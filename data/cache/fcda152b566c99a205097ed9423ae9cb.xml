<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0"><channel><title>Presentations &gt; Page #1 - InfoQ</title><link>https://www.infoq.com/presentations/</link><atom:link href="http://rsshub.isrss.com/infoq/presentations" rel="self" type="application/rss+xml"></atom:link><description>Presentations from QCon London 2018, QCon New York 2018, SpringOne Platform 2018, and more - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><image><url>https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo-big.jpg</url><title>Presentations &gt; Page #1 - InfoQ</title><link>https://www.infoq.com/presentations/</link></image><lastBuildDate>Tue, 25 Mar 2025 00:36:19 GMT</lastBuildDate><ttl>5</ttl><item><title>How GitHub Copilot Serves 400 Million Completion Requests a Day</title><description>&lt;figure&gt;&lt;img alt=&quot;How GitHub Copilot Serves 400 Million Completion Requests a Day&quot; src=&quot;https://res.infoq.com/presentations/github-copilot/en/card_header_image/davidcheney-twittercard-1741334769344.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s1_20250320073842_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-nov-githubcopilot.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-nov-githubcopilot.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-nov-githubcopilot.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;David Cheney discusses the intricate architecture of GitHub Copilot&#39;s code completion service, explaining the challenges of achieving low-latency responses for millions of daily requests. He delves into HTTP/2 optimizations, global scaling strategies, and the critical role of their internal proxy.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;David Cheney is an open source contributor and project member for the Go programming language. David is a well-respected voice within the tech community, speaking on a variety of topics such as software design, performance, and the Go programming language.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon San Francisco empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Cheney: GitHub Copilot is the largest LLM powered code completion service in the world. We serve hundreds of millions of requests a day with an average response time of under 200 milliseconds. The story I&#39;m going to cover in this track is the story of how we built this service.&lt;/p&gt;

&lt;p&gt;I&#39;m the cat on the internet with glasses. In real life, I&#39;m just some guy that wears glasses. I&#39;ve been at GitHub for nearly five years. I&#39;ve worked on a bunch of backend components, which none of you know about, but you interact with every day. I&#39;m currently the tech lead on copilot-proxy, which is the service that connects your IDEs to LLMs hosted in Azure.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;What is GitHub Copilot?&lt;/h2&gt;

&lt;p&gt;GitHub is the largest social coding site in the world. We have over 100 million users. We&#39;re very proud to call ourselves the home of all developers. The product I&#39;m going to talk to you about is GitHub Copilot, specifically the code completion part of the product. That&#39;s the bit that I work on. Copilot does many other things, chat, interactive refactoring, things like that, and so on. They broadly use the same architecture and infrastructure I&#39;m going to talk to you about, but the details vary subtly. GitHub Copilot is available as an extension. You install it in your IDE. We support most of the major IDEs, VS Code, Visual Studio, obviously, the pantheon of IntelliJ IDEs, Neovim, and we recently announced support for Xcode. Pretty much you can get it wherever you want. We serve more than 400 million completion requests. That was when I pitched this talk. I had a look at the number. It&#39;s much higher than that these days.&lt;/p&gt;

&lt;p&gt;We peak at about 8,000 requests a second during that peak period between the European afternoon and the U.S. work day. That&#39;s our peak period. During that time, we see a mean response time of less than 200 milliseconds. Just in case there is one person who hasn&#39;t seen GitHub Copilot in action, here&#39;s a recording of me just working on some throwaway code. The goal, what you&#39;ll see here is that we have the inbuilt IDE completions, those are the ones in the box, and Copilot&#39;s completions, which are the gray ones, which we notionally call ghost text because it&#39;s gray and ethereal. You can see as I go through here, every time that I stop typing or I pause, Copilot takes over. You can see you can write a function, a comment describing what you want, and Copilot will do its best to write it for you. It really likes patterns. As you see, it&#39;s figured out the pattern of what I&#39;m doing. We all know how to make prime numbers. You pretty much got the idea. That&#39;s the product in action.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Building a Cloud Hosted Autocompletion Service&lt;/h2&gt;

&lt;p&gt;Let&#39;s talk about the requirements of how we built the service that powers this on the backend, because the goal is interactive code completion in the IDE. In this respect, we&#39;re competing with all of the other interactive autocomplete built into most IDEs. That&#39;s your anything LSP powered, your Code Sense, your IntelliSense, all of that stuff. This is a pretty tall order because those things running locally on your machine don&#39;t have to deal with network latency. They don&#39;t have to deal with shared server resources. They don&#39;t have to deal with the inevitable cloud outage. We&#39;ve got a pretty high bar that&#39;s been set for us. To be competitive, we need to do a bunch of things.&lt;/p&gt;

&lt;p&gt;The first one is that we need to minimize latency before and between requests. We need to amortize any setup costs that we might make because this is a network service. To a point, we need to avoid as much network latency as we can because that&#39;s overhead that our competitors that sit locally in IDEs don&#39;t have to pay. The last one is that the length and thus time to generate a code completion response is very much a function of the size of the request, which is completely variable. One of the other things that we do is rather than waiting for the whole request to be completed and then send it back to the user, we work in a streaming mode. It doesn&#39;t really matter how long the request is, we immediately start streaming it as soon as it starts. This is quite a useful property because it unlocks other optimizations.&lt;/p&gt;

&lt;p&gt;I want to dig into this connection setup is expensive idea. Because this is a network service, we use TCP. TCP uses the so-called 3-way handshake, SYN, SYN-ACK, ACK. On top of that, because this is the internet and it&#39;s 2024, everything needs to be secured by TLS. TLS takes between 5 to 7 additional legs to do that handshaking, negotiate keys in both directions. Some of these steps can be pipelined. A lot of work has gone into reducing these setup costs and overlaying the TLS handshake with the TCP handshake. These are great optimizations, but they&#39;re not a panacea. There&#39;s no way to drive this network cost down to zero. Because of that, you end up with about five or six round trips between you and the server and back again to make a new connection to a service.&lt;/p&gt;

&lt;p&gt;The duration of each of those legs is highly correlated with distance. This graph that I shamelessly stole from the internet says about 50 milliseconds a leg, which is probably East Coast to West Coast time. Where I live, on the other side of an ocean, we see round trip times far in excess of 100 milliseconds. When you add all that up and doing five or six of them, that makes connection setup really expensive. You want to do it once and you want to keep it open for as long as possible.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Evolution of GitHub Copilot&lt;/h2&gt;

&lt;p&gt;Those are the high-level requirements. Let&#39;s take a little bit of a trip back in time and look at the history of Copilot as it evolved. Because when we started out, we had an extension in VS Code. To use it, the alpha users would go to OpenAI, get an account, get their key added to a special group, then go and plug that key into the IDE. This worked. It was great for an alpha product. It scaled to literally dozens of users. At that point, everyone got tired of being in the business of user management. OpenAI don&#39;t want to be in the business of knowing who our users are. Frankly, we don&#39;t want that either. What we want is a service provider relationship. That kind of thing is what you&#39;re used to when you consume a cloud service. You get a key to access the server resources. Anytime someone uses that key, a bill is emitted. The who is allowed to do that under what circumstances is entirely your job as the product team. We&#39;re left with this problem of, how do we manage this service key? Let&#39;s talk about the wrong way to do it.&lt;/p&gt;

&lt;p&gt;The wrong way to do it would be to encode the key somehow in the extension that we give to users in a way that it can be extracted and used by the service but is invisible to casual or malicious onlookers. This is impossible. This is at best security by obscurity. It doesn&#39;t work, just ask the rabbit r1 folks. The solution that we arrived at is to build an authenticating proxy which sits in the middle of this network transaction. The name of the product is copilot-proxy because it&#39;s just an internal name. I&#39;m just going to call it proxy for the rest of this talk. It was added shortly after our alpha release to move us out of this period of having user provided keys to a more scalable authentication mechanism.&lt;/p&gt;

&lt;p&gt;What does this workflow look like now? You install the extension in your IDE just as normal, and you authenticate to GitHub just as normal. That creates a kind of OAuth relationship, like each where there&#39;s an OAuth key which identifies that installation on that particular machine for that person who&#39;s logged in at that time. The IDE now can use that OAuth credential to go to GitHub and exchange that for what we call a short-lived code completion token. The token is just like a train ticket. It&#39;s just an authorization to use it for a short period of time. It&#39;s signed. When the request lands on the proxy, all we have to do is just check that that signature is valid. If it&#39;s good, we swap the key out for the actual API service key, forward it on, and stream back the results. We don&#39;t need to do any other further validation. This is important because it means for every request we get, we don&#39;t have to call out to an external authentication service. The short-lived token is the authentication.&lt;/p&gt;

&lt;p&gt;From the client&#39;s point of view, nothing&#39;s really changed in their world. They still think they&#39;re talking to a model, and they still get the response as usual. This token&#39;s got a lifetime in the order of minutes, 10, 20, 30 minutes. This is mainly to limit the liability if, say, it was stolen, which is highly unlikely. The much more likely and sad case is that in the cases of abuse, we need the ability to shut down an account and therefore not generate new tokens. That&#39;s generally why the token has a short lifetime. In the background, the client knows the expiration time of the token it was given, and a couple of minutes before that, it kicks off a refresh cycle, gets a new token, swaps it out, the world continues.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;When Should Copilot Take Over?&lt;/h2&gt;

&lt;p&gt;We solved the access problem. That&#39;s half the problem. Now to talk about another part of the problem. As a product design, we don&#39;t have an autocomplete key. I remember in Eclipse, you would use command + space, things like that, to trigger the autocomplete or to trigger the refactoring tools. I didn&#39;t use that in the example I showed. Whenever I stop typing, Copilot takes over. That creates the question of, when should it take over? When should we switch from user typing to Copilot typing? It&#39;s not a straightforward problem. Some of the solutions we could use is just a fixed timer. We hook the key presses, and after each key press, we start a timer. If that timer elapses without another key press, then we say, we&#39;re ready to issue the request and move into completion mode.&lt;/p&gt;

&lt;p&gt;This is good because that provides an upper bound on how long we wait and that waiting is additional latency. It&#39;s bad, because it provides a lower bound. We always wait at least this long before starting, even if that was the last key press the user was going to make. We could try something a bit more science-y and use a tiny prediction model to look at the stream of characters as they&#39;re typed and predict, are they approaching the end of the word or are they in the middle of a word, and nudge that timer forward and back. We could just do things like a blind guess. Any time there&#39;s a key press, we can just assume that&#39;s it, no more input from the user, and always issue the completion request. In reality, we use a mixture of all these strategies.&lt;/p&gt;

&lt;p&gt;That leads us to the next problem, which is, despite all this work and tuning that went into this, around half of the requests that we issued are what we call typed through. Don&#39;t forget, we&#39;re doing autocomplete. If you continue to type after we&#39;ve made a request, you&#39;ve now diverged from the data we had and our request is now out of date. We can&#39;t use that result. We could try a few things to work around this. We could wait longer before a request. That might reduce the number of times that we issue a request and then have to not use the result. That additional latency, that additional waiting penalizes every user who had stopped and was waiting. Of course, if we wait too long, then users might think that Copilot is broken because it&#39;s not saying anything to them anymore. Instead, what we&#39;ve built is a system that allows us to cancel a request once they&#39;ve been issued. This cancellation request using HTTP is potentially novel. I don&#39;t claim it to be unique, but it&#39;s certainly the first time I&#39;ve come across this in my career.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Canceling a HTTP Request&lt;/h2&gt;

&lt;p&gt;I want to spend a little bit of time digging into what it means to cancel a HTTP request. You&#39;re at your web browser and you&#39;ve decided you don&#39;t want to wait for that page, how do you say, I want to stop, I want to cancel that? You press the stop button. You could close the browser tab. You could drop off the network. You eat your laptop away. These are all very final actions. They imply that you&#39;re canceling the request because you&#39;re done. Either you&#39;re done with using the site or you&#39;re just frustrated and you&#39;ve given up. It&#39;s an act of finality. You don&#39;t intend to make another request. Under the hood, they all have the same networking behavior. You reset the TCP stream, the connection that we talked about setting up on previous slides. That&#39;s on the browser side. If we look on the server side, either at the application layer or in your web framework, this idea of cancellation is not something that is very common inside web frameworks.&lt;/p&gt;

&lt;p&gt;If a user using your application on your site presses stop on the browser or if they Control-C their cURL command, that underlying thing translates into a TCP reset of the connection. On the other end, in your server code, when do you get to see that signal? When do you get to see that they&#39;ve done that? The general times that you can spot that the TCP connection has been reset is either when you&#39;re reading the request body, so early in the request when you&#39;re reading the headers in the body, or later on when you go to start writing your response back there.&lt;/p&gt;

&lt;p&gt;This is a really big problem for LLMs, because the cost of the request, that initial inference before you generate the first token, is the majority of the cost. That happens before you produce any output. All that work is performed. We&#39;ve done the inference. We&#39;re ready to start streaming back tokens. Only then do we find that the user closed the socket and they&#39;ve gone. As you saw, in our case, that&#39;s about 45% of the requests. Half of the time, we&#39;d be performing that inference and then throwing the result on the floor, which is an enormous waste of money, time, and energy, which in AI terms is all the same thing.&lt;/p&gt;

&lt;p&gt;If this situation wasn&#39;t bad enough, it gets worse. Because cancellation in HTTP world is the result of closing that connection. In our case, in the usage of networking TCP to talk to the proxy, the reason we canceled that request is because we want to make another one straightaway. To make that request straightaway, we don&#39;t have a connection anymore. We have to pay those five or six round trips to set up a new TCP TLS connection. In this naive idea, in this normal usage, cancellation occurs every other request on average. This would mean that users are constantly closing and reestablishing their TCP connections in this kind of signaling they want to cancel and then reestablishing connection to make a new request. The latency of that far exceeds the cost of just letting the request that we didn&#39;t need, run to completion, and then just ignoring it.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;HTTP/2, and Its Importance&lt;/h2&gt;

&lt;p&gt;Most of what I said on the previous slides applies to HTTP/1, which has this one request per connection model. As you&#39;re reading on this slide, HTTP version numbers go above number 1, they go up to 2 and 3. I&#39;m going to spend a little bit of time talking about HTTP/2 and how that was very important to our solution. As a side note, copilot-proxy is written in Go because it has a quite robust HTTP library. It gave us the HTTP/2 support and control over that that we needed for this part of the product. That&#39;s why I&#39;m here, rather than a Rustacean talking to you. This is mostly an implementation detail. HTTP/2 is more like SSH than good old HTTP/1 plus TLS. Like SSH, HTTP/2 is a tunneled connection. You have a single connection and multiple requests multiplexed on top of that. In both SSH and HTTP/2, they&#39;re called streams. A single network connection can carry multiple streams where each stream is a request. We use HTTP/2 between the client and the proxy because that allows us to create a connection once and reuse it over and again.&lt;/p&gt;

&lt;p&gt;Instead of resetting the TCP connection itself, you just reset the individual stream representing the request you made. The underlying connection stays open. We do the same between the proxy and our LLM model. Because the proxy is effectively concatenating requests, like fanning them in from thousands of clients down onto a small set of connections to the LLM model, we use a connection pool, a bunch of connections to talk to the model. This is just to spread the load across multiple TCP streams, avoid networking issues, avoid head-of-line blocking, things like that. We found, just like the client behavior, these connections between the proxy and our LLM model are established when we start the process and they leave assuming there&#39;s no upstream problems.&lt;/p&gt;

&lt;p&gt;They remain open for the lifetime of the process until we redeploy it, so minutes to hours to days, depending on when we choose to redeploy. By keeping these long-lived HTTP/2 connections open, we get additional benefits to the TCP layer. Basically, TCP has this trust thing. The longer a connection is open, the more it trusts it, the more it allows more data to be in fly before it has to be acknowledged. You get these nice, warmed-up TCP pipes that go all the way from the client through the proxy, up to the model and back.&lt;/p&gt;

&lt;p&gt;This is not intended to be a tutorial on Go, but for those who do speak it socially, this is what basically every Go HTTP handler looks like. The key here is this req.Context object. Context is effectively a handle. It allows efficient transmission of cancellations and timeouts and those kind of request-specific type meta information. The important thing here is that the other end of this request context is effectively connected out into user land to the user&#39;s IDE. When, by continuing to type, they need to cancel a request, that stream reset causes this context object to be canceled. That makes it immediately visible to the HTTP server without having to wait until we get to a point of actually trying to write any data to the stream.&lt;/p&gt;

&lt;p&gt;Of course, this context can be passed up and down the call stack and used for anything that wants to know, should it stop early. We use it in the HTTP client and we make that call onwards to the model, we pass in that same context. The cancellation that happens in the IDE propagates to the proxy and into the model effectively immediately. This is all rather neatly expressed here, but it requires that all parties speak HTTP/2 natively.&lt;/p&gt;

&lt;p&gt;It turns out this wasn&#39;t all beer and skittles. In practice, getting this end-to-end HTTP/2 turned out to be more difficult than we expected. This is despite HTTP/2 being nearly a decade old. Just general support for this in just general life was not as good as it could be. For example, most load balancers are happy to speak HTTP/2 on the frontend but downgrade to HTTP/1 on the backend. This includes most of the major ALB and NLBs you get in your cloud providers. It, at the time, included all the CDN providers that were available to us. That fact alone was enough to spawn us doing this project. There are also other weird things we ran into.&lt;/p&gt;

&lt;p&gt;At the time, and I don&#39;t believe it&#39;s been fixed yet, OpenAI was fronted with nginx. nginx just has an arbitrary limit of 100 requests per connection. After that, they just closed the connection. At the request rates that you saw, it doesn&#39;t take long to chew through 100 requests, and then nginx will drop the connection and force you to reestablish it. That was just a buzz kill.&lt;/p&gt;

&lt;p&gt;All of this is just a long-winded way of saying that the generic advice of, yes, just stick your app behind your cloud provider&#39;s load balancer, it will be fine, didn&#39;t work out for us out of the box. Something that did work out for us is GLB. GLB stands for GitHub Load Balancer. It was introduced eight years ago. It&#39;s one of the many things that has spun out of our engineering group. GLB is based on HAProxy. It uses HAProxy under the hood. HAProxy turns out to be one of the very few open-source load balancers that offers just exquisite HTTP/2 control. I&#39;ve never found anything like it. Not only did it speak HTTP/2 end-to-end, but offered exquisite control over the whole connection. What we have is GLB being the GitHub Load Balancer, which sits in front of everything that you interact with in GitHub, actually owns the client connection. The client connects to GLB and GLB holds that connection open. When we redeploy our proxy pods, their connections are gracefully torn down and then reestablished for new pods. GLB keeps the connection to the client open. They never see that we&#39;ve done a redeploy. They never disconnected during that time.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;GitHub Copilot&#39;s Global Nature&lt;/h2&gt;

&lt;p&gt;With success and growth come yet more problems. We serve millions of users around the globe. We have Copilot users in all the major markets, where I live in APAC, Europe, Americas, EMEA, all over the world. There&#39;s not a time that we&#39;re not busy serving requests. This presents the problem that even though all this HTTP/2 stuff is really good, it still can&#39;t change the speed of light. The round-trip time of just being able to send the bits of your request across an ocean or through a long geopolitical boundary or something like that, can easily exceed the actual mean time to process that request and send back the answer. This is another problem. The good news is that Azure, through its partnership with OpenAI, offers OpenAI models in effectively every region that Azure has. They&#39;ve got dozens of regions around the world. This is great. We can put a model in Europe, we can put a model in Asia. We can put a model wherever we need one, wherever the users are. Now we have a few more problems to solve.&lt;/p&gt;

&lt;p&gt;In terms of requirements, we want users, therefore, to be routed to their &quot;closest&quot; proxy region. If that region is unhealthy, we want them to automatically be routed somewhere else so they continue to get service. The flip side is also true, because if we have multiple regions around the world, this increases our capacity and our reliability. We no longer have all our eggs in one basket in one giant model somewhere, let&#39;s just say in the U.S. By spreading them around the world, we&#39;re never going to be in a situation where the service is down because it&#39;s spread around the world. To do this, we use another product, again, that spun out of GitHub&#39;s engineering team, called octoDNS. octoDNS, despite its name, is not actually a DNS server. It&#39;s actually just a configuration language to describe DNS configurations that you want. It supports all the good things: arbitrary weightings, load balancing, splitting, sharing, health checks. It allows us to identify users in terms of the continent they&#39;re in, the country.&lt;/p&gt;

&lt;p&gt;Here in the United States, we can even work down to the state level sometimes. It gives us exquisite control to say, you over there, you should primarily be going to that instance. You over there, you should be primarily going to that instance, and do a lot of testing to say, for a user who is roughly halfway between two instances, which is the best one to send them to so they have the lowest latency? On the flip side, each proxy instance is looking at the success rate of requests that it sees and it handles. If that success rate drops, if it goes below the SLO, those proxy instances will use the standard health C endpoint pattern. They set their health C status to 500. The upstream DNS providers who have been programmed with those health checks notice that.&lt;/p&gt;

&lt;p&gt;If a proxy instance starts seeing its success rate drop, they vote themselves out of DNS. They go take a little quiet time by themselves. When they&#39;re feeling better, they&#39;re above the SLO, they raise the health check status and bring themselves back into DNS. This is now mostly self-healing. It turns a regional outage when we&#39;re like, &quot;All of Europe can&#39;t do completions&quot;, into a just reduction in capacity because traffic is routed to other regions.&lt;/p&gt;

&lt;p&gt;One thing I&#39;ll touch on briefly is one model we experimented with and then rejected because it just didn&#39;t work out for us was the so-called point of presence model. You might have heard it called PoP. If you&#39;re used to working with big CDNs, they will have points of presence. Imagine every one of these dots on this map, they have a data center in, where they&#39;re serving from. The idea is that users will connect and do that expensive connection as close to them as possible and speed up that bit.&lt;/p&gt;

&lt;p&gt;Then those CDN providers will cache that data, and if need to, they can call back to the origin server. In our scenario, where I live in Asia, we might put a point of presence in Singapore. That&#39;s a good equidistant place for most of Asia. A user in Japan would be attracted to that Singapore server. There&#39;s a problem because the model is actually still hosted back here on the West Coast. We have traffic that flows westward to Singapore only to turn around and go all the way back to the West Coast. The networking colloquialism for this is traffic tromboning. This is ok for CDN providers because CDN providers, their goal is to cache as much of the information, so they rarely call back to the origin server. Any kind of like round tripping or hairpinning of traffic isn&#39;t really a problem.&lt;/p&gt;

&lt;p&gt;For us doing code completions, it&#39;s always going back to a model. What we ended up after a lot of experimentation was the idea of having many regions calling back to a few models just didn&#39;t pay for itself. The latency wasn&#39;t as good and it carried with it a very high operational burden. Every point of presence that you deploy is now a thing you have to monitor, and upgrade, and deploy to, and fix when it breaks. It just didn&#39;t pay for us. We went with a much simpler model which is simply, if there is a model in an Azure region, we colocate a proxy instance in that same Azure region and we say that is the location that users&#39; traffic is sent to.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;A Unique Vantage Point&lt;/h2&gt;

&lt;p&gt;We started out with a proxy whose job was to do this authentication, to authenticate users&#39; requests and then mediate that towards an LLM. It turns out it&#39;s very handy to be in the middle of all these requests. Some examples I&#39;ll give of this are, we obviously look at latency from the point of view of the client, but that&#39;s a very fraught thing to do. It&#39;s something I caution you, it&#39;s ok to track that number, just don&#39;t put it on a dashboard because some will be very incentivized to take the average of it, or something like that. You&#39;re essentially averaging up the experience of everybody on the internet&#39;s requests, from somebody who lives way out on bush on a satellite link to someone who lives next to the AMS-IX data center, and you&#39;re effectively trying to say, take the average of all their experiences. What you get when you do that is effectively the belief that all your users live on a boat in the middle of the Atlantic Ocean.&lt;/p&gt;

&lt;p&gt;This vantage point is also good, because while our upstream provider does give us lots of statistics, they&#39;re really targeted to how they view running the service, their metrics. They have basic request counts and error rates and things like that, but they&#39;re not really the granularity we want. More fundamentally, the way that I think about it, to take food delivery as an example, use the app, you request some food, and about 5 minutes later you get a notification saying, &quot;Your food&#39;s ready. We&#39;ve finished cooking it. We&#39;re just waiting for the driver to pick it up&quot;. From the restaurant&#39;s point of view, their job is done, they did it, their SLO, 5 minutes, done. It&#39;s another 45 minutes before there&#39;s a knock on the door with your food. You don&#39;t care how quickly the restaurant prepared your food. What you care about is the total end-to-end time of the request. We do that by defining in our SLOs that the point we are measuring is at our proxy. It&#39;s ok for our service provider to have their own metrics, but we negotiate our SLOs as the data plane, the split where the proxy is.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Dealing with a Heterogeneous Client Population&lt;/h2&gt;

&lt;p&gt;You saw that we support a variety of IDEs, and within each IDE, there is a flotilla of different client versions out there. Dealing with the long tail of client versions is the bane of my life. There is always a long tail. When we do a new client release, we&#39;ll get to about 80% population within 24 to 36 hours. That last 20% will take until the heat death of the universe. I cannot understand why clients can use such old software. The auto-update mechanisms are so pervasive and pernicious about getting you to update, I don&#39;t quite understand how they can do this, but they do. What this means is that if we have a bug or we need to make a fix, we can&#39;t do it in the client. It just takes too long, and we never get to the population that would be successful with rolling out that fix. This is good because we have a service that sits in the middle, the proxy that sits in the middle, that we can do a fix-up on the fly, hopefully.&lt;/p&gt;

&lt;p&gt;Over time, that fix will make it into the client versions and they&#39;ll sufficiently roll out, there&#39;ll be a sufficient enough population. An example of this, one day out of the blue, we got a call from a model provider that said, you can&#39;t send this particular parameter. It was something to do with log probabilities. You can&#39;t send that because it&#39;ll cause the model to crash, which is pretty bad because this is a poison pill. If a particular form of request will cause a model instance to crash, it will blow that one out of the water and that request will be retried and it&#39;ll blow the next one out of the water and keep working its way down the line. We couldn&#39;t fix it in the client because it wouldn&#39;t be fast enough. Because we have a proxy in the middle, we can just mutate the request quietly on the way through, and that takes the pressure off our upstream provider to get a real fix so we can restore that functionality.&lt;/p&gt;

&lt;p&gt;The last thing that we do is, when we have clients that are very old and we need to deprecate some API endpoint or something like that, rather than just letting them get weird 404 errors, we actually have a special status code which triggers logic in the client that puts up a giant modal dialog box. It asks them very politely, would they please just push the upgrade button?&lt;/p&gt;

&lt;p&gt;There&#39;s even more that we can do with this, because logically the proxy is transparent. Through all of the shenanigans, the clients still believe that they&#39;re making a request to a model and they get a response. The rest is transparent. From the point of view of us in the middle who are routing requests, we can now split traffic across multiple models. Quite often, the capacity we receive in one region won&#39;t all be in one unit. It might be spread across multiple units, especially if it arrives at different times. Being able to do traffic splits to combine all that together into one logical model is very handy. We can do the opposite. We can mirror traffic. We can take a read-only tap of requests and send that to a new version of the model that we might be either performance testing, or validating, or something like that.&lt;/p&gt;

&lt;p&gt;Then we can take these two ideas and mix and match them and stack them on top of each other and make A/B tests, experiments, all those kinds of things, all without involving the client. From the client&#39;s point of view, it just thinks it&#39;s talking to the same model it has yesterday and today.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Was It Worth the Engineering Effort?&lt;/h2&gt;

&lt;p&gt;This is the basic gist of how you build a low latency code completion system with the aim of competing with IDEs. I want to step back and just ask like, as an engineering effort, was this worth it? Did the engineering effort we put into this proxy system pay for itself? One way to look at this is, for low latency, you want to minimize hops. You certainly want to minimize the number of middlemen that are in the middle, the middleware, anything that&#39;s kind of in that request path adding value but also adding latency. What if we just went straight to Azure instead, we had clients connect straight to Azure? This would have left authentication as the big problem, as well as observability. They would have really been open questions. It would have been possible to teach Azure to understand GitHub&#39;s OAuth token. The token that the IDE natively has from GitHub could be presented to Azure as an authentication method. I&#39;m sure that would be possible. It would probably result in Azure building effectively what I just demonstrated on this.&lt;/p&gt;

&lt;p&gt;Certainly, if our roles were reversed and I was the Azure engineer, I would build this with an authentication layer in front of my service. Some customer is coming to me with a strange authentication mechanism, I&#39;m going to build a layer which converts that into my real authentication mechanism. We would have probably ended up with exactly the same number of moving parts just with more of them behind the curtain in the Azure side. Instead, by colocating proxy instances and model instances in the same Azure region, we, to most extent, ameliorated the cost of that extra hop. The inter-region traffic is not free, it&#39;s not zero, but it&#39;s pretty close to zero. It&#39;s fairly constant in terms of the latency you see there. You can characterize it and effectively ignore it.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;War Stories&lt;/h2&gt;

&lt;p&gt;I&#39;m going to tell you a few more war stories of what&#39;s happened over the life of this product just to emphasize that the value of having this intermediary really paid for itself over and over. One day we upgraded to a new version of the model which seemed to be very attracted to a particular token. It really liked emitting this token. It was some end of file marker, and it was something to do with a mistake in how it was trained that it just really liked to emit this token. We can work around this essentially in the request by saying, in your response, this very particular token, weight it down negative affinity, never want to see it. If we didn&#39;t have an intermediary like the proxy to do that, we would have had to do that in the client. We would have had to do a client rollout which would have been slow and ultimately would not have got all the users.&lt;/p&gt;

&lt;p&gt;Then the model would have been fixed and we&#39;d have to do another client change to reverse what we just did. Instead, it was super easy to add this parameter to the request on the fly as it was on its way to the model. That solved the problem immediately and it gave us breathing room to figure out what had gone wrong with the model training and fix that without the Sword of Damocles hanging over our head.&lt;/p&gt;

&lt;p&gt;Another story is that, one day I was looking at the distribution of cancellation. For a request that was cancelled, how long did it live until it was cancelled? There was this bizarre spike effectively at 1 millisecond, effectively immediately. It was saying, a lot of requests come from the clients and are immediately cancelled. As in, you read the request and then instantly afterwards the client is like, I&#39;m sorry, I didn&#39;t mean to send that to you, let me take it back. The problem is by that time we&#39;ve already started the process of forwarding that to Azure and they&#39;re mulling on it. We immediately send the request to Azure and then say to them, sorry, I didn&#39;t mean to send that to you. May I please have it back? Cancellation frees up model resources quicker but it&#39;s not as cheap as just not sending a request that we know we&#39;re going to cancel is.&lt;/p&gt;

&lt;p&gt;It took us some time to figure out what was exactly happening in the client to cause this fast cancellation behavior, but because we had the proxy in the middle, we could add a little check that just before we made the request to the model, we would check, has it actually been cancelled? There were mechanisms in the HTTP library to ask that question. We saved ourselves making and then retracting that request. Another point talking to metrics, from the metrics that our upstream model provider provides us, we don&#39;t get histograms, we don&#39;t get distributions, we barely get averages. There would be no way we would have been able to spot this without our own observability at that client proxy layer. If we didn&#39;t have the proxy as an intermediary, we still could have had multiple models around the world.&lt;/p&gt;

&lt;p&gt;As you saw, you can have OpenAI models in any Azure region you want. We would just not have a proxy in front of them. We probably would have used something like octoDNS to still do the geographic routing, but it would have left open the question of what do we do about health checks. When models are unhealthy or overloaded, how do we take them out of DNS? What we probably would have had to do is build some kind of thing that&#39;s issuing synthetic requests or pinging the model or something like that, and then making calls to upstream DNS providers to manually thumbs up and thumbs down regions. HTTP/2 is critical to the Copilot latency story. Without cancellation, we&#39;d make twice as many requests and waste half of them. It was surprisingly difficult to do with off-the-shelf tools.&lt;/p&gt;

&lt;p&gt;At the time, CDNs didn&#39;t support HTTP/2 on the backend. That was an absolute non-starter. Most cloud providers didn&#39;t support HTTP/2 on the backend. If you want to do that you have to terminate TLS yourself. For the first year of our existence of our product, TLS, like the actual network connection, was terminated directly on the Kubernetes pod. You can imagine our security team were absolutely overjoyed with this situation. It also meant that every time we did a deploy, we were literally disconnecting everybody and they would reconnect, but that goes against the theory that we want to have these connections and keep them open for as long as possible.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;GitHub&#39;s Paved Path&lt;/h2&gt;

&lt;p&gt;This is very GitHub specific, but a lot of you work for medium to large-scale companies, you probably have a tech stack that is, in GitHub paths, we call it the paved path. It is the blessed way, the way that you&#39;re supposed to deploy applications inside the company. Everything behind GLB, everything managed by octoDNS made our compliance story. You can imagine, we&#39;re selling this to large enterprise companies. You need to have your certifications. You need to have your SOC 2 tick in the box. Using these shared components really made that compliance story much easier. The auditors say, this is another GLB hosted service, using all the regular stuff, not exactly a tick in the box but got a long way towards solving our compliance story. The flip side is because these are shared components rather than every individual team knowing every detail up and down of terminating TLS connections on pods hosted in Kubernetes clusters that they run themselves, we delegate that work to shared teams who are much better at it than that.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Key Takeaways&lt;/h2&gt;

&lt;p&gt;This is a story of what made Copilot a success. It is possible that not all of you are building your own LLM as a service-service. Are there broader takeaways for the rest of you? The first one is, definitely use HTTP/2. It&#39;s dope. I saw a presentation by the CTO of Fastly, and he viewed HTTP/2 as an intermediary. He really says HTTP/3 is the real standard, the one that they really wanted to make. From his position as a content delivery partner whose job is just to ship bits as fast as possible, I agree completely with that. Perhaps the advice is not use HTTP/2, the advice would probably be something like, use something better than HTTP/1. If you&#39;re interested in learning more, if you look that up on YouTube, that&#39;s a presentation by Geoff Huston talking about HTTP/2 from the point of view of application writers and clients, and how it totally disintermediates most of the SSL and the middle VPN nonsense that we live with day to day in current web stuff.&lt;/p&gt;

&lt;p&gt;The second one is a Bezos quote, if you&#39;re gluing your product together from parts from off-the-shelf suppliers and your role in that is only supplying the silly putty and the glue, what are your customers paying you for? Where&#39;s your moat? As an engineer, I understand very deeply the desire not to reinvent the wheel, so the challenge to you is, find the place where investing your limited engineering budget, in a bespoke solution, is going to give you a marketable return. In our case, it was writing a HTTP/2 proxy that accelerated one API call. We&#39;re very lucky that copilot-proxy as a product is more or less done, and has been done for quite a long time, which is great because it gives our small team essentially 90% of our time to get dedicated to the operational issues of running this service.&lt;/p&gt;

&lt;p&gt;The last one is, if you care about latency, if your cloud provider is trying to sell you this siren song that they can solve your latency issues with their super-fast network backbone. That can be true to a point, but remember the words of Montgomery Scott, you cannot change the laws of physics despite what your engineering title is. If you want low latency, you have to bring your application closer to your users. In our case that was fairly straightforward because code completion at least in the request path is essentially stateless. Your situation may not be as easy. By having multiple models around the globe, that turns SEV1 incidents into just SEV2 alerts. If a region is down or overloaded, traffic just flows somewhere else. Those users instead of getting busy signal, still get a service albeit at a marginally higher latency. I&#39;ve talked to a bunch of people, and I said that we would fight holy wars over 20 milliseconds. The kind of latencies we&#39;re talking about here are in the range of 50 to 100 milliseconds, so really not noticeable for the average user.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/github-copilot/</link><guid isPermaLink="false">infoq-presentations/github-copilot</guid><pubDate>Sun, 23 Mar 2025 16:00:00 GMT</pubDate><author>David Cheney</author><enclosure url="https://res.infoq.com/presentations/github-copilot/en/card_header_image/davidcheney-twittercard-1741334769344.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-nov-githubcopilot.mp4" type="video/mp4"></enclosure><itunes:duration>49:24</itunes:duration><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>QCon San Francisco 2024</category><category>.NET Core</category><category>InfoQ</category><category>Scalability</category><category>Machine Learning</category><category>Performance</category><category>Microservices</category><category>Artificial Intelligence</category><category>AI, ML &amp; Data Engineering</category><category>Availability</category><category>QCon Software Development Conference</category><category>Java9</category><category>DevOps</category><category>Large language models</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>Rebuilding Prime Video UI with Rust and WebAssembly</title><description>&lt;figure&gt;&lt;img alt=&quot;Rebuilding Prime Video UI with Rust and WebAssembly&quot; src=&quot;https://res.infoq.com/presentations/prime-video-rust/en/card_header_image/alexandru-ene-twitter-card-1741257206752.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s1_20250320073842_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-nov-primevideoui.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-nov-primevideoui.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-nov-primevideoui.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Alexandru Ene features details of a new UI SDK in Rust for Prime Video that targets living room devices.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Alexandru Ene is a Principal Engineer in Prime Video at Amazon. Previously he’s worked in video games, game engines and has a passion for interactive media and high-performance systems.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon San Francisco empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Ene: We&#39;re going to talk about how we rebuilt a Prime Video UI for living room devices with Rust and WebAssembly, and the journey that got us there. I&#39;m Alex. I&#39;ve been a principal engineer with Amazon for about eight years. We&#39;ve been working with Rust for a while actually in our tech stack for the clients. We had our low-level UI engine in WebAssembly and Rust for that log. Previously I worked on video games, game engines, interactive applications like that. I&#39;ve quite a bit of experience in interactive applications.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Content&lt;/h2&gt;

&lt;p&gt;I&#39;ll talk about challenges in this space, because living room devices are things like set top boxes, gaming consoles, streaming sticks, TVs. People don&#39;t usually develop UIs for these devices, and they come with their own special set of challenges, so we&#39;re going to go through those. Then I&#39;ll show you how our architecture for the Prime Video App looked before we rewrote everything in Rust. We had a dual tech stack with the business code in React and JavaScript, and then low-level bits of the engine in Rust and WebAssembly, a bit of C++ in there as well. Then I&#39;ll show you some code with our new Rust UI SDK and how that looks, which is what we use right now in production. We&#39;re going to talk a little bit of how that code works with our low-level existing engine and how everything is organized. At the end, we&#39;re going to go a little bit to results, lessons learned.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Challenges In This Space&lt;/h2&gt;

&lt;p&gt;Living room devices, so as I said, these are gaming consoles, streaming sticks, set top boxes. They come with their own challenges, and some of them are obvious. There&#39;s performance differences that are huge. We&#39;re talking about a PlayStation 5 Pro, super nice gaming console, lots of power, but also a USB power streaming stick. Prime Video, we run the same application on all of these device types. Obviously, performance is really important for us. We can&#39;t quite have teams per device type, so one team that does set top boxes, then another team does gaming consoles, because then everything explodes. When you build a feature, you have to build it for everything. We were building things once and then deploying on all of these device categories here. We don&#39;t deploy this application that I&#39;m talking about on mobile devices, like iPhone, iOS, mobile devices don&#39;t have this. This is just living room devices. Again, a huge range of performance. We&#39;re trying to write our code as optimal as possible.&lt;/p&gt;

&lt;p&gt;Usually, high performant code is code that you compile natively. Let&#39;s say Rust compiled to native, C++ compiled to native, but that doesn&#39;t quite cut it in this space and we&#39;ll see why. Another pain point and challenge is hardware capabilities between these devices is a pain. As SDK developers, we need to think a lot about what are reasonable fallbacks that application developers who write the app code and the app behavior, they don&#39;t need to think about when they write that code and every little hardware difference. We try to have some reasonable defaults. That&#39;s not always possible, so we use patterns like feature flags and things like that to let them have a bit more control. It&#39;s a fairly challenging thing.&lt;/p&gt;

&lt;p&gt;Another thing is we&#39;re trying to make this application as fast as possible with as many features as possible to every customer, but then updating native code on these device types is really hard. Part of that is these devices don&#39;t even have app stores, most of them. Maybe it goes up with a firmware update. That&#39;s a pain. It requires a manual process interacting with a third party that owns the platform. Even on places that do have an app store, if you try to update an app on an app store, it&#39;s quite a challenge as well. You need to wait. It&#39;s highly likely a manual process. We&#39;re having this tension between code that we&#39;re downloading over the air, like JavaScript, WebAssembly, and so on, fairly easy, and then code that works on a device that is very fast, but then really hard to update. We want to have this fast iteration cycle. Updating the app in a short amount of time is huge for us.&lt;/p&gt;

&lt;p&gt;Again, this is how the application looks like today. I&#39;ve been there eight years and we changed it many times. I&#39;m sure it&#39;s going to change again sometime as it happens with the UIs. We&#39;ve added things to it like channels, live events, all sorts of new features that weren&#39;t there in the starting version. Part of us being able to do that was this focus on updatability that we had all the way in the beginning. Most of these applications were in a language like JavaScript that we can change basically everything on it and add all of these features almost without a need to go and touch the low-level native code. I&#39;ll show you the architecture and how it looks like.&lt;/p&gt;

&lt;p&gt;Today, if a developer adds some code, changes a feature, fixes a bug, does anything around the UI, that code goes through a fully CI/CD pipeline, no manual testing whatsoever. We test on virtual devices like Linux and on physical devices where we have a device farm. Once all of those tests pass, you get this new experience on your TV in your living room. That is way faster than a native app update for that platform.&lt;/p&gt;

&lt;p&gt;Right now, you&#39;ll see it working and you&#39;ll see a couple of features. This is a bunch of test profiles I was making because I was testing stuff. We have stuff like layout animation, so the whole layout gets recalculated. This is the Rust app in production today. Layout animations are a thing that was previously impossible with JavaScript and React, and now they just work. When you see this thing getting bigger, all the things get reordered on the page. These are things that are just possible right now due to the performance of Rust. Almost instant page transitions as well are things that weren&#39;t possible with TypeScript and React due to performance constraints. This is live today and this is how it looks like, so you have an idea on what is going on in there. We&#39;re going to get a little bit back to layout animations and those things later. For people who are not UI engineers, or don&#39;t quite know, this is the slide that will teach you everything you need to know about UI programming.&lt;/p&gt;

&lt;p&gt;Basically, every UI ever is a tree of nodes, and the job of a UI SDK is to manipulate as fast as possible this tree as a reaction to user inputs or some things that happen like some events. You either change properties on nodes, like maybe you animate a value like a position, and then the UI engine needs to take care of updating this tree and creating new nodes, deleting new nodes, depending on what the business logic code tells you to do. Those nodes could be view nodes that are maybe just a rectangle. Then, text nodes are quite common, and image nodes, those type of things. Nothing too complicated. It&#39;s really annoying that it&#39;s a tree, but we&#39;re going to move on because we&#39;re still having a tree, even in our Rust app we didn&#39;t innovate there, but it&#39;s just important to have this mental model. We call this in our UI engine, a scene tree, browsers call it a DOM, but it&#39;s basically the same thing everywhere.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;High-Level Architecture&lt;/h2&gt;

&lt;p&gt;This is the high-level architecture before we rewrote everything in Rust. As you can see, we already added Rust, I think two years, three years ago, we already had it there for the low-level UI engine. There&#39;s a QCon talk about this journey. There&#39;s another part which is saying JavaScript here, but actually developers write TypeScript, that has the business logic code for the Prime Video App. This is the stuff we download. This is downloaded every time the application changes. This is what we output at the end of that full CI/CD pipeline. It&#39;s a bundle that has some WebAssembly compiled Rust code and some JavaScript that came from TypeScript and got transpiled to JavaScript. It maybe changes once per day, sometimes even more, sometimes less, depending on our pipelines and if the tests pass or not, but it&#39;s updated very frequently on all of the devices that I spoke about, the device categories.&lt;/p&gt;

&lt;p&gt;Then we have the stuff on device in our architecture. We&#39;re trying to keep it as thin as possible because it&#39;s really hard to update, so the less we touch this code, the better. It has a couple of virtual machines, some rendering backend, which mainly connects the higher-level stuff we download to things like OpenGL and other graphics APIs, networking. This is basically cURL. Some media APIs and storage and a bunch of other things, but they&#39;re in C++. We deploy them on a device and they sit there almost untouched unless there&#39;s some critical bug that needs to be fixed or some more tricky thing. This is how things work today.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Prime Video App, (Before) With React and WebAssembly&lt;/h2&gt;

&lt;p&gt;You might wonder, though, these are two separate virtual machines, so how do they actually work together? We&#39;re going to go through an example of how things worked before with this tech stack. The Prime Video App here takes high-level decisions, like what to show the user, maybe some carousels, maybe arrange some things on the screen. Let&#39;s say in this example, he wants to show some image on your TV. The Prime Video App is built with React. We call it React-Livingroom because it&#39;s a version of React that we&#39;ve changed and made it usable for living room devices by pairing them some features, simplifying them, and also writing a few reconcilers because we have this application that works on this type of architecture, but also in browsers because some living room devices today have just an HTML5 browser and don&#39;t even have flash space big enough to put our native C++ engine. We needed that device abstraction here. Prime Video App says, I want to put an image node. It uses React-Livingroom as a UI SDK.&lt;/p&gt;

&lt;p&gt;Through the device abstraction layer, we figure out, you have a WebAssembly VM available. At that point in time, instead of doing the actual work, it just encodes a message and puts it on a message bus. This is literally a command which says, create me an image node with an ID, with a URL where we download the image from, some properties with height and position, and the parent ID to put it in that scene tree. The WebAssembly VM has the engine, and this engine has low-level things that actually manage that scene tree that we talked about.&lt;/p&gt;

&lt;p&gt;For example, the scene and resource manager will figure out, there&#39;s a new message. I have to create a node, put it in the tree. It&#39;s an image node, so it checks if that image is available or not. It issues a download request. Maybe it animates some properties if necessary. Once the image is downloaded, it gets decoded, uploaded to the GPU memory, and after that, the high-level renderer here, from the scene tree that could be quite big, it figures out what subset of nodes is visible on the screen and then issues commands, the C++ layer, that&#39;s with gray, to draw pixels on the screen. At the end of it all, you&#39;ll have The Marvelous Mrs. Maisel image in there as it should be.&lt;/p&gt;

&lt;p&gt;This is how it used to work. When we added Rust here, we had huge gains in animation fluidity and these types of things. However, things like input latency didn&#39;t quite improve, so the input latency is basically the time it takes from when you press a button on your remote control, in our case, until the application responds to your input. That&#39;s what we call input latency. That didn&#39;t improve much or at all because, basically, all of those decisions and all that business logic, like what happens as a response to an input event to the scene tree, is in JavaScript. That&#39;s a fairly slow language, especially since some of this hardware can be as, maybe, dual-core devices with not even 1 gigahertz worth of CPU speed and not much memory.&lt;/p&gt;

&lt;p&gt;Actually, those are medium. We have some that are even slower, so running JavaScript on those is time-consuming. We wanted to improve this input latency, and in the end, we did, but we ended up with this architecture. The engine is more or less the same, except we added certain systems that are specific to this application. For example, focus management, layout engine is now part of the engine. I didn&#39;t put it on this slide because it goes into that scene management. On top of it, we built a new Rust UI SDK that we then use to build the application. Everything is now in Rust. It&#39;s one single language. You don&#39;t even have the message bus to worry about. That wasn&#39;t even that slow anyway. It was almost instantaneous. The problem was JavaScript, so we don&#39;t have that anymore. We&#39;re actually not quite here because we are deploying this iteratively, page by page, because we wanted to get this out faster in front of customers, but we will get here early next year.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;UI Code Using the New Rust UI SDK&lt;/h2&gt;

&lt;p&gt;This is going to be a bit intense, but here&#39;s some UI code with Rust, and this is actually working UI code that our UI SDK supports. I&#39;m going to walk you through it because there&#39;s a few concepts here that I think are important. When it comes to UI programming, Rust isn&#39;t known for having lots of libraries, and then the ecosystem is not quite there. We had to build our own. We&#39;d use some ideas from Leptos, like the signals that I&#39;m going to talk about, but this is how things look like today. If you&#39;re familiar with React and SolidJS and those things, you&#39;ll see some familiar things here.&lt;/p&gt;

&lt;p&gt;First, you might notice, is that Composer macro over there, that gets attached to this function here that returns a composable. A composable is a reusable piece of tree, of hierarchy of nodes that we can plug in with other reusable bits and compose them, basically, together. This is our way of reusing UI. This Composer macro actually doesn&#39;t do that much except generate boilerplate code that gives us some nicer functionality in that compose macro you see later down in the function. It allows us to have named arguments as well as nicer error messages and optional arguments that might miss for functions.&lt;/p&gt;

&lt;p&gt;This is some quality-of-life thing. Then our developers don&#39;t need to specify every argument to these functions, like this hello function here that just takes a name as a string. In this case, the name is mandatory, but we can have optional arguments with optional values that you don&#39;t need to specify. Also, you can specify arguments in any order as long as you name them, and we&#39;ll see that below. It&#39;s just super nice quality-of-life thing. I wish Rust supported this out of the box for functions, but it doesn&#39;t, so this is where we are.&lt;/p&gt;

&lt;p&gt;Then, this is the core principle of our UI SDK. It uses signals and effects. The signal is a special value, so this name here will shadow the string above. Basically, this name is a signal, and that means when it changes, it will trigger effects that use it. For example, when this name changes, it will execute the function in this memo, which is a special signal, and it creates a new hello message with the new value the name has been set to. It executes the function you see here. It formats it. It concatenates, and it will give something like Hello Billy, or whatever. Then hello message is a special signal that also will trigger effects. Here you see in this function, we use the hello message.&lt;/p&gt;

&lt;p&gt;Whenever the hello message is updated, it will trigger this effect that we call here with create effect. This is very similar to how SolidJS, or if you&#39;re familiar with React, works. Actually, this is quite important because this is also what helps UI engineers be productive in this framework without knowing much Rust actually. The core of our UI engine is signals, effects, and memos, which are special signals that only trigger effects if the values that they got updated to are different from the previous value. By default, they just trigger the effect anyway.&lt;/p&gt;

&lt;p&gt;Then, we have this other macro here, which is the compose macro, and this does the heavy lifting. This is where you define how your UI hierarchy looks like. Here we have a row that then has children, which are label nodes. You see here the label has a text that is either a hardcoded value with three exclamation marks as a string, or it can take a signal that wraps a string. The first label here will be updated whenever hello message gets updated. Without the UI engineer doing anything, it just happens automatically that hello message gets updated, the label itself will render the new text, and it just works. If you&#39;re a UI engineer, this is the code you write. It&#39;s fairly easy to understand once you get the idea. Here we have some examples, for example, ChannelCard and MovieCard are just some other functions that allow you to pass parameters like a name, a main_texture, and maybe a title, a main_texture, and so on.&lt;/p&gt;

&lt;p&gt;Again, they could have optional parameters that you don&#39;t see here. You can even put signals instead of those hardcoded values. It doesn&#39;t quite matter, it&#39;s just these will be siblings of those two labels. All the way down we have button with a text, that says Click. Then it has a few callbacks on select, on click, and stuff like that, that are functions that get triggered whenever those events happen in the UI engine. For example, whenever we select this button, we set a signal. That name gets set to a new name. This triggers a cascade of actions, hello message gets updated to hello new name. Then, the effects gets trigger because that&#39;s a new value, so that thing will be printed.&lt;/p&gt;

&lt;p&gt;Then, lastly, the first label you see here, will get updated to a new value. Lastly, this row has properties or modifiers, so we can modify the background color. In this case, it&#39;s just set to a hardcoded value of blue. However, we support signals to be passed here as well. If you have a color, that&#39;s a signal of a color. Whenever that gets set, maybe on a timer or whatever, the color of the node just gets updated and you pass it here exactly like we set this parameter. That&#39;s another powerful way where we get behavior or effects as a consequence of business logic that happens in the UI code. This is what your engineers deal with, and it&#39;s quite high-level, and it&#39;s very similar to other UI engines, but it&#39;s in Rust this time.&lt;/p&gt;

&lt;p&gt;When we run that compose macro, this is how the UI hierarchy will look like in the scene tree. You have the row, and then it has a label. Then labels are special because they&#39;re widgets. Composables can be built out of widgets, which are special composables our UI SDK provides to the engineers, or other composables that eventually are built out of widgets. Widgets are then built out of a series of components. This is fairly important because we use entity component systems under the hood. Components, for now, you can think of them as structures without behavior, so just data without any behavior. The behavior comes from systems that operate on these components.&lt;/p&gt;

&lt;p&gt;In this case, this label has a layout component that helps the layout system. A base component, let&#39;s say maybe it has a position, a rotation, things like that. RenderInfo components, this is all the data you need to put the pixels on the screen for this widget once everything gets computed. A text component, this does text layout and things like that. Maybe a text cache component that is used to cache the text in the texture so we don&#39;t draw it letter by letter.&lt;/p&gt;

&lt;p&gt;The important bit is that widgets are special because they come as predefined composables from our UI SDK. Then, again, composables can be built out of other composables. This row has a few children here, but eventually it has to have widgets as the leaf nodes because those are the things that actually have the base behavior. Here maybe you have a button and another image, and the button has, all the way down, a focus component. This allows us to focus the button, and it gets automatically used by that system. The image component, again, just stores a URL and maybe the status, has this been downloaded, uploaded to GPU, and so on. It&#39;s fairly simple. Basically, this architecture in our low-level engine is used to manage complexity in behavior. We&#39;ll see a bit later how it works. Then we had another Movie Card in that example and, again, it eventually has to be built out of widgets.&lt;/p&gt;

&lt;p&gt;Widgets are the things that our UI SDK provides to UI developers out of the box. They can be row, columns, image, labels, stack, rich text, which is special text nodes that allows you to have images embedded and things like that. Stacks, row list, and column list, and these are scrollable either horizontally or vertically. I think we added grid recently because we needed it for something, but basically, we build this as we build the app. This is what we support now. I think button is another one of them that&#39;s just supported here out of the box that I somehow didn&#39;t put. Then, each widget is an entity ID. It has an ID and a collection of components. Then, the lower-level engine uses systems to modify and update the components. ECS is this entity component system. It&#39;s a way to organize your code and manage complexity without paying that much in terms of performance. It&#39;s been used by game engines, not a lot, but for example, Overwatch used it.&lt;/p&gt;

&lt;p&gt;Thief, I think, was the first game in 1998 that used it as a piece of trivia. It&#39;s a very simple idea, which is, you have entities, and these are just IDs that map to components. You have components that are data without behavior. Then you have systems, which are basically functions that act on tuples of components. It always acts on more things at the time, not on one thing at a time. It&#39;s a bit different than the other paradigms. It&#39;s really good to create behavior, because if you want a certain behavior for an entity, you just add the component and then the systems that need that component automatically will just work because the component is there.&lt;/p&gt;

&lt;p&gt;Here is how it might work in a game loop. For example, these systems are on the left side and then the components are on the right side. When I say components, you can basically imagine those as arrays and entity IDs as indices in those arrays. It&#39;s a bit more complicated than that, but that&#39;s basically it. Then the things on the left side with the yellow thing, those are systems, and they&#39;re basically functions that operate on those arrays at the same time. Let&#39;s say the resource management system needs to touch base components, image components, and read from them. This reading is with the white arrow, and it will write to RenderInfo components. For example, it will look where the image is, if it&#39;s close to the screen, look at the base component. It looks at the image component that contains the URL. It checks the image status that will be there. Is it downloaded? Has it been uploaded to the GPU? If it has been decoded and uploaded to the GPU, we update the RenderInfo components so we can draw the thing later on the screen.&lt;/p&gt;

&lt;p&gt;For this system, you need to have all three components on an entity, at least. You can have more, but we just ignore them. We don&#39;t care. This system just looks at that slice of an object, which is the base components, the image components, and RenderInfo components. You have to have all three of them. If you have only two without the third one, that entity just isn&#39;t touched by this system and it does nothing, the system widget. Then we have the layout system. Of course, this looks at a bunch of components and updates one at the end. It&#39;s quite complicated, but layout is complicated anyway. At least that complication and that complexity sits within a file or a function. You can tell from the signature that this reads from a million things, writes to one, but it is what it is. You can&#39;t quite build layout systems without touching all of those things. Maybe we have a text cache component that looks at text components and writes to a bunch of other things.&lt;/p&gt;

&lt;p&gt;Again, you need to have all three of these for an entity such that is updated by this system. All the way at the end, we have the rendering system that looks at RenderInfo components, reads from them. It doesn&#39;t write anywhere because it doesn&#39;t need to update any component. It will just call the functions in the C++ code in the renderer backend to put things on the screen. It just reads through this and then updates your screen with new pixels. It sounds complicated, but it&#39;s a very simple way to organize behavior. This has paid dividends organizing our low-level architecture like this for reasons that we&#39;ll see a little bit later, how and why. Not only for the new application, but also the old application because they use the same low-level engine.&lt;/p&gt;

&lt;p&gt;Again, going back to the architecture, this is what we have, Prime Video App at the top. We&#39;ve seen how developers write the UI with composables using our UI SDK. Then we&#39;ve seen how the UI SDK uses widgets that then get updated by the systems, and have components that are defined in the low-level engine. This is again, downloaded. Every time we write some new code, it goes through a pipeline, it gets built to WebAssembly, and then we just execute it on your TV set top box, whatever you have in your living room. Then we have the low-level stuff that interacts with the device that we try to keep as small as possible. This is what we shipped, I think, end of August. It&#39;s live today.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;The Good Parts&lt;/h2&gt;

&lt;p&gt;Good parts. Developer productivity, actually, this was great for us. Previously, when we rewrote the engine, we had a bunch of developers who knew C++ and switched to Rust, and we had good results there. In this case, we switched people who knew only JavaScript and TypeScript to Rust, and they only knew stuff like React and those frameworks. We switched them with our Rust UI SDK with no loss in productivity. This is both self-reported and compared with. Whenever we build a feature, we have other clients that don&#39;t use this, so, for example, like the mobile client or the web client and so on. The Rust client, actually, when we were discussing some new features to be built now on all of the clients, was, I think, the second one in terms of speed, behind web. Then even mobile developers had higher estimations than we did here. Also, we did this whole rewrite in a really short amount of time. We had to be productive. We built the UI SDK and a large part of the app quite fast.&lt;/p&gt;

&lt;p&gt;The reason why I think this is true is because we did a lot of work in developer experience with those macros that maybe look a bit shocking if you don&#39;t know UI programming, but actually they felt very familiar to UI engineers. They could work with it right off the bat, they don&#39;t have to deal with much complexity in the borrow checker. Usually, in the UI code, you can clone things if necessary, or even use a Rc and things like that. You all know, this is not super optimal. Yes, we came from JavaScript, so this is fine, I promise. The gnarly bits are down in the engine, and there we take a lot of care about data management and memory and so on. In the UI code, we can afford it easy. Even on the lowest level hardware, I have some slides that you&#39;ll see the impact of this.&lt;/p&gt;

&lt;p&gt;Another thing in the SDK, as the SDK and engine team, we chose some constraints and they helped us build a simpler UI SDK and ship it faster. For example, one constraint our UI engine has, I might show it to you, is that when you define a label or a widget or something like that, you cannot read properties from it unless you&#39;ve been the one setting properties. It&#39;s impossible to read where on the screen an element ends up after layout from the UI code. You never know. You just put them in there. We calculate things in the engine, but you can&#39;t read things unless you&#39;ve been the one saying, this is your color, blue. Then you&#39;re like, yes, it&#39;s in my variable. I can read it, of course. Things like that, you can&#39;t read. This simplified vastly our UI architecture and we don&#39;t have to deal with a bunch of things, and slowness because of it. It seems like a shocking thing. Maybe you need to know where on the screen. No, you don&#39;t, because we shipped it.&lt;/p&gt;

&lt;p&gt;There was no need to know where you are on the screen, and there was no need to read a property that you haven&#39;t set. There are certain cases where we do notify UI developers through callbacks where they can attach a function and get notified if something happens. It&#39;s very rare. It happens usually in case of focus management and things like that. You will get a function call that you&#39;re focused, you&#39;re not focused anymore, and that works fine. Again, it&#39;s a tradeoff. It has worked perfectly fine for us. That&#39;s something that I think also has helped productivity. We only had one instance where developers asked to read a value of layout because they wanted something to grow, and maybe at 70% of the thing, they wanted something else to happen. Just use a timer and that was fixed.&lt;/p&gt;

&lt;p&gt;Another good thing is that we iteratively shipped this. This is only because we used, I think in my view, entity component systems as the basis of our lower-level engine. That low-level engine with the systems it has and the components it has, currently supports JavaScript pages. By pages, I mean everything on the screen is in Rust or everything on the screen is in JavaScript. For example, we shipped the profiles page, which is the page you select the profile. The collections page, that&#39;s the page right after you select the profile and you see all of the categories, all of the movies and everything. The details page, which is, once you choose something to watch, you can go to that place and see maybe episodes or just more details about the movie, and press play. We still have to move the search page, settings, and a bunch of other smaller pages. Those are still in JavaScript. This is work in progress, so we&#39;re just moving them over. It&#39;s just a function of time. We only have 20 people for both the UI SDK and the application. It takes a bit to move everything. It&#39;s just time.&lt;/p&gt;

&lt;p&gt;Another reason, it&#39;s just work in progress. We think it was good. That entity component system managed perfectly fine to have these two running side-by-side. I don&#39;t think we had one bug because of this. We only had to do some extra work to synchronize a bunch of state between these things, like the stack that you used to go back, the back stack and things like that, but it was worth it in the end. We got this out really fast. We actually first shipped the profiles page and then added the collections page and then the details page and then live and linear and whatnot. That&#39;s nice.&lt;/p&gt;

&lt;p&gt;Another good part is, in my opinion, we built tools as part of building this UI SDK. Because we built an SDK, so we had to build tools. I think one winning move here was, it&#39;s really easy in our codebase to add a new tool, mostly because we use egui, which is this Rust immediate mode UI library. You see there like the resource manager just appears on top of the UI. This is something a developer built because he was debugging an issue where a texture wasn&#39;t loading and he was trying to figure out, how much memory do we have? Is this a memory thing? The resource manager maybe didn&#39;t do something right. It just made it very easy to build tools. We built tools in parallel with building the application and the UI SDK.&lt;/p&gt;

&lt;p&gt;In reality, these are way below what you&#39;d expect from browsers and things like that, but with literally 20% of the tools, you get 80% done. It&#39;s absolutely true. You just need mostly the basics. Of course, we have debugger and things like that that just work, but these are UI specific tools. We have layout inspectors and all sorts of other things, so you can figure out if you set the wrong property. Another cool thing, in my opinion, so we built this, which is essentially a rewrite of the whole Prime Video App. Obviously, we&#39;re against these things without a lot of data. One thing that really helped us make a point that this is worth it is to make a prototype that wasn&#39;t cheating, that we showed to leadership around, this is how it feels on the device before what we did, and this is with this new thing.&lt;/p&gt;

&lt;p&gt;Literally, features that were impossible before, like layout animations, are just super easy to do now. You see here, things are growing, layout just works, it rearranges everything. Things appear and disappear. Again, this is a layout animation here. Of course, this is programmer art, but has nothing to do with designers. We are just showcasing capabilities on a device. As you can see, things are over icons and under, it&#39;s just a prototype, but it felt so much nicer and responsive compared to what you could get on a device that it just convinced people instantly that it&#39;s worth the risk of building a UI in Rust and WebAssembly. Because even though we added Rust and it was part of our tech stack, we were using it for low-level bits, but this showed us that we can take a risk and try to build some UI in it.&lt;/p&gt;

&lt;p&gt;Here are some results. This is a really low-end device where input latency for the main page for collection page was as bad as 247 milliseconds, 250 milliseconds, horrible input latency, with the orange color, this is in JavaScript. With Rust in blue, 33 milliseconds, easy. Similarly, details page, 440 milliseconds. This also includes layout time, because if you press a button as the page loads and we do layout, you might wait that much. This is max. The device is very slow. Again, 30 milliseconds, because layout animations means we need to run layout as fast as an animation frame, which is usually 16 milliseconds or 30 milliseconds at 30 FPS. It&#39;s way faster and way more responsive. Again, that line is basically flat. It was great. Other devices have been closer to those two lines, but I picked this example because it really showcases even on the lowest-end device, you can get great results. The medium devices were like 70 milliseconds, and they went down to 16 or 33, but this is like the worst of them all. We have that.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;The Ugly Parts&lt;/h2&gt;

&lt;p&gt;Ugly parts. WebAssembly System Interface is quite new. WebAssembly in general is quite new. We&#39;re part of the W3C community. We&#39;re working with them around features, things like that. There are certain things that are lacking. For example, we did add threads, but also there&#39;s things that happen in the ecosystem that break our code sometimes because we use something that&#39;s not fully standardized in production for a while. One such example was recently Rust 1.82, enabled some feature by default for WebAssembly WASI builds, that basically didn&#39;t work on older WebAssembly virtual machines that we had in production. We basically now have a way to disable it, even if you have a new default and things like that. It&#39;s worth it for us. That&#39;s something to think about.&lt;/p&gt;

&lt;p&gt;Also, WebAssembly System Interface keeps evolving and adding new features, and we&#39;re trying to be active as part of that effort as well. It requires engineering effort. We can&#39;t just quite take a dependency, or specifically on WebAssembly, and just be like, let&#39;s see where this ship goes. You need to get involved in there and help with feedback, with work on features and so on. Another one we found out is panic-free code is really hard. Of course, exceptions should be for exceptional things, but that&#39;s not how people write JavaScript. When the code panics in our new Rust app, the whole app gets just basically demolished, it crashes. You need to restart it from your TV menu. It&#39;s really annoying. Panics shouldn&#39;t quite happen. It&#39;s very easy to cause a Rust panic, just access an array with the wrong index, you panic, game over. Then, that&#39;s it. If you&#39;re an engineer who only worked in JavaScript, maybe you&#39;re familiar with exceptions, you can try-catch somewhere.&lt;/p&gt;

&lt;p&gt;Even if it&#39;s not ideal, you can catch the exception and maybe reset the customer at some nice position, closer to where they were before or literally where they were before. It&#39;s impossible with our new app, which is really annoying. We, of course, use Clippy to ban unwraps and expect and those things. We ban unsafe code, except in one engine crate that has to interact with the lower-level bits. Again, it required a bit of education for our UI engineers to rely on this pattern of using the result type from Rust and get comfortable with the idea that there is no stack unwinding, especially there is no stack unwinding in WebAssembly, which is tied to the first point. You can&#39;t even catch that in a panic handler. It just aborts the program. Again, this pretty big pain point for us.&lt;/p&gt;

&lt;p&gt;In the end, obviously we shipped, so we&#39;re happy. We almost never crashed, but it requires a bunch of work. This also generated a bunch of work on us because we were depending on some third-party libraries that were very happily panicking whenever you were calling some functions in a bit of a maybe not super correct way. Again, we would rather have results instead of panics for those cases. It led to a bit of work there that we didn&#39;t quite expect. That&#39;s something to think about especially in UI programming, or especially if you go, like we did, from JavaScript to Rust and WebAssembly.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;The Bytecode Alliance&lt;/h2&gt;

&lt;p&gt;The Bytecode Alliance is this nonprofit organization we&#39;re part of, a bunch of companies are part of it, and builds on open-source standards like WebAssembly, WebAssembly System Interface. Then, the WebAssembly Micro Runtime, which is the virtual machine we use, is built over there, as well as Wasmtime, which is another popular Rust one, implemented in Rust this time. WebAssembly Micro Runtime is C. It&#39;s a good place to look at if you&#39;re interested in using Rust in production, and especially using WebAssembly in production more specifically. In our case, it comes with Rust and everything.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant: You mentioned you don&#39;t use this for your web clients. Would you think that something like this could work with using WebGL as the rendering target?&lt;/p&gt;

&lt;p&gt;Ene: We did some comparisons on devices. There&#39;s a bunch of pain points. First pain point is on the ones we do have to use a browser, because there&#39;s no space on the flash, on some set top boxes. The problem is those are some version of WebKit that has no WebAssembly. That&#39;s the big hurdle for us there. It could be possible. We did some experiments and it worked, but you do lose a few things that browsers have that we don&#39;t. Today, it&#39;s not worth it for us because those have very few customers. They work fairly ok in terms of comparing them to even the system UI. Even though they don&#39;t hit these numbers, it would be a significant amount of effort to get this SDK to work on a browser.&lt;/p&gt;

&lt;p&gt;Right now, it&#39;s just quite simple because it has one target, the one that has the native VM. It requires a bunch of functions from the native VM that we expose that aren&#39;t standard. Getting those would probably require to pipe them to JavaScript. Then you&#39;re like, what&#39;s going on? You might lose some performance and things like that. It&#39;s a bit of a tricky one, but we&#39;re keeping an eye on it.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/prime-video-rust/</link><guid isPermaLink="false">infoq-presentations/prime-video-rust</guid><pubDate>Thu, 20 Mar 2025 16:00:00 GMT</pubDate><author>Alexandru Ene</author><enclosure url="https://res.infoq.com/presentations/prime-video-rust/en/card_header_image/alexandru-ene-twitter-card-1741257206752.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-nov-primevideoui.mp4" type="video/mp4"></enclosure><itunes:duration>48:06</itunes:duration><category>WebAssembly</category><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>Amazon</category><category>QCon San Francisco 2024</category><category>.NET Core</category><category>Cloud Computing</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>Web Development</category><category>Case Study</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>Java9</category><category>DevOps</category><category>Reactive Programming</category><category>System Programming</category><category>Rust</category><category>Transcripts</category><category>Development</category></item><item><title>The Harsh Reality of Building a Real-Time ML Feature Platform</title><description>&lt;figure&gt;&lt;img alt=&quot;The Harsh Reality of Building a Real-Time ML Feature Platform&quot; src=&quot;https://res.infoq.com/presentations/sharechat/en/card_header_image/ivan-burmistrov-twitter-card-1741179272139.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-apr-mlplatform.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-apr-mlplatform.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-apr-mlplatform.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Ivan Burmistrov shares how ShareChat built their own Real-Time Feature Platform serving more than 1 billion features per second, and how they managed to make it cost efficient.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Ivan Burmistrov is a software engineer with a passion in building large-scale distributed systems, real-time data processing and low-latency. Currently Burmistrov is working at Indian&#39;s largest social network ShareChat, where he is leading the work on Realtime ML Feature Store, powering ShareChat&#39;s recommendation system.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Burmistrov: I&#39;ll start with a short story that happened to me. It was a normal working day. I came from work to join my wife and daughter for dinner. I&#39;m really excited because the model our team has been working on has finally started showing promising results. I&#39;m really eager to share this with my family. My wife is busy though, because she&#39;s trying to feed our daughter who is a fast eater. She doesn&#39;t want to talk about models or anything else at that point. Then, so excited I cannot wait, so I started telling her, &quot;We had this model we built, and it wasn&#39;t working. We&#39;ve been debugging it for more than a month.&lt;/p&gt;

&lt;p&gt;Finally, turns out that we thought was just stupid bugs like typos, like prompt feature name, this kind of stuff. We finally hunted them enough, so the model is now performing as we expected&quot;. I shared with excitement and I expect some reaction or something, but she&#39;s been busy. She barely listened, but she doesn&#39;t want to be rude, so she understands that she needs to do something, so she&#39;s offering her comments, like, &quot;Nice, this model behaves just like you do at times&quot;. I&#39;m like, &quot;What do you mean, how is it even relevant?&quot; She said, &quot;When you&#39;re mad at me, you don&#39;t simply tell me what&#39;s wrong. I have to spend so much time figuring out what&#39;s going on, just like you guys with this model&quot;. She&#39;s actually spot on.&lt;/p&gt;

&lt;p&gt;The models are really hard to debug. If you build a model and it&#39;s underperforming, it can be a nightmare in debugging. It can cost days, weeks, or months, like in our case. The least we can do to ease the pain is to ensure that the data that we feed to the model is ok, so the data doesn&#39;t contain these bugs. This is what feature platforms aim to do. They aim to deliver good data for machine learning models. Feature platforms is what we will be discussing.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Background, and Outline&lt;/h2&gt;

&lt;p&gt;My name is Ivan. I&#39;m a student engineer at ShareChat. ShareChat is an Indian company that builds a couple of social networks in India, like the largest domestic social networks. I am primarily focusing on this data for ML, and in particular, this feature platform. Before ShareChat, I had experience working at Meta and ScyllaDB. One of the social networks that ShareChat builds is called Moj. It&#39;s a short video app, pretty much like TikTok. It has fully personalized feed, around 20 million daily active users, 100 million monthly active users. The model in the story is actually the main model for this app, so the main model that defines the ranking, that defines which videos we will show to users. We will be talking about how we built the feature platform for this app, or any app like this, in particular. We will have the background about what a feature platform is, or what are features, high-level architecture of any feature platform. The majority of time will be spent on challenges and how to overcome them, based on our examples. Finally, some takeaways.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Introduction to Feature Platform&lt;/h2&gt;

&lt;p&gt;Let&#39;s refresh the memory of what the feature is for ML. A feature is pretty much anything we can derive from data. Typically, it&#39;s attached to some entity in the system. For instance, it can be the user as an entity, or post, or creator, which is also a user, but the one who posts the content. There are multiple types of features one may be interested in. Like, for instance, window counters is like a feature, give me the number of likes for this post in the last 30 minutes, or give me the number of engagements from this user in the last one day. These kind of window counters, because they have time window. There can be lifetime counters. Lifetime don&#39;t have windows, so like total number of posts or likes for the given post, or total number of engagements from the given user. There can be some properties, like date of birth for a user, or post language, or this kind of stuff. Or something like last N, give me last 100 interactions with this given post, or give me the last 1,000 engagements for the given user.&lt;/p&gt;

&lt;p&gt;In this talk, we&#39;ll primarily focus in on window features, the somewhat most interesting. To summarize what the feature platform is, feature platform is a set of services or tools that allows defining features, like give some feature API. They list features and read what they mean. It allows launching some feature pipeline to compute. Finally, feature store is to read the data when it&#39;s needed. One important aspect I wanted to stress is that it&#39;s really important that a feature platform helps in engineering velocity, meaning that it doesn&#39;t stay in the way. Instead, it allows for fast iteration. The people who build the model, they can easily define features, update them if needed, or read what they mean. Because when it comes to some kind of investigation, especially a model underperforming, let&#39;s say, it&#39;s really important that people can go and easily read, these are the features that we feed, and this is what they mean. It&#39;s pretty clear. It is not hidden somewhere in tens of files, in cryptic language or whatever.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Architecture - High-Level Overview of Feature Platform Components&lt;/h2&gt;

&lt;p&gt;In typical architecture for any feature platform, it starts with some streams of data. In a social network, it may be streams of likes, streams of views, video plays, this kind of stuff. Some processing engine that gets these streams of data, computes something, and writes to some kind of database. Finally, in front of this database, there is some service that gets a request for features, transforms it to whatever queries it requires for the database, and probably performs some last aggregations, and returns. Speaking about last aggregations, in particular for window counters, how to serve the query, like give me number of likes in the last 30 minutes or one day. Typically, the timeline is split into pre-aggregated buckets, we call them tiles, of different size.&lt;/p&gt;

&lt;p&gt;For instance, we can split the timeline into buckets of 1 minute, 30 minutes, 1 day. When the request comes, like let&#39;s say at 3:37 p.m., we want number of likes in the last two hours. We can split this window of two hours, cover it by the relevant tiles, and so then we request the database for these tiles, get the response back, and aggregate across this tile and return the result. It&#39;s a typical technique to power these window counters.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Challenges and Solutions: Story 1 (The Boring One)&lt;/h2&gt;

&lt;p&gt;That was it about architecture. Now let&#39;s go to the challenges, solutions that I will be talking about in form of stories. The first story is a boring one. It&#39;s about the choice of streaming platform and database. For streaming, we picked Redpanda. For database, we picked ScyllaDB. ScyllaDB and Redpanda, they are like siblings. They share a lot of similarities.&lt;/p&gt;

&lt;p&gt;First, they were born with API compatibility with existing systems, like Scylla was born with Cassandra API, and later it did DynamoDB API. Redpanda is Kafka compatible. They&#39;re both obsessed with performance. They do this via so-called shard-per-core architecture. Shard-per-core is when we split the data in some shards. Each shard is processed by a given core in the system. This architecture allows eliminating synchronization, overhead, or logs, this kind of stuff. This helps them to get to the performance numbers they desire. It&#39;s not easy to build applications using this shard-per-core technique. They both leverage Seastar framework. It&#39;s an open-source framework. It&#39;s built by Scylla team. It allows to build these kinds of applications. They both don&#39;t have autoscaling. It&#39;s actually not entirely true anymore for Redpanda. They just announced that they launched Redpanda serverless in one of their clouds. If you install them in your own stack, they don&#39;t have autoscaling out of the box.&lt;/p&gt;

&lt;p&gt;Despite that, they&#39;re extremely cost efficient. In our company, we use them not only in this feature platform thing, but actually migrated a lot of workloads to ScyllaDB: to Scylla we migrated from Bigtable, and to Redpanda we migrated from GCP Pub/Sub and in-house Kafka, and achieved really nice cost saving. They both have really cute mascots. If you work with the systems, you get to work with some swag, like this one. That&#39;s it about the story, because these systems are tragically boring. They just work. They really seem to deliver on the promise of operational easiness. Because when you run them, you don&#39;t need to tune, so they tend to get the best out of the given hardware. My view on this is biased a bit, because we use managed versions. We tested non-managed versions as well, and this is the same. We tested it, and we picked managed, just because we didn&#39;t want to have a team behind this.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Story 2 (Being Naive)&lt;/h2&gt;

&lt;p&gt;Let&#39;s go to the less boring part. When it came to streaming and database, there were a bunch of options. When it comes to processing engine, especially at the moment when we made the decision, there were not so much options. In fact, there was only one option, when you want real-time stream processing. What&#39;s important, we want real-time stream processing, and we want some expressive language for the stream processing.&lt;/p&gt;

&lt;p&gt;Apache Flink is the framework that can process a stream of data in real-time, has some power capabilities. What&#39;s important, it has Flink SQL. One can define the job in SQL language, and it&#39;s really important for feature platform, because of that property we want to deliver so that it&#39;s easy to write features, and it&#39;s most importantly easy to read features. We picked Flink. We can build features using this streaming SQL. What it does, this query, it basically works on top of streaming data, selects them, GROUP BY by some entity ID and some identifier of the tile, because we need to aggregate these tiles. This query forms this so-called virtual table. Virtual, because it&#39;s not really materialized, it keeps updating as the data comes. For instance, some event comes, and now we updated the shares and likes. Some new event comes, we again update it. This process continues. Super nice, we build this proof of concept pretty quickly, and we&#39;re happy. From developer experience, Flink is really nice. It&#39;s easy to write on using developer workflow.&lt;/p&gt;

&lt;p&gt;Then the question comes, so we build this query, we launch the job, it&#39;s running, it&#39;s updating this virtual table, all good. Now we want to update the query. Flink has this concept called savepoint, checkpoints. Basically, because it&#39;s stateful processing, it has some state. What savepoint does is that we can stop the job, take the snapshot of the state, then we can update the job, and start again, and start from this snapshot. It continues working without huge backlog or something. This was the expectation. Nice, we have Flink, so Flink has savepoints. Now we have Flink SQL, so we can update this SQL and restore from savepoint. Unfortunately, it doesn&#39;t work like this.&lt;/p&gt;

&lt;p&gt;In fact, it&#39;s impossible to upgrade Flink SQL job. For us, at that moment, kind of unexperienced Flink engineers, it came like a big shot. Because like, come on guys, are you kidding? What are we supposed to do? We launched the job and should expect it never fails? Like, works forever or what? When the first shock faded, and we thought about this a little bit more, it&#39;s not that surprising. Because this query actually gets translated to a set of stateful operators. When we do even slight change in the query, these stateful operators may completely differ. Of course, mapping one set of stateful operators to another is a super-difficult task. It&#39;s not yet implemented. It&#39;s not a Flink fault that it&#39;s not implemented. Knowing this doesn&#39;t make our life easier, because for us, we want to provide a platform where users want to go, update SQL, and just relaunch the job. It should just pick and continue working. The typical recommendation is to always backfill.&lt;/p&gt;

&lt;p&gt;If you want to update Flink SQL, we compose the job in such a way that it first runs in batch mode, already process data, and then continue on the new data. It&#39;s really inconvenient, and it&#39;s also costly. If you want to do this every single time we update these features, it will cost us a lot of money. It&#39;s just super inconvenient, because backfill is also taking time. We want the experience so that we updated the job and just relaunched them, and it continues working.&lt;/p&gt;

&lt;p&gt;One thing where Flink shines is so-called DataStream API. DataStream, in comparison to SQL API, DataStream is where we express the job in a form like some JVM language, like Java, Kotlin, or Scala. There is a really nice interop between SQL and DataStream. In particular, it&#39;s called Changelog. When we want to get from SQL to DataStream, there is a thing called Changelog. Basically, SQL will send so-called Changelog rows. What is it? It&#39;s a row of the data with this marker, like on this list, +I. +I means that it&#39;s a new row on our virtual table. There could be -U, +U. They come in pairs. -U means that this was the row before update, and +U means this is the row after update. Once SQL is running, it keeps issuing these Changelog entries. What&#39;s interesting about this Changelog, if you look at this, you can notice that the final values of our columns, our features, is aggregation over this Changelog.&lt;/p&gt;

&lt;p&gt;If you consider + operation is plus, and - operation is minus. Basically, here we see three rows. If we want to count shares, we do, 3 - 3 + 5, final result is 5. The same continues. We can keep treating these Changelog entries like commands, either + command or - command. If we want to express this in form of this DataStream, there will be a function like this. Super simple. We have row update. We have current state. We get the state. We see if it&#39;s positive update or negative update, and we perform this update. Super simple. So far, it&#39;s super obvious. What&#39;s interesting about this model is that it survives job upgrades. Because when we upgrade the job, we lost SQL state, fine. This SQL will continue issuing these Changelog entries. We have this set of +I, -U, +U.&lt;/p&gt;

&lt;p&gt;Then job upgrades, and now this row from the SQL engine perspective, is a new row. It will start sending these +I entries. It&#39;s fine. This +I represents exactly the change from the previous data. From the logic that performs aggregation over the Changelog, nothing happens. It just keeps treating these entries as commands, like + command, - command. We see, we had this aggregation. Now job upgrade happened. We don&#39;t care. We keep aggregating these Changelog entries. We can compose the job in this way. There is SQL part, and there is Changelog aggregation part. SQL part, we don&#39;t control the state there, because of this magic and complexities of this SQL and so on. In this Changelog aggregation part, it&#39;s expressed in DataStream. This is where we control the state. We can build the state in such a way that it survives job upgrades. This part of the job can be restored from savepoint and continue. The flow will look like we updated the query and just relaunched the job from savepoint. Computation will continue.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Story 3 (Being Tired)&lt;/h2&gt;

&lt;p&gt;Now we have the system that we can write and also update. It&#39;s pretty exciting. The next problem that we might face is that when we launch the job, performance over time may decline. We call this the job getting tired. It&#39;s clear why. Because this is a stateful job, and we have state. The bigger the state, the less performant the job. It&#39;s a pretty clear dependency. The typical recommendation, you have the state that just applied TTL. The state will expire. It will be constant size. The job will not get tired. It&#39;s a file recommendation, but it&#39;s not really clear what kind of TTL to pick. For instance, in our case, the largest tile that we use in this system is a five-day tile. If you want counters to be accurate, the TTL must be at least five days. It&#39;s not even entirely solved the problem of lifetime counters, where we don&#39;t have window. It&#39;s not really clear what kind of TTL to pick. Assuming they&#39;re fine with lifetime counters being inaccurate, and assuming that we find the five-day TTL in the context of lifetime counters, the problem is that five days is too much.&lt;/p&gt;

&lt;p&gt;In our experience, the job processing hundreds of thousands of events and performing millions of operations per second with the state, it shows signs of being tired pretty quickly, just a few hours after launch. Five days is just too much. Of course, we can upscale the job, but it comes with a cost. What can we do about it? Remember that we now have two types of state. One is SQL state, and now, Changelog aggregation state. The good news about SQL state is that we shouldn&#39;t do anything, because we already survived the job upgrade because of this Changelog mode.&lt;/p&gt;

&lt;p&gt;Now from the SQL perspective, it doesn&#39;t matter if it lost state because of job upgrade, or it lost state because of TTL. It doesn&#39;t matter. We just set TTL on the SQL and keep treating these Changelog commands as commands. We shouldn&#39;t do anything. For Changelog aggregation state, we can modify our function a bit. When we access the state and it got expired, we can just query our Scylla, because the data exists in Scylla. We modify, like have this in green update of our function. Now we can set TTL for Changelog aggregation as well. It will keep running and recovering itself from the main database. Now the jobs are no longer getting tired, so it has consistent performance because the state has a consistent size.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Story 4 (Co-living Happily)&lt;/h2&gt;

&lt;p&gt;Good. Now we can launch the job, update the job, and it&#39;s not dying. The problem though now with previous change, now jobs not only write to database, but also read from them. It&#39;s actually a big deal. A big deal for the job to read from the database, because for a database like Scylla or Cassandra or similar kind, reads are somewhat more expensive than writes, especially cold reads. If we read something which is cold, which doesn&#39;t contain database cache, a lot of stuff happens. Because we need to scan multiple files on the disk to get the data, merge them, update the cache, a lot of stuff. What&#39;s interesting about jobs, is that they are more likely to hit cold reads than the service that serves features. What would happen is that when we do something on the job site, for instance, we launched a bunch of test jobs. We want to launch test jobs because we want to unlock this engineering velocity, and we want to experiment, and so on. Or maybe some job got a backlog for some reason and needs a huge backfill or whatever.&lt;/p&gt;

&lt;p&gt;We launch the job, and they start hitting these cold reads, especially if it&#39;s in backfill, and they try to process the backlog: they hit a lot of these cold reads. It thrashes main database and affects the service latency that accesses the features. What do we do about that? The first thought may be that we need some throttling. The problem, though, it&#39;s not really clear the level where we should apply the throttling. We cannot throttle on individual worker&#39;s level in the Flink job, because at least we have multiple jobs, and a new job can come and go.&lt;/p&gt;

&lt;p&gt;Instead, we can have some kind of coordinator in between jobs and Scylla, which is basically a proxy. It&#39;s a tricky thing, though, because this proxy, Scylla and Scylla clients are really super uber-optimized. If you want the same efficiency for the reads, this proxy should be at least optimized as well as Scylla itself, which is rather tricky. Overall, this solution is complex. It actually has extra cost, because we need this component, which needs to be scaled appropriately. Likely, it&#39;s not efficient, because it&#39;s not really easy to write this proxy in the same way that it will be as efficient as Scylla itself.&lt;/p&gt;

&lt;p&gt;The second option is called data centers. Scylla, the same as Cassandra, it has data center abstraction. It&#39;s purely logical abstraction. It doesn&#39;t need to be a real data center, real physical. Basically, we can split our cluster into two logical data centers. Job will hit data center for a job, and feature store will hit data center for the feature store. It&#39;s pretty nice, because it has super great isolation. We also can independently scale these different data centers. The downside is that the cluster management for Scylla becomes much more complex. It also comes with cost, because even though we can independently scale these data centers, it still means extra capacity. Also, the complexity of cluster management shouldn&#39;t be underestimated, especially if you want real data centers. Now, for instance, we wanted our database to be in two data centers, and now it&#39;s in four. The complexity actually increases quite a lot.&lt;/p&gt;

&lt;p&gt;The third option, the one that we ended up with, is so-called workload prioritization. There is this feature in Scylla called workload prioritization. It&#39;s that we can define multiple service levels inside Scylla and attach different workloads to different service levels. How does it work? Any access to any resource in Scylla has the queue of operations. For instance, we have job queue and service queue: job has 200 shares and service has 1,000 shares. What does it mean? It means that for any unit of work for the job queries, Scylla will perform up to five units of work for service queries. There is the scheduler that picks from these queues and forms the final queue. What does it mean? It means finally we will have consistent latency. Of course, job latency will be higher than service latency. This is fine because job is a background thing and doesn&#39;t care about latency that much. It cares about throughput.&lt;/p&gt;

&lt;p&gt;Service, on the other hand, it cares about latency. Summarizing this final solution, how it will look like, basically we don&#39;t do anything with the cluster, with Scylla itself. We just set up these different service levels. From the job, we access the cluster using user for the job workload. From feature service, we access user for the service-service level. This is super simple. No operation overhead. Best cost because we don&#39;t need to do anything with the cluster. The only downside is that job and service, they connect to the same nodes in Scylla. Theoretically, they are not entirely isolated. It&#39;s a nice tradeoff between cost and safety.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Story 5 (Being Lazy)&lt;/h2&gt;

&lt;p&gt;Final story is about being lazy so that now we can launch the job, they are running, and they don&#39;t impact our main service. Now it&#39;s time to think about the data model in database to serve the queries. We need to be able to query these tiles to aggregate window counters. The natural data model is like this. Scylla has the notion of partitions. A partition is basically a set of rows ordered by some keys. We can attach each entity ID to partition. Inside partition, we can store each feature in its row. Basically, feature will be identified by timestamp of the tile and feature name. We have these rows. This schema is nice because we don&#39;t need to modify the schema when we add each feature. It&#39;s schemaless in terms of, we can add as many feature types as we want, and the schema survives. However, if you do some math, we have 8,000 feed requests. On average, we rank around 2,000 candidates. For each candidate, we need to query 100 features.&lt;/p&gt;

&lt;p&gt;For these features, we need to query like 20 or something tiles. Also, assuming that our feature service has some local cache, and assume we have 80% cache hit rate, then multiplying all of this, we will get more than 7 billion rows per second. This is the load that our database needs to perform in order to satisfy this load. This is totally fine for Scylla. It can scale to this number. The problem is that it will use some compute. Of course, our cloud provider, GCP, and Scylla itself, they will be happy to scale. Our financial officer might not be that happy. What can we do? Instead of storing each feature in the individual row, we can compact multiple features into the same row, like this, so that now rows identify only by tile timestamp. Row value is basically bytes, some serialized list of pairs, like feature name to feature value, feature name to feature value. This is nice, because now we don&#39;t have this 100 multiplier anymore, because we will query 100 less rows, and this number of rows per second looks much better.&lt;/p&gt;

&lt;p&gt;The question may arise, whether it&#39;s really a cost saving, because it can be that we just shifted the compute from the database layer to basically the job. Because we used to have this nice schema, when we updated each feature independently, it was nice. Now we have these combined features. In protobuf, it can be expressed in a message like this. We have FeaturesCombined and map in string to some feature value. What does it mean? It means that whenever a feature is updated, we need to serialize all of them together, every single time. Basically, it may look like the cost of updating a single feature now gets 100 times bigger. It&#39;s a fair question. There are fairly easy steps to mitigate it. The first is a no-brainer, is that we don&#39;t need to store strings, of course. We always can store some kind of identifiers of the feature. We can always have a dictionary mapping feature values to some IDs. Now we need to serialize mapping of int to feature value, which, of course, for protobuf is much easier.&lt;/p&gt;

&lt;p&gt;The second observation is that protobuf format is pretty nice, in the sense that this map, can actually via equivalent to just a repeated set of key-value pairs. Basically, we have two messages. One is FeaturesCombined, and another is FeaturesCombinedCompatible, which is just repeat MapFieldEntry. We can serialize the FeaturesCombined, and deserialize FeaturesCombinedCompatible, and vice versa. They&#39;re equivalent in the form of bytes that get produced. Moreover, they&#39;re actually equivalent to the just repeat bytes feature, so basically, array of arrays. All these three messages, FeaturesCombined, FeaturesCombinedCompatible, FeaturesCombinedLazy, they&#39;re equivalent in the form of the bytes that get produced by protobuf. How does it help? It helps that in the Flink state, we can store the map from feature ID to the bytes. Bytes serialize this MapFieldEntry. When we need to serialize all of these features, we just combine these bytes together, have this array of arrays, form these FeaturesCombinedLazy message, and serialize with protobuf.&lt;/p&gt;

&lt;p&gt;This serialization of protobuf, it&#39;s super easy because protobuf itself will just write these bytes one after another. This serialization is extremely cheap. It&#39;s much cheaper than serialization of original message. In fact, when we implemented that, we didn&#39;t need to scale the job at all. In comparison to other things that the job is doing, this step is basically negligible.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Assuming that you decided to build a feature platform, first, good luck. It&#39;s going to be an interesting journey. Second, take a look on ScyllaDB and Redpanda. The odds are that they may impress you and be your friends. Third thing is that Flink is still the king of real-time stream processing, but it takes time to learn and to use it in the most efficient way. The fourth thought, there are multiple vendors now who build SQL-only stream processing. My thought is that, in my opinion, SQL is not enough. I don&#39;t understand how we can build what we build without this power of Flink DataStream API. Probably, it&#39;s possible via some user-defined functions or something, but it likely would look much uglier and harder to maintain.&lt;/p&gt;

&lt;p&gt;In my opinion, Flink&#39;s ability to have this DataStream API is really powerful. Finally, lazy protobuf trick is a pretty nice trick that can be used pretty much anywhere. For instance, in addition to this place, we also use it on our service to cache gRPC messages. Basically, we have gRPC server, and there is a cache in front of it. We can store serialized data in the cache. When we need to respond to gRPC, we just send these bytes over the wire without a need to deserialize the message, to serialize it back.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 1: You&#39;re looking into C++ variants of various classically Java tools. Have you looked into Ververica&#39;s new C++ version of Flink yet? Partially backed by some team at Alibaba, Ververica are launching a C++ rewrite of Flink. It will have similar properties to like Scylla&#39;s rewrite of Cassandra. It&#39;s called VERA, their new platform. Have you looked into using that as another way to get more performance out of Flink? There&#39;s a new one called VERA by the Ververica team, which is a rewrite of the core Flink engine in C++.&lt;/p&gt;

&lt;p&gt;Burmistrov: The question is, there is some solution, which is a Flink rewrite to C++, pretty much like what happened to Scylla, rewriting Cassandra, which is Java to C++, and Redpanda the same, like Kafka, which is in Java to C++. In fact, I don&#39;t know about the solution in C++, but there are other solutions that claim to be Flink competitors, which are written in Rust. The downside of all of them that I mentioned, they claim to be all SQL-only. This is harder. We took a look at multiple of them. Performance may be indeed good, but how to adapt them with the same level of what we can do with Flink, we didn&#39;t manage to figure out.&lt;/p&gt;

&lt;p&gt;Participant 2: Can you comment a little bit on the developer experience of adding new features, and how often does that happen? You said you have 100 features. How often does that grow, and what&#39;s the developer experience of that?&lt;/p&gt;

&lt;p&gt;Burmistrov: What is the developer experience of adding features, and how does it look like in our solution? It&#39;s far from great. For each feature, we have some kind of configuration file. We split features into so-called feature set. Feature set, they are logically combined. Logically, for instance, we have one model, and user features for this model, or this kind of stuff, post features for this model. They are somehow logically combined. This configuration file is basically YAML that contains some settings, and also query. Query in Flink or SQL, but the query is simple, so it&#39;s equivalent in any SQL dialect. This query is basically select.&lt;/p&gt;

&lt;p&gt;Then, people can do something with select, like transform something, whatever. They define the query, and then basically they can launch the job. There is deployment pipeline. They can push the button, and a job gets upgraded. That&#39;s the flow to define the features. There is also the process of how this feature gets available for training. We still use so-called wait and lock approach. Basically, through the lifetime of accessing the feature, we lock the values, and using this lock, model gets trained. There is process. When we edit features, we now start to access it for locking. Then enough time passed, so model can start being trained on this data.&lt;/p&gt;

&lt;p&gt;Participant 3: Can you maybe elaborate on why did you choose Redpanda over Kafka?&lt;/p&gt;

&lt;p&gt;Burmistrov: Why did we choose Redpanda over Kafka? The answer is the cost. First of all, we wanted managed, because we didn&#39;t want to manage. We didn&#39;t have people to manage Kafka. We use the experience of managed Kafka in-house, we just didn&#39;t have a team to continue this. Then, we started evaluating the solutions. There is Confluent and other vendors. Then we compared prices. Kafka was the winner for the cost. Also, there are a few other considerations, like they have remote read replicas. They&#39;re moving towards being a data lakehouse. Every vendor is actually moving to that direction. We just liked their vision.&lt;/p&gt;

&lt;p&gt;Participant 4: The DataStream API for Flink is very similar to Spark structured streaming, in terms of the ability to do upserts on the tables. If jobs fail, we can use the checkpoints to trigger jobs. What about the disaster recovery strategies if the data is lost? What usually have you thought in terms of a data backup. Then the trouble becomes that the checkpoints are not portable to another location, because of issues of hardcoding of the table IDs and stuff like that. Have you thought about that?&lt;/p&gt;

&lt;p&gt;Burmistrov: What are the best practices of using Flink or Spark streaming, which is equivalent, in terms of disaster recovery, like if job died or something happened? First of all, we have checkpoints enabled. They&#39;re actually taken pretty regularly, like once per minute. Checkpoints get uploaded to cloud storage, S3 equivalent in GCP. We have a history of checkpoints. Also, we take savepoints once in a while. We have all this stuff ready for a job to recover from.&lt;/p&gt;

&lt;p&gt;Sometimes, to be fair, with Flink at least, actually the state can get corrupted. The state can get corrupted in such a nasty way that all checkpoints that we store, let&#39;s say we store like last 5, 10, whatever checkpoints, they all can get corrupted. Because the corruption can propagate from checkpoint to checkpoint. Now we have the job with unrecoverable state, what do we do? The good thing about the approach I described, that we don&#39;t do anything, we just start a job from scratch. Because it will just recover from the main database by itself. There will be a little bit of incorrectness probably due to last minute or whatever of data. In general, it can get to the running state pretty quickly.&lt;/p&gt;

&lt;p&gt;Participant 5: I have a follow-up question to the developer experience. I can imagine that when you&#39;re trying to understand, especially the part where you talk about the Changelog and the Flink operators, as a developer, I would love to be able to interact with the state. I know that the queryable state feature from Flink was deprecated. I don&#39;t know whether you were able to figure out a different way to see what&#39;s in the state and help you in your ability to create new features and stuff.&lt;/p&gt;

&lt;p&gt;Burmistrov: What&#39;s the developer experience when it comes to figuring out what&#39;s going on inside the job? Basically, we have two states: one SQL state and then this Changelog, aggregation of a Changelog. What do we do? It&#39;s in the works now for us. We didn&#39;t have this for a while. Relied on basically feature lock already down the line. When a feature computed and access it, we lock it and we can have basically match rate over raw data plus feature lock. Of course, it&#39;s pretty tricky. We actually want to dump this intermediate Changelog operations to some OLAP database like ClickHouse or similar. In this way, we will have this full history of what happened and ability to query and see. It&#39;s not yet ready, so we&#39;re working on it too.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/sharechat/</link><guid isPermaLink="false">infoq-presentations/sharechat</guid><pubDate>Wed, 19 Mar 2025 16:00:00 GMT</pubDate><author>Ivan Burmistrov</author><enclosure url="https://res.infoq.com/presentations/sharechat/en/card_header_image/ivan-burmistrov-twitter-card-1741179272139.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-apr-mlplatform.mp4" type="video/mp4"></enclosure><itunes:duration>47:16</itunes:duration><category>Architecture &amp; Design</category><category>Apache Flink</category><category>Culture &amp; Methods</category><category>.NET Core</category><category>InfoQ</category><category>Scalability</category><category>Machine Learning</category><category>Performance</category><category>Microservices</category><category>AI, ML &amp; Data Engineering</category><category>Low Latency</category><category>QCon Software Development Conference</category><category>QCon London 2024</category><category>Java9</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>How to Ship Updates to 40+ Apps Every Week With Nx</title><description>&lt;figure&gt;&lt;img alt=&quot;How to Ship Updates to 40+ Apps Every Week With Nx&quot; src=&quot;https://res.infoq.com/presentations/nx/en/card_header_image/santosh-yadav-twitter-card-1740568525418.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s1_20250320073842_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-sep-updateswithnx.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-sep-updateswithnx.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-sep-updateswithnx.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Santosh Yadav discusses the world of Nx, maintaining a large Angular code base, CI/CD, release strategy, feature flags, introducing new tools to the codebase.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Santosh Yadav is a Senior Software Engineer at Celonis, a GDE for Angular, GitHub Star, Nx Champion. He loves contributing to Angular and its ecosystem. He is a co-founder of This is Learning. He is also the author of the Ngx-Builders package and part of the NestJsAddOns core Team. He is also running This is Tech Talks talk show, where he invites industry experts to discuss different technologies.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;InfoQ Dev Summit Munich software development conference focuses on the critical software challenges senior dev teams face today. Gain valuable real-world technical insights from 20+ senior software developers, connect with speakers and peers, and enjoy social events.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Yadav: When I was asked to come here and give a talk, I was thinking about how not to give a talk which we have been through, because at Celonis, we went through a lot of troubles to where we are right now, so it&#39;s like a journey, and the tools which we used in the process to make us deliver faster. Nx is one of the most important tools which we use in our ecosystem. That&#39;s why I just mentioned it in the talk title as well. We&#39;ll talk about Nx as well. Let&#39;s see what we are going to talk about.&lt;/p&gt;

&lt;p&gt;First, I want to show you how our application looks like. This is our application. If you see the nav bar, actually each nav bar is actually an application. It&#39;s a separate application which we load inside our shell. Even inside the shell, there can be multiple components which we can combine together and create a new dashboard for our end users. It means there are different teams which are building these applications. This is where we are right now.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;p&gt;How did we start? I just want to show you what the problem statement was. What was the problem or the issue which we were trying to resolve? Then we ended up here. This was our old approach. I was speaking to some of my friends, and we were talking about the same issue where we have multiple repositories, but we are still thinking about, of course, moving all the code to a single monorepo stuff, but we are not able to do it. Or we are struggling, like we know that there might be challenges. This is where we were three years ago. We had separate apps with separate repositories. We used to load each app using URL routing. It&#39;s not like SPA or module federation or microcontent, which we know today. Because in the past few years, tools have added more capabilities.&lt;/p&gt;

&lt;p&gt;For example, webpack came with the support of module federation, which was not there earlier. Everyone was solving module federation in some different ways, just not in the right way. This is another issue which we had. Of course, we had close to 40 different repositories, and then we used to build those code. We are using GitHub Actions. Of course, we used to build that code, push it in the artifact or database, because that was the one way to load the application. We used to push the entire build into the database and then load it on frontend. The only problem is we were doing it X times. The same process, same thing, just 30 times. Of course, it costs a lot of money. The other issue which we had was, of course, we have a design system.&lt;/p&gt;

&lt;p&gt;Which company doesn&#39;t have a design system? The first thing which a company decides to do is, let&#39;s have a design system. We don&#39;t have a product, but we should have a design system. This was another issue which we had. Now this became a problem. Of course, we had a design system, but now different applications started using different versions of design system, because sometimes they had time. Some teams started pushing back, we don&#39;t have frontend developers or we don&#39;t have time to upgrade it right now. This was, of course, a big pain. How should we do it? This caused another issue.&lt;/p&gt;

&lt;p&gt;Some of our customers are actually seeing the same app, but as soon as they move to a different application or part of the application, they see a different design system. There&#39;s probably a dark theme and light theme, just as an example. Think about a different text box. Someone is seeing a different text box and someone is seeing different.&lt;/p&gt;

&lt;p&gt;What were the issues we went through? Page reloads, for example, of course, now with HTML5, everyone knows of course, the experience should be smooth. As soon as I click on the URL, there should not be page refresh. That&#39;s the expectation of today&#39;s users. This is not early &#39;90s or 2000 where we can just click on a URL and wait for one hour to download my page. This is the thing of the past. Our users were facing this issue. Every page, every app, it reloads the entire thing. Bundle size, of course, we could not actually tree shake anything, or there was no lazy loading. Of course, there was a huge bundle size which we used to download.&lt;/p&gt;

&lt;p&gt;Of course, when we have to upgrade Angular or any other framework, this can be any other framework which you are using in your enterprise. We are using Angular, of course. We had too much effort upgrading Angular because we have to do it 30 times. Plus, our reusables and design system. Maintaining multiple versions of shared libraries and design system became a pain because we cannot move ahead and adopt the new things which are available in Angular or any other ecosystem because it&#39;s always about backward compatibility.&lt;/p&gt;

&lt;p&gt;Everyone knows, backward compatibility is not a thing. It&#39;s just a compromise. It&#39;s a compromise we do that, ok, we have to support this. That&#39;s why we are just still here. Now, as we said, we had 30-plus apps and then we used to deploy them separately. We had to sync our design system, which we saw in the previous slide. Which was, again, very difficult because for a few seconds or a few minutes, if your releases are not synchronized, you will see different UIs.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;What Is Nx?&lt;/h2&gt;

&lt;p&gt;Then came Nx. Of course, we started adopting Nx almost three years back. Let&#39;s see what is Nx. It&#39;s a build tool. It&#39;s an open-source build tool, which is available for everyone. You can just start using it for free. There&#39;s no cost needed. It also supports monorepo. Monorepo is just an extra thing which you get. The main thing is it&#39;s a build tool. It&#39;s a build tool you can use. Let&#39;s see. It actually provides build cache for tasks like build and test. As of today, one thing which we all are doing is we are building the same code again and again. Nx takes another approach. The founders actually are from Google. Everyone knows Google has different tools.&lt;/p&gt;

&lt;p&gt;If you have a colleague from Google, you keep hearing about, we had this tool and we had that tool, and how crazy it was. Of course, these people, they used to work in the Angular team. They took this idea of Bazel. Bazel was, of course, the idea, because Google uses it a lot. They built the entire Nx based on it. Eventually, they launched it for Angular first, and then now it&#39;s platform technology independent. As I said, it&#39;s framework and technology agnostic. You can use it for anything. It&#39;s plugin based, so you can bring your own framework. If there is no support for any existing technology, you can just add it. Or if you have any homegrown framework, you build it on your own. You can also bring it as a plugin, as part of Nx, and you can start getting all the features which Nx offers.&lt;/p&gt;

&lt;p&gt;For example, build cache. It supports all the major frameworks out of the box. For example, Angular, React, Vue. On top of it, it supports micro-frontend. If you want to do micro-frontend with React or Angular, it&#39;s just easy. I&#39;ll show you the commands. It also supports backend technologies. They have support for .NET, Java, Spring. They have support for Python. They also added support for Gradle recently. As I said, it&#39;s wide.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Celonis Codebase&lt;/h2&gt;

&lt;p&gt;This is our codebase as of today. We have 2 million lines of code. We have close to 40-plus applications. We have 200 projects. Why are applications not projects? Because we also have libraries. We try to split our code into smaller chunks using libraries, so that&#39;s why we have close to 200 projects. Then, more than 40-plus teams which are contributing to this codebase. We get close to 100 PRs per day. That&#39;s average. There are some times where we get more. With module federation, this is what we do today. We are not loading those applications via URL routing. It&#39;s just the Angular application loads natively. We have multiple applications here. Shell app is something which just renders your nav bar.&lt;/p&gt;

&lt;p&gt;Then you can access any apps. It just feels like you&#39;re a single page application. There is no reload. We can do tree shaking. We can actually do code splitting. We can also share or reduce the need to share our design system across the application, because now we have to do it only once. These are some tasks which we run for each and every PR. Of course, we do build. Once you write your code, the first thing which you do is you build your project. Then we write unit tests. We use Jest. We also have Cypress component test to write our test. Then we, of course, run it on the CI as well. Before we merge our PR, we also run end-to-end test. We are using Playwright for writing our end-to-end test or user journey.&lt;/p&gt;

&lt;p&gt;Then, let&#39;s see how to start using module federation with Angular. You can just use this command. You can generate nx generate. For any framework, you will find nx generate. Then you will say nx, and the framework name. You can just here, for example, replace Angular with React, and you get your module federated app or micro-frontend app for your React application. These remotes are actually applications which will be loaded when you route through your URLs. For example, home, about, blogs, this can be different URLs which we have. They are actually different applications. It means your three teams can work on three different applications but, at the end, they will be loaded together.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Feature Flags&lt;/h2&gt;

&lt;p&gt;We use feature flags a lot because when we started migrating all of the codebase, it became a mess. Of course, a lot of teams started pushing their code in a single codebase. We were coming from different teams. A different team had their different ways to write code. We had feature flags for backend. Of course, that was something which was taken care of. At the frontend, we were seeing a lot of errors. We thought of creating a feature flag framework for our frontend application. This is how it feels like without feature flag. I&#39;ve seen this meme many times. This always says, this is fine. We believe this is not fine. If your organization is always on fire, this is not fine. This is not fine for everyone. You should not do 24 by 7 just monitoring your systems because you just did a release. This is where we started. Of course, we had a lot of fires.&lt;/p&gt;

&lt;p&gt;Then we decided, of course, we will have our own feature flag framework for frontend applications. This is what we used to think before we had a feature flag. We&#39;re used to, ok, backend, frontend, we will merge it. Then everything goes fine. We&#39;ll do a release, and everyone is happy. This is not the reality. This looks good on paper but, in reality, this is what happens once you merge your code. Everything just collapses. We started with this. We started creating our frontend feature flag to do this. We now have the ability to ship a feature based on a user, based on a cluster. We can also define how many percentages of users or customers we want to ship this feature to. Or we can also ship a specific build. We generally try to avoid this. This is something which we use for our POCs.&lt;/p&gt;

&lt;p&gt;Let&#39;s say if you want to do a POC for a particular customer, we can say, just use this build. That customer will do its POC, and if they&#39;re fine or they&#39;re happy with this, we can go ahead and write for the code. For example, of course, we have to still write tests. We have to write user journey test. This is just for POC. We can also combine all of the above. We ended up with this. We started seeing, now there are less bugs, because now the bugs are isolated, because they are behind a feature flag. We also have the ability to roll back a feature flag if anything goes wrong. We don&#39;t have to roll back the entire release, which was the case earlier. Now we are shipping features with more confidence, which we need.&lt;/p&gt;

&lt;p&gt;Before you ask me which feature flag solution we are using, I&#39;m not here to sell anything. We built our own. We decided to build our own. How? Again, Nx comes into the picture. Because Nx, as I said, is plugin based. You can build anything and just create it as a plugin. You get everything out of the box. It feels native. It feels like you are still working with Nx. This is the command. You can just say, nx add and a new plugin. You can define where you want to put that plugin into. For our feature flag solution, we use a lot of YAML files. We added all the code to read those YAML files as part of our plugin. It&#39;s available for everyone.&lt;/p&gt;

&lt;p&gt;One thing which you have to focus more on, in case you are creating a custom solution, is developer experience. Otherwise, no one will use it. We also added the ability to enable/disable flags. Developers can just raise a PR and enable and disable a feature flag. We also added some checks that no one should disable a flag in case it&#39;s already being used, and no one knows about it. There are some checks. Like, for example, your release manager or your team lead has to approve it.&lt;/p&gt;

&lt;p&gt;Otherwise, someone just does it by mistake. Then we also have a dashboard where you can see which features are enabled and in which environment. Our developers can also see that. We also have a weekly alert, just in case there is a feature flag which is GA now, and it&#39;s available for everyone. We also send a weekly alert so developers can go ahead and remove those feature flags. This is fine, because we know where the fire is, and we can just roll it back.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Proof of Concepts&lt;/h2&gt;

&lt;p&gt;Of course, when you have a monorepo, the other problem which we have seen is that a lot of teams are actually not fans of monorepos, because they think they&#39;re being restricted to do anything. This is where we came up with the idea like, what if teams want to do a proof of concept? Recently, there were a few teams which said, we want to come into the monorepo, but the problem is our code is something which is a POC. We don&#39;t want to write tests, because we also have checks. I think most of you might have checked for your test coverage. You should have 80%, or 90%, or whatever. I don&#39;t know why we keep it, but it&#39;s useful, just to see the numbers.&lt;/p&gt;

&lt;p&gt;Then we said, let&#39;s give you a way so you can start creating POCs, and we will not be a blocker for you anymore. In Angular, you can just say, I&#39;ll define a new remote, and that&#39;s it. A new application is created. They can just do it. Another issue is, most of the enterprises, they have their own way of creating applications. They may need some customization. That, I want to create an application, but I need some extra files to be created when I create this application. Nx offers you that. Nx offers you the way to customize how your projects will be created. For example, in our use case, what we do is whenever we create an Angular application, we also add the ability to write component test. What we did is we just took the functionality from Nx, added all this into a single bundle or a single plugin, and we gave it to our developers.&lt;/p&gt;

&lt;p&gt;That whenever you create a new application, you will also get component test out of the box. Or let&#39;s say it can be your Cypress, or it can be your Playwright, or it can be anything which you like. For example, you want to create some extra files, for example, maybe Dockerfile, or maybe something related to your deployment, which is mandatory for each and every app. You can customize the way your applications are created by using the generators. This is called Nx Generator. As I said, you can also create files. You can define the files wherever you want to. Generally, we use files as a folder. You can put all the files.&lt;/p&gt;

&lt;p&gt;For example, as I said, Dockerfile, or any other files which you need for configuration. You can pass them as a parameter. It uses a format called EJS. I&#39;m not sure how many people are aware of EJS. It uses a syntax called EJS to replace any variables into the actual file. Here, I&#39;m talking about the actual file. This is not any temporary files. I&#39;m talking about the actual files which will be written on the drive. You can all do this with the help of Nx Generator. This is what we do whenever someone creates a new application. We just add some things out of the box.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Maintaining a Large Codebase&lt;/h2&gt;

&lt;p&gt;When it comes to maintaining a large codebase, because now we are here, we have 2 million lines of code in a single repository, there are a few things which we have to take care of. For example, refactoring. We do a lot of refactoring because we got the legacy code. I&#39;m sure everyone loves legacy code, because you love to hate it. Then, we keep doing deprecations. This is one thing I think we are doing better, that we are doing deprecations. As soon as we see some old code, we start deprecating that code if it&#39;s not used. Then, migration. Of course, over the period of time, we have migrated multiple apps into our monorepo.&lt;/p&gt;

&lt;p&gt;We still support, just in case anyone wants to migrate their code to our monorepo. It took us time. It took us close to two years. Now we are at the stage where I think we have only one app, which is outside our monorepo. This is not going to happen in a day, but you have to start someday. Then, adding linters and tools. Of course, this is very important for any project. You need to have linters today. You may need to add tools tomorrow. Especially with the JavaScript ecosystem, there is a tool every one hour, I think. Then, helping team members. This is very important in case you are responsible for managing your monorepo. I&#39;m sure if you end up doing this, initially you will end up actually doing this a lot.&lt;/p&gt;

&lt;p&gt;Most of the time, you&#39;ll be helping your new developers onboard into a monorepo. This is very important, again. Documentation, this is critical, because if you don&#39;t do this, then more developers will rely on you, which you don&#39;t want to. It will take your time away. Then the ability to upgrade Angular framework for everyone. Whatever framework you use, we use Angular, but in case you use React or Vue. This is what we wanted. This is what comes under the maintaining our monorepo. How do we do this? For example, Nx offers something called nx graph. If I run nx graph, I get this view, where I can see all the applications, all the projects.&lt;/p&gt;

&lt;p&gt;I can figure out which library is dependent on which app. If I want to refactor something, I can just check if this is being used or not by using the nx graph. Or if there is something refactored which is required, I can just look at this graph and say, probably this UI should not be used in home, it should be used in blogs. Then you can just refactor your code. It helps a lot during refactoring and during deprecations as well.&lt;/p&gt;

&lt;p&gt;Now, talking about the migrations. As I said, you may have to migrate a lot of code to your monorepo once you start, because all the code is available in different repositories. Nx offers you a command called nx import, where you can define your source repository and your destination repository, and it will migrate your code with your Git history. This command just came in the last release. From past years, we have been doing it manually. We did it for more than 30 repositories, but we did it manually. The same thing is now available as part of Nx. You can just run this command and do everything automatically. We deploy our documentation on Backstage.&lt;/p&gt;

&lt;p&gt;This is what we do, so everyone is aware of where the documentation is. We use Slack for all the communications or any new initiatives or deprecations which we are announcing. We have a dedicated Slack channel, so just in case developers have any questions, they can ask on this channel. It actually improves the knowledge sharing as well, because if someone already knows something, we don&#39;t have to jump in and say, this is how you should do it. It reduced a lot of dependency from us, the core team. Education is important.&lt;/p&gt;

&lt;p&gt;We started doing a lot of workshops initially when we moved to a monorepo, just to give the confidence to the developers that we are not taking anything from you. We are actually giving you more control over your codebase, and we are just here to support. We started educating. We did multiple workshops. Whenever we add a new tool, we do a workshop. That&#39;s very important.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Tools&lt;/h2&gt;

&lt;p&gt;As I said, every other hour, you are getting a tool. What should you use? Which tool should you add? This is true that, of course, introducing a tool in a new codebase is very time consuming. You may actually end up doing probably two, three days just to figure out how to make this tool work. At the same time, sometimes adding a tool is easy, but maintaining it is hard. Because as soon as you add it, there is a new tool, which is available the next hour, which is much more powerful than this. Now you are maintaining this tool, because there is no upgrades. Most of your code is already using this tool, so you cannot actually move away from this now.&lt;/p&gt;

&lt;p&gt;At the end of the day, you have to just maintain this code or maintain this tool. Nx makes it easy. It also makes it easy to introduce a new tool and maintain a new tool. Let&#39;s see how. Nx offers you support out of the box for the popular tools, for example, Cypress and Playwright. This is now a go-to tool for writing end-to-end tests. I&#39;m not sure about the others, but it&#39;s widely used in the JavaScript ecosystem. Anyone who starts a new project probably now goes for Playwright, but there was a time that many people were going with Cypress. Nx, just a command, and then you can just start adding or start using this tool. You don&#39;t have to even invest time configuring this. You just start using it. That&#39;s what I&#39;m talking about.&lt;/p&gt;

&lt;p&gt;For unit tests, it gives you Jest and Vitest out of the box. You can just add this and then start using it. No time needed to configure this tool. What about the upgrades? Nx offers you something called Migrate. With the migrate command, you can just migrate everything to the latest version. For example, if you&#39;re using React and you want to move to the new React version, you can just say nx migrate latest, and it will migrate your React version. Same for Angular. This is what we do now. We don&#39;t invest a lot of time doing manual upgrades or something. We just use this nx migrate, and our code gets migrated to the new version. It works for all the frameworks, all the technologies which is supported by Nx, but you can also do it for your plugins.&lt;/p&gt;

&lt;p&gt;For example, let&#39;s say if you end up writing something for your own company, a new plugin, and you want to push some new updates, you can just write a migration, where this migration tool will just automate the migration for your codebase, and your developers don&#39;t have to even worry about what&#39;s happening. Of course, you have to make sure that you test it properly before shipping.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;I&#39;ll show you a small demo, because everything we saw was a picture. Always believe when you see something running, otherwise, don&#39;t. This is how your nx graph looks like, whenever you run nx graph, and you can click on Show all projects. Then you can hover on any project and see how it is connected, like how it&#39;s being used, which application is dependent on which application. For example, shell, you see dotted lines. Dotted lines is lazy loading. It means they are not directly related, but they are related.&lt;/p&gt;

&lt;p&gt;For example, Home and UI, it says that there is a direct dependency. You can figure out all this from nx graph. It also gives you the ability to see tasks, tasks like build or lint. Let&#39;s say if you make a code change, you can figure out what tasks will be run after my code change. Which builds will be running? Which applications will be affected? Everything you can figure out from this nx graph. This is free, so you don&#39;t have to pay. I&#39;m just saying this is one of the best features which I have seen, which is available for free. Let me show you the build. I talked about caching. Let&#39;s run a build, nx run home:build. I&#39;ll just do production build. It&#39;s running the build. This line is important. It says, 1 read from cache. Let&#39;s say if you make some changes, like right now, one thing about monorepo, people think I have 40 projects.&lt;/p&gt;

&lt;p&gt;Whenever I make changes, my 40 projects will be built. Monorepos have actually a bad name for this. I have done .NET, so I know. We used to have so many projects, and then rerun the same code or build the same code again and again, but not with Nx. Nx knows your dependency graph, so it can figure out what needs to be built again and what needs to be read from the cache. They do it really well. Here we can see one read from cache, because I already built it before. It just retrieved the same build from the cache. Now let&#39;s say, 40 teams working on 40 different apps, but one team makes changes to its own app, then 39 apps are not built again, because Nx knows from dependency graph that this application is not affected, so I don&#39;t have to build anything.&lt;/p&gt;

&lt;p&gt;If I try to build it again, so next time it will just retrieve everything from cache. Now it&#39;s faster than before. It says now it took 3 seconds, which earlier was 10 seconds. This is what Nx offers you out of the box. Nx is available for your builds, your test, your component test, or your end-to-end test, anything. All the tasks can be cached. This is caching.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;CI/CD&lt;/h2&gt;

&lt;p&gt;Of course, CI/CD, there is always one guy in your team who is asking for faster builds. I was one of them. We use GitHub Actions with Nx, which gives us superpower. How do we do it? We use actually larger runners on GitHub Actions. We use our own machines. We used to use GitHub-provided machines, but it was too expensive for us. We moved on to using our own machines now. We use Merge Queue to run end-to-end tests. I&#39;ll talk about Merge Queue, because this is an amazing feature given by GitHub. This is only available for enterprises. We can cache build for faster build and test, which we saw on the local. What we saw was on the local. I&#39;ll show you how we do it on CI. Let&#39;s talk about Merge Queue and user journey test first.&lt;/p&gt;

&lt;p&gt;One thing about user journey test is they are an excellent way to avoid bugs. Everyone knows. Because you are testing in a real-time simulation, because you are actually going to log in and click on a button to process something. We all know that if you try running user journey on every PR, it will be very expensive, because we are interacting with the real database. It may take a lot of time to complete your build. We also know that when you are running multiple branches, this is another issue. Because the next branch will soon go out of sync with the main branch because you already have latest changes in main branch.&lt;/p&gt;

&lt;p&gt;Then running the user journey test again on an old branch is pointless because now you don&#39;t have latest changes. It means there is chances that you may introduce errors. This is where actually Merge Queue was introduced by GitHub. Let&#39;s see how it works. Let&#39;s say these are four PRs in your pipeline, PR is pull request, and PR 4 fails, so it&#39;s removed from your queue. These three PRs, PR 1, PR 2, PR 3, will be sent to your Merge Queue. Merge Queue is actually a feature provided by GitHub, which you can enable from your settings. You can define how many PRs you need to consider for Merge Queue. We do 10. Ten PRs will be pushed to Merge Queue at once. You can change. Because we have 100 PRs per day, we found that this is our average. We can do 10.&lt;/p&gt;

&lt;p&gt;In your case, if you get more PRs, you can just increase the number of PRs which you want to push into Merge Queue. Then once it goes to Merge Queue, this is how it works. GitHub will create a new branch from your PR, the first PR, and the base branch will be main. Then it will rebase your changes from PR 1 to this new branch, which is created, but it will not do anything else. The branch is created. That&#39;s it. Then it creates another branch called PR 1, PR 2. Now the PR 1 branch is your base. Then it will merge PR 2 changes into this branch. Now it&#39;s latest code. Same with PR 3. Now it will create PR 1, PR 2, PR 3, take PR 1, PR 2 as base, and PR 3 changes will be merged to this branch.&lt;/p&gt;

&lt;p&gt;After this, it will run all the tasks which are actually available on your CI/CD. For example, you run build, you run test, you run your component test, plus user journey test. Whenever you are running user journey test, you are running it on latest code. It&#39;s not the old code which is out of sync. Yes, it reduces the number of errors you have.&lt;/p&gt;

&lt;p&gt;Before I go with affected, I want to give some stats, like how we are doing today. With 2 million lines of code, 200 projects, as of today, our average time for each PR is 12 minutes. For entire rebuild, it&#39;s 30 minutes. It&#39;s all possible because we take usage of affected builds. Because Nx knows what has been affected, so this is what it does internally. For example, Lib1, Lib2, it affects five different applications. Your change is this. You push a new code, which affects your library 1, in turn affects App1 and App3. What we will do is we will just run affected tasks. We will say, run affected and do build, lint, test. That&#39;s it. We retrieve the cache from S3 bucket.&lt;/p&gt;

&lt;p&gt;As of today, we are using S3 bucket to push our cache and then retrieve it back whenever there is a change. We just retrieve it back from the S3 bucket. You can do it if you have money. There is a paid solution by Nx, it&#39;s called Nx Cloud. You can just remove this. You don&#39;t have to do it on your own. Nx Cloud can take care of everything for you. It can actually even do cache hit distribution. I&#39;m talking about cache hit distribution on your CI pipeline as well as on your developer&#39;s machine. Your developers can get the latest build, which is available on the cache, and they don&#39;t have to build even a single thing. It&#39;s very powerful, especially if you are onboarding new developers. They can just join your team on day one, within one hour, they are running your code without doing anything, because everything is already built.&lt;/p&gt;

&lt;p&gt;As soon as they make changes, they are just building their own code and not everything. If you want to explore Nx Cloud, just go to nx.dev, and then you will find a link for Nx Cloud. As of today, we are not using Nx Cloud because it was probably too expensive for us and not a good fit, but if you have a big organization? As I said, Nx Cloud works for everyone. It&#39;s not only for frontend or backend: any technology, any framework. This is an example from our live code. We have our design system. For example, when I tried to run it for the first time, it took 48 seconds. The next run took us 0.72 seconds, not even a second. This is a crazy level of time which we save on every time we build something. Our developers are saving a lot of time. They are drinking less coffee.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Release Strategy&lt;/h2&gt;

&lt;p&gt;The last thing is about the release strategy. One thing at Celonis, is we love our weekends. I&#39;m sure everyone loves their weekend, but we really care about it. Our release strategy is actually built around the same, that we don&#39;t have to work on weekends. This is what we do. Of course, we have 40-plus apps, so we know that this is risky, so we don&#39;t do Friday releases. Because it&#39;s not fun, going home and working on Saturdays and Sundays to fix some bugs. What we do today, we create a new release candidate every Monday morning. Then we ask teams to run their test. It&#39;s a journey. There are teams who have automated tests. There are teams who don&#39;t have automated tests. They do manual or whatever way they are doing, or they just say, ok, it&#39;s fast. You should not do that, but, yes, that might be a possibility. They execute their tests, automated or manual.&lt;/p&gt;

&lt;p&gt;If everything goes fine, we deploy by Wednesday or Thursday. Wednesday is our timeline that we ask every team to finish their test by Wednesday, or worst case, Thursday. If something goes wrong, we say, no release this week. Because we are already on Thursday, if we do a release, it means our weekends are destroyed. We don&#39;t like that. We really care about our weekends, so we cancel our release, and then we say, we&#39;ll come back on Monday and then see if it goes ahead and we can do a deployment. If everything goes green, we just deploy and then go home and monitor it for Monday, either Thursday or Friday, based on when we release. Everything is happy. Then we do this again next week.&lt;/p&gt;

&lt;p&gt;Of course, there are some manual interventions which are required here. This is where we want to be. Of course, every company has a vision. Every person has a vision. We also have a vision. This is what we want to do. We want to create a release candidate every day. If CI/CD is green, we want to deploy to production. That&#39;s it. If there&#39;s something which goes wrong, we want to cancel our deployment and do it next day. Renato accidentally mentioned 40 releases per week. We at least want to do five releases a week. That&#39;s our goal. Probably we will be there one day. We are probably very close to that, but it will take us some time.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 1: I have a question about end-to-end test. As I understand you call it user journey test. How do you debug that in this huge setup of 40 teams? Let&#39;s say if test is red, how do I understand root causes? It can be a problematic red.&lt;/p&gt;

&lt;p&gt;Yadav: Playwright actually has a very good way to debug the test. We use Playwright. Then it comes with a debug command. You can just pass, --debug, and whichever application is giving us an error, you can just debug that particular application. You don&#39;t have to debug 40 applications. We also have insights. Whenever we run tests, we push the data of success and failure on Datadog. We display it on our GitHub summary. Even the developer knows which test is failing. They don&#39;t have to look into the void and see, what&#39;s going wrong? They know, this is the application, and this is what I have to debug.&lt;/p&gt;

&lt;p&gt;Participant 2: I was wondering if you also integrate backend systems into this monorepo, or if it was a conscious decision not to do so.&lt;/p&gt;

&lt;p&gt;Yadav: It does support. As I said, you can actually use your backend, like .NET Core. I think it supports Spring, as well as Maven. Now they added support for Gradle as well. You can bring whatever framework or whatever technology you want to. We are not using it because I think that&#39;s not a good use case for us. I think more teams will be happy with having the current setup where they own the backend, and the frontend is owned by a single team.&lt;/p&gt;

&lt;p&gt;Participant 3: How do you handle major framework updates or, for example, design system updates? Because I think in the diagram you showed that you try to do it like every day release. I can imagine that with many breaking changes, this is not how it can work. You need more time to test and make sure it&#39;s still working.&lt;/p&gt;

&lt;p&gt;Yadav: We actually recommend every developer write their own test. It&#39;s not like another team who is writing the test. That&#39;s one thing. Of course, about the upgrades, this is what we do. We have the ability to push a specific build. For example, Angular 14 upgrade, which was a really big upgrade for us, because after Angular 13, we were doing it for the first time, and there were some breaking changes. We realized very early that there are some breaking changes. We wanted to play safe. What we did is with feature flag, we started loading only Angular 14 build for some customers and see how it goes. We rolled it out initially for our internal customers, like our users.&lt;/p&gt;

&lt;p&gt;Then we ran it for a week. We saw, ok, everything is fine. Everything is good. Then we rolled it out for 20% of the users. Then we monitored it again for a week. Then 50%, and now we will go 100%. This is very safe. We don&#39;t have any unexpected issues. With design system, we do it weekly. It&#39;s like design system is owned by another team, so they make all the changes. They also do it on Monday. They get enough time, like four or five days, to test their changes, and then make it stable before the next release goes.&lt;/p&gt;

&lt;p&gt;Participant 4: You explained about the weekly release. How do you handle hotfix with so many teams?&lt;/p&gt;

&lt;p&gt;Yadav: Of course, there will be hotfixes, we cannot avoid this. There will be some code which goes by mistake on release. We try to capture hotfixes or any issues on release before they go on to production. Just in case there is anything which needs to be hotfixed, they generally create a PR with the last release, which is there. Then we create a new hotfix. It&#39;s all automated. You just need to create a new release candidate from the last build, which we had, and just push a new build again. Good thing is, with the setup, it&#39;s not like we have to roll back the entire release.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/nx/</link><guid isPermaLink="false">infoq-presentations/nx</guid><pubDate>Tue, 18 Mar 2025 16:00:00 GMT</pubDate><author>Santosh Yadav</author><enclosure url="https://res.infoq.com/presentations/nx/en/card_header_image/santosh-yadav-twitter-card-1740568525418.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-sep-updateswithnx.mp4" type="video/mp4"></enclosure><itunes:duration>43:00</itunes:duration><category>Agile Techniques</category><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>Agile</category><category>Continuous Integration</category><category>.NET Core</category><category>Cloud Computing</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>Automated Deployment</category><category>Release Management</category><category>IT Service Management</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>InfoQ Dev Summit</category><category>Java9</category><category>DevOps</category><category>InfoQ Dev Summit Munich 2024</category><category>Reactive Programming</category><category>Transcripts</category><category>Continuous Delivery</category><category>Development</category></item><item><title>Building Your First Platform Team in a Fast Growing Startup</title><description>&lt;figure&gt;&lt;img alt=&quot;Building Your First Platform Team in a Fast Growing Startup&quot; src=&quot;https://res.infoq.com/presentations/platform-engineering-strategy/en/card_header_image/jessica-andersson-twitter-card-1741177200218.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-apr-growingstartup.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-apr-growingstartup.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-apr-growingstartup.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Jessica Andersson discusses Platform Engineering, how to start small, strategies and tradeoffs, gaining adoption from developers, and the future.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Jessica Andersson is Product Area Lead for Engineering Enablement at Kognic, providing products and services for internal development teams. Jessica is also a CNCF Ambassador and engaged in the Nordic and local Meetup communities, as an attendee, speaker and organizer.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Andersson: Once upon a time, there was a startup. This startup was running code in production, because that&#39;s how we get our product out. They have been running for a couple of years, and they started to identify the need for all the things DevOps. They knew that they wanted to keep developer focus on delivering software value, and they knew that they wanted to do all them things cloud. Recognize this story? I think this is common for a lot of people.&lt;/p&gt;

&lt;p&gt;They realized that the solution was to invest into a base platform and platform engineering in order to solve the problem of CI/CD, runtime, and observability. It&#39;s been evolving ever since. I&#39;m here to tell you that you need a platform regardless of the size of your organization. I would even go as far as to say, you already have a platform, you just don&#39;t know it yet. You need to treat your platform as a product. It&#39;s no different than delivering the software product that your company is probably surviving on. Trust is a currency, and you need to treat it as such, because otherwise you will fail. Being a small company, the tradeoffs that you make will be the key to your success.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;I&#39;m Jessica. I&#39;m the product area lead for developer experience or engineering enablement at Kognic, the startup that I&#39;m working at. I&#39;m also a CNCF ambassador and a speaker at conferences. Before joining Kognic, I was a platform engineer working at another company and delivering platform as a service to more than 40 teams globally across the world. We were focusing mainly on Kubernetes as a platform and logging as a service. We had other things that we provided as well. I first heard of Kognic from a friend of mine who worked there when he sent me a job ad with a motivation, like, &quot;You do cloud and Kubernetes. How about this one?&quot; Who can resist that? It read, &quot;Wanted, Head of DevOps&quot;. Let&#39;s give it a chance. I like my friend. He&#39;s nice. I start reading the ad and I realize what they describe is something that actually sounds fairly similar to what I&#39;m already doing.&lt;/p&gt;

&lt;p&gt;I set up a meeting with our co-founder, Daniel, and we met up in a cafe looking like this, actually. We took a cup of coffee and we talked about it. I told him about all the things that I&#39;ve been doing with trying to empower and enabling different development teams to solve their needs more efficiently. He told me about their situation and how far they had gotten, that they had some software running, but they realized that they need to invest more into making it continue to run smoothly over time, and solve some of the common needs of how to operate it in production. I told him my other hot take that I don&#39;t dare to put in text, is that, I think a DevOps team is a glorified operations team, and I don&#39;t work with operations. It&#39;s nothing wrong doing that, but I feel that I really enjoy being a platform team more because I can affect a lot of people and I can try to make their lives easier.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Empowering and Enabling Product Teams&lt;/h2&gt;

&lt;p&gt;As a platform team, making people&#39;s lives easier, someone wrote on Twitter, &quot;Being a platform engineer is the closest that I will probably ever be to become a 10x developer&quot;. This was all the rage when I was looking for this job. We also know that by empowering and enabling our product teams, they can focus on the things that they care about a lot, which is delivering product value. We tried to look at the needs for this head of DevOps, the reason why this ad was out. What Daniel described the need of the company was that they needed to do quick iterations because they didn&#39;t really know exactly what the product would end up being in the long run. This is a startup. You&#39;re trying to figure out what is the perfect fit for my product. They wanted to do quick iterations. They also wanted to have a lot of flexibility in changing direction if they discovered this is not the way we want to go. They wanted to maintain the developer focus on delivering value. They didn&#39;t want all the developers to understand everything about Kubernetes.&lt;/p&gt;

&lt;p&gt;I don&#39;t want them to have to do that either because there&#39;s a lot of things to know. They also knew that they wanted to keep a cloud native modern tech stack. We saw on the hype scale that cloud native was right up there with generative AI. We also talked about developers and operations versus DevOps. I&#39;ve done a talk only about this thing previously. I think the main thing is that when you have a situation where you have a developer and operations team as we had at my previous company and we transformed that into DevOps, we have several reasons of doing so. We discovered that having the developers focusing only on pushing out new features and code changes was very disconnected from operating them in production because we started to see a lot of issues such as failures in production. We got long time to get new things pushed out because operations were busy firefighting. They were pushing back when developers wanted to deploy more things.&lt;/p&gt;

&lt;p&gt;Operations had a problem getting the feedback back to the developers and prioritize the fixes for solving the things failing in production. It was more or less just throwing things over the wall and hoping it all works out. It did not work out.&lt;/p&gt;

&lt;p&gt;At my previous company, we did a large effort in trying to transform into DevOps and have all the product teams work with an end-to-end full application lifecycle workflow. When we talk about end-to-end, like only in the full application lifecycle, we can also talk about how we get there. We get there through having empowered product teams. If you haven&#39;t read Empowered Product Teams by Marty Cagan, it&#39;s still a really good book, and there&#39;s a lot of great ideas in it. You don&#39;t have to read all of it. There&#39;s a lot of blog posts that summarize some of the main points, or talk to someone smart around you that actually read it. That also works for me. Check it out if you haven&#39;t. Marty Cagan describes empowered product teams as being about ordinary people delivering extra-ordinary products. You want to take any product teams, empower them so that they can focus on delivering great products.&lt;/p&gt;

&lt;p&gt;Difference between, as I mentioned, the developers pushing features and empowered product teams can be described as, product teams, they are cross-functional. They have all the functionality or all the skillsets that they need in order to deliver their part, their slice of the product. They might have product managers, they might have designers, engineering, whatnot, they need to deliver their slice. They are also measured by outcomes and not output. Output is, I did a thing, yes, me. Outcome is, I made an impact. There&#39;s a difference in that. We want to optimize for making good outcomes rather than a lot of output. They&#39;re also empowered to figure out the best way to solve the problems that they&#39;ve been asked to solve. This is a quote from the blog post that describes exactly this. It says that solving problems in ways our customers love, yet work for our business. It&#39;s not all about only focusing on making customers happy, about doing it in such a way that the business can succeed, because we&#39;re all here to earn money.&lt;/p&gt;

&lt;p&gt;Very much related to this is &quot;Team Topologies&quot; by Matthew Skelton and Manuel Pais. I will focus on it because I think this is strongly related and this is something that we looked at on how to structure our teams. Stream-aligned teams have also a slice of the cake. They have like a you build it, you run it thing. They have a segment of the business domain, and they&#39;re responsible for that, end-to-end. Then you have the enabling team. I said, I offer engineering enablement. There&#39;s a reason why it&#39;s here. They work on trying to help and unblock the stream-aligned teams. They are there to figure out the capabilities that we need to make in order to improve the life of the stream-aligned teams.&lt;/p&gt;

&lt;p&gt;Then we have the platform team, and they are supposed to build a compelling internal product to accelerate delivery by the stream-aligned teams. We&#39;re here to empower and enable our stream-aligned teams to deliver good outcomes to create business value. As a small company and a small organization, I argue that you probably can&#39;t afford to have both an enabling team and a platform team. In our case, we decided to combine these two. We decided that we should have both engineering enablement and platform engineering within the same function.&lt;/p&gt;

&lt;p&gt;Given what I told you about empowered product teams and trying to focus on good outcomes, do you think we should have a DevOps team or a platform team? It&#39;s a given. We&#39;re going for the platform team. That&#39;s why I&#39;m here. Back to the coffee shop, me and Daniel, we talked for longer than we set off time. We&#39;re both big talkers. In the end, we think that this might be something and we decided to give it a go. In June 2020, I joined Kognic with the mission of starting up a platform engineering team and try to solve the problem of empowering the product teams to deliver more value.&lt;/p&gt;

&lt;p&gt;By the time we had the platform team hired because there was new hiring going on, the engineering team had grown to 31 people. Four of these were working in the platform team. That means that about 13% of our headcount were dedicated to platform and internal tooling. The reason why I tell you is not because 13 is a magic number, I just thought it could be nice to know, like put it in numbers. Tell us what you really were doing. Being a small company, this is 13% of the capacity, whatever you want to call it, that we had to deliver new value, and we thought that we could actually gain this in more empowerment.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Implicit Platform&lt;/h2&gt;

&lt;p&gt;We had a team. We could start with a platform. We didn&#39;t start from scratch, because, as I said, we were running code in production. It turns out, if you&#39;re running code in production, you already have an implicit platform. The first thing that I had to do was try to figure out what is already here, what do we have right now. This platform, it exists, but it&#39;s less structured and less intentional. This is what happens when you don&#39;t do an intentional effort in trying to build your platform. There were some really good things in place, but a lot of it had happened in the way of, we need something for this, let&#39;s do it, and then go back to the things that I&#39;m really supposed to be working on. Meaning that we had Kubernetes running, but it had not been upgraded since it was started. Yes, that&#39;s true. We had other things that were working good enough to solve the immediate need, but maybe not good enough to empower the teams fully. We had a lot of things in place.&lt;/p&gt;

&lt;p&gt;They were done with the best effort, good enough approach, and we needed to turn this around. I&#39;m going to show you what we had in our implicit platform. This is not me telling you this is what you should run in any way, but I want you to have this in mind. We use Google Cloud Platform as our cloud provider. We use GKE, Kubernetes running on top of it for our runtime. We had CircleCI for our CI. We are writing code in TypeScript and Scala and Python. We had bash scripts for deploying to production. We also had InfluxDB and Grafana for observability. You can see that there&#39;s nothing here about logs because I don&#39;t think we work with logs at this point in time.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;A Base Platform&lt;/h2&gt;

&lt;p&gt;What do I mean when I say platform? Because this is what we were hired to fix. This is where we started out and what we wanted to build. I define this as a base platform. This is, for me, a new term. Focus on solving the basic needs. We&#39;re talking about CI/CD. You want to build, package, and distribute your code. You want to have it run somewhere in what&#39;s equivalent to production for you. You want to have the observability to be able to know in case something goes wrong so that you can operate and maintain your applications. Without these three, it&#39;s really hard to have something continuously deployed to production and keep working in production. I see those as the bare necessities of platform. With this concept in mind, we want to take our implicit platform and turn it into an intentional platform.&lt;/p&gt;

&lt;p&gt;Our platform, it changed a tiny bit, not much. You can see it&#39;s basically the same picture that I showed you before. We took Google Cloud and we introduced infrastructure as code to make sure that we have resources created in the same way and that it&#39;s easy to upgrade when we want to make changes to how we utilize them. We improved the separation of concern for cloud resources. There was a lot of reusing the same database instances for staging and production and other things. We also took Kubernetes and we upgraded it and applied security patches, and then continuously maintain it. Kubernetes does several releases per year, so it&#39;s a lot of years to keep up. CircleCI, we were running a monorepo where all the code was, for both TypeScript, Scala, and Python. We broke it apart and we reduced the build times a lot. We went from 40-plus minutes to less than 5 minutes.&lt;/p&gt;

&lt;p&gt;We also introduced GitHub Actions in order to have smaller, more efficient jobs because our developers really felt those were easy to integrate with. We didn&#39;t even provide it as a platform. It was just something they started using and then we adopted it. We removed InfluxDB and replaced it with OpenTelemetry and automagic instrumentation of applications. When I say automagic, I really mean automagic. If you haven&#39;t looked at it, it&#39;s as good as they say, and I didn&#39;t believe it until we tried it. Then we removed the bash scripts for deployments and we introduced Argo CD and GitOps for better version controlling, and control and easier upgrades and rollbacks of applications.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Platform as a Product&lt;/h2&gt;

&lt;p&gt;How did we go about getting to this place? We treated our platform as a product. It&#39;s an important part. I think the first thing you need to do is to understand your market. This is your empowered product teams. What do they need to do? How do they work today? What pain points do they have? You need to understand, what would be the good direction to go with this? You need to iterate and validate your solutions. You can&#39;t just go away for a year and do something and come back and deploy it, and hope everyone is happy. Because you need to always constantly work together with the teams in order to make sure that you have something good. If you&#39;re new to working with a product mindset, I can recommend looking at something that is called a Double Diamond, where you have two different phases. One is you go broad for problem discovery, and then you narrow down on a problem solution.&lt;/p&gt;

&lt;p&gt;Then you go broad on a solution discovery and then narrow down on a solution decision, and then you iterate on that. When we look at our platform team, we do similar things as our empowered product teams, meaning that we try to be cross-functional. We try to figure out, what capabilities do we need in our team in order to deliver the things that we need? We are looking at cloud infrastructure. We need that skill. We also need containers and Kubernetes skills because there&#39;s a lot to learn about those.&lt;/p&gt;

&lt;p&gt;Observability is a big thing. It&#39;s good to have that skill. Also, the enablement and the teaching. You need to teach your empowered product teams to adopt and work with these services that you provide. You need to be able to show them new ways of working. You need people that can actually both teach and communicate with other people. Communication is actually one of the bullets that we had in job ads for skills we&#39;re looking for. You also need product management skill combined into this team. Obviously, since we&#39;re doing product. If you want to learn more about working with product thinking for platform teams, I can recommend checking out this talk by Samantha Coffman. It was at KubeCon + CloudNativeCon EU in Paris. The recording is up on YouTube. Check it out. She does a really good description of it and she has really concrete examples of what it means to figure out what the real problem is rather than fixing the symptoms.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Finding the Right Problems&lt;/h2&gt;

&lt;p&gt;Talking about figuring out what the real problems are, remember what we wanted to achieve? We wanted to achieve quick iterations, flexibility to change direction, and maintain a developer focus on delivering product value. Given that, we wanted to figure out what is holding us back from achieving that. Let&#39;s start with understanding the market. Autonomous teams and empowered product teams, I have trouble separating those terms. Where does one end, where does another start? Autonomous teams is something that we have talked a lot about at Kognic. One thing that it says is that autonomous teams can deliver, beginning to earn value, and with minimum supervision. It&#39;s similar to empowered product teams: do all the things that you need to solve the problem, but also with the caveat of minimum supervision. They&#39;re in charge of defining the daily tasks and the work processes that need. It&#39;s very similar.&lt;/p&gt;

&lt;p&gt;If we think about autonomy and the free choice, we can&#39;t force our product teams to use our platform because then we are removing the autonomy and the freedom to choose. As a platform team, it&#39;s very important that we try to make it enticing and something they want to use. We can have things as our platform as a default setting, but maybe we can&#39;t hinder them from choosing something else. To achieve that, we want to create a paved road for the developers. What does that even mean? What is paved road? We want to empower and enable our product teams or our stream-aligned teams to deliver valuable outcomes. For every decision and every action they need to take that doesn&#39;t build towards that, we can view that as a cost that takes away from the outcomes that they are able to deliver. If we provide a paved road, something that is easy to follow, they don&#39;t have to make a decision of, how do I want to deploy to production? They can follow the already paved road of that.&lt;/p&gt;

&lt;p&gt;Then we can solve the basic needs of building, running, and operating applications. We allow our teams to focus on the things that make a difference, and we reduce that cost. This paved road should be effortless. It should not cost them energy to stay on the paved road because then your paved road is not that great. As a platform team, I mentioned Kubernetes has a lot of upgrades. How many of you do migrations continuously, because I feel like we are always in a migration from one thing to another? If we as a platform team make those migrations feel cumbersome or heavy to do, people would try to like, &quot;Maybe I don&#39;t have to migrate. Maybe I can do something else. Maybe I don&#39;t have to follow this thing that platform is now forcing me to do again&quot;. You need to make those things effortless so that people can maintain on the paved road without adding more energy.&lt;/p&gt;

&lt;p&gt;Is autonomy limitless? Are there boundaries to what you&#39;re allowed to do as an autonomous team? Who is responsible for setting those limits? If everyone decides to go for their own solution, it will be really hard to be a platform team. Like you say, this is how you should deploy your applications. Then the team goes like, I think I have a better way. There are two reasons for that: either your platform is shit or your culture is shit. Both of those is something that you should try to figure out as soon as possible so you can address it. I also think that there are occasions where autonomy is good, but like if you have people running around freely, just doing the things that they find really interesting, it will be very costly in the long run because it will happen that you have to do something. With a very diverse codebase, it&#39;s super hard to handle that as a platform team and as an organization. The longer you wait to figure out where the limits for your autonomy goes, the harder it will be to address it once you decide to do it.&lt;/p&gt;

&lt;p&gt;There are things that might be good to put as not optional for the paved road. When I talk about that, I usually think about compliance and I think about security. Everyone loves compliance and security. I&#39;m sure you do because I do know that I do. A paved road or a platform is something that can really help you figure those things out. If you make it easy for the teams to be compliant and be secure by building innately into your platform, you can reduce the load for them to do so, and be able to focus on the things that they want to do, like valuable outcomes. I think there are situations where the paved road might not be optional and that you can build it into the platform in order to solve that.&lt;/p&gt;

&lt;p&gt;Back to finding the right problem. If we build a paved road that enables quick iterations, flexibility change while allowing product teams to focus on product value while staying empowered, if we want to do that, then we need to figure out what is holding us back from doing so right now. We need to figure out what the right problems are. We knew that with our limited resources, being a small team, 31 people, 4 people in platform, we needed to figure out what we wanted to focus on and be intentional with what we invest to. We want to take our implicit platform, apply strategy in order to make it intentional. We wanted to reduce the pain points and the time sinks, and improve developer experience to increase our ability to deliver product value.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Problem Statements&lt;/h2&gt;

&lt;p&gt;I have some problem statements that we can use as a tool when asking ourselves what should we focus on. The first thing is like, teams are blocked from performing the tasks that they need to do. Maybe they have to wait for someone to help them in order to move forward. This is, of course, bad. I&#39;m only listing bad things here. The second one could be like the task that the team performed takes a long time and they are hindered from moving on until it&#39;s done. The third thing is the tasks teams perform are unreliable and prone to failure. I&#39;m going to give you three examples of where this applied to us. The first one was DNS. DNS was not failing, but DNS was blocking. When our teams wanted to deploy a new service and they wanted to attach a DNS to it, they had to go and ask one or two people that can help them create a DNS record and give it back. They were blocked from moving on until they got that support.&lt;/p&gt;

&lt;p&gt;Something that was taking a very long time, I mentioned before, we had a monorepository with a lot of long builds. You had to wait for the build to build your package so you could deploy to production. We had build times of over 40 minutes. This was taking a lot of time and hindering people from moving forward. When it comes to unreliable and failures, we had deploying to production with bash scripts. Because there was a lot of hidden functions within this bash script that was not clear, it was a black box to the developers, and it failed several times a week. It was becoming painful. The team members were not sure how to figure it out themselves. They couldn&#39;t know for sure, even if it seemed to go fine, if it actually also got reproduced in production. It was prone to errors. It was unreliable.&lt;/p&gt;

&lt;p&gt;This was something that they were not able to solve themselves. They were hindered from moving forward. We looked at these tasks and we figured out what we should focus on. Hint, we took all three of those and tried to look at. We tried to look at our implicit platform. We tried to figure out where can we streamline it, where can we upgrade it, and where can we improve it in order to remove those pain points and time sinks? When we have tackled how to solve those problems, we also need to figure out how to roll this out to the teams, and how we can get them started using it, and how can we gain adoption of a platform.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Trust is Currency&lt;/h2&gt;

&lt;p&gt;Which nicely leads me to the next section, which says, trust. Gaining adoption is closely related to the amount of trust your product teams have in you as a platform team. As eng says, trust is currency and you should treat it as such. You have to gain some currency before you can spend it. Credibility is a currency and it is what you earn and spend as a platform team. When we talk about trust, trust goes up and trust goes down. When I say up, I mean up in the organization. You have to keep your trust with leadership because they are the ones that decide to continue to invest into your platform team. If you look at the budget, you&#39;re just cost. If you&#39;re unlucky as well, you also get the cloud bill on your cost part of their budget, and then it looks like you&#39;re very expensive. You need to build trust with your organization that you are actually improving things so that you can keep doing it. It&#39;s already been talked about, and DORA metrics is something that you can work with in order to show some kind of improvement and show what value to deliver.&lt;/p&gt;

&lt;p&gt;This link goes to the &quot;Accelerate&quot; book which is written by Dr. Nicole Forsgren, Jez Humble, and Gene Kim. If we think about DORA metrics, they are four metrics and they are focused on deployment frequency, how often do you get new things into production? Lead time for changes, how long does it take that you start working on something until it actually reaches an end user? Mean time to recovery, if you have a failure, how quickly do you recover? Change failure rate, how often does a change lead to failure? Those four metrics is something that you measure your empowered product teams on but can be a nice indicator on your effect as a platform team. If you think about down, you want to have trust with your product teams in order to gain adoption of your platform.&lt;/p&gt;

&lt;p&gt;If they don&#39;t trust you to solve their pain points, then you need to figure out why they don&#39;t trust you and what you can do to change that. I would suggest starting with something easy but painful or time consuming. Make that work so much easier for them, and then go on to the next thing. Start small, start building up credibility, because when you have built some trust, people will start coming to you and then you will have an easier time understanding their point of view. For us, something that we did, the DNS thing, we introduced external DNS into Kubernetes which means that you can use Kubernetes configuration in order to allocate a DNS record. This was very easy for the developers to understand how to use and it was very quick for them to start using it as well, meaning that from one day to another basically, they were no longer blocked by anyone ever when wanting to change DNS.&lt;/p&gt;

&lt;p&gt;Once you have tackled some of the small things, you can go on to the bigger things, and then you will probably spend some credits so you can earn them back again. Based on my experience, these are some of the things that you can do in order to earn or spend the credits. When we talk about earning credits, we can talk about removing pain points, and really anything that is painful for developers will do. As a platform team, it&#39;s good to be approachable and helpful. You want people to reach out to you so you can learn about what they are doing. Something that we do for this, is that in Slack, we have a team channel that is team platform engineering in which we have a user group that is a goalkeeper, platform engineering goalkeeper. Teams know that they can ping this goalkeeper with questions regarding the platform and to get help to figure out how they can solve something in case something breaks and they need help understanding what went wrong, they can do that.&lt;/p&gt;

&lt;p&gt;If they want help understanding how they can utilize some part of the platform, they can do that. By being very approachable and helpful, and by approachable, I mean there are no stupid questions, we know this. Also make sure that they understand it. Be nice. Be kind. If someone comes to you with a question and you go like, here&#39;s a link to wiki. Do you think they will ask you again? They will probably be like, no, they don&#39;t want to help. If you go like, we wrote this part about it but if there&#39;s anything that&#39;s unclear, please let me know and we can work on it together. That&#39;s more approachable. That is something that makes people want to come back and ask again. You can still give them the link to wiki because like, check the documentation. You can do it in a kind way so people want to reach out again.&lt;/p&gt;

&lt;p&gt;You want to be proactive. You want to be able to fix some things before people have to ask, especially if it&#39;s something that you really should know, like something is broken in your platform. It would be nice if you know it before some developers come and ask you, why is this thing not working anymore? You need to understand the team perspective, like, where do they come from? What do they know? What do they not know? What do they want to achieve? In spending credits, we have enforcing processes. Sometimes you can&#39;t avoid it, like compliance, like security. It costs you credits. In my experience, teams really don&#39;t like when you tell them you have to do this just because. Be mindful of how you enforce your processes.&lt;/p&gt;

&lt;p&gt;Also, blocking processes. Empowered teams, they know that they are empowered. They know they&#39;re supposed to be. If you take that away from them, you&#39;re blocking them from being empowered, they&#39;re not going to like it. Migrations, we can&#39;t get away from it, but depending on how you perform them, it will cost you credits. It might even cost you credits even if you do it well. Assumptions, I know everyone working on my platform team is really smart and capable at what they&#39;re doing. I also know that there are several times when we made assumptions of what the teams needed and how they wanted it to work, and we were wrong. It&#39;s very easy to take your world view and project it on something where it doesn&#39;t really fit. Make sure that you validate your assumptions and understand the team perspective in combination with your assumptions. Otherwise, you might be spending credits.&lt;/p&gt;

&lt;p&gt;I want to tell you, this could be our trust credit score over time, from June 2020 up until now. Please don&#39;t mind the time warp. It was hard. I don&#39;t have any numbers as well because we haven&#39;t been tracking this, but it could look like this. On the y-axis, we have the trust credits. On the x-axis, we have the time. You can see that we did a really big drop in trust. This is when we were making the assumption, enforcing a process, not talking to the teams to anchor or understand their perspective before migrating everyone to a new way of working. I&#39;m talking about how we introduced Argo CD and GitOps instead of a bash script for deploying into Kubernetes.&lt;/p&gt;

&lt;p&gt;For us, it was clear that everyone wants to work with GitOps, because you have it version controlled. It&#39;s very nice. You have control always running. You can follow the trail of everything. It&#39;s easy to do rollbacks and all the things. We knew this. This was clear. The whole industry is talking about how this is a good way of working, but we did not anchor it with the teams. We did not understand how they viewed working with the bash script and interacting with it. We took something away from them, we forced something on them, and we did not make them understand why.&lt;/p&gt;

&lt;p&gt;In the long run, we actually managed to gain back some trust on this because this change and the new process that we enforced, it proved itself time over time. In the long run, we gained more trust than we spent. I would rather not have that dip if I were doing it again, because I think it could have been avoidable, and I think we could have mitigated it through spending more time understanding the developers and making sure they&#39;ve anchored the change before performing it. In the long run, great investment.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Small Teams and Tradeoffs&lt;/h2&gt;

&lt;p&gt;Speaking of investment, as a small team, you will have to do tradeoffs. The link goes to the CNCF landscape. The CNCF landscape is a map of all the open-source projects that is under the Cloud Native Computing Foundation umbrella. I don&#39;t know the number, but if you zoom in, it looks like this. There&#39;s a lot of project icons, and they are structured into different areas. Being a small team, you will not be able to use all these tools. You need to figure out what you need for your use case. You need to be mindful of what you take on, because if you take on too many things, it will be hard for you to maintain the speed, and it will be hard for you to adapt once you have to really work with the business value. Let&#39;s say you&#39;re working and you have some slack time, and so you go like, &quot;What should we do now? How about this cool thing over here that I found? I think this would be a really nice feature for our product teams. I think they would really love it. Let&#39;s do that&quot;.&lt;/p&gt;

&lt;p&gt;Then you start. Then you make everyone start using it. Then you get a new version, and you have to migrate everyone. Then suddenly the business comes and says like, we need you to fix this thing, because we need to add this capability into our platform. You go like, but we are working on this nice-to-have thing over here. You have must needs that you will not be able to address because suddenly you have filled all your time with nice-to-haves. Be mindful of what you take on. Be mindful that the open-source community is both a power and a risk, because there&#39;s a risk of drowning into a lot of things, but it&#39;s also the power of standing on the shoulders of other people and utilizing what they already have done. Have reservation, but make use of the things that you must have. Ask yourself, what can you live without? What do I not need?&lt;/p&gt;

&lt;p&gt;For us, we realize we can live without a service mesh. A service mesh is a dedicated infrastructure layering for facilitating service-to-service communications within Kubernetes or other things. It&#39;s really nice. You can get these fancy maps where you can see the graph of services talking to each other and all the things. You can do network policies, all them things. Really nice, but not a must for us. We don&#39;t need it. In a similar way, we don&#39;t need mutual TLS between applications in our Kubernetes cluster, because that&#39;s not the main concern for us right now. Caveat, I really love Backstage.io as a project, but we don&#39;t need a developer portal. It can be extremely nice to have. It can solve many issues that you have, but as a small company, we don&#39;t have those pain points that motivate people to start using Backstage.&lt;/p&gt;

&lt;p&gt;We don&#39;t need to invest into a developer portal. Design system. Starting out, design system is like clear standards and component libraries that you can reuse for the frontend. Starting out, we did not want to invest into this because we didn&#39;t see the need. Actually, in the last year, we have started to invest into a design system. It&#39;s really valuable. We started out with the components that were mostly used throughout the application, and we started by standardizing those. Not every component is in the design system, but the ones that are, are used a lot, which is really nice for our frontend developers and our designers who can work together in collaborating on how they want the standard to work. Starting out, ask yourself, what things can you live without? What is on your nice-to-have list, but maybe not worth investing into?&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;With this knowledge, if you want to get started on your own paved road, what should your paved road contain? When you know what your business needs are, what your team needs are, how you can continuously build trust with your organization and your teams, and what tradeoffs you&#39;re willing to make, then you&#39;re ready to start paving your own road of empowered product teams. Do remember, you already have a platform. Start investing strategically into it. You should treat your platform as a product and unleash new capabilities. Trust is a currency, and you use it to gain adoption from your product teams. Tradeoffs is a key to success. Pick the right ones, and you can win again and again.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 1: You started with your personal journey, and I appreciate that a lot. Forgive me for saying so. It seems to me like the deal was already in place when you joined the new company. You didn&#39;t have to fight for a consensus. You didn&#39;t have to convince the company to start building a platform team. Myself, I&#39;m in a different situation. I&#39;m still fighting that battle, and I am exactly against the things you were saying. We are very small as a company. Maybe we just need a bigger DevOps team. Based on my experience, based on my reading, it seems to me like to win these arguments, what one would have to do is, small POCs improve value, but you also need a little bit of help from the top down. I need to manage upwards. I need to manage downwards. I&#39;m looking for some advice, basically.&lt;/p&gt;

&lt;p&gt;Andersson: Eb was talking about change, how to drive change without having the authority. He talked about finding allies for your cause, and I think finding allies in the leadership is what you need to do. Maybe not someone directly in your line, if you have trouble convincing them. Find someone else in leadership that will listen to you and that can be an ally for you. Then we had a talk about bits and bots and something else around DevOps, and she talked about how the different team structures can look. I think she made a really good case for why a DevOps team is not the way to go. She had a really nice dumpster picture on the DevOps team and everything. Check that talk out if you haven&#39;t. I think you can use that as a motivation for why a big DevOps team is not the solution.&lt;/p&gt;

&lt;p&gt;Then I think, yes, it really helped to have our co-founder, Daniel, convinced before joining the company. He knew they needed to change something. He wasn&#39;t sure how to approach it. Talking together, we could come to a shared vision of what that would look like, which was very useful.&lt;/p&gt;

&lt;p&gt;Participant 2: Luckily, for us, we&#39;re on the right path towards this, so we just started something like this. I&#39;m trying to know, did you have something like a roadmap from the beginning? How long did it take you to achieve this?&lt;/p&gt;

&lt;p&gt;Andersson: I&#39;m a really bad product manager because I never had a roadmap that I successfully maintained, unfortunately. Luckily, no one really blamed me for it either. What we did mainly was, it took about a little bit over half a year from me joining before we actually had a team in place. There was a lot of recruitment and getting to know the company first. That&#39;s where I spent the first time. Then when the team joined, we started to remove the blockers because there were some things that the teams were not able to do. Those were often quicker fixes, so we started with that. Within a year from June 2020, we had changed the DNS thing and we had changed the GitOps thing, but we had not started on the monorepo and the build times. Half of the things I&#39;ve told you now was basically within the first year. This second half spread out over the last three years, but also all the things that I did not mention happened in the last three years.&lt;/p&gt;

&lt;p&gt;Participant 3: If I&#39;m a developer within your company, what does the platform look like? Is it a website? Is it documentation, API? If I want to deploy something on your platform as a developer, where do I go? How does it work on a day-to-day basis?&lt;/p&gt;

&lt;p&gt;Andersson: As a developer wanting to use our platform, it&#39;s back to the keynote thing. If you want to introduce a Kanban for a team that never worked with Agile, or Jira, or one of these things, start simple. They used an Excel sheet. We used mainly GitHub repositories where it was like, this is how you can copy-paste code to get started. It&#39;s not a fancy platform in any way. It&#39;s more like, here&#39;s where you put your Kubernetes configuration and here&#39;s what you can copy-paste to get started. Here&#39;s where you can clone a GitHub repository to get that base application. It&#39;s a little bit crude still, but it&#39;s still making it more streamlined and everything. Boilerplates, we are currently working on rolling that out. It takes a while. Boilerplates are part of the fancy part of the platform. Bare necessities is just, make it run.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/platform-engineering-strategy/</link><guid isPermaLink="false">infoq-presentations/platform-engineering-strategy</guid><pubDate>Mon, 17 Mar 2025 16:00:00 GMT</pubDate><author>Jessica Andersson</author><enclosure url="https://res.infoq.com/presentations/platform-engineering-strategy/en/card_header_image/jessica-andersson-twitter-card-1741177200218.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-apr-growingstartup.mp4" type="video/mp4"></enclosure><itunes:duration>46:19</itunes:duration><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>Agile</category><category>.NET Core</category><category>Strategy</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>AI, ML &amp; Data Engineering</category><category>Platform Engineering</category><category>QCon Software Development Conference</category><category>QCon London 2024</category><category>Java9</category><category>DevOps</category><category>Leadership</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>Recommender and Search Ranking Systems in Large Scale Real World Applications</title><description>&lt;figure&gt;&lt;img alt=&quot;Recommender and Search Ranking Systems in Large Scale Real World Applications&quot; src=&quot;https://res.infoq.com/presentations/recommender-search-ranking/en/card_header_image/moumita-bhattacharya-twitter-card-1741256493367.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-nov-searchrankingsys.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-nov-searchrankingsys.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-nov-searchrankingsys.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Moumita Bhattacharya overviews the industry search and recommendations systems, goes into modeling choices, data requirements and infrastructural requirements, while highlighting challenges.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Moumita Bhattacharya is a senior Research Scientist at Netflix, where she works on developing at-scale machine learning models for Search and Recommendation Systems. Prior to Netflix, she was a Senior Applied Scientist at Etsy. She has been actively serving as Program Committees for WebConf and is a reviewer for conferences such as RecSys, SIGIR, WebConf, AAAI, and various journals. &lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon San Francisco empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Bhattacharya: We&#39;re going to talk about large-scale recommender and search systems. Initially, I&#39;ll motivate why we need recommendation and search systems. Then I&#39;ll further motivate by giving one example use case from Netflix for both recommendations and search. Then identify some common components between these ranking systems, our search and recommendation system. What usually ends up happening for a successful deployment of a large-scale recommendation or search system. Then, finally, wrap it up with some key takeaways.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;I think it&#39;s no secret that most of the product, especially B2C product, have in-built search and recommendation systems. Whether it&#39;s video streaming services such as Netflix, music streaming services such as Spotify, e-commerce platforms such as Etsy, Amazon, usually have some form of recommendations and some form of search systems. The catalogs in each of these products are ever-growing. The user base is growing. The complexity of these models and this architecture and these overall systems keeps growing.&lt;/p&gt;

&lt;p&gt;This whole talk is about, with examples, trying to motivate what it takes to build one of these large recommendation or search systems at scale in production. The reality of any of the B2C, business to customer products are that there are almost like, often depending on the product, there could be 100 million plus users, like Netflix has more than 280 million users, 100 million plus products. In general, to rank for these many users, these many products at an admissible latency, is almost impossible. There are some tricks that we do in industry to still keep the relevance of what items we are showing to our users, but be able to be realistic in the time it takes to render the service.&lt;/p&gt;

&lt;p&gt;Typically, any ranking system, whether it&#39;s recommendation system or search system, we break it down into two steps. One is candidate set selection, or oftentimes also referred to as first pass ranking, wherein you take all these items for user and instead of those millions of items, you narrow it down to hundreds and thousands of items. That&#39;s basically called candidate set selection. You are selecting a candidate that you can then rank. In this kind of set selection, we&#39;ll typically try to retain the recall. We want to ensure that it&#39;s high recall system. Then, once these hundreds of thousands of items are selected, then we have a more complex machine learning model that does second pass ranking. That leads to the final set of recommendation or results for search, query that then gets shown to the user. Beyond this stratification of first pass and second pass ranker, there are many more things that need to be considered.&lt;/p&gt;

&lt;p&gt;Here is an overview of certain components that we think about, we look into irrespective of whether it&#39;s a search system or a recommendation system. First is the first pass ranking that I showed before. The second one is the second pass ranking. For the first pass ranking, typically, depending on the latency requirement, one needs to decide whether we can have a machine learning model, or we can build some rules and heuristics, like for query, you can have lexical components that retrieves candidates. Versus for recommendation, you can have simple non-personalized model that can retrieve candidates. The second pass ranking is where usually a lot of heavy machine learning algorithms are deployed. There, again, there are many subcomponents like, what should be the data for training the model? What are the features, architecture, objectives, rewards? I&#39;m going to go into much more details on some example second pass ranking.&lt;/p&gt;

&lt;p&gt;Then there is a whole system of offline evaluation. What kind of metrics should we use? Should we use ranking metrics? Should we use human in the loop, like human annotation for quality assessment, and so on? Then there is the aspect of biases where when we deploy a system, all our users are seeing the results from that system. There is a selection bias that loops in. How we typically address that is by adding some explore data. How can we set up this explore data while not hitting the performance of the model? Then there is a component of inference where once the model is trained, we want to deploy it and then do the inference within the acceptable latency, throughput, what is the compute cost, GPU, and so on? Ultimately, any experience typically in a B2C product, we are A/B testing it as well.&lt;/p&gt;

&lt;p&gt;Then in the A/B test, we need to think about the metrics. I wanted to show this slide first. If you want to take away one thing, you can just take away this, that these are the different things you need to think about and consider. During this talk, I&#39;m going to focus on the second pass ranking, offline evaluation, and inference setup with some examples. Just in case you were not able to see some of the details in the previous slide, here are the sub-bullet points where data, feature, model architecture, evaluation metric, explore data, all these things are crucial for any of these ranking systems.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Recommendation: A Netflix Use Case&lt;/h2&gt;

&lt;p&gt;Now let&#39;s take a recommendation use case from Netflix. On Netflix, when a user comes to Netflix, usually there is a lot to watch from, and we often hear in our user research, it sometimes feels overwhelming. How do we find what I want to watch in the moment? Netflix oftentimes has so many recommendations on the homepage. It just feels overwhelming. One approach that our members do take is they often go to search, and then they will type something. For example, here, show me a stand-up comedy. The results from these search ranking systems are either personalized or just relevant to the query. I think 60% of our users are on TV, and typing query is still a very tedious task. Just to motivate the search use case, most of the discovery still happens on homepage on Netflix, but 20% to 30% of discovery happens from search, second only to the homepage. There is a nice paper that talks about the challenges of search in the context of Netflix linked here.&lt;/p&gt;

&lt;p&gt;Usually, when a member comes to search, there are three different types of member intent. Either the member directly knows what they want, so typing, Stranger Things, and then you specifically want Stranger Things. Versus you know something, but you don&#39;t know exactly, so that&#39;s find intent. Then there is explore intent where you type as broad things like, it&#39;s a Saturday evening, show me comedy titles, or something like that. Depending on this different intent, how the search ranking system responds are different.&lt;/p&gt;

&lt;p&gt;Going back to the aspect of a member coming from homepage to search and having to type this long query on a TV remote, which is very tedious. What if we can anticipate what the member is about to search and update the recommendation before the member needs to start typing? That&#39;s why this particular example I&#39;m referring to as a recommendation use case, even though it is after you click on the search page. Internally, we refer to as pre-query, but in industry it often is also referred to as no-query systems. This is a personalized recommendation canvas, which is also trying to capture search intent for a member. Here, let me motivate the purpose of this canvas a little bit more. On a Thursday evening, Miss M is trying to decide whether she goes to Netflix, HBO, Hulu, and so on.&lt;/p&gt;

&lt;p&gt;Then she comes to Netflix because she heard that they have good Korean content. There is a model that understands this member&#39;s long-term preference, but in the moment, she recently heard that Netflix has some Korean content that is really good from her friend, and her intent changed. What she did is she browsed on the homepage with some horror titles, and then she browsed on the homepage with some Korean titles. Now she still didn&#39;t find the title that she wants to start watching on homepage. Now she went on search. In this moment, if you&#39;re able to understand this user&#39;s long-term preference, but also the short-term intent, that is, she&#39;s looking for a Korean movie, and before she has to search Korean horror or something, we can just update the recommendation to show her a mix of Korean and Korean horror movies on the no-query, pre-query canvas. We can really capture her intent without the struggle of typing.&lt;/p&gt;

&lt;p&gt;If you imagine, to build a system like this, there is, of course, modeling consideration, but a large part of it is also software and infrastructural consideration, and that&#39;s what I&#39;m going to try to highlight. This is anticipatory search because we want to anticipate before the user has to search based on the in-session signal and browsing behavior that the member did in that current session. Overall, pre-query recommendation needs this kind of approach where it not only learns from long-term preference, but also utilizes short-term preference. We have seen in industry that being able to leverage browsing signals in the session, it&#39;s able to help the model capture user short-term intent, while the research question there is, how do we balance the short-term and long-term intent and not make the whole recommendation just Korean horror movies for this member?&lt;/p&gt;

&lt;p&gt;There are some advantages of these kinds of in-session signals. One is, as you can imagine, freshness, where if the model is aware of the user in-the-moment behavior, then it will not go into a filter bubble of only showing a certain taste, so you can break out of that filter bubble. It can help inject diversity. Of course, it&#39;ll introduce novelty because you&#39;re not showing the same old long-term preference to the member. Make it easy for findability because you&#39;re training the model or you&#39;re tuning the model to be attuned to the user&#39;s short-term intent. It also helps user and title cold starting. Ultimately, how we call it in Netflix is it sparks member joy. We see in our real experience, so this is a real production model, that it ultimately reduces abandoned session. In the machine learning literature, there is a ton of research of how do we trade off between user long-term interest with short-term interest.&lt;/p&gt;

&lt;p&gt;In the chronological order of research done many years ago to more recent, earlier we used to use Markov chain, Markovian methods, then there is reinforcement learning, there are some papers that tries to use reinforcement learning. Then, more recently, there is a lot of transformer and sequence model that captures the user long-term preference history while also adding some short-term intent as a part of the sequence and how they balance the tradeoff. I&#39;m not going into details about these previous studies, but some common consideration if you want to explore this area is, what sequence length to consider. How long back in the history should we go to capture user long-term and short-term interest? What are the internal order of actions? In the context of e-commerce, for example, purchase is the most valuable action, add to cart is a little less, click might be much less informative than purchase, and the different types of action.&lt;/p&gt;

&lt;p&gt;What is the solution that we built? I&#39;ll go into the model itself later, but first I wanted to show the infrastructure overview. A member comes to Netflix and the client tells the server that the member is on pre-query canvas, fetch the recommendation. In the meantime, as JIT&#39;s just-in-time server request call happens, we are also in parallel accessing every engagement that the member did elsewhere on the product. There has to be a data source that can tell, one second ago the member thumbs up a title and two seconds ago member clicked on a title. That information needs to come just in time to be sent to the server. While we also, of course, need to train the future model, so we also set up logging.&lt;/p&gt;

&lt;p&gt;Ultimately, this server then makes a real-time call with the in-session signals as well as the historical information to the online model, which is hosted somewhere. This online model was trained previously and has been hosted, but it&#39;s capable of taking these real-time signals to make the prediction. Ultimately, this model then returns a ranked list of results within a very acceptable latency. In this case, the latency is lower than 40 milliseconds, and ultimately sends the results to a client. In the process, we are also saving the server engagement, client engagement into logging so that the offline model training can happen in the future. There is a paper in RecSys on this particular topic. If you&#39;re more interested, feel free to dig deeper. That is the overall architecture.&lt;/p&gt;

&lt;p&gt;Here are some considerations that we had to think through when implementing a system like that. Actually, one of the key things for a system like that was this just-in-time server call. We really have to make the server call or access the model when the member is on that canvas. We have to return the result before the member even realizes, because we want to take all the in-session signals that the member did the browsing on the product in that session to the model. Because otherwise we lose the context. Let&#39;s say in the Korean horror movie context, the member is seeing a Korean horror movie and immediately goes to search, and if you&#39;re not aware of the interaction that the member did on homepage, then we will not really be able to capture the member intent. The recommendations will not be relevant to the member in that short-term intent of the member. Here are some considerations. The server call pattern is the most important thing we needed to figure out in this world.&lt;/p&gt;

&lt;p&gt;More interestingly, different platform, I don&#39;t know if that&#39;s the case for other companies. In this particular case, different platforms had different server call patterns. How do you figure out and work together with engineers and infra teams to change the service call pattern and make sure that the latency of the model and the end-to-end latency is within acceptable bound that the member doesn&#39;t realize that so much action is happening within such few milliseconds. Of course, throughput SLA becomes even more important depending on the platform, depending on the region, and so on. Because we want to do absolute real-time inference to capture the user in-session browsing signals, we had to remove caching. Any kind of caching had to be removed or the TTL had to be reduced a lot. These three components really differentiated a work like this from more traditional recommendation where you can prefetch the recommendation for a member. You can do offline computation.&lt;/p&gt;

&lt;p&gt;The infrastructural and software constraint is much lenient in a more traditional prefetching recommendation versus this system has to be really real-time. Then, of course, the regular things like logging. We need to make sure client-side and server-side logging is correctly done. Near real-time browsing signal is available through some data source, or there is a Kafka stream or something also making sure those streams have very low latency so that the real-time browsing signal can become available to the model during inference time without much delay. Ultimately, then the model comes, that the model needs to be able to handle these long-term and short-term preference and be able to predict relevant recommendation. There is a reason why I did a priority listing like that. The first three components are really more important than the model itself in this particular case, which is server call, latency, and caching.&lt;/p&gt;

&lt;p&gt;What is the model? The model itself in this case is a multi-task learning, deep learning architecture, which is very similar to traditional content-based recommendation model where we have a bunch of different types of signals that gets trained, go into the model. It&#39;s, I think, a few layered deep learning model with some residual connection and some real sequence information of the user. There is a profile context. That is where the user is from, country, language, and so on. Then there is video-related data as well, things like tenure of the title, how new or old the title is, and so on. Then there is synopsis and these other information about the video itself. Then, more importantly, there is video and profile information, so engagement data. Those are really powerful signals, whether the member had thumbs up a title in the past, or is this a re-watch versus a new discovery, new title that the member is discovering? In this particular work, there was this addition of browsing signals that we had added.&lt;/p&gt;

&lt;p&gt;This is where the short-term member intent is being captured, where in real time, we know whether the member did a my list add on this title or thumbs up this title or thumbs down some title. Negative signal is also super important. That immediately goes and feeds into the model during inference time, letting the model then trade off between short-term and long-term. We do have some architectural consideration here to trade off between short-term and long-term. Unfortunately, that&#39;s something I could not talk about. I&#39;m just giving you this thought that it&#39;s important to trade off between short-term and long-term in this model architecture. Overall, with this improvement of absolute real-time inference as well as the model architecture incorporating in-session browsing signal, offline we saw over 6% improvement, and it is currently 100% in production. All of Netflix users, when you go to pre-query canvas, this is the model that shows you your recommendation.&lt;/p&gt;

&lt;p&gt;Here is a real example. This was the initial recommendation when the user session started. This is on a test user. Then the member went and did a bunch of browsing on homepage, and did browsing on woman in the lead role shows and movies. Then they came back to no-query or pre-query page, and their recommendation immediately within that session got updated to have shows like, Emily in Paris, New Girl, and so on, which has woman in the lead character. Then they did, again, go back to category page or homepage and did some shows related to cooking or baking. Ultimately, in the same session, when they went back to search page, their recommendation immediately changed. You can see it&#39;s a combination of all three. It&#39;s not just souping the whole thing to make it baking show or something. This is where the tradeoff between short-term and long-term preference comes into play. That you want to capture what member is doing in the session, but don&#39;t want to overpower the whole recommendation with that short-term intent only.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Challenges and Other Considerations&lt;/h2&gt;

&lt;p&gt;What were some of the challenges and other considerations that we took into account? I think something that I alluded to in the previous slide, where filter bubble and concentration effect is a big problem, and still an open question in the space of recommendation and search, where, how do we make sure that when we understand a member need, we are not saying this is the only need you have, and the whole page, whole product gets overwhelmed with that one taste profile or one kind of recommendation. In this, both short-term, long-term tradeoff is important, but also explore-exploit or reinforcement learning, these are areas that are usually explored to break out of filter bubble and avoid concentration effect. Because this is such a real-time system, as you would imagine, depending on the latency and the region the model has been served, sometimes there is increased timeout, which leads to increased error rate. What we don&#39;t want is a member to see an empty page.&lt;/p&gt;

&lt;p&gt;There was a lot of infrastructural debugging and stuff we had to do to make sure that the error rate and increased timeout is not affected, which include multi-GPU inference, but also thinking about how do we deploy the model and additional consideration like the feature computation, whether there is some caching in some of the features that doesn&#39;t need to be real-time and so on. Overall, we also want to be careful about not making the recommendation too dynamic. We do want to capture the user&#39;s short-term intent and hence update the recommendation in real-time, but we also don&#39;t want to completely move the floor behind the member&#39;s feed by just changing the page every time the member is coming into that part of the product. We want to have a tradeoff between how much we are changing versus how much we are keeping constant. Because it&#39;s such a real-time system and because it&#39;s so dynamic, it is more difficult to debug.&lt;/p&gt;

&lt;p&gt;Also, it becomes more susceptible to network unreliability, which can ultimately cause degraded user experience. Another thing that is important is depending on how you are building these short-term signals, browsing signals. Some of these signals are very sparse, like how many times do you actually thumbs up a show when you enjoy something on Netflix, or on Hulu, or somewhere. Signals like thumbs up, my list add, or thumbs down, they are usually very sparse. Typically, we need to do something to the model to generalize these signals that are otherwise very sparse, and make sure the model is not over-anchoring on one signal versus another. That was my recommendation use case.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Defining Ranking - How and When It Was Right&lt;/h2&gt;

&lt;p&gt;Participant 1: You mentioned ranking, and I&#39;m assuming that after you computed and you had a list of things, you had to rank them based on, because that page is limited, so you can only show so much. How did you guys go about defining that ranking? How did you know it was right or when did you know it was right?&lt;/p&gt;

&lt;p&gt;Bhattacharya: This is where that happens. When we train the model, that&#39;s the example that I shared, the deep learning model with short-term, long-term intent and so on. Then we have offline evaluation. With offline evaluation, we evaluate for ranking. Some metrics for ranking is NDCG and MRR. What rankings mean typically is the model generates a likelihood of something. In this case, let&#39;s say likelihood of playing a title. Then we order that likelihood in decreasing order and cut it off. Let&#39;s say top 20, if you just want to show the member top 20 titles, we rank the probability scores and then take top 20 probability scores for a given context. In this case, let&#39;s imagine the context is just profile ID.&lt;/p&gt;

&lt;p&gt;Then we take that as top-k, and then we use some metric, for example, NDCG or an MRR, to evaluate how good the model is doing. There&#39;s something called golden test set here, where usually we would build a temporarily independent dataset, temporarily independent to the training data, to evaluate how good the model is doing. That&#39;s the offline metric. Then we go to the A/B test, which tells us what we saw offline, whether that&#39;s what our members are seeing. A/B test gives us a real test.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Balancing What Is Happening During Searches vs. Tagging with Metadata&lt;/h2&gt;

&lt;p&gt;Participant 2: As customers are changing their language about how they&#39;re searching, as well as the metadata that&#39;s associated with all of the content that&#39;s available inside of Netflix, it seems like there is this constant change, as you maybe had missed some metadata, because what was being pulled back in terms of recall and precision wasn&#39;t matching actually what the customer&#39;s language was trying to represent. How are you all trying to balance how things are tagged with metadata versus what is taking place during searches?&lt;/p&gt;

&lt;p&gt;Bhattacharya: We usually try to incorporate some of those metadata in the model as features so that that correspondence between the query or the user engagement with other titles and the metadata is used for the model to learn the connection. Usually the metadata is static, but the query is dynamic. When the query comes in, depending on the title and metadata that the model thinks is the right relevant result for that query, it gets pulled as top-k ranking. In general, there is also certain lexical and certain filters as well as guardrails in the actual production system. There are some boosting or some lexical matching that happens as well to make sure the model do not surface something that is completely irrelevant to the query or the context.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Search Use Case: A Netflix Use Case&lt;/h2&gt;

&lt;p&gt;The next use case is a search use case. Although it is a search use case, it&#39;s actually a search and recommendation use case. We built this model called UniCoRn, Unified Contextual Recommender. I&#39;ll get to in a couple of slides why is it called UniCoRn. Similar to what I already motivated, many products, especially B2C products, have both search and recommendations use case. In the context of Netflix, here is an example where we have traditional search, which is query to video ranker. For example, if you type P-A-R-I, we want to make sure the model is being able to show you Emily in Paris or some kind of P-A-R-I related titles.&lt;/p&gt;

&lt;p&gt;Then there is purely recommendations use case, which is what we saw in the previous slide, example is no-query or pre-query. Then there is other kind of recommendations, such as title-title recommendation, video-video recommendation. In the context of e-commerce, Canvas is more like this, or in the context of Netflix, it&#39;s more like this, wherein you click on a title, here, Emily in Paris, and you see other titles that are similar to it. That&#39;s a recommendation use case as well.&lt;/p&gt;

&lt;p&gt;The overarching thesis for this work was, can we build a single model for both search and recommendation task? Is it possible? Do we need different bespoke models, or can we build one model? The answer is, yes, we can build one model, we don&#39;t need different models, because both of them, the search and recommendation task, are two sides of the same coin. They are ultimately ranking tasks, and ultimately, we want, for a given context, top-k results that are relevant to the context. What really changes is part of this example: how we went about identifying what are the differences between search and recommendation tasks, and how we built one model.&lt;/p&gt;

&lt;p&gt;What are the differences between search and recommendation tasks, typically? The most important difference is the context itself. When you think about search in the context, we think about query. We type a query, we see results. Query is in the context. Whereas when we think about recommendation, we usually think about the person, it&#39;s usually personalized, so profile ID. Similarly, for more like this, or video-video, or title-title recommendation, the context is the title itself. You are looking at Emily in Paris, you want to see similar shows to Emily in Paris. The next big difference is the data itself, which is a manifestation of the product. They&#39;re in different parts of the product. The data that is collected based on the engagement are different.&lt;/p&gt;

&lt;p&gt;For example, when you go to search, you type a query, you see the results, you engage with the result, you start watching it, you start purchasing it. Versus when you go on homepage, you are seeing the recommendation, which is a laid-back recommendation, then you engage on it. The data, how it&#39;s being logged, and what the engagement the user is, is different. Similarly, the third difference would be candidate set retrieval itself, where for query, you might want to make sure that there is lexical relevance. For personalization, a purely recommendation task, the candidate set itself, the first pass, could be different. Ultimately, to the previous question, there is usually canvas specific or product specific business logic that puts guardrails on what is allowed on that part of the product. What we do is first identify what are these differences, and then we set a goal to set out to combine these differences.&lt;/p&gt;

&lt;p&gt;Overall, the goal is to develop a single contextual recommender system that can serve all search and recommendation canvases. Not two different models, five different models, just one model that is aware of all these different contexts. What are the benefits? I think the first benefit is these different tasks learn from each other. When you go on Netflix, if you&#39;re typing, Stranger Things, the result that you see versus on more like this, when you click on Stranger Things, the recommendations that you see, what we see from our members is they don&#39;t want different results for the same context on different parts of the canvas. Or, do they? We want the model to learn this information. We want to leverage these different tasks for benefiting the other tasks.&lt;/p&gt;

&lt;p&gt;Then, innovation applied to one task can be immediately scaled to other tasks. The most important benefit is, instead of five models, now we have to maintain one model. It&#39;s overall much reduced tech debt and much lower maintenance cost. Overall, engineering cost reduces, PagerDuty, like overall, on-calls become easier because instead of debugging five models and their issues, you&#39;re debugging one model. It&#39;s an overall pretty big win-win.&lt;/p&gt;

&lt;p&gt;How we go about doing it? Essentially, we unify the differences. The first important difference was context. We, instead of having a small context, training one model, gathering data and feature for the small context, we expand the context. Then we do the same things, gathering data, features for the whole context. Instead of just having query or just profile ideas context, we build a model that has this large context, query, country, entity in the context of Netflix&#39;s video ID, and a task type. Task type is telling the model that this is a search task, this is a more like this task, this is a pre-query task, and so on. In a way, we are injecting this information in the data while giving all the information this particular task needs as one dataset. Then, in this particular case, in the context of Netflix, entity here refers to beyond just video, we have out-of-catalog videos.&lt;/p&gt;

&lt;p&gt;For example, we often get queries like Game of Thrones, and we have to tell our users, we don&#39;t have Game of Thrones. First, to tell our users, we need to identify what is Game of Thrones. It is an out-of-catalog entity. Similarly, person, like people search Tom Cruise. We need to understand, what is Tom Cruise? It&#39;s not a title. It&#39;s person. Similarly, genre, and so on. An example of context for a specific task would be, for search, the context is query, country, language, and task is search. For title-title recommendation, the context is source video ID. In our example, Emily in Paris, the ID of it. Then country, language, and the task is, title-title recommendation. They&#39;re different tasks. Then the data, which is what we have logged in different parts of the product, we merge them all together while adding this context, this task type, as a part of the data collection. Ultimately, we know which engagement is coming from which task or which part of the product that is associated with which task, but we let the model learn those tradeoffs.&lt;/p&gt;

&lt;p&gt;Finally, the target itself, whether it&#39;s a video ranker, whether it&#39;s an entity ranker, like now on Netflix, we also have games, whether it&#39;s a game ranker, so we unify that as well and make the model rank the same entity for all these different tasks.&lt;/p&gt;

&lt;p&gt;Here&#39;s a setup. We basically build a multi-task learning model, but multi-task via model sharing. Actually, I&#39;m not sure here if people have built multi-task learning model. Typically, we would have different parts of the objective. Let&#39;s say an example would be, train a model to learn play, thumbs up, and click. There are three parts of the objective, and we are asking the model to learn all the three objectives and learn the tradeoff between these objectives. Whereas in our case, we did the multi-task through data, where we mix all the data, with the context and with the task type tagged to the data, and we&#39;re asking the model to learn the tradeoff between these different tasks from the data itself without explicitly calling out the objectives. Similar to the previous example use case of recommendation that I showed, here also there are different types of features, the big one being the entity features, which is basically the video features or the game features.&lt;/p&gt;

&lt;p&gt;Then now a big difference compared to traditional recommendation or search system is, here we have context features, which is much larger. We have query-based features. We have profile features. We have video ID features. We have task-specific features as well. Because the context is so broad, this information has to be expanded. Then we have context and entity features. All these different types, when it&#39;s numeric feature, it gets feed into the model in a different way, versus if it&#39;s a categorical feature, we have the corresponding embeddings in the model. Then, ultimately, the model is a similar architecture to the previous one, which is a large deep learning model with a lot of residual connection, and some sequence features, and so on. Ultimately, the target or the objective of this model is to learn probability of positive engagement for a given profile and context, and title, because we are ultimately ranking the titles.&lt;/p&gt;

&lt;p&gt;Let&#39;s take an example. This same model, when a user comes to Netflix and types a query, P-A-R-I, the same model takes that context query and does create all these features and ultimately generates the likelihood of all the videos that are relevant to this query, P-A-R-I. Then the same model, when it&#39;s used on more like this canvas, when a user clicks on Emily in Paris, it generates all these features for the context 12345. Let&#39;s say that&#39;s the ID of Emily in Paris, and generates the likelihood of all the titles in our catalog that are similar to Emily in Paris. That&#39;s the power of unifying this whole model where even though the product itself are in different parts of canvases of the product, we are just using the same infra, same ML model to make inference and ultimately generate rank list of given tasks for a given context.&lt;/p&gt;

&lt;p&gt;How is this magic happening? Here are some of the hypotheses based on a lot of ablation studies that we have done. I think the key benefit of an effort like this is each task benefits from each other, each of the auxiliary tasks. In this case, search is one of the tasks is benefiting for all these different recommendation tasks. This model replaced four different ML models. We were able to sunset and deprecate four different ML models and replace it with one model. Clearly, there was benefit from one task to another task. The task type as a context was very important. Then the feature specific to these different tasks, was allowing the model to learn tradeoffs between these different tasks. Another key important thing is, how do we handle these different contexts and missingness of these different contexts? We took like an approach of imputing the missing context.&lt;/p&gt;

&lt;p&gt;For example, in the context of more like this, we don&#39;t really have a query, but we can think of some heuristic and impute query. Also, things like feature crossing, which is a specific ML architecture consideration helped. Then, with this unification, we were able to achieve either a lift or parity in performance for different tasks. As a first step, we wanted to just at least be able to replace the four different models and not take a hit in performance. Then, once we were able to do that, we brought in all sorts of innovation, which was immediately applicable to four different parts of the product rather than one place. Here&#39;s an example where, ultimately, we replace initially pre-query search and more like this canvas with this one model. Then we also brought in personalization in it. This is a traditional UniCoRn model. Then we took a user foundation model that is trained separately and merged with this UniCoRn model.&lt;/p&gt;

&lt;p&gt;Then, ultimately, immediately we were able to bring personalization to pre-query, to search, and more like this. In the previous world where we had three different models for three different tasks, we would have to bring in these similar features to three different models. Instead of taking three quarters, we ended up doing it in one quarter. Again, there is a recent paper on this work in RecSys. Feel free to take a look. Offline, we got an improvement of 7% and 10% lift on search and recommendation tasks by combining these. That makes the point that these different tasks are benefiting from each other.&lt;/p&gt;

&lt;p&gt;This is a redundant slide, just the way I showed you before that we are able to merge in a personalization signal, a separate TF graph into the UniCoRn model to bring personalization to all canvases. Here is an example. After we deployed UniCoRn in production and then we deployed the personalized version of UniCoRn, I usually don&#39;t see this on my profile, so I clicked on s as a query, and I don&#39;t see kids show. Before personalization, I was getting some kids show here, Simon, Sahara. Then, after the personalization model was pushed, all those kids show disappeared and then these were very relevant personalized titles for me for the very broad query, s. Go give it a try because currently Netflix production search more like this and entity suggestion is being powered by these models, this specific model, UniCoRn.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Considerations&lt;/h2&gt;

&lt;p&gt;What were the considerations? In addition to the other infra considerations that I shared in my previous use case, here, because we are merging search and recommendation, a very big consideration is how we make sure the tradeoff between personalization and relevance. What does relevance mean here? Relevance to the context. If you type s and on Netflix you see a lot of titles that starts with a, I think you&#39;ll find it a pretty bad experience. If you&#39;re typing s, you would expect things to have s, so that&#39;s lexical relevance. Similarly, if you&#39;re typing a genre of romance and you start seeing a lot of action movies on the results, it will be irrelevant. Even though it might be very personally relevant to you, but in the context of the query, it&#39;s irrelevant. We want to make sure that we trade off between personalization and relevance pretty well.&lt;/p&gt;

&lt;p&gt;Then, because query is real-time, all these engagements are real-time, we want to make sure that our model is really good, but it&#39;s not hurting latency. We don&#39;t want our member to wait around after typing a query, for 5 minutes. In fact, the latency consideration is very strict, like something around 40 milliseconds to 100 milliseconds, P50. Similarly, depending on the region the app Netflix has been opened, throughput becomes important. Handling missing context is important for this particular case because we are expanding the context quite a lot. Features specific to the context, and ultimately, what kind of task-specific retrieval logic we have becomes important. In this case, one thing to note is we just unify the second pass ranker and not the first pass ranker. The retrieval or the retrieval logic remains different for different tasks.&lt;/p&gt;

&lt;p&gt;Some additional consideration. In general, when you&#39;re building ranking systems, in addition to everything I showed, there are things like negative sampling. What should be the negative sampling? Should you look at random negative sampling or should you look at impressions as negative sampling? Overall sample weighting, is one action more important than another action? Then, a very important thing is cost of productization. Even though it&#39;s a winning experience during A/B test, we might not be able to productize because it&#39;s too expensive. We ended up training it on too many GPUs that the company cannot support. Multi-GPU training, even for inference if there are GPUs used, cost of productization becomes a very critical thing to be considered. Then, ultimately, during online A/B testing, what kind of metrics to look at. How do we analyze and tell a story of what really is happening? Debugging from what the members are really liking in an experience becomes very important.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Key Takeaways&lt;/h2&gt;

&lt;p&gt;Overall, it&#39;s beneficial to identify common components among production ranking systems, because then we can really unify those differences and reduce tech debt, improve efficiency, and have less on-call issues. A single model aware of diverse contexts can perform and improve both search and recommendation tasks. The key advantages in consolidating these tech stacks and model, is that these different tasks can benefit from one another. It reduces tech debt, and higher innovation velocity. Real-time in-session signals are important to capture member short-term intent, while we want to be sure of also trading off with long-term interest.&lt;/p&gt;

&lt;p&gt;Overall, infrastructural considerations are equally important as the model itself. I know, saying machine learning, modeling is really the cool or the sexy part, but in real production model, infrastructure becomes even more important. Oftentimes, I&#39;ve seen that being the bottleneck rather than the model itself. Latency, throughput, training, and inference time efficiency, all these are super critical considerations when we build something for at-scale production real-time member.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 3: You mentioned a lot about how having one model can benefit the search and recommendation system. Beside the fact that there&#39;s consideration to take into training and CPU, what were the real drawbacks having to condense four different models into just one?&lt;/p&gt;

&lt;p&gt;Bhattacharya: The first point here is the biggest drawback or something to keep in mind. We spend a bunch of time trying to ensure that personalization is not overpowering relevance, because recommendation tasks typically over-anchors our personalization, where a search task is more relevance-oriented. How we merged in this particular context here, the same picture, the left-hand side is bringing personalization and the right-hand side is the relevance server. How we merge these two is very important. Because if we do some of these merges very early on, it could hurt relevance. If you don&#39;t merge it with the right architectural consideration, then it could not bring in the personalization for the relevant queries. Going back to this, I think the first one is the personalization-relevance tradeoff, which is a difficult thing to achieve and you have to do a bunch of experimentation.&lt;/p&gt;

&lt;p&gt;Then, in general, bigger model helps, but bigger model come with higher latency. How do we do that? We have a few tricks that we used to address latency, which I cannot share because we haven&#39;t publicly written that in the paper. Latency becomes a big consideration and can be one of the blockers to be able to combine.&lt;/p&gt;

&lt;p&gt;Participant 4: In terms of unifying the models between search and recommender system, the number of entities in context of Netflix is limited to a number of genres, then movie titles, and the person. Let&#39;s say if it was something like X, social media platform, where the entities would be of unlimited number, will the approach of unifying those models still scale in terms of those kind of applications?&lt;/p&gt;

&lt;p&gt;Bhattacharya: I think that&#39;s where this disclaimer, that this is a second pass ranker unification. Prior to Netflix, I was in Etsy, which is an e-commerce platform where the catalog size was much bigger than Netflix catalog size. We usually do first pass ranking and then second pass ranking. This unification is a second pass ranker. I believe this would scale to any other product application. As long as we have first pass ranking, which retrieves the right set of candidate and has high recall, then usually the second pass ranking, the candidate set size is much smaller. To also be able to unify the first pass or the retrieval phase, actually, there are a few papers now with generative retrieval, but this world did not focus on that.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/recommender-search-ranking/</link><guid isPermaLink="false">infoq-presentations/recommender-search-ranking</guid><pubDate>Sun, 16 Mar 2025 16:00:00 GMT</pubDate><author>Moumita Bhattacharya</author><enclosure url="https://res.infoq.com/presentations/recommender-search-ranking/en/card_header_image/moumita-bhattacharya-twitter-card-1741256493367.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-nov-searchrankingsys.mp4" type="video/mp4"></enclosure><itunes:duration>48:46</itunes:duration><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>QCon San Francisco 2024</category><category>.NET Core</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>Artificial Intelligence</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>Java9</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>Dare Mighty Things: What NASA&#39;s Endeavors Teach Us about the Power of Calculated RISCs</title><description>&lt;figure&gt;&lt;img alt=&quot;Dare Mighty Things: What NASA&#39;s Endeavors Teach Us about the Power of Calculated RISCs&quot; src=&quot;https://res.infoq.com/presentations/nasa-calculated-risks/en/card_header_image/khawaja-shams-twitter-card-1740573020029.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-nov-calculatedriscs1.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-nov-calculatedriscs1.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-nov-calculatedriscs1.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Khawaja Shams explores the essence of calculated risks, discussing why these risks are worth taking, how to identify and mitigate potential downsides, and the characteristics of good risks.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Khawaja Shams is a passionate QCon advocate. He earned the NASA Early Career Medal for his contributions to the Mars Rovers, from the cameras to the image processing pipeline. At Amazon Web Services, he ran DynamoDB and was subsequently the VP of Engineering for AWS Elemeal. He is the CEO and a co-founder of Momento.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon San Francisco empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Shams: &quot;The International Space Station represents a groundbreaking collaboration between five space agencies, transcending geopolitical boundaries to extend scientific knowledge. Centuries later, images recaptured from the ISS showcases the Earth&#39;s splendor in ways unimaginable in this era. The vibrant colors, the delicate atmosphere, and the brilliant reflection of sunlight affirm Galileo&#39;s assertion that the Earth, seen from space, is indeed more splendid than the moon. Defying the powerful institutions of his day, he defended the heliocentric model despite fierce opposition from the church and traditional scholars. Even when placed under house arrest for his beliefs, Galileo continued to publish his work. Had he stopped and conformed to the pressure of his time, he might have been forgotten.&lt;/p&gt;

&lt;p&gt;Instead, his defiance paved the way for the scientific method as we know it today. His courage to pursue observation, experimentation, and evidence-based reasoning revolutionized how we approach science and inquiry. Inspiring generations of thinkers to take risks, push boundaries, and to redefine what is possible. To be more like Galileo today, we must embrace boldness and take risks when faced with challenges. Often, the greater risk is inaction, staying comfortable with the status quo, rather than pushing for change. Galileo understood the pursuit of truth sometimes requires stepping into uncertainty, even at great personal cost. In a world that demands innovation, we must dare to question assumptions, challenge conventional wisdom by taking calculated risks, as Galileo did.&quot;&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Scope (Fear and Risk-Taking)&lt;/h2&gt;

&lt;p&gt;I&#39;m going to talk to you about the power of calculated RISCs. The word RISC is not spelled wrong. It&#39;s a play on words that will make sense in a little bit. Let&#39;s get started with talking a little bit more about risks. Each of you took a risk today. You chose to leave your daily jobs to come to this conference. You risked coming back to a huge backlog on your mailboxes, your Slacks, meetings, emails. While you&#39;re here at QCon San Francisco, some decisions get made while you&#39;re at QCon.&lt;/p&gt;

&lt;p&gt;You&#39;re not going to be able to champion necessarily for the issues that you really wanted to champion for in debates that may be happening right now. Before you look to the door asking yourself how awkward it would be if you were to just exit right now, let me at least make a case to you why the FOMO is real. The FOMO is worth it, but the upside is better. You&#39;re surrounded here by other bold risk takers who also abandoned their jobs to be here with you, to learn from each other, to network, and to make new connections that may make a lasting impact on your career, and perhaps beyond your career, too.&lt;/p&gt;

&lt;p&gt;I started coming to QCon 15 years ago as a young engineer at NASA. It was incredibly hard for me to step out of my daily routine. The FOMO consumed me. It is here that I learned the value of networking, disconfirming my beliefs with new ideas. It is here that I learned to put aside my fears and present what I had been working on. It&#39;s scary. As a volunteer organizer at QCon for a few of the conferences, I learned to overcome my fear of sending emails to complete strangers who are renowned experts in their fields to ask them to come speak at QCon for free.&lt;/p&gt;

&lt;p&gt;I got so many rejections, usually by people just not responding. I got so many rejections that I became numb to it. I got so many unexpected accepts that I realized that it was ok to just ask. Fear is the primary reason we often avoid taking risks, but fear is often irrational. Had I not given up the week to come to QCon 15 years ago, my career might have been very different. I would not have had the confidence to go ask people, go ask strangers for things that I would normally be too shy to do so.&lt;/p&gt;

&lt;p&gt;We&#39;re going to discuss overcoming fear to take calculated risks. I&#39;ll share a framework for risk-taking with examples from history, from space, and industry. I hope that you walk out of here inspired to make a few bold moves with a framework on how to make the most out of your newfound love of risk-taking. Before you go decide which risk to take, it is incredibly crucial to make sure that the risk is worth taking. Is your risk grounded by passion? Risks without passion is like setting sail without a destination. The best risks, or the risks with the best outcome, often take years to manifest and to materialize. You will encounter countless challenges along the way. Make sure, when you&#39;re thinking of a bold move, that you are passionate and you know what the upside is.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;A Risky Endeavor (The Mars Rover)&lt;/h2&gt;

&lt;p&gt;Let me tell you a little bit about a risky endeavor. This is a journey of 140 million miles. The rover, which is a size of a Mini Cooper, is just entering the Martian atmosphere. The heat shield does its best to protect the rover from temperatures exceeding 1600 degrees. The parachutes deploy, but then we realize that the Martian atmosphere is 100 times thinner than the Earth&#39;s atmosphere, so they don&#39;t help that much. Like any mad scientist&#39;s wildest dream, we deploy jetpacks. After traveling 140 million miles, this rover is to land on its wheels on Mars. The sky crane flies away. This is no ordinary rover. It&#39;s got a rocker-bogie suspension so that when it&#39;s traversing the challenging terrain of Mars, the body of the rover can stay leveled. It&#39;s got lasers. It&#39;s got an arm that you can use to acquire samples and do science on board. This is how we explore Mars today.&lt;/p&gt;

&lt;p&gt;This little rover, this landed in 2012. Perhaps one of the most profound inventions that this rover did was popularize the selfie. Selfies were not as common in 2012 as they are today. This little rover is obsessed with itself. If you think this is one selfie, it&#39;s not. This is not an artist rendering. This is a collection of photos that the rover took just by covering every inch of its existence. This is dozens of images that have been stitched together. It&#39;s fulfilling to know that the image processing pipeline I built to process images is being put to good use. Why is it there taking selfies? It gets a little bored. It&#39;s lonely on Mars. You got to have something to do with your recreational time. The view is nice, but it gets tiring, so it takes selfies and sends it back.&lt;/p&gt;

&lt;p&gt;Metrics really matter when you&#39;re taking risks. This is a really important metric, 299,792,458. Any guesses as to what that is? Speed of light, that&#39;s right. Mars is anywhere between 54-and-a-half to 400 billion meters away, depending on where the planets are in their orbit. When Earth and Mars are on the opposite side of the sun, it&#39;s going to be 400 billion meters away. For any high bandwidth data transfer use cases, data gets communicated and relayed via a relay network of about five satellites that are orbiting Mars to send the data over to the rover.&lt;/p&gt;

&lt;p&gt;If you take the 400 billion meters, the farthest that the planets get, and you divide it by the speed of light, pure coincidence, it&#39;s 1337 seconds. A more socially versed friend told me that this is how you spell leet with numbers. The point here is that even if you really wanted to, you can&#39;t operate the Mars rover with a joystick. You have to send it commands, and that fulfill is whole day. It gets a set of commands. It tries to do its best, and then it reports back. Then you lather, rinse, repeat every sol. A sol is a Martian day, 37 minutes longer than the Earth day.&lt;/p&gt;

&lt;p&gt;What were the risks here? There are a few risks that were taken in order to pull this mission off. A really surprising risk is that the flight software and the driving software for this rover wasn&#39;t actually done when it was launched. It takes nine months to get to Mars. While the rover is in its cruising altitude, which means different things here than it does on the plane right here, while it&#39;s in its cruising altitude, or while it&#39;s in cruise, scientists were busy writing up the software to make sure that the rover can operate itself safely. Bugs were being found, they were getting fixed. We were preparing a package to upload to it so it can actually land properly and do science in a safe manner. Inaction is often a really bad move when risks come.&lt;/p&gt;

&lt;p&gt;You can wait till everything is done, but you&#39;ll never actually ever be done with software. There&#39;s always going to be more bugs. The problem is that if you wait too long, you miss the launch window, and you have to wait two years. Some of this is just necessity. Oftentimes, you will find yourselves in the midst of a bold bet or a risk that&#39;s already been decided. You have no choice. You&#39;re in the middle of the freeway. You got to go across. You have to make the most of it. It&#39;s really important in this framework to understand the difference between productive and unproductive stress that comes with risks. If you find yourself in a situation, it&#39;s easy to just spend time commiserating on how terrible the idea is, or how screwed we all really are. It&#39;s cathartic. It feels like it might help your emotional state, but it doesn&#39;t.&lt;/p&gt;

&lt;p&gt;What really helps is being action oriented. What can we do to make the most out of the situation that we&#39;re in? The other part of this particular rover&#39;s journey was testing in prod. This is as YOLO as it gets. You can&#39;t simulate the Martian atmosphere on Earth. The jetpack would react very differently on Earth than it does on Mars. The parachute will react very differently. There&#39;s a whole lot of deploy straight to production. We do this all the time on Earth. I don&#39;t care how beautiful your staging environment is, you will always be learning lessons in prod. Everything is a simulation. You learn lessons in prod, except here, there&#39;s no rollback, there&#39;s no canaries, no pagers. Just one shot to bet it all.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Framework 1: Think Bigger&lt;/h2&gt;

&lt;p&gt;The first advice I have, or the framework, there&#39;s three parts of it, is oftentimes when we are considering a particular bold bet to make, it is incredibly easy to bikeshed on, or to get fixated on the downsides. That&#39;s where most of us start. When you&#39;re about to take on a new job, when you&#39;re about to start a business, it&#39;s easy to fixate on what could go wrong. That is productive, but only with a balance of a focus on what can go right. When I started in the entrepreneurial world, I started going to VC pitches, as an industry person to listen in. I thought that my job in these pitches was to point out all of the problems that the entrepreneurs had in their business plan.&lt;/p&gt;

&lt;p&gt;I realized that I was the odd person out in these VC pitch sessions, because while I was there, looking at how there are 99 ways in which the particular company being pitched can fail, the really seasoned investors were looking at, what is the one thing that can go right? It&#39;s a very different framework. It&#39;s a hard framework for me as an engineer to employ, because as engineers, we&#39;re trained to look for things that can go wrong, to look around the corner, to have that paranoia. When it comes to making bold moves, make sure that you&#39;re starting and not thinking about what can go wrong, your starting point has to be what can go right. How do I make that bigger? How do I double down on the largest possible outcome? If you can&#39;t answer that, then the risk may not be worth taking.&lt;/p&gt;

&lt;p&gt;In terms of reckless entrepreneurship, there&#39;s an entrepreneur that mentioned that I believe we are the best place in the world to fail, and we have plenty of practice. Failure and invention are inseparable twins. Any guesses to who said this? Jeff Bezos. Amazon has had its own set of failures, and we&#39;ll talk more about some of them. When you are making bold risks, you&#39;re going to fail. It&#39;s ok. Even if you do everything right, somebody is going to sit there and point the things that you did wrong. After the Curiosity rover landed, this is CBS News, space, and they had this awesome article that said, slow but rugged. It basically pointed out that the microchip controller that&#39;s operating the Mars rover has less horsepower than a typical cell phone. That&#39;s frustrating to read, but it&#39;s accurate.&lt;/p&gt;

&lt;p&gt;That rover that you saw, everything inside of it, the entry, descent, landing, the autonomous driving, the path planning, everything gets done on this incredible RAD750 processor. It&#39;s a beast, 200 megahertz, 256 megabytes of RAM, 400 million floating point operations per second. That last one, just to put that in context, the iPhones are measured in teraflops, this one is in megaflops. It&#39;s just got this one really cool feature, though, which is that it is radiation hardened. The scientists and the software developers at NASA JPL have to work probably 10 times as hard to make the rover do things on this particular piece of hardware that also runs a real-time operating system. It&#39;s not running your typical Linux. You&#39;re not running TensorFlow on it. It&#39;s semi arcane.&lt;/p&gt;

&lt;p&gt;Eight years later, when the Perseverance rover was being built, the scientists really wanted to land it at a site that was a lot more challenging. They found Jezero Crater. There&#39;s a lot of craters on Mars, but this particular crater was very interesting because it had a very clear delta, like water flowing. It had an outlet. It had an inlet. There&#39;s a very good chance that this crater contained water at some point in time. The terrain around it was incredibly challenging. The Curiosity rover had a 20-kilometer accuracy in where it would land compared to where we wanted it to land.&lt;/p&gt;

&lt;p&gt;For Perseverance, because of the challenging terrain, the accuracy range had to be within 40 meters. That&#39;s 500 times smaller radius that the rover has to land in. The engineers at NASA worked on ways to modulate the rover and to improve the algorithms to do entry, descent, and landing with a higher accuracy. They invented what&#39;s called terrain relative navigation. Terrain relative navigation basically is evaluating the visual features that are on the ground and then using that to steer the rover in the right direction, much like your self-driving cars would do, except there&#39;s no GPS and the terrain is a little different, and a lot less features on Mars. Terrain relative navigation was required to pull this off. It&#39;s a lot of software that has to be written to pull this off.&lt;/p&gt;

&lt;p&gt;If anybody here really wants to be a roboticist at NASA, and interviews and find yourself at the interview question where somebody says, write terrain relative navigation for me. I&#39;ve got a cheat sheet for you. You take pictures as you&#39;re descending, you compare it to an orbital map that you have, and you go left or right based on where things are. That&#39;s essentially what terrain relative navigation is. This was something that could have really benefited from a massive compute platform. We had very quantified objectives here, 40 meters as opposed to 20 kilometers. While it&#39;s tempting to just say, let&#39;s just change the compute platform to be able to land within that 40-meter radius, it&#39;s important to internalize that the problem to be solved is not the chipset.&lt;/p&gt;

&lt;p&gt;The problem to be solved is, can we pull this off? There&#39;s a famous alleged quote from Einstein that does a better job than this one. It&#39;s not clear whether he actually said this or not. Apparently, when asked what he would do if he had 60 minutes to save the Earth by solving a problem, he would spend 59 minutes deciding what the problem is and 1 minute solving it. What he&#39;s actually said, though, is that the formulation of a problem is more essential than the solution. Sometimes it&#39;s really important in a design review to just step out and say, what is the problem we&#39;re trying to solve? It&#39;s one of the most profound questions that principal engineers ask when they see junior engineers arguing over a particular technology choice. It&#39;s really important to understand what is the problem that we&#39;re solving.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Framework 2: Derisk&lt;/h2&gt;

&lt;p&gt;The other part of this, why we didn&#39;t jump to a new platform right away, is you got to derisk your bold risks as well. The choice isn&#39;t always as simple as taking the risk or not taking it. It&#39;s about maximizing the upside and then reducing the blast radius of the downside. That&#39;s what distinguishes your ability to take bold risks or not. You got to stop thinking about it as a black and white, taking it or not taking it. This is where the Snapdragon 801 chip came into play. It&#39;s a Qualcomm chip. It&#39;s actually quite common. There aren&#39;t that many RAD750 chips, since Mac used them in the &#39;90s. That was great. You see a lot of Snapdragon 801s being developed.&lt;/p&gt;

&lt;p&gt;This particular model, if anybody here likes to use Android, if you had a Galaxy S5 in 2014, that&#39;s the chip in the Galaxy S5, and a bunch of other Android phones. Many Android phones use some variant of a Snapdragon chip. This, from a perspective that you can take, I&#39;m not saying that I take this perspective, but one perspective could be that this is a toy. This is not meant for serious space exploration on a billion-dollar rover, it&#39;s for a $1,500 phone.&lt;/p&gt;

&lt;p&gt;Even if you don&#39;t take that uncharitable of a perspective, it is hard to imagine a consumer grade chip surviving the thermal cycles, the vibration at the launch, the radiation on Mars. We go back and look at the upside. Chip is incredible. It runs Linux, as opposed to a real-time operating system. Don&#39;t get fixated on the processing power, gigaflop numbers, those are like miles per gallon on the car that you might buy. Real-world application really matters. If you need more evidence of that, just see how fast I can drive a race car on the track versus an actually good driver. Application matters a lot more than what the device can do.&lt;/p&gt;

&lt;p&gt;Because the device has Linux, because scientists can run TensorFlow on it, they can run PyTorch on it, the productivity goes up meaningfully as well, and the gains that we get are meaningfully larger than the 100x gains that are over there. Power draw, that&#39;s another thing. In a spacecraft, you don&#39;t have an infinite supply of power that you do in an autonomous car or on Earth. This thing draws a couple of watts of power, like 2 to 5 watts of power. It&#39;s 10% of what the RAD750 requires. It weighs a tenth of the RAD750. It&#39;s good on every dimension except radiation hardening.&lt;/p&gt;

&lt;p&gt;This chip, it could change the world in terms of how we explore. Can we risk this multibillion-dollar rover? This particular rover, Perseverance, has a better arm, better instruments, better cameras, better sample acquisitions, but if the main brain of the system fails, all of that goes away. How can we derisk it? A lot can go wrong, but what can go right? What is something that we could do that we could never do before? Putting Snapdragon as the main processor was too risky, and it was a one-way door. A last-minute bug could delay the launch. Hardware failure could jeopardize the whole mission. What if we could amortize the risk? What if we could try it for the first time in a way that reduces blast radius?&lt;/p&gt;

&lt;p&gt;Ingenuity is a helicopter that flew on Mars. Remember, the thinner atmosphere? Helicopters elevate by generating lift as their wings propel, but if there&#39;s less air density, they got to work that much harder. That&#39;s really the easy part. I&#39;m not being charitable to the aerospace engineers there, but you make the propeller spin faster and it&#39;ll go higher. I&#39;m a software person, so I have to do this.&lt;/p&gt;

&lt;p&gt;The hard part is because of the atmosphere being the way it is, the real-time controls require a lot more precision. Little moves can really mess up the helicopter. The terrain relative navigation here is a lot harder. Terrain relative navigation when you&#39;re descending from space covering a very large area and have a reference map to work on, is a lot easier problem computationally than doing it in real time, and especially when flying across boring terrain on Mars. When I say boring, it just means there&#39;s no building, sometimes things look really the same. It makes life a lot harder.&lt;/p&gt;

&lt;p&gt;Doing this, the Ingenuity helicopter turned out to be a platform where we could turn this into a two-way door. Two-way door is a concept introduced by Jeff Bezos in a shareholder letter for Amazon in 2015, where there are some decisions that are nearly impossible or very expensive to undo, the other decisions are a lot easier. As companies get larger, oftentimes, we end up spending the same amount of time debating one-way doors as we debate the two-way doors. This is hard. If you take a mathematical perspective on this, it means that the one-way door decisions are getting the same amount of time as a two-way door decision.&lt;/p&gt;

&lt;p&gt;The one-way door decisions are getting a lot less time than they should, and the two-way door decisions, things that are simpler are being bikeshed upon and there&#39;s indecisiveness. The job of a senior leader is to not take one-way door decisions. The job of a senior leader in a company is to convert as many of your one-way door decisions into two-way door decisions. You can take a unique perspective. You can reduce the blast radius. Reducing blast radius is a really good way to transform a one-way door decision into a two-way door decision.&lt;/p&gt;

&lt;p&gt;That&#39;s exactly what we did with Ingenuity. Ingenuity was a huge success. The target was very modest. We wanted to see if we can fly it five times. This is not artist rendering, this is actual photos. This is it filming its shadows. This video is a little dated. It was made to commemorate Ingenuity&#39;s 50th flight. It eventually completed a total of 72 flights. It went as high as 79 feet, 128 minutes in flight. It explored for 11 miles. Ingenuity can&#39;t fly anymore, and it&#39;s not because the chip got stuck with a cosmic ray, it had a bad landing, and the propellers were damaged. It&#39;s far exceeded it duty cycle. It&#39;s inspiring and paving the way for future missions.&lt;/p&gt;

&lt;p&gt;There&#39;s another concept that&#39;s being talked about right now, which is a hexacopter. It&#39;s got six wings to not only go explore, but also carry samples around Mars. There&#39;s also Dragonfly. This is a mission that is funded to go to Titan, a moon of Saturn. Titan is a very different place than Mars, with its own unique challenges. This is a fully contained mission. This is not a little helicopter that sits on the mother ship. It&#39;s got its own RTG. It&#39;s got its own scientific capabilities where it can acquire samples, analyze them, fly to the next spot, and do so again.&lt;/p&gt;

&lt;p&gt;Let&#39;s bring this down to be a little more terrestrial. The ability to transform one-way door decisions into two-way doors is not unique to Amazon or to NASA. We see this with Apple. In 2020, Apple released the M1 processor for the MacBooks. Some of us, especially developers, might remember, this was a little scary to transition over from Intel to the ARM chip. Was it worth it? I got one of those, and I was so happy with the battery power. What did Apple do? They launched three of their products. It was the MacBook Air, MacBook Pro, and Mac Mini, while they simultaneously continued to carry the Intel processors. This was a way to reduce blast radius.&lt;/p&gt;

&lt;p&gt;That story didn&#39;t start in 2020, it actually started in 2010 when the iPhone 4 was launched. This was the first time Apple put in its own silicon on a device. Before that, it was Samsung silicon. This is with the launch of the iPad. Today, the M4 chip that is on the latest MacBook Pro is also available in your iPad Pros, which is basically giving Apple a much higher leverage in terms of the innovations that they do on that particular platform for multiple devices. This is all just phased deployment. Anybody who&#39;s an SRE, who&#39;s doing continuous deployment, this is just phased deployment.&lt;/p&gt;

&lt;p&gt;If you are deploying to one AZ at a time and two availability zones after that, and then spreading across multiple regions slowly and being ready to roll back, it&#39;s essentially that, but in industry. When you&#39;re doing this, when you&#39;re taking these bold bets, you got to inspire your team. Oftentimes, it&#39;s not taking the risk. It&#39;s about making sure that everybody in the team knows what the upside is.&lt;/p&gt;

&lt;p&gt;Now, armed with the success of what the Snapdragon chipset has allowed JPL to do, we&#39;re seeing more teams pop up, and more scientists popping up and saying, what can we do with more advanced capabilities? This is the Nancy Roman Space Telescope. We&#39;re experimenting on doing high-order wavefront sensing. This particular telescope, you&#39;ve heard of Hubble or James Webb, this is the next version of that. Basically, it&#39;s got optics. Optics have aberrations. HOFS just simply deals for those aberrations, so that you can denoise the light that is coming for stars and focus on dark energy and dark matter and really learn the secrets of the cosmos.&lt;/p&gt;

&lt;p&gt;A lot of those algorithms are now being ported over to the Snapdragon as well. Synthetic Aperture Radar, this is another really cool instrument, where this is technology from space or from an airplane, it can detect the topology of the terrain that it&#39;s over. What scientists typically do is they study how the topology changes across multiple passes of the orbit. It can see through clouds. It works at night. It has some really profound purposes, like being able to assess damage after an earthquake. It was used for that in previous earthquakes as well. This is all happening across JPL. Lots of new applications are showing up across radar, signal processing, telecom, autonomy.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Framework 3: Be Wrong, a Lot&lt;/h2&gt;

&lt;p&gt;I&#39;ll leave you with the third and final piece of a risk-taking framework, is to be wrong a lot. If you&#39;re good at course correcting, then being wrong may be less costly than you think. Imagine if you&#39;re at a bar and you&#39;re shooting darts, and the game is to see who can get closest to the center. You can spend a lot of time aiming and doing the whole ready, aim, fire thing, or you can do ready, fire, aim, and if you shoot the dart fast enough, you can get a second shot, and you can get more shots than your opponent. That&#39;s often true. In a lot of decisions that you make, indecision is probably way worse than not taking a risk at all. What you&#39;ll have to do is, instead of waiting for more data to arrive, you have to just get better at course correcting.&lt;/p&gt;

&lt;p&gt;Just like at the bar, if you&#39;re just shooting a lot, you&#39;re going to get better with practice as well. Failures are going to happen, and you have to embrace them. Sometimes you just have to lean into those failures. Everything fails all the time. What we learned from Ingenuity, perhaps we were too concerned with the processor, how the processor could fail.&lt;/p&gt;

&lt;p&gt;Primarily because of radiation, the cosmic rays that are coming on Mars, the JPL has full Markov models on how much extra radiation there is and the likelihood of a cosmic ray hitting a chip on Mars versus Earth. What if we could really lean in and just know that, this processor, which is a tenth of the weight and a fifth of the power draw is more prone to radiation driven upsets? I know something a platform engineer can teach NASA, redundancy. You can have two processors, one watching the other. That&#39;s exactly what this particular paper covers.&lt;/p&gt;

&lt;p&gt;In a platform world, this is just music to my ears, &quot;The groundbreaking scientific benefits of these modern high-performance computing platform is of course balanced with the concerns for known transient radiation upsets, as demonstrated to exist with the data provided in this paper. The answer to this concern is to develop resilient and robust architecture that adapt and repair errors while still providing functionality&quot;. What they did, is they just put two processors in this model, and one just watches the other. You can get pretty good 99.99% availability in a continuous operation, by having these two chips. You don&#39;t have to stop at two, you can actually go deeper and make quorum-based systems that are verifying each other&#39;s work as well.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Taking a risk, think bigger, derisk, and be willing to change your mind. Galileo is who we started talking about. His life was not all that glamorous. People think he invented the telescope, he didn&#39;t. He just improved it. A Dutch scientist had built a telescope that had 3x magnification, he built one that had 20x magnification. He improved the design. What he&#39;s really contributed to science is he discovered a whole bunch of moons that we didn&#39;t know existed. He discovered that the Moon&#39;s surface is not smooth, it&#39;s got craters, which was hard to believe at the time. He insisted that the Earth revolves around the sun, rather than the other way around. He had his failures.&lt;/p&gt;

&lt;p&gt;He was actually a medical student, and could not really succeed at medicine because he didn&#39;t like it. He started the trend of dropping out well before Silicon Valley did. There are failures, and it&#39;s ok. I&#39;ll share a couple of really quick failures that we don&#39;t talk much about, but we learn from them. There was the Fire Phone. Anybody remember the Mac Cube? This was I think in the &#39;90s. It looked like a toaster oven, and people thought it was broken when they got it, because it had this little hole in there to put your CD-ROM in. It was a total flop. Those are not the products that we remember Amazon and Apple by. Today, as you head off, engage. It&#39;s ok to be disconnected from work for a little bit. Meet some friends. Make some connections. Think about the choices that you can make that are not just optimizing for the next step, but your eventual goals.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/nasa-calculated-risks/</link><guid isPermaLink="false">infoq-presentations/nasa-calculated-risks</guid><pubDate>Thu, 13 Mar 2025 16:00:00 GMT</pubDate><author>Khawaja Shams</author><enclosure url="https://res.infoq.com/presentations/nasa-calculated-risks/en/card_header_image/khawaja-shams-twitter-card-1740573020029.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-nov-calculatedriscs1.mp4" type="video/mp4"></enclosure><itunes:duration>42:55</itunes:duration><category>Keynote</category><category>Architecture &amp; Design</category><category>Risk</category><category>Culture &amp; Methods</category><category>QCon San Francisco 2024</category><category>.NET Core</category><category>Risk Management</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>NASA</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>Security</category><category>Java9</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>A Walk along the Complexity-Performance Curve</title><description>&lt;figure&gt;&lt;img alt=&quot;A Walk along the Complexity-Performance Curve&quot; src=&quot;https://res.infoq.com/presentations/java-string-performance/en/card_header_image/richard-startin-twitter-card-1740132256484.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s1_20250320073842_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-apr-complexityperformance.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-apr-complexityperformance.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-apr-complexityperformance.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Richard Startin explores the relationship between complexity and performance through the lens of recent improvements to the Java String class, visiting continuous profiling and some assembly code.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Richard Startin has over ten years experience working as a software engineer, primarily on big data and observability. Richard is an avid open source contributor and blogger. Richard currently works as a Senior Software Engineer at Datadog, where he has been working on tracing and profiling of applications running on the JVM.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Startin: My talk is about the performance complexity curve. What is a performance complexity curve? It comes from the performance work phase diagram by Aleksey Shipilev. It&#39;s an observation he made on how performance and complexity change as we engage in performance work. It&#39;s quite common, before we do any kind of performance work, that we actually have a low-performance and high-complexity system. This often comes from the mantra, make it work, make it right, make it fast, in that order. What we do, in a lot of systems, at the application layer, is we&#39;ll incur technical debt to meet deadlines, maybe to get an MVP out.&lt;/p&gt;

&lt;p&gt;Then, later, people will start using the app, and we&#39;ll need to improve the performance of it. The characteristic of the first phase of the performance tuning is usually bug fixing. These are simplifying changes. Later on, if we still need to tune the system more, we might actually have to trade some complexity for performance. This is something that&#39;s always really resonated with me, because it&#39;s how I&#39;ve seen things play out many times. What we can see is, in the different phases, as we get to the most simple possible point, we get these really big gains at the beginning. If you hear about 10x gains in performance, it&#39;s probably that nobody&#39;s tried to optimize the system before.&lt;/p&gt;

&lt;p&gt;Later on, it gets much harder. We have to incur a complexity for diminishing returns as we improve the performance. I split this into three main phases. We have the low-hanging fruit, which is from the initial state to the simplest possible point we can get to. We make lots of performance gains in this stage. After that, we might get into tuning. After that, there&#39;s this competitive advantage stage, which is a stage we shouldn&#39;t really go into unless we&#39;re in real-time competition or we want to reduce costs to improve margins. Because at that point, we&#39;re taking on a lot of complexity for marginal gains. We can use it as a framework for thinking to compare problems.&lt;/p&gt;

&lt;p&gt;If we have a problem which gives us more performance for the same level of complexity, then that&#39;s a nicer problem. If we can simplify it further, then we&#39;ve got an even nicer problem, because we should usually prefer simplicity in the application there. It also informs the tools we might use. Early on, when we&#39;re doing performance optimization, we might want to use metrics. Something I work on, which is continuous profiling, can help you find bottlenecks. Later on, we might change tack and move into microbenchmarks, perf counters, instruction profiles, as we try to improve the code.&lt;/p&gt;

&lt;p&gt;Who am I? My name is Richard Startin. I&#39;m a software engineer at Datadog. I work on continuous profiling. I will talk about continuous profiling here, but we&#39;re not the only vendor. It&#39;s not a sales pitch. I&#39;m splitting the rest of the talk into three phases based on the low-hanging fruit, the tuning, and the competitive advantage stage that I&#39;ve shown you before. We&#39;ll start in the green zone, where we&#39;ll look at how we might start a performance tuning exercise, and some of the kinds of low-hanging fruit that we might encounter.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Decide on an Objective&lt;/h2&gt;

&lt;p&gt;Before you do anything, if you want to optimize, you need an objective. You need to really know what you&#39;re trying to do. It could be reduced latency, or increased throughput, or maybe reduced cost. These are activities which might actually trade off against each other. You could imagine, to improve latency, you might take some processing off the critical path onto a background thread. That might cost you some throughput if you have more context switches, and you have the concurrency control that you need to take care of. It may even increase cost. You really need to know what you&#39;re going for, what your objective is.&lt;/p&gt;

&lt;p&gt;Most of the focus of this talk will be in terms of cost reduction, because that&#39;s quite generic. The ultimate aim is to reduce hardware, so we want to focus on what&#39;s using memory or CPU, so we can use less of it. For the sake of simplicity of the talk, I&#39;m focusing on CPU bottlenecks, because this is a fairly ubiquitous problem. It applies nicely to whatever your objective is. I&#39;m going to talk about continuous profiling later, but it&#39;s a good idea to start with metrics, so you actually know that you have a CPU utilization problem before you start looking at profiles. Otherwise, going bottom-up, it can be too much information, and you&#39;re not even sure you&#39;re going to get anywhere useful.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Continuous Profiling&lt;/h2&gt;

&lt;p&gt;Something dear to my heart is continuous profiling. It&#39;s really useful if you&#39;re always running a profiler, whether you know that you have a problem to analyze or not, because when an unexpected problem comes about, then you have the profiles. You can just go and look at them, and you&#39;ll get a breakdown of what&#39;s using the resource that you&#39;re concerned with. This is only actually possible because sampling CPU profilers are very low overhead. What they do is they continuously sample threads in proportion to CPU time. The Linux kernel has tools which allow you to generate a signal whenever 10 milliseconds of CPU time or a period of CPU time has passed, and then take a sample.&lt;/p&gt;

&lt;p&gt;If you take a sample every 10 milliseconds of CPU time, and it takes less than 100 microseconds to record a sample, then you&#39;re staying under 1% CPU overhead. It&#39;s possible for a lot of applications to actually just profile, or at least do CPU profiling continuously in production. Once you&#39;ve profiled your entire fleet, you can aggregate all of the profiles together, and you can look at fleet level bottlenecks. There are numerous technical options. There&#39;s two that I know more about through my work, which is JFR and async-profiler. These are JVM profilers, which run in-process. These have their merits and their drawbacks.&lt;/p&gt;

&lt;p&gt;JFR, for example, it doesn&#39;t actually have a true CPU profiler. It&#39;s got execution sample events. They&#39;re not scheduled in response to CPU time. It doesn&#39;t actually report any of its errors. When you&#39;re taking profile samples, there can be a lot of error cases, and it&#39;s important not to destroy the statistics of the dataset by misreporting errors. Async-profiler, which is written by Andrei Pangin at AWS, solves an awful lot of problems. It&#39;s got two true CPU profilers. It uses perf events, and it has a timer-based solution. It has very rigorous error reporting.&lt;/p&gt;

&lt;p&gt;On the downside, it&#39;s using undocumented JVM internals, unsupported APIs, which in principle could be taken away at some point in the future. Those are the only two in-process JVM profilers which have low enough overhead to use in production. There&#39;s eBPF and perf, which have their own pros and cons.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Low-Hanging Fruits&lt;/h2&gt;

&lt;p&gt;In this first phase of optimization, once we&#39;ve got a profiling tool that can show us where problems are, we want to focus on low-hanging fruit. These are generally problems that are essentially bugs. I&#39;m going to start with something that I call non-functional bugs. This is code which produces the correct result, but goes about it in the wrong way, and in such a way so the error can only be detected by performance measurement. I&#39;m an Apache Pinot committer, which is a distributed OLAP database. It originated at LinkedIn. I used to work on Pinot when I was working at StarTree.&lt;/p&gt;

&lt;p&gt;At some point, I wanted to speed up our test suite, because it took quite a long time, so I profiled it. I profiled the test suite rather than Pinot in production. I noticed that the profile was dominated by this library called Apache Helix, which is used by Pinot&#39;s controller to schedule work to other participants in the Pinot cluster. Here we can see the scheduling of jobs to something called the minions, which are basically workers which do work on behalf of the rest of the cluster. The profile shows that most of these controller CPU timers is scheduling this job list, which seems a bit unusual. The problem here is quite a common one in libraries. It&#39;s an expectations mismatch on either side of the module boundaries.&lt;/p&gt;

&lt;p&gt;We can see at the top, this is on the Pinot side, it&#39;s setting a parallelism level, and it&#39;s saying, give me all you&#39;ve got, because it sets Integer_MAX_VALUE. You can&#39;t have more than that. On the other side, it&#39;s assuming that there will be many more jobs than the level of parallelism. The loop termination condition only checks for the level of parallelism, and it will just keep on looping round until it gets to Integer_MAX_VALUE. That can easily take 20 seconds of a busy spin on CPU. It&#39;s not a very difficult problem to fix, because all you have to do is check that all of the jobs have been consumed. LinkedIn upgraded the Helix library after this fix had been made.&lt;/p&gt;

&lt;p&gt;They actually published a blog post about it, which is really great, because you can really dig into the details of the outcomes of what happened here. This led to a massive reduction in processing latency for jobs by 7x. You can figure out whether you have a performance problem with things like back of the envelope calculations, where you can figure out what&#39;s reasonable for a task to take, how much resources should it take with back of the envelope calculations. It&#39;s very difficult when you have the abstraction of libraries in the way. If the libraries are doing things for you that you don&#39;t know how to do it, you don&#39;t understand the problem. Abstraction can make it hard, whereas a profiler will just show you that there&#39;s a problem, and you can go and fix it. It&#39;s normally quite easy.&lt;/p&gt;

&lt;p&gt;There&#39;s also poor programming practices. This is something that I dug out of a profile at some point. It&#39;s basically programmed by exception. To convert a Double to a Long, if the mantissa is zero, if it&#39;s an integral value. Then construct a big decimal, which is expensive, and then get the exact Long value, which will throw an exception if the input was a Double. Then catch the exception and return that. It&#39;s obvious that this can be done better by doing some arithmetic. If you compare this, it&#39;s like a 150x improvement. These are just the kinds of problems that work their way into application code when you&#39;re bootstrapping a product. Also, algorithmic issues. This is an interesting one from Datadog.&lt;/p&gt;

&lt;p&gt;We can get algorithmic issues for a number of reasons, mostly because of abstraction. You can get accidentally too high algorithmic complexity because of composition in ways that wasn&#39;t expected. Or you can have rare cases that you didn&#39;t expect to be rare. In fact, you might have a comment saying, this never happens, but this is a fallback, and you don&#39;t put too much effort into implementing the most optimal fallback. Then it turns out in production that that fallback is not so rare. This actually came up at Datadog in one of our backend processors, which processes JFR files from our customers. We used a JMC JFR parser to parse the JFR files. All of a sudden, they started timing out, but only for one customer.&lt;/p&gt;

&lt;p&gt;Fortunately, we profiled that service, which is doing the parsing, so it&#39;s like profiling inception. We quickly identified that all of the time is being spent in this method, DisjointBuilder.add. This is just a quick overview so we can understand the problem. This is trying to turn events which have a start and an end timestamp into lanes so that the intervals are disjoint in a lane. If none of the events overlap, then we only have one lane. That was expected to be the common case. If all of the events overlap, then we have lots of lanes. What the code was doing to maintain this, it was basically saying, I don&#39;t expect it ever to have more than one lane.&lt;/p&gt;

&lt;p&gt;If that ever happens, to maintain an invariant of sort order, I&#39;ll just sort the entire set of lanes over again. Because of a bug in the JDK, actually, where some tracing around file write events was recording the end timestamp rather than the duration as the duration, so we would always get events which don&#39;t overlap if you&#39;re using that file API. We&#39;d be taking these events in, and the assumption would be quite catastrophic. We&#39;d end up with a cubic time algorithm. It&#39;s very easy to fix this just by finding the insertion point. We still don&#39;t get a really optimal solution. It would be very difficult to change everything else. This was enough to fix the problem here. Just something unexpected happened in production, basically.&lt;/p&gt;

&lt;p&gt;There&#39;s also going against the grain. There are lots of existing optimizations in frameworks which target idiomatic code. If you don&#39;t write idiomatic code, then you won&#39;t profit from these optimizations. A common one is composite lookups. You have a hash map with two or more values. You could concatenate the values together and look them up in the map, or you could construct a record. If we compare the performance here, there&#39;s basically a 3x to 4x performance difference. I called it stringly-typed and typesafe because there are prefix and suffix overlap bugs in the string version, which you just don&#39;t need to think about with typesafe. You&#39;re getting better performance, but you&#39;re avoiding bugs.&lt;/p&gt;

&lt;p&gt;Ultimately, you&#39;re just writing objectively better code. Why not just find all of the bad code in your codebase with static analysis? There might be quite a lot of it. Some of it might never run. Some of it might not run very much. The benefit of having a profiler is that you know it&#39;s worth fixing if you see it in the profile because it definitely happened that way. People might have different thresholds on when to act here. Normally, these kinds of optimizations, they&#39;re just win-win.&lt;/p&gt;

&lt;p&gt;All of this assumes that the profiling data is accurate enough to act on. Profiling, you think that you&#39;re just interrupting a thread and collecting a stack trace, and that should be quite simple. It&#39;s actually not because there are so many different kinds of code running on a JVM stack. We have code running in the interpreter, which hasn&#39;t been compiled yet. Then it gets JIT compiled if it gets hot. We have generated native code, which is basically C++ code, which is generated at runtime based on the platform capabilities, which is slightly different to JIT compiled code. It&#39;s not come from bytecode in the first place.&lt;/p&gt;

&lt;p&gt;Then, finally, we have native code that we might call from JNI, and that&#39;s dangerous as well. There are complications that I won&#39;t go into now in unwinding Java stacks outside of a safepoint, because some of the code in the JVM to do this just never expected to be called outside of a safepoint. It&#39;s not always safe or possible to get a sample, and this can lead to errors. Let&#39;s compare the stringly-typed lookup profiles taken by JFR, where we get these figures, notably 27% in String#hashCode, and 8% of samples in String#getBytes. An async-profiler reports something completely different.&lt;/p&gt;

&lt;p&gt;It reports much more time in String#getBytes and a little less time in the hashCode method. This is because of this red frame here, which is a runtime stub. jbyte_disjoint_arraycopy is a runtime stub. This is code which is generated at runtime. JFR can&#39;t unwind these, because the unwinding code never expected to encounter them, because they can&#39;t be encountered outside of a safepoint. Whereas async-profiler knows about them. It pops this frame, and then it gets into the Java code where it can unwind from.&lt;/p&gt;

&lt;p&gt;If you have two profilers, you&#39;re not really sure where your bottlenecks are, because they report completely different things. They do agree on the typesafe lookup. We get some benefits here. We&#39;re not constructing a new string. We&#39;re not concatenating strings. Also, interestingly, there&#39;s next to no time in String#hashCode. JFR does agree. It&#39;s not like JFR&#39;s a hopeless case. There are specific cases where it goes wrong, and it doesn&#39;t tell you that it&#39;s gone wrong. It can agree with async-profiler.&lt;/p&gt;

&lt;p&gt;This is important for the rest of the talk, because we&#39;re going to change tack as we get into the different phases of the curve. The important thing here is that a string caches its hash code. If you write code which does things with strings, where you have to hash the strings in such a way that you work with the caching of this hash code, you don&#39;t need to compute the hash code. If you don&#39;t, then we&#39;ll be computing the hash code, and that might get to be expensive.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Tuning&lt;/h2&gt;

&lt;p&gt;We&#39;re moving into the next zone in the talk now. This is the amber zone. This is tuning. Some of the stuff I&#39;ll talk about here I don&#39;t necessarily suggest that people try. It&#39;s not necessarily helpful. In some cases, it&#39;ll make a difference. What do we do at Datadog? We ingest a lot of observability data. A lot of our backend is written in Go, but we also have a lot of Java services. They&#39;re large enough to be worth optimizing for the sake of cost, and also meeting SLAs. When you profile your entire fleet, you can do things like take all of the data, aggregate the data together, convert it into cores, and then multiply the number of cores in a unit time by the cost from your cloud provider in that unit of time.&lt;/p&gt;

&lt;p&gt;You can basically compute the cost, how much you&#39;re paying for different methods. It turns out that actually at Datadog, for some reason, we spend an awful lot of time in String#hashCode. This turned out to be the most expensive method. What we could do is we could find all of the uses of it and optimize them individually, but it&#39;s death by a thousand cuts. If we went and found them all and we optimized them in a way to do something different, there&#39;d be more of them in two weeks&#39; time. We actually have to optimize this method. We found this hot method. What can we actually do? We can stare really hard at the code, but the problem&#39;s not actually jumping out at me just looking at this. What we could do is we could relax the requirements.&lt;/p&gt;

&lt;p&gt;In the context of String#hashCode, we might want to compute the hash code in a completely different way, produce a different result. This is like moving to have a nicer problem. This is going to have a dual effect, because a lot of people in the Java community know that String#hashCode is not particularly good in terms of randomness. Why are we computing hash codes in the first place? It&#39;s because we&#39;re using hash maps, and so we don&#39;t want collisions in the hash maps. If we had a better algorithm which produced a more random result, then we could solve two problems.&lt;/p&gt;

&lt;p&gt;We&#39;d have a faster algorithm to compute, and we&#39;d have fewer collisions in the hash maps, which would have secondary benefits. The problem with changing the algorithm is it&#39;s specified in the JLS, but it&#39;s hard to believe that this is really an immovable obstacle, because it was originally specified incorrectly in the &#39;90s. The specification was reverse engineered from the implementation, but it was reverse engineered incorrectly. They changed the specification to make it correct. Then it was noticed because the hash code algorithm, the original one, it only depended on the first 16 characters of the string, which trivially produces a lot of collisions.&lt;/p&gt;

&lt;p&gt;For the sake of reducing the number of collisions, the algorithm was changed. I think this was resolved in 1999, so a long time ago now. The real problem that this can&#39;t be changed is that Java 7 introduced switch statements on strings. This is a switch expression, which is a more modern way to write Java code, but it&#39;s fundamentally very similar to a switch statement, which happened back in Java 7. If we actually look at the bytecode for how this is compiled, we&#39;ll see that the invokevirtual up there is on the String#hashCode, and then we basically have a lookup switch on the hash code values.&lt;/p&gt;

&lt;p&gt;The output of the function, as of Java 17, is hardcoded into class files. We don&#39;t know how many of those there could be in the last 13 years or whatever it&#39;s been since Java 7. To maintain backward compatibility, we just can&#39;t change the algorithm. We can&#39;t avoid solving this problem.&lt;/p&gt;

&lt;p&gt;We have to change tack. We can&#39;t really stare at the code. The line number information we might get from a profiler isn&#39;t going to be very revealing because the code is so simple. We can produce a microbenchmark with JMH. What this microbenchmark here is doing is creating random byte arrays of parameterized range of sizes. Try not to choose multiples of eight, because that&#39;s going to give the hardware an easy time. Try to choose something like seven. It&#39;s constructing a new string to make sure that the hash code doesn&#39;t get cached.&lt;/p&gt;

&lt;p&gt;If we compare this to anything to have a fair test, we&#39;re going to have to also do these same operations so we have the same baseline cost. JMH gives you all of these microprofilers. All of this was written by Aleksey Shipilev, very useful tooling, to help you explain the benchmark results. You&#39;re not just saying A is faster than B. You have things like perf counters from the perfnorm profiler, which basically normalizes the perf counters to the benchmark invocations. Then it will give you something like IPC, which is instructions per cycle. This is a good measure of how efficient the code is. It&#39;s essentially dividing the number of instructions during the benchmark run by the number of cycles per invocation.&lt;/p&gt;

&lt;p&gt;We can see we&#39;re starting at 4, and as the string gets longer, we&#39;re going down under 2, which is not particularly good. There&#39;s another profiler for explaining benchmark results in a different way, which is the perfasm profiler. That works by loading hsdis, which is the HotSpot Disassembler, putting that in a location where the JVM can load it. Then that will allow the JVM to decompile code blobs wherever they land. You can build that with binutils or Capstone or whatever disassembler you like. Then it goes further than that, and it links the program counters, which is what perf samples with the JIT compiled code blobs.&lt;/p&gt;

&lt;p&gt;It can produce a histogram like this, which is very useful because you can check whether what you think should be running is running. Here we can see that 80% is in the benchmark stub. A total of 8% is in constructing strings and copying the strings, which we know is coming from the benchmark stub. Then there&#39;s another interesting thing, which is, in my system, I haven&#39;t set up debug symbols for the kernel. It&#39;s saying that 9% of the time is spent in the kernel, and we don&#39;t know where. That&#39;s a really important thing to check before you interpret the data you get from here. It produces this output. It&#39;s worth getting to understand this before going any further, because it&#39;s important for understanding the rest of the talk.&lt;/p&gt;

&lt;p&gt;On the left-hand side, you have percentages. The scary-looking addresses in the middle are program counters. The percentages are how many times of all of the program counter samples that program counter was sampled by perf. Then we have the instructions on the right-hand side. We have an instruction profile. It&#39;s very hard to interpret the percentages on the left-hand side, because there are lots of confounding factors. There are things like skid, where a few instructions late gets blamed, so you have the wrong suspect. We have things like pipelining, but there&#39;s only one instruction pointer. If you have multiple instructions in flight, which one do you blame? It does give you very useful output, which can help you to understand problems.&lt;/p&gt;

&lt;p&gt;This is the output for String#hashCode, which should have a lot of multiplications in it, but we don&#39;t see any. If you don&#39;t read x86 assembly, you can see there&#39;s an add, mov, shl, sub, but there&#39;s no multiplication. This is actually intentional. Compilers do something called a strength reduction, which is an optimization the compiler can apply when it has information about constants. In this particular case, we know that a multiple by 31 is the difference of a multiple by 32, and the value subtracted from that multiple. Multiplying by 32 is nice, because the compiler can replace that multiplication with a left shift. The value of 31 was chosen back in the &#39;90s to enable this optimization.&lt;/p&gt;

&lt;p&gt;If we look at the structure of the code, we can see that this code has been unrolled. We can see the same code over again. In between each of these blocks, we don&#39;t have any control flow, so we&#39;re not checking the loop conditions. That&#39;s good. That&#39;s saving some overhead per byte. Each of these blocks is processing 1 byte. Unfortunately, we have what&#39;s known as a dependency chain. First, we start by bumping the loop induction variable. Then we load a load of data, four at a time. These instructions don&#39;t block each other. In principle, they can happen at the same time. Then we have this long sorry chain of instructions which can only happen one at a time. That makes us sad.&lt;/p&gt;

&lt;p&gt;It may or may not seem like a problem to you, because the program&#39;s sequential, and the execution is also sequential, so where were we expecting to get parallelism from? The instruction latencies are very low. It&#39;s worth looking at how instructions are actually executed. First of all, in the frontend, they&#39;re decoded into these things called micro-ops or uOps, by the frontend. The uOps are then executed out of order and scheduled across a number of ports. Instructions have affinity with ports, so certain instructions can only execute on certain ports. The execution engine is designed to exploit any non-sequentialism in the program, and so it can do things out of order.&lt;/p&gt;

&lt;p&gt;It&#39;s designed to execute out of order, and this works so long as you don&#39;t have a data dependency. We can think of IPC, which I showed you earlier, as almost like a utilization metric for the execution engine. If we get a lot of instruction-level parallelism, IPC is high, that means that we&#39;re executing lots of instructions over the execution ports. There&#39;s a great simulator where you can just load some assembly code on uops.info, which shows you basically like a Gantt chart waterfall for how the code is actually executing. Basically, we can see this long diagonal waterfall of green retired blocks, and that&#39;s something that we&#39;d like to avoid.&lt;/p&gt;

&lt;p&gt;The next thing to do, to think about how to improve the performance of the hash code is to look at the chain latency, these instructions for each byte, and go back to Pentium when the optimization, like choosing 31 as the multiple was applied. Back in those days, multiplying integers was really expensive. It cost nine cycles, whereas the add, mov, shl, sub routine that we&#39;ve seen in the disassembly only cost 4. This was actually a pretty decent optimization back in the day.&lt;/p&gt;

&lt;p&gt;Nowadays, multiplication is much faster, and there&#39;s nothing between them. I got these numbers from Agner Fog&#39;s instruction tables, which are really useful. I don&#39;t actually have a Pentium to measure this on. That&#39;s where I got those numbers from. If we want to optimize this, then we need to replace total order with partial order. The approach we&#39;ll take is to set up a recurrence relation, and we&#39;re going to substitute the last value into the next iteration for each iteration. We&#39;re substituting this in, and then keeping track of these multiples of 32, and the subtractions gets really unwieldy.&lt;/p&gt;

&lt;p&gt;If we just cave in and we just multiply by 31 instead, we start to get this polynomial expression, and it gets very simple. If we go all the way down to the base case, we can see this is just a polynomial, so it&#39;s quite easy to deal with. This leads to this possible, I call it crazy unroll. This code looks pretty ugly, but there&#39;s something to it. I don&#39;t suggest people do this, but people tend to assume in high-level languages that it&#39;s not possible to outdo the JIT compiler. If you have two programs which produce the same output, the compiler will somehow magically optimize them both the same way. By understanding the gaps in the JIT compiler, you can find ways to beat it.&lt;/p&gt;

&lt;p&gt;I wouldn&#39;t bother most of the time, but this was a case where we could do this. First of all, we need to deal with the remainder. Then we have this big unrolled loop which does all of these multiplications, and it&#39;s basically looking ahead in the loop to pre-compute some of the multiplications. We&#39;re not doing lots of extra multiplications because we have constant folding, and so we can see that those powers of 31 are pre-computed and loaded into the bytecode, so we don&#39;t need to worry about that. If we actually measure the performance of this, we&#39;ve improved IPC, so we know that we&#39;re making better use of the execution engine with this code, and we managed to outsmart the JIT compiler.&lt;/p&gt;

&lt;p&gt;If we actually inspect the JIT compiled code, we can see why this is. Here we are. In principle, we can load all of the data, 8 bytes at a time in parallel. They&#39;re not going to block each other. Then we can do these at the same time in principle and so on. That gives us a faster implementation by about 2x. Why is that? It shouldn&#39;t be faster, really, because the strength reduction sequence of instructions has the same latency. If you look at the reciprocal throughput of the instructions, reciprocal throughput of 1 means that once you&#39;ve issued one instruction, you can wait one cycle to issue another.&lt;/p&gt;

&lt;p&gt;If you have a reciprocal throughput of 0.25, you can issue four per cycle, and you don&#39;t need to wait for the first instruction to complete. We have a Gantt chart like this, and we can see that after six cycles, we&#39;ve actually processed 3 bytes, which gives us two cycles per byte versus four cycles per byte, so that&#39;s twice as fast. That explains why we&#39;re twice as fast to compute the result. We can also put the disassembly into the instruction scheduling simulator, and we can see that it&#39;s much more parallel. If we put them side by side, we used to have this diagonal waterfall, and now we have something which is much steeper.&lt;/p&gt;

&lt;p&gt;We do have a problem, though, because we have this data dependency, again, on this ECX register. We can see all of these add instructions are trying to add to the same register, which creates a data dependency. That is because we&#39;re loading the hash at the start of each loop iteration, then we have to add all of these numbers together into the hash, so we&#39;re doing this computation effectively in parallel and then merging back into the intermediate result for each iteration.&lt;/p&gt;

&lt;p&gt;That&#39;s basically limiting the amount of parallelism we can get. This is arguably an optimization because it&#39;s twice as fast. It&#39;s pure Java. It&#39;s pretty easy to understand, even if it&#39;s ugly. It does increase the bytecode size, which is a bit of a problem if we want to get this into OpenJDK. It may penalize shorter strings if we no longer get inlining because the bytecode size is too large.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Competitive Advantage&lt;/h2&gt;

&lt;p&gt;This was actually submitted as a proposal to OpenJDK, to solve our cost problem because we&#39;d identified that the hash code was costing us so much money to compute. We get into the red zone now because straightaway, rightly, OpenJDK came back to us and said, &quot;Don&#39;t do it this way. Do it another way. Can you implement a vectorized intrinsic? Here&#39;s an idea&quot;. What is vectorization? What we&#39;re trying to do is synonymous with SIMD, which is Single Instruction, Multiple Data, and so we&#39;re trying to process more data per instruction. This can be really useful for increasing the throughput and reducing the number of instructions that we need to apply per byte by operating in chunks.&lt;/p&gt;

&lt;p&gt;We&#39;re basically applying the same operation to a vector value simultaneously in one go. It&#39;s a bit like instruction-level parallelism, but there are explicit execution units, special instructions, and pipelining applies to vectorization too. Schematically what we&#39;re doing is we would no longer have this reduction bottleneck. We could keep all of the data in separate lanes and then reduce horizontally at the end. There&#39;s autovectorization applied by the JIT compiler in OpenJDK. Why can&#39;t it do anything with this? We&#39;ve already seen the disassembly for the improved version, and so we know that the disassembly doesn&#39;t have any vectorized instructions because we&#39;ve seen it.&lt;/p&gt;

&lt;p&gt;The problem with the optimized solution that we&#39;ve shown is basically we&#39;ve unrolled the loop. The JIT compiler basically competes with the application for CPU cycles, so it&#39;s limited in the analysis it can really do. It&#39;s not going to look for similarity in blocks of instruction. If we do manual unrolling, we&#39;re basically switching off vectorization because unrolling is actually the trigger for vectorization analysis. Why don&#39;t we just try and make it easier by pre-computing the coefficients, and then we&#39;ll evaluate the polynomial as a dot product, which should be pretty simple to optimize.&lt;/p&gt;

&lt;p&gt;Unfortunately, we don&#39;t get any SIMD. We get these register spills. Register spills are where there&#39;s too much register pressure and integer values are saved into floating-point registers, and we still have this per iteration reduction anyway, so it&#39;s not really a very attractive solution.&lt;/p&gt;

&lt;p&gt;What we could do is pre-compute enough powers of 31 so that we can just keep on generating the next set of powers in a loop, and we&#39;ll multiply the vector of values by the powers of 31, and compute a dot product one vector at a time. Then, finally we&#39;ll reduce the vector horizontally at the end. There&#39;s a blog post here about how to do that. That code looks like this using the vector API, which still hasn&#39;t been released, and this code won&#39;t compile anymore. This had very attractive performance back in 2018 using a prototype of the vector API. Here&#39;s a diagram of how this algorithm is actually working. We&#39;re going to load the data backwards. We&#39;re going to load that into a small register.&lt;/p&gt;

&lt;p&gt;Then we&#39;re going to have to sign extend, so we&#39;re going to put a load of zeros next to the data. Then we&#39;ll multiply that data as an integer with the powers of 31. We&#39;re going to add that to the accumulator. Then we&#39;re going to update the coefficient so we have the next powers of 31. Then, finally, because we&#39;re going backwards, we&#39;re going to go and load the next vector. We&#39;ll keep on going backwards until we&#39;ve got nothing more.&lt;/p&gt;

&lt;p&gt;Finally, in the end, we&#39;ll reduce the accumulator horizontally. This works pretty well because we&#39;re processing so much data per instruction. Adding up the latencies of these instructions, we have 24 cycles, but we&#39;re doing 8 values per chain, so we&#39;ve basically got 3 instructions. We got three cycles per byte, which isn&#39;t that much better than we had before, so we should probably find out why. If we have wider vectors, so we have AVX-512, and so we can process 16, then we&#39;re down to 1.5 cycles per byte, which is quite a big improvement. Vector multiplications are quite slow, we have a latency of 10.&lt;/p&gt;

&lt;p&gt;The nice thing is in AVX2, the multiplication has a throughput of 1, so we only need to wait one cycle until we can have another one. If we just wait 27 cycles by doing four of these, unrolling this four ways, in 27 cycles, we can process four times more data. Three cycles extra, we get four times more data processed. Finally, we get under one cycle per byte with this unrolled approach.&lt;/p&gt;

&lt;p&gt;Ludovic Henry, who used to work at Datadog, went off and implemented this. Then he left Datadog before it could get finished. We were really lucky because Oracle and Intel saw a lot of value in this approach because it massively improved the performance of such a common JDK method that they came in, they helped, and they productionized it, and it went into JDK 21. If you&#39;re running JDK 21 now on x64, then you&#39;re benefiting from this optimization. We can see, here&#39;s a kernel of the loop now. We can see that we have a vectorized loop. The instruction simulation is much more vertical.&lt;/p&gt;

&lt;p&gt;That&#39;s a nice comparison with what we started off with. We have much more parallelism, is the interpretation of these two charts. Crazy unroll was somewhere in the middle. With JDK 21 for a range of lengths of string, computing hash codes is much faster. What kind of impact does this actually have? It&#39;s a shame it&#39;s had limited impact at Datadog because this hasn&#39;t been implemented for ARM. It took about 18 months for this change to get into OpenJDK.&lt;/p&gt;

&lt;p&gt;In the meantime, we actually moved most of our backend processing to ARM. We need a new intrinsic now. It&#39;s going to be a little while more to finally profit from this. Although we have profited a lot from that very simple migration to a different microarchitecture. Potentially, it&#39;s got a huge impact outside of Datadog, if you&#39;re running Java on x64, you&#39;re running Java 21, that is. That&#39;s the state of things now.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Key Takeaways&lt;/h2&gt;

&lt;p&gt;The key takeaways are, early on in performance work, we have these simplifying optimizations, which, they&#39;re objective improvements. It&#39;s hard to argue with them because you&#39;re probably improving your code quality at the same time. 10x really sounds amazing, but it probably means that the system hasn&#39;t been optimized before, and you&#39;re just the first one to get there. Don&#39;t be too disappointed with 10% later on. I think that continuous profiling is a great tool for finding low-hanging fruit. It&#39;s not so useful for fine-tuning of the kind of work that Datadog did with the String#hashCode.&lt;/p&gt;

&lt;p&gt;Thomas said at QCon last year that all software is optimized for out-of-date hardware. That&#39;s an interesting example of this here because String#hashCode was optimized for the Pentium in the &#39;90s. The algorithm was chosen for the Pentium to be more efficient. Over time, that optimization became less relevant. Specifying that algorithm and relying on it put OpenJDK in a difficult place because it couldn&#39;t be changed when hardware changed.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 1: How long did it take from identifying the issue to actually implementing the solution?&lt;/p&gt;

&lt;p&gt;Startin: Actually implementing the solution was quite quick. Obviously, OpenJDK has a massive impact. If you break something like the String#hashCode, it can really affect a lot of users. OpenJDK is a very cautious project. Actually, getting it merged and released took more like 18 months. Implementing the solution was relatively quick.&lt;/p&gt;

&lt;p&gt;Participant 2: You said earlier that profilers will often report different things. Does this mean that we should be using different profilers in different circumstances, or is there a way for us to know when a profiler is best suited for a certain problem?&lt;/p&gt;

&lt;p&gt;Startin: I think it&#39;s worth having more than one observability tool to test the observability. I think in general for Java, anything based on async-profiler is a really accurate profile. JFR has some gaps. We&#39;d like to actually fix those gaps to make it better. That&#39;s something we&#39;re working on at Datadog now. I wouldn&#39;t suggest running two in production, because it&#39;s going to increase the overhead. Profilers, especially if they use signals, they don&#39;t play too nicely. I think it&#39;s definitely worth not trusting your tools and testing them against each other.&lt;/p&gt;

&lt;p&gt;Participant 3: The condition is to start using continuous profilers to do these jumps, [inaudible 00:48:50] for services under load. It&#39;s a little bit risk averse to add a little bit more to it. You mentioned it&#39;s below 1%. Are there cases where this isn&#39;t the case or stuff you need to watch out for?&lt;/p&gt;

&lt;p&gt;Startin: Anything that applies tracing is difficult to reason about, because when you apply tracing you don&#39;t know all of the programs that the instrumentation will be applied to. The overhead depends on what&#39;s being traced. With CPU sampling, it&#39;s by construction. If you set the sampling frequency low enough, you can&#39;t have more than a certain level of overhead. Allocation profiling, on the other hand, that can be quite high overhead. It can be more like 5% depending on the workload. Be careful with allocation profiling if you want to enable that in production. Often, it&#39;s lower than that, but it can be higher depending on the workload.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/java-string-performance/</link><guid isPermaLink="false">infoq-presentations/java-string-performance</guid><pubDate>Wed, 12 Mar 2025 16:00:00 GMT</pubDate><author>Richard Startin</author><enclosure url="https://res.infoq.com/presentations/java-string-performance/en/card_header_image/richard-startin-twitter-card-1740132256484.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-apr-complexityperformance.mp4" type="video/mp4"></enclosure><itunes:duration>50:04</itunes:duration><category>Architecture &amp; Design</category><category>Java</category><category>Culture &amp; Methods</category><category>.NET Core</category><category>InfoQ</category><category>Machine Learning</category><category>Performance</category><category>Microservices</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>QCon London 2024</category><category>Java9</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>From a Lambda-Lith to an Event Driven Architecture – Lessons Learned</title><description>&lt;figure&gt;&lt;img alt=&quot;From a Lambda-Lith to an Event Driven Architecture – Lessons Learned&quot; src=&quot;https://res.infoq.com/presentations/lambda-lith/en/card_header_image/leo-hanisch-twitter-card-1740568142902.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-sep-lambdalith.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-sep-lambdalith.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-sep-lambdalith.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Leo Hanisch discusses the challenges of the &quot;Lambda-Lith&quot; approach and the benefits of embracing EDAs.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Leo Hanisch is a Solutions Architect at Siemens who is passionate about all things serverless technologies and event-driven architectures. He loves diving deep into the world of cloud computing and actively engages with the local AWS community.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;InfoQ Dev Summit Munich software development conference focuses on the critical software challenges senior dev teams face today. Gain valuable real-world technical insights from 20+ senior software developers, connect with speakers and peers, and enjoy social events.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Hanisch: My talk, &quot;From a Lambda-Lith to an Event-Driven Architecture&quot;,&amp;nbsp;let&#39;s take a look at what we are actually talking about. First, I&#39;m giving you a little introduction. Then we are going to explore, what is actually a Lambda-Lith. We are first going to introduce that term, what it is actually. Then, we are going to reflect on how can we maybe do a little bit better. This is more like the journey we at Siemens did these three past years, so where we started building our systems and how we build them today. To showcase that, I then brought three different projects, and of course, in the end, we are going to wrap it up.&lt;/p&gt;

&lt;p&gt;Who am I? I&#39;m Leo, solutions architect with Siemens, involved with AWS and mostly into serverless stuff. That&#39;s also why we are going to talk about mostly serverless architectures.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;The Lambda-Lith&lt;/h2&gt;

&lt;p&gt;When it comes to architecture, as a solutions architect, we always aim for the best solution, around optimal ways, like it&#39;s supposed to cost nothing, it&#39;s supposed to scale high. Back then, this is what our architecture looked like. This is when you get started with AWS, when you get started with serverless architectures, this is probably what you first are going to see. We have an Amazon API gateway, AWS Lambda, and Amazon DynamoDB, and they are great. Serverless services and all good, but there&#39;s no Lambda-Lith yet, or what is a Lambda-Lith? To discover the Lambda-Lith, we actually have to zoom in a bit in the Lambda function.&lt;/p&gt;

&lt;p&gt;A Lambda-Lith is more like a word combination of Lambda, because AWS Lambda is our compute platform, and a monolith, because we are building monoliths. We would do something like this. We have our favorite Node.js framework, in our case, NestJS, and there you just define your REST API. Your routes, in this case, we have a POST route for orders, we want to create new orders, and then you do all your stuff in code. That&#39;s how we traditionally build servers, and it&#39;s all good. This time we say, we don&#39;t want to deploy it on-premise, we just put it in the Lambda function, and there you get the Lambda-Lith.&lt;/p&gt;

&lt;p&gt;With each of those solutions, there are tradeoffs. With the Lambda-Lith, there&#39;s advantages. When you&#39;re talking about AWS Lambda, you soon get to experience cold starts. There is, because on an initial invocation, AWS actually has to download your source code files, before it&#39;s able to execute it. That time is referred to as a cold start. Then, every subsequent invocation of the same Lambda function, you don&#39;t have that cold start anymore, because the source code files are already there. Of course, it&#39;s easy to get started, because maybe you already built that server for an on-premise use case, and now you want to leverage all the advantages of the cloud. With very little glue code, you can actually manage to migrate your existing applications to the cloud. Of course, every solution ships with advantages and disadvantages.&lt;/p&gt;

&lt;p&gt;Some disadvantages are that even though you have less cold starts, because all in a single Lambda function, your cold starts tend to take longer. Why is that? You&#39;re bundling all in one bundle. That bundle is bigger, because everything is in there, therefore, it takes longer. With this type of architecture, we have a REST API, we have the Lambda function in our database, we only have synchronous invocation. Of course, that&#39;s sufficient for many use cases. There&#39;s nothing wrong with that at all.&lt;/p&gt;

&lt;p&gt;Of course, as things grow, those projects we&#39;re building, those applications, they&#39;re evolving, we see new requirements coming in over again, it might hit the limit. In the case of AWS Lambda, you actually have a hard limit you cannot avoid. That means the bundle size you&#39;re uploading is a maximum of 50 megabytes zipped and 250 megabytes unzipped. If you hit that limit, you&#39;re really in trouble, and you need to rearchitect your solutions to somehow circumvent that. Of course, the big monolith disadvantage, you have a single bottleneck. Even if you&#39;re using AWS, you&#39;re scaling, you have tremendous scaling capabilities, but if you introduce a bug that prevents your server from starting, it will break that Lambda function, it will break your application.&lt;/p&gt;

&lt;p&gt;With that in mind, we have one more thing, AWS has its own take on the Lambda-Lith. That&#39;s nothing I came up with. When we zoom in on the very top left, we actually see it&#39;s an anti-pattern. That raises the question, are we now doing everything wrong, or are we supposed to do better? One quote on the very bottom, I really like because I felt when I read that, it really much related, it definitely increases the cognitive burden for developers, at least that was the case for us. Because if you go back to Lambda-Lith itself, if you imagine this, we have here the product service, we don&#39;t have any error handling. What is supposed to happen if we fail to create an order? Are we rolling back? Should we unreserve the product again? All those things are not handled. If you imagine those four lines of codes with tremendous try-catch blocks, that&#39;s what causes the cognitive burden. With that, we successfully introduced Lambda-Lith.&lt;/p&gt;

&lt;p&gt;As we will do with Lambda-Lith and all subsequent projects I&#39;m going to bring today, we are going to think about, why have we used it in the first place and what have we learned by applying this? In our case, we used it because we just did not know any better. Three years ago, that was mostly the start of our cloud journey. We had them deployed very fast and it worked. It scales, it works, and you&#39;re happy.&lt;/p&gt;

&lt;p&gt;Again, you have those tradeoffs, you have to have those tradeoffs in mind. Again, like the re-platforming case, if you have already a REST server running somewhere, it&#39;s an easy way to migrate to the cloud. What have we learned? The whole cold start thing. You want to know the implication it ships with, so it&#39;s a tradeoff. Do I want to have multiple faster cold starts, or do I want only one cold start or less cold starts? I accept that a single cold start might take longer to spin up. Of course, what we just said, it&#39;s like a cognitive burden, or can become a cognitive burden. Of course, not all monoliths are, deferring to all monoliths, but speaking how it was for us, and we had that single bottleneck. That&#39;s, of course, something to avoid.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Project 1 - Use Case (Siemens COIN)&lt;/h2&gt;

&lt;p&gt;That actually brings us to our first project I want to share with you. We call it COIN. The scope of that COIN project is to actually inform fellow Siemens employees about their stock options, and equity plans they are eligible. Once a year, as a Siemens employee, you can subscribe for one of those plans, and then you&#39;re fine. It&#39;s mostly informational web app, but also then you&#39;re allocating your options through that web app. You want to learn. We learned, Lambda-Lith, it&#39;s ok, but maybe we can do a little bit better. Let&#39;s take a look at how we built it. That looks very similar to what we already got. We only swapped out the DynamoDB, like the NoSQL database of Amazon, with an Aurora instance. Aurora is the relational database, comes in different flavors. We used PostgreSQL.&lt;/p&gt;

&lt;p&gt;To actually get the difference, we have to zoom in, because now we would again have an API gateway endpoint, a Lambda function, reader and writer instance from the Aurora. We would have multiple instances of those. We would start calling the microservices. Now we have an absolutely fine architecture. We eliminated our single bottleneck, the Lambda function, and we deploy it to production on a Friday, and go home, and are satisfied. That&#39;s what we do. Turns out, on a Monday, you go back to office, and then you realize, we actually not only have a few clients, like HR is actually on an annual basis running email campaigns, informing all 300,000 eligible Siemens employees, &quot;Dear Siemens employees, now please go to COIN, and allocate your stock options&quot;. What&#39;s going to happen?&lt;/p&gt;

&lt;p&gt;We are having a lot of clients, causing a lot of traffic on our API gateways, so that&#39;s fine. We&#39;re using serverless technologies. That&#39;s why we use them in the first place, because we are expecting that peak workload. API gateway, I think it has a quota, about 10,000 requests a second, so that&#39;s ok. Our Lambda function also, they spin up multiple instances. That&#39;s ok. Each Lambda function, which spins up on invocation to serve the high amount of requests, is attempting to connect to the database. We still have a single writer, and a single reader. Of course, what&#39;s going to happen? We are trying to open multiple connections, and we will just overload the database, and we fail to serve our requests. What we accomplished, we got rid of our compute bottleneck, but we successfully introduced a database bottleneck. How can we circumvent that?&lt;/p&gt;

&lt;p&gt;First thing you actually want to do is, you want to use the so-called RDS Proxy. That thing, what it&#39;s meant for is, it is actually taking care of those multiple connections. This component is the AWS solution for connection pooling for your database instances. Still, we had many, still many requests. It was a very read-heavy application. We then could also swap out a single reader, and introduce a so-called Aurora autoscaling group. What it does is, it lets you track a certain metric, for instance, database CPU load, and then based on that metric, add additional readers to your database cluster. With that, we eventually managed to serve all requests, and survive the annual Siemens email campaign, and all Siemens employees got happy that they could allocate their options. That&#39;s for the first application.&lt;/p&gt;

&lt;p&gt;Again, why would you use it? It&#39;s the re-platforming thing. We still have smaller Lambda-Liths, but still somewhat the same. We still only have synchronous execution, and we already managed to decouple parts on the compute side. What have you learned? Don&#39;t use a single bottleneck as a database. It sounds stupid. When I&#39;m talking about that, it sounds obvious, but it was not obvious back then. With the small Lambda-Liths, it&#39;s the same thing. You have to tradeoff the cold starts. I would not say it&#39;s a problem, but you should know the implications that it ships with.&lt;/p&gt;

&lt;p&gt;Of course, with the database, a single database is a bottleneck. If you have that, and if it&#39;s an AWS Aurora, make sure you want to use an RDS Proxy if you have multiple connections, and if that&#39;s not sufficient, introduce the autoscaling. That&#39;s why we go to the cloud provider in the first place because we want them to do the scaling for us.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Project 2 - Use Case (Siemens DVM - Digital Visit Management)&lt;/h2&gt;

&lt;p&gt;That actually leads us to the second project I want to share with you. After we eventually managed to deliver COIN, we got a new project, and it&#39;s called DVM, it&#39;s the Digital Visit Management, the abbreviation for that. What do you do with that? With Siemens, we have a lot of business partners we want to get involved, we want to get in touch, we want to do business, and we want to invite them because we want to show what is Siemens capable of doing. We have new facilities, new factories, like Industry 4.0, like we have those showcases.&lt;/p&gt;

&lt;p&gt;For instance, I want to say, let&#39;s invite the whole InfoQ attendees to a Siemens site, and then I&#39;m going to show you what we can all do. This tool allows me to plan that. I can schedule meetings. I can schedule tours through our factories, you get QR codes so that you get through the gates. As you can imagine, I should not be capable to just schedule a meeting, or such a tour for 100 people, there should be an approval process. Probably my manager wants to know what Leo is planning on this event, on this conference. This is a lot of approval, a lot of states, a lot of management needed. That&#39;s actually a screenshot of a first version of a state machine we had for all those approvals. You can even imagine, one of those small boxes even unfolds in its own state machine. It&#39;s a lot to consider.&lt;/p&gt;

&lt;p&gt;You have different factories, different interfaces all over the place, and you somehow want to manage it. We need to build an approval workflow. The good thing, AWS got us covered when it comes to approval workflows, namely, AWS Step Functions. Step Functions, I refer to it as the workflow engine that AWS provides for us. What you can do is you can define a workflow. In this example, we have the start and the end, and we have a single action, we want to emit an event to Amazon EventBridge, so the event bus. Usually, when you just normally invoke such a Step Function, it does all the steps you&#39;re defining. We&#39;re writing something to the database, an additional API call, putting an event to EventBridge, and in the end, it will succeed, and the execution is done.&lt;/p&gt;

&lt;p&gt;However, there&#39;s this wait for callback mechanism I want to introduce a bit more in depth, because with that enabled, if you think about having an event, we can link that Step Function to an event bus, so whenever an event occurs, that Step Function starts. Usually, we emit an event, again, on the event bus, and we end. However, with that wait for callback mechanism, we actually emit still that event, but we pause the Step Function, so we don&#39;t finish the Step Function just yet, we pause it. That event that was emitted, that includes a token, and now we&#39;re waiting. What you usually do is you subscribe to another listener, in this case, a Lambda function, to that event, including the token, and now you want to have that Lambda function to save the token to a database, because now we are waiting for an approval.&lt;/p&gt;

&lt;p&gt;Is Leo allowed to invite all the people to the next Siemens site? Yes or no. We don&#39;t know. Of course, when have you asked your manager the last time for an approval, maybe for a vacation, to come here to this conference? It can take some time. The good thing about this type of workflow is, while you&#39;re waiting for the approval, you actually are billed almost nothing as infrastructure costs. The only thing you&#39;re billed for is a little bit of storage because you store the token within a database, but you have no compute costs, because the Step Function is paused. EventBridge is like the serverless event bus, so if there are no events, no costs. The same goes for the Lambda function.&lt;/p&gt;

&lt;p&gt;Then, when I&#39;m approved to show you the next Siemens site, we again have an event on event bus, a Lambda function listening to that, reading a token from the database, and then finally submitting the token back to the Step Functions API. Just now, once the approval was granted, the Step Function would continue, and in this case, it finishes. Of course, now you could do everything else, any subsequent things, like sending out emails, phone confirmation of the approval, those would be like the usual subsequent steps you want to do.&lt;/p&gt;

&lt;p&gt;With that in mind, we actually can take a look how we built our architecture. Again, we want to do microservices, now we&#39;ve learned, don&#39;t use database, a single bottleneck, so now we moved databases to each microservice, so each microservice has its own database. Also, microservices, just to visualize it, it&#39;s not only a Lambda function API gateway, it can have multiple components, like queues, additional listeners, and so on. Then, of course, we have EventBridge for our choreography, so now we can actually communicate in an asynchronous fashion. Then the counterpart orchestration, we&#39;re using Step Functions for that. In our case, we put that Step Function in a scope we called the orchestration service, but we&#39;ll see how that turned out. Now, again, we can think of why have we used it. Why did we choose such a type of architecture?&lt;/p&gt;

&lt;p&gt;First of all, asynchronous communication, so everything is asynchronous, so we want to also be able to implement those requirements. Also, for us, like this whole event-driven mindset was also closely aligned to how we actually think or how we would receive the requirements. Because it usually was, when a booking was approved, please send out an email to the following stakeholders. When the catering is scheduled, then we need to feed that back to some other channel. Those requirements naturally were defined with, when something happens, please do something else.&lt;/p&gt;

&lt;p&gt;For us, it turned out that was actually quite nice and it aligned with this event-driven approach. We managed to decouple our systems, so if a microservice now fails, only that single feature or microservice failed and not our whole application. That&#39;s a huge benefit, of course. The whole Step Function to implement the orchestration, also a huge benefit to do so.&lt;/p&gt;

&lt;p&gt;What have we learned? We need to plan for observability, because now, suddenly, we have dozens of microservices, dozens of Lambda functions listening, and Step Functions, and other pieces and components asynchronously somehow interacting with each other, and we want to keep track what&#39;s going on. That&#39;s a big lesson, because maybe you&#39;re submitting some event to some microservice over here, but some subsequent microservice horribly fails, and you want to know that, actually, that event caused the downstream failure. That&#39;s a big thing, big lesson learned. Because in the end, it blew up, and then we were like, we have no idea why. You want to actually trace what&#39;s going on in your system.&lt;/p&gt;

&lt;p&gt;It&#39;s, again, one big Step Function we introduced, we have this orchestration service. It&#39;s not a good thing, similar to how the initial Lambda monolith was like an ok-ish idea, we want to break things up, we want to put them in different scopes. Also, it was hard for us from a mental model, because, in this application, we&#39;re orchestrating not only bookings, we&#39;re orchestrating factory onboarding users, and many things. Then we were like, ok, where should we put that piece? We&#39;re doing orchestration, so it should go in the orchestration service but, actually, it&#39;s heavily related to a user so maybe it should go in the scope of a user service. I would not do that again, like creating its own orchestration service, because that, at some point, did not align well with our mental model.&lt;/p&gt;

&lt;p&gt;The last one is actually a tricky one. Within AWS, especially with these serverless event processing solutions, namely EventBridge, like the event bus, but also SQS, where we can queue different events and process them, they all have a so-called delivery policy. When you look it up, the delivery policy for most services is at least once, so that doesn&#39;t mean zero times, mostly one time, but it can also be two times.&lt;/p&gt;

&lt;p&gt;If you remember what I showed when I walked you through the Step Function part, that was triggered by an event emitted by the event bus. Now what happens if you have two events, two Step Functions start, trying to do the same thing, that caused side effects you cannot imagine. It&#39;s hard to describe. It&#39;s hard to grasp, but it&#39;s a mess. You don&#39;t want to debug those kind of side effects. We maybe see with the final and last architecture how you can actually deal with duplicated events.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Project 3 - Use Case (Siemens MDLA - My Digital Lab Assistant)&lt;/h2&gt;

&lt;p&gt;That leads us to the last architecture I brought with me. It&#39;s called MDLA, an abbreviation for My Digital Lab Assistant. We built that for our Siemens Healthineers colleagues. What problem are we trying to solve? In the end, it&#39;s a customer portal. If you&#39;re a customer of Siemens Healthineers, you buy your favorite medical device, but then, of course, different things can fail. It does not get shipped. It&#39;s damaged. It has the wrong color. It&#39;s broken, whatsoever. In that case, you usually contact your Healthineers contact. To somehow streamline this in a more structured manner, we built MDLA. When I said those duplicated events can become a mess, there&#39;s actually a concept that allows us to take care of that, and that&#39;s namely idempotency.&lt;/p&gt;

&lt;p&gt;The definition I&#39;ve looked up here is that idempotency is like the capacity of an application to identify repeated events, prevent duplicated, inconsistent, or lost data. We learned those duplicate events, they can happen. That&#39;s not good. We want to know when they arise, and then we want to treat them properly and handle them. That&#39;s what idempotency is about. I&#39;ve brought an example. Let&#39;s say we have a Lambda function, and we&#39;re trying to insert a new record, a new row within our database. Now, when we have duplicate events, we would insert two records, so that&#39;s obviously not a good thing. What you now do, instead of directly inserting that event, you want to somehow identify that event.&lt;/p&gt;

&lt;p&gt;Either you have your own ID that uniquely identifies an event, or you could think of hashing the whole event to get a unique identifier. Then, before you put it in the database, you actually want to look up that event ID in a so-called control table. In this case, could be some cheap DynamoDB table. Then, you have two cases. Either the event does not exist in a control database, so we know, ok, we have not processed that event already, so we are safe to put it in the original database. Then, finally, also have to update the control table. The other case, we already have processed that event. We get a hit in our control database, and then we know, we already processed that event. What should we do? Most of the time, you don&#39;t want to do anything, you just drop that event, because you know you already did that.&lt;/p&gt;

&lt;p&gt;With that in mind, we can actually take a look at how we built MDLA. Lessons learned: we don&#39;t want to do its own orchestration service, therefore we put Step Functions in each microservice where needed. We would again have multiple microservices, because microservices are allowed to communicate with each other. We usually would have client credentials for each microservice, so we have an authorization in place. Again, have an EventBridge, but for asynchronous communication. What I did not mention, actually, is it&#39;s fairly tedious for clients to connect to all of our microservices. We have dozens, or even hundreds of microservices, and we don&#39;t want the clients to connect to each microservice, aggregate data across microservices, that&#39;s very tedious.&lt;/p&gt;

&lt;p&gt;Instead, what we introduced in this project is so-called gateway service. It&#39;s again, an API gateway and a Lambda function. We would this time deploy it in a private subnet, because then we can create so-called interface endpoints and assign those downstream microservices, so-called resource policies. With that, we switched these client credentials overflow, or exchanged it with AWS IAM, so the AWS authorization service. That&#39;s why we choose the cloud provider. We want to pass on the responsibility, because we don&#39;t want to keep track of our secrets and so on. That&#39;s why we chose to implement it in this way.&lt;/p&gt;

&lt;p&gt;With that, we actually unveiled the last architecture, but again, why did we choose it? All the previous benefits. We entered this rabbit hole of event-driven architecture, so of course all those advantages apply. In this case, we really benefited from those private API gateways because we switched out our custom authorization mechanism and leveraged what AWS already provides for us. Of course, the whole client connections with the gateway service. Lessons learned: I cannot stress it out enough, it&#39;s again, observability. You want to start tracing. X-Ray is the AWS service to do so, but you of course can also use OpenTelemetry. Again, those private APIs, this was both like a benefit where you wanted to use it, but we discovered it on the way. It was also a lesson learned for us.&lt;/p&gt;

&lt;p&gt;With the Step Functions, that&#39;s actually an interesting one because you can actually version your Step Functions and assign different aliases for them. It&#39;s actually a very neat feature because that allows you to publish new versions, new adapted workflows without breaking anything old. That ensures you some backward compatibility. Because imagine you have your approval workflow, it&#39;s running, you&#39;re waiting for approval, but now you&#39;re changing the overall process, you&#39;re deploying a new Step Function. How would you do that? You have to still wait until the old workflows succeeded. With that, you can actually run multiple versions of your Step Functions simultaneously.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We started with a fairly simple serverless architecture when it comes to AWS, with first, an API gateway, a similar Lambda function where we have all our code inside, and a database. We switched it up. We changed that compute bottleneck in favor of a database bottleneck. Then we discovered Step Functions to actually implement approval workflows, which then allowed us to implement a more complex system with a lot of orchestration going on, such as the Digital Visit Management. Then the final architecture with MDLA. We implemented our handlers in an idempotent way. We were checking, have we processed an event already or not? With that, we eventually built more resilient architectures. That was more like the journey we&#39;ve taken.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 1: I had a question about the best of the best architecture that you built. You introduced the Lambda function that is going to be taking in all the requests coming from the users. Did you make your architecture synchronous at the end of the day? Is your Lambda function waiting on the process to complete? Is this your new bottleneck?&lt;/p&gt;

&lt;p&gt;Hanisch: Yes, it is. First of all, why did we do that? We learned that already, a single Lambda bottleneck, it&#39;s a bad thing. We want to avoid that. In this case, we are not doing the heavy business logic in there. We are aggregating across multiple services. Of course, there&#39;s the risk to have that single bottleneck, but in our case, we accepted that risk because back in time, we&#39;ve been good at building small Lambda-Liths, so we built a small Lambda-Lith, obviously. How would it do things today? There are different solutions you could tackle that.&lt;/p&gt;

&lt;p&gt;What also we are investigating is, for instance, using AppSync. That&#39;s GraphQL. The managed allows you to build GraphQL APIs, and it&#39;s also a fully serverless service managed by AWS. Then you would get rid of your bottleneck. Obviously, yes, very good catch. Again, a bottleneck. In this case, we accepted that risk. It&#39;s an intended bottleneck.&lt;/p&gt;

&lt;p&gt;Participant 2: One question more towards the application side. One issue that I always found with the serverless architectures is the unit testing, because the whole application is interdependent on calling both the tree, and then to properly test it, you need to have it in cloud. How did you solve the unit testing issues?&lt;/p&gt;

&lt;p&gt;Hanisch: All the application layers, like all our controllers, all our Lambda function code, we would unit test.&lt;/p&gt;

&lt;p&gt;Participant 2: For example, Lambda is going to call EventBridge, do you have some mocks for that?&lt;/p&gt;

&lt;p&gt;Hanisch: Yes. I would love to say that we are doing it all in a fully automated way, and we build up whole test environments within our CI/CD environment, do our end-to-end test, and then tear it all down, maybe at some point, but we don&#39;t do that right now. What we do instead is, at least locally, we use LocalStack.&lt;/p&gt;

&lt;p&gt;That&#39;s a framework or a tooling that allows you to emulate some AWS environment, and then you can locally start your AWS services. Within our CI/CD integration, we run unit tests, we do linting, and we do some integration tests where we mock databases. Because for DynamoDB, there exists a Docker image, of course, also for PostgreSQL and those relational databases, so we spin those up. Yes, like EventBridge, or those other managed or proprietary AWS services, we just mock them away in our tests. There&#39;s not full end-to-end tests, but at least from the application layer to the database layer.&lt;/p&gt;

&lt;p&gt;Participant 3: We don&#39;t use AWS in our case, but generally we have huge volume APIs that are going to the database for data, and normally this data is very large. We queue the request to the database. What if the containers receive the request and they are willing to accept the request, but then when the data is processed in the database, these results are coming back to the container, and then the container basically is having a problem. There is a queuing mechanism when the requests go to the database, but what of the reverse queuing? Is there a way to queue also the response that comes from the database?&lt;/p&gt;

&lt;p&gt;Hanisch: We are sending a request to the database, and you&#39;re asking whether we can queue or receive the response? Definitely there is a way to do that. You have to build it. When you use from your Lambda function, or from your compute environment, you send the request to the database, you get a result, and then you just can save that result somewhere. I don&#39;t see anything that would restrict you from doing so.&lt;/p&gt;

&lt;p&gt;Participant 4: Let&#39;s say you have a big monolith and you want to split it up. How small should a small piece be? Should it be just one REST endpoint or one entire domain.&lt;/p&gt;

&lt;p&gt;Hanisch: We ask that question ourselves a lot because we have a lot of monoliths we&#39;re trying to refactor. Of course, it depends. There is no hard rule that you say, if your Lambda function code bundle size exceeds a certain threshold, you&#39;re doing it wrong. However, there are methods like domain-driven design and stuff like that that allow you to define the scope of what are suitable scopes of your microservices, and then you will naturally see where to put which part. There are methods to build or to get started with those event-driven architectures.&lt;/p&gt;

&lt;p&gt;Participant 4: If you have a Lambda which only contains one REST endpoint, do you really then still use an entire REST framework like Flask?&lt;/p&gt;

&lt;p&gt;Hanisch: Obviously, you don&#39;t need that. Also, the question we had in the beginning, we have, again, a Lambda-Lith and having that single point of failure, what you also can do is to have a single Lambda function for all your REST endpoints. Your get orders endpoint invokes a dedicated Lambda function, but also your post orders endpoint invokes another Lambda function. With that, you would also eliminate that bottleneck.&lt;/p&gt;

&lt;p&gt;Participant 5: Across your three different versions of application, you&#39;re always stuck with Lambda functions. Have you ever evaluated going for something like Kubernetes? At least at the very beginning, you had this one fat Lambda, and then with Kubernetes, you could have run that apart with autoscaling, you get similar functionality. Why did you stick with Lambda?&lt;/p&gt;

&lt;p&gt;Hanisch: Because we have absolutely no idea of Kubernetes. Lambda also has its limits, so certain payload limits you can pass through. When we would hit those limits, either we can refactor our application so that it fits, or we would then opt in for a provisioned solution such as AWS ECS, that&#39;s the container service. Then we would go more into this image-based. Kubernetes was never an option for us.&lt;/p&gt;

&lt;p&gt;Participant 6: My question is about your second project. You mentioned having multiple microservices, and those microservices having their databases. In this kind of solution, should we measure infrastructural costs with the benefits of this kind of architecture? Should the data be written synchronously to those databases?&lt;/p&gt;

&lt;p&gt;Hanisch: I&#39;d like to answer your first question, having a single database over multiple ones. Definitely that&#39;s going to increase your bill. Again, it&#39;s a tradeoff. You can also build such a microservice environment with having a single database.&lt;/p&gt;

&lt;p&gt;Then, again, it&#39;s up to you whether you are accepting that risk to have that single bottleneck, to have that single point of failure. As we did with this architecture, now we also have that single bottleneck, but we accept the risk. It&#39;s more about awareness and making conscious decisions. When I talked in the beginning, like the optimal architecture, so with all your questions it&#39;s not the optimal architecture. That&#39;s what we consider good, like an optimal in a local optimum. There&#39;s no global optimal solution for that. It&#39;s always a decision you have to take.&lt;/p&gt;

&lt;p&gt;Participant 6: The second question was about writing data to those databases. Should this be synchronous, or should you just know where you wrote the specific data, and while reading, extract data from that specific database?&lt;/p&gt;

&lt;p&gt;Hanisch: There&#39;s no global recommendation or general recommendation. If you, for instance, rely on transactions when you&#39;re writing, then of course, you want to do it in a synchronous fashion. If some inserts are taking ages, then why not offload that to a downstream process? It depends.&lt;/p&gt;

&lt;p&gt;Participant 7: You said that Lambda processes an event at least once. In case your Lambda has multiple transactions, then do you need to implement like a rollback mechanism outside of the Lambda, or how would you advise to manage?&lt;/p&gt;

&lt;p&gt;Hanisch: You&#39;re asking where to handle those duplicated events, whether we want to do this in the Lambda? Personally, I would recommend to build all your handlers in a resilient and idempotent way because you never know. Generally speaking, you want to protect your system. If you would consider this whole AWS cloud as your system, you only want correct data entering your system, but there&#39;s serverless, so you cannot avoid it. There&#39;s also tooling, how you can implement that in an idempotent way.&lt;/p&gt;

&lt;p&gt;In case of AWS or with Lambda functions, you can use the Lambda Powertools. This is a very lightweight package, which does it all for you, you only have to pass in your control table, and then it&#39;s like a small wrapper, and then you&#39;re safe. There&#39;s tooling out there. Generally speaking, I&#39;d recommend you build it all in an idempotent way, because it&#39;s just more resilient. It&#39;s a very little overhead to implement it.&lt;/p&gt;

&lt;p&gt;Participant 6: In terms of migration of existing systems to serverless architectures, is there any way to migrate just part of it? Let&#39;s say you take your GraphQL, and then you put API gateway in front, or you take one domain and create a Lambda function for that domain, or how is it to migrate existing systems to a serverless architecture?&lt;/p&gt;

&lt;p&gt;Hanisch: We also have a lot of cases where we have to do that. We have on-prem applications running somewhere deep down the Siemens intranet, and we want to elevate them to the cloud. Of course, if you have a complex system, you do it all at once. You should probably have some roadmap. Because it could be easier to migrate the whole monolith to the cloud, and then within the cloud, you refactor, try to identify domains, try to identify service boundaries, and then migrate.&lt;/p&gt;

&lt;p&gt;Of course, also, this is like a step-by-step process. What we usually would do is we would have a connection from our on-prem data center to the cloud environment. There are different options how you can do that with probably any cloud provider. Then, you just allow them to communicate, and then you can move part by part to the cloud.&lt;/p&gt;

&lt;p&gt;Participant 8: Modeling with state machines and events can be a bit tricky. How do you handle when the events come out of order? If the events arrive out of order, not in the intended order that you expect, suppose you model a physical system with a state machine.&lt;/p&gt;

&lt;p&gt;Hanisch: In our use cases, we don&#39;t rely on event ordering. We just don&#39;t have that requirement that we process all events in the same order. That does not apply, at least for us. What you could do, again, AWS got your back on that. There are different tools on how to enforce or keep ordering of your events. For instance, using a certain type of queue, first-in, first-out queue, stuff like that. That way you could ensure processing all your events.&lt;/p&gt;

&lt;p&gt;Participant 8: Is the EventBridge guaranteed to deliver the events in the order they were published?&lt;/p&gt;

&lt;p&gt;Hanisch: I think they do best-effort ordering, again, with the same thing with the at least once. If you rely on strict ordering, this might not be sufficient for your use case.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/lambda-lith/</link><guid isPermaLink="false">infoq-presentations/lambda-lith</guid><pubDate>Tue, 11 Mar 2025 16:00:00 GMT</pubDate><author>Leo Hanisch</author><enclosure url="https://res.infoq.com/presentations/lambda-lith/en/card_header_image/leo-hanisch-twitter-card-1740568142902.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-sep-lambdalith.mp4" type="video/mp4"></enclosure><itunes:duration>49:29</itunes:duration><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>.NET Core</category><category>InfoQ</category><category>Event Driven Architecture</category><category>Machine Learning</category><category>Microservices</category><category>Architecture</category><category>AI, ML &amp; Data Engineering</category><category>InfoQ Dev Summit</category><category>QCon Software Development Conference</category><category>Java9</category><category>DevOps</category><category>InfoQ Dev Summit Munich 2024</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>Borderless Cloud: Designing, Implementing, and Securing Apps across Multiple Clouds</title><description>&lt;figure&gt;&lt;img alt=&quot;Borderless Cloud: Designing, Implementing, and Securing Apps across Multiple Clouds&quot; src=&quot;https://res.infoq.com/presentations/multiple-clouds/en/card_header_image/adora-nwodo-twitter-card-1740569205699.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s1_20250320073842_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-apr-borderlesscloud.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-apr-borderlesscloud.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-apr-borderlesscloud.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Adora Nwodo explores the complexities of seamlessly integrating multiple clouds into an application&#39;s architecture, deployment processes, and CI/CD pipelines.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Adora Nwodo is a multi-award winning Senior Software Engineer with 8 years of industry experience. She currently works at the intersection of the Cloud Engineering, and Developer Platforms and is passionate about the Cloud and Emerging Technologies. She is also the Vice President of the Nigerian chapter for VRAR Association.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Nwodo: I&#39;m going to be talking about borderless cloud, basically how to design, implement, and secure your apps across different clouds, multiple clouds.&lt;/p&gt;

&lt;p&gt;I&#39;m going to tell you a short story. Once upon a time there was a global company called Moota. They are a company that offers translation services for the different languages all across the world. They had millions of daily active users, from students trying to decipher their homework to international corporations that are bridging communication gaps. They run everything on a single cloud platform. This was a reliable solution at first but eventually some cracks began to appear, and the first issue that they had was redundancy. I know that this doesn&#39;t really happen a lot but bear with me for this story.&lt;/p&gt;

&lt;p&gt;One summer a heatwave ravaged the data center that Moota&#39;s servers were in, and that obviously destroyed a bunch of things for them, and their translations had to halt. Their users were frustrated, businesses were stalled, there was downtime, and obviously they lost revenue as a result of this. They also had to figure out a way to fix their damaged reputation. Then they desperately needed a backup plan. Their costs were also spiraling. Their single cloud provider offered a one-size-fits-all solution and it wasn&#39;t the most cost effective. They were paying a premium for features they didn&#39;t necessarily need for all their workloads.&lt;/p&gt;

&lt;p&gt;The surge in user traffic pushed them into expensive higher tiers, and obviously that was affecting their profit margins and squeezing them even if they were making revenue. They also had regulatory compliance issues, because as they expanded into new markets with strict data privacy laws, they had to figure out how to navigate these compliance issues in these different markets that existed because the cloud providers that they used didn&#39;t have data centers across these different places that they wanted to launch in. Their growth was stifled because of the limited service offerings. Their cloud provider excelled in basic storage and computing but lacked advanced features like robust AI for translations or analytics.&lt;/p&gt;

&lt;p&gt;They had a competitor called Polyglot that was offering real-time translation powered by AI, but they were stuck, and they were not obviously able to leverage the latest cloud innovation. They decided to go multi-cloud and it solved a bunch of problems for them. It solved the redundancy problem which I talked about earlier, because now they replicated their infrastructure across different cloud platforms, and a single outage wouldn&#39;t necessarily cripple their business anymore because you could always just route the traffic somewhere else and it makes everything easy.&lt;/p&gt;

&lt;p&gt;They were also able to optimize their costs in some way because the different cloud providers offer competitive pricing for specific workloads. They could leverage on-demand pricing models and spot instances. They could pay only for resources that they used. They were able to achieve the flexibility that they needed for optimizing their cloud spending. They were also able to achieve the compliance that they needed to achieve across the different regions and the new markets that they were trying to get deployed into. They also had different cloud services so they were not locked into a particular vendor. The move to multi-cloud wasn&#39;t obviously without challenges for them. In this talk, I&#39;m going to show you how they did it and how you can do it too if you ever want to do it.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Professional Background&lt;/h2&gt;

&lt;p&gt;My name is Adora. I&#39;m a Senior Platform Engineer currently working at SurrealDB. I&#39;m a non-profit founder. I&#39;m the founder of NexaScale. It&#39;s a non-profit helping people and connecting them to simulated work experiences. I used to work at Microsoft. I spent 4-plus years there right before joining SurrealDB. I&#39;ve won different awards across different countries. I&#39;m the author of three cloud engineering books. I&#39;ve spoken at over 150-plus conferences. I&#39;m a DJ during the weekends, and in December. I&#39;m also a content creator featured across major media. I&#39;m very interested in tech, education, and music.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Challenges When Adopting Multi-Cloud&lt;/h2&gt;

&lt;p&gt;I talked about some of the benefits of multi-cloud when I was telling Moota&#39;s story, but we can see that some other benefits that I&#39;ve talked about before, are regulatory compliance, depending on the region that you&#39;re deployed in. Enhancing redundancy and reducing downtime. You get the best of each of the different clouds that you use. You&#39;re able to avoid vendor lock-in because now you&#39;re not forced to use everything a particular vendor offers because you are only tethered to that vendor.&lt;/p&gt;

&lt;p&gt;The truth is, there are challenges when you adopt multi-cloud and there are challenges around architecture, CI/CD, and security as well. One of the architectural challenges when you adopt multi-cloud is that there is increased complexity, because when you have one cloud provider it&#39;s very easy. Especially, if you don&#39;t do things the right way, things might be tightly coupled and it might just be very hard to even figure out how you should use those cloud providers in a way that makes sense. When you have two, it becomes a bit more complicated. When you have three, more complicated. When you have more than three, let&#39;s say five different providers that you&#39;re trying to use, it&#39;s just scary.&lt;/p&gt;

&lt;p&gt;You don&#39;t even want to know what that looks like. When you&#39;re thinking of architecture as well, there&#39;s also the vendor lock-in disadvantage or the vendor lock-in challenge, which is quite interesting, because vendor lock-in is supposed to be something that you&#39;re running away from when moving into multi-cloud. The truth is, over time, an organization could actually become overly dependent on the specific service that the cloud provider is offering and they get locked into that service into that provider as well. It&#39;s always important to do your architecture in such a way that that doesn&#39;t become a problem. There&#39;s also the data portability issue as well.&lt;/p&gt;

&lt;p&gt;If you have to move data between different cloud providers, it can be cumbersome and sometimes annoying if the architectural foundation of that service wasn&#39;t properly done right. When we think about GDPR&#39;s rights to data portability as well, it can clash in multi-cloud environments. That&#39;s because there are data silos across different clouds. You could have technical challenges with data extraction and transfer. Cloud providers have different ways that they interpret different things. Data portability will pre-purpose your data and make sure it&#39;s readily available and transferable.&lt;/p&gt;

&lt;p&gt;Unlike with multi-cloud, your data might be scattered across different cloud providers and everybody has their own different data storage formats and their different access control mechanisms as well. If you want to actually gather and deliver data in a structured format while abstracting the fact that you have multi-cloud architecture from your customers, it could get a bit complicated if you don&#39;t lay the foundations right. There&#39;s also the challenges of technical data extraction and transfer, like I talked about before.&lt;/p&gt;

&lt;p&gt;If you can locate your data and you want to extract it from different provider systems, if you have to convert it to a usable format for another provider, it might be a technical hurdle as well, which is something to think about. Like I said, the right to data portability is part of the GDPR which is an EU thing, but not every provider exists in the EU, so different cloud providers may interpret that regulation differently and that could also be a problem.&lt;/p&gt;

&lt;p&gt;Like I said, there&#39;s also challenges with security. First of all, monitoring security across one environment can be chaotic enough. When you think of multi-cloud where you have different other cloud providers, it could get more complicated. That&#39;s also something to think about. Dealing with compliance challenges as well, because if you have to be compliant, let&#39;s say you have to do the SOC 2 compliance, or GDPR, or all the other kinds of compliance that you have to normally do, and you&#39;re doing it for one cloud service across different regions, that&#39;s one thing to think about.&lt;/p&gt;

&lt;p&gt;Then, when you now have to do it across multiple regions for multiple cloud services again, that&#39;s another layer of complexity that&#39;s added to what you have to deal with. In programming or tech generally, the more data centers you are deployed to, the more VMs that are running your services, the more accessible your service is the easier it is for bad actors to gain access to your system, because you have an increased attack surface.&lt;/p&gt;

&lt;p&gt;Basically, there are more entry points for them to try and get access to you. That&#39;s also something that you should think about. The final challenge when you try to adopt multi-cloud, in the context of CI/CD, is, how do we integrate our tooling to be able to deploy to these multiple clouds in a way that it doesn&#39;t affect developer experience on our team? How do we do version control? How do we make sure that these things are consistent? How do we test our application across these different clouds before we deploy to these different environments, because the fact that something runs perfectly on Azure doesn&#39;t necessarily mean that if I deploy that same thing to GCP it would run as I expect it to.&lt;/p&gt;

&lt;p&gt;When we&#39;re thinking about doing DevOps and CI/CD, and we&#39;re creating environments that mirror production so that we can test a bunch of things before we get into production, this is something that we have to think about when we&#39;re now also dealing with multiple providers as well.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Fixing Issues - Part A: Architecture&lt;/h2&gt;

&lt;p&gt;There are many ways to fix these issues. I&#39;m going to start with part 1, which is the architecture. As we can see here, imagine if we had a multi-cloud scenario where we had different user requests coming in and we had some kind of geo-routing mechanism that either maybe routes the requests coming from U.S. regions to AWS, and does the Azure routing for the EU regions. We have to think of a way to create that standardization. In the context of architecture, the first thing that we can do is use cloud-agnostic IaC. We can use tools like Terraform, we can use tools like Pulumi to define our infrastructure configuration as code.&lt;/p&gt;

&lt;p&gt;The great things about these tools is that they support these major providers that we use every day. We&#39;re not going to be having to use something like Azure Bicep, which is Azure&#39;s way of doing infrastructure automation nowadays, or Google&#39;s own Deployment Manager, and AWS CloudFormation. Imagine having to manage all these different things at the same time, it could get chaotic. If you are dealing with Terraform, one Terraform project is enough. If you are dealing with Pulumi, one Pulumi project is enough. Then you will be able to configure your AWS resources, your Azure resources, and the resources for the other clouds that you want to do as well.&lt;/p&gt;

&lt;p&gt;You also want to use cloud-agnostic tools and cloud-agnostic libraries, so programming languages that you know could be deployed across different clouds and different things. I don&#39;t know many programming languages nowadays that are not cloud-agnostic, but just in case there is any, maybe you probably want to avoid that. You want to use a tool like Knative as well for your serverless workloads as opposed to maybe using something like Azure Functions or AWS Lambda. If you are running a serverless application you probably want to find a way to run, use Knative so that you can run those workloads in Kubernetes and you&#39;re able to deploy it across your different clouds.&lt;/p&gt;

&lt;p&gt;You want to use queuing systems that are cloud-agnostic as well that you can integrate into anywhere, so things like Kafka, RabbitMQ. You want to make sure that you have an open-ended architecture basically, with a bunch of cloud-agnostic tools that you are able to bring in and plug and play whenever you want. You can also use containers to run your application so that you can run those apps across different clouds regardless of who is hosting it. You want to use databases like Surreal, MongoDB, Postgres as well that, like I said, don&#39;t belong to a particular cloud, so you are not restricted to that.&lt;/p&gt;

&lt;p&gt;You also want to implement a central API gateway to manage that external access to your applications functionalities. There are different things that you could use to route traffic, so whatever geo-routing mechanism or load balancing or traffic managing whatever, there are different mechanisms for doing that. You want to also make sure that you have that API gateway in such a way that it routes to either your U.S. regions, your EU regions depending on whatever customer is calling your application at that time.&lt;/p&gt;

&lt;p&gt;You could also think about leveraging microservices here, just so that you can break down your application into independent reusable services that perform a particular function. When you have microservices, then you could always just deploy things in containers and run those workloads across different clouds, and you would be good as well. You could also consider architectures like the event-driven architecture. Because when you&#39;re doing service-to-service communication, services aren&#39;t directly communicating with each other. There is a queue somewhere.&lt;/p&gt;

&lt;p&gt;There is a Pub/Sub mechanism, something somewhere that handles those events, and all the different services are just subscribed to those different things and can listen and pick up what they need to do to get done what needs to be done. It helps when you have different services, and if you are trying to do asynchronous programming as well because not everything exactly directly has to be real-time. One thing I want to note is that these different options that I&#39;m mentioning are different things that you can try to do. You can pick up some of them, and depending on your use case and depending on the kind of application that you&#39;re building, that would matter. You could also try to use the hexagonal architecture.&lt;/p&gt;

&lt;p&gt;This is something that people also call the ports and adapters architecture. It promotes flexibility and testability because it decouples the core business logic from the external dependencies, so the application itself would have a port and adapters can fit into that port. It creates that kind of abstraction where, let&#39;s say, I am supposed to be dealing with a database-like structure, but I don&#39;t know what the external dependencies are. I just create a read and write method, and whoever is supposed to give me the functionality provides the functionality for me to read or write without me thinking about whether it is DynamoDB for AWS or Cosmos DB for Azure, or all these kinds of things.&lt;/p&gt;

&lt;p&gt;These different hexagons in this thing shows different services using the hexagonal architecture that just fits together because their ports and the adapters basically match. Hexagonal architecture can be beneficial for multi-cloud for a couple of reasons. The first one is decoupling. Like I said, it separates the core application logic from the infrastructure, so your application doesn&#39;t really care where the data is coming from or how the thing is even supposed to be processed, or where it&#39;s stored. You can implement different adapters for the different cloud providers that you have. It&#39;s just plug and play, essentially.&lt;/p&gt;

&lt;p&gt;There&#39;s also the loose coupling thing which is the fact that when you use ports and adapters, your core logic remains independent of this specific cloud provider, going back to what I talked about before. It creates a way for you to do cloud-agnostic design, cloud-agnostic architecture if you are maybe drawing your architectural diagrams or thinking of a way to build out that system.&lt;/p&gt;

&lt;p&gt;You are not forced to build your application in such a way that it is very Azure dependent or AWS dependent because you are using ports and adapters basically, and wherever the provider is giving you whatever you need to use it for, you&#39;ve created that contract in the way that your application is supposed to interact with those providers. You&#39;ve made it in a way that it would be cloud-agnostic. This obviously is a modular design so it makes it easy for you to maintain and update your different configurations and your components within your application.&lt;/p&gt;

&lt;p&gt;Now let&#39;s talk about the data management strategy as well. It&#39;s also important to design your data layer to be cloud-agnostic. This code is an example. Let&#39;s say I have maybe an application that is supposed to write to a database, but because I&#39;m using multi-cloud I have different kinds of databases, and let&#39;s say the application for the different regions that I operate in, I&#39;m not supposed to cross-pollinate for whatever reason. I&#39;m using Azure in the EU, I&#39;m using AWS in the U.S., I&#39;m using GCP in Asia, or whatever, so I don&#39;t even have to think of data replication mechanisms across because I don&#39;t need to.&lt;/p&gt;

&lt;p&gt;One thing I need to do is make sure that I&#39;m able to provide the functionality for my different applications to read from whatever database depending on where the user&#39;s call is coming from. I can create a data access interface and have my different providers implement that interface. Maybe for Azure I could have a Cosmos DB interface, and I can implement all my get users, get user by ID, save user, all the different functions that I need. I can do the same thing for Postgres. Let&#39;s say I use Postgres in the U.S., for example, I can do the same thing for Postgres.&lt;/p&gt;

&lt;p&gt;How you know when to call whatever database, depending on how you deploy the application, whether you deploy the application through Kubernetes or you deploy it in a regular web API, you can have configurations. You can have configurations that allow your application to run. In one of those configurations, you could add that the cloud provider is Azure, or the cloud provider is AWS, or the cloud provider is GCP, or Cloudflare, or whatever the provider is.&lt;/p&gt;

&lt;p&gt;Now, in the startup of your code, and this is like C# code, so depending on whatever programming language it is that you&#39;re using, it would differ a little bit but the concept is still pretty much the same. When you are doing all your dependency injection and thinking about what service to add into your DI Container, you can see that I&#39;ve said that if the cloud provider is Azure then I want to add the Cosmos DB user data adapter as the implementation to that interface.&lt;/p&gt;

&lt;p&gt;That&#39;s what I&#39;m going to use throughout every instance for that application. If it&#39;s not Azure then I want to use Postgres instead. This is a way to just make sure that you have a dynamic data management strategy as well. You have different implementations because you have different providers, and you&#39;ve written your code in such a way that your data layer is not tightly coupled to any data provider.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Part B: Security&lt;/h2&gt;

&lt;p&gt;The next thing is security. It&#39;s important to have centralized logging and monitoring. You want to make sure that you have security information and events management systems so that you can aggregate logs and security events from all your different providers, and monitor them. You also want to make sure that you use cloud native security tools. As much as it&#39;s important to use cloud-agnostic tools, I would always advocate for using cloud native security tools. It just requires that you do a lot more work. The reason for that is that these tools have been designed to work seamlessly with the particular platform.&lt;/p&gt;

&lt;p&gt;Obviously, Azure security tools will work fantastic on Azure as opposed to a third-party tool from somewhere. Same thing with the AWS security tools as well. You want to have those things also integrated into your systems. You also want to make sure that your application&#39;s network traffic is segmented, and you&#39;ve isolated sensitive components so that there is no lateral movement of malicious actors, like I said. Because you now have different places that your application is being deployed to, there&#39;s a possibility that you would have a larger attack surface. You want to make sure that you have like VNet, subnets, and you&#39;ve done your network segmentation in a way that is not just going to be easy for traffic to hop from one provider to another, because sometimes that traffic could be malicious traffic.&lt;/p&gt;

&lt;p&gt;Obviously, it&#39;s important for your security team to follow security best practices for network configurations generally. I would always advocate for policy as code, because I&#39;m an infrastructure engineer. Right now, I&#39;m a platform engineer, but policy as code helps you have consistent security policies, because you can now define your security policies as code in terms of like even naming conventions, or you want to force certain things within your application, you want to have automated enforcement as well. You want to make sure that you&#39;ve enforced certain things, and you can easily audit things.&lt;/p&gt;

&lt;p&gt;For policy as code, let me just give a random example, let&#39;s say you only want it to be possible that your Azure storage account only permits HTTPS traffic. If you have policy as code, and when you are doing your infrastructure declarations, you&#39;ve set it up in a way that before your infrastructure gets to the point where it&#39;s being provisioned, it passes through the policy engine. The policy engine checks that everything is right with whatever infrastructure you&#39;ve created before it goes ahead to do the whole deployment thing. Maybe engineers on the team have worked with the security team, and they&#39;ve said, we want to make sure that storage accounts on Azure only permits HTTPS traffic.&lt;/p&gt;

&lt;p&gt;If I create a new storage account in my infrastructure as code, and I accidentally forget to enable the HTTPS traffic thing, my infrastructure will not get created. That&#39;s a way to make sure that when a security team gives the platform team security things that they have to do, those policies they have to enforce, you&#39;ve written code that can automate the enforcement of those things for you, as opposed to going manually and checking these things out yourself.&lt;/p&gt;

&lt;p&gt;For general security as well, you want to have security automation. You want to make sure that you train people on your team about these things. You want to run regular security audits as well so that you can be proactive as well as reactive to security incidents when they happen.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Part C: CI/CD&lt;/h2&gt;

&lt;p&gt;The final part is the CI/CD part. You also want to make sure that you&#39;re using standard CI/CD tools. You want to choose a CI/CD tool that supports multi-cloud deployments. Because you don&#39;t want to have different CI/CD pipelines as well to monitor, or different providers for doing CI/CD, because the goal is to move to multi-cloud in a way that makes it easy for everyone else on your team. You want to use things like Jenkins. I know that with Azure DevOps and integrating multi-cloud extensions into Azure DevOps, you can actually deploy to other providers. Same thing with AWS as well. You want to have these things in your CI/CD tool so that it&#39;s easy for you to do these deployments.&lt;/p&gt;

&lt;p&gt;For your CI/CD, again, you want to make sure that you&#39;re using cloud-agnostic infrastructure as code tools. I&#39;ve said this thing before, I&#39;m saying it again. It&#39;s better to use things like Terraform or Pulumi as opposed to using AWS CDK and Azure Bicep because it&#39;s just harder. Another thing that is very important as to why I would say use cloud-agnostic tools, is because you&#39;d only have to train your team once, essentially. If you train them on how to use Terraform or you train them on how to use Pulumi, everyone is fine. As opposed to training them on how to use CDK, then you train them on how to use the Google version, and then you train them on how to use Azure Bicep.&lt;/p&gt;

&lt;p&gt;You are wasting engineering effort as you do that. It&#39;s from a technical point but also from an optimization point on your team, especially if you&#39;re a manager, that this makes sense. Containerize your applications as well. You want to use container technologies like Docker. I don&#39;t know if there&#39;s any other one because it&#39;s only Docker that I&#39;ve known all my career, so that you can have a standard way to package your application and your application dependencies and deploy it across the different multi-clouds that you&#39;re using.&lt;/p&gt;

&lt;p&gt;I was talking about CI/CD pipelines just now. This is a CI/CD pipeline, a sample pipeline. This is an Azure DevOps pipeline actually, that you can use to deploy to Azure and deploy to AWS. What&#39;s happening here is that in the first image, it&#39;s installing all the npm dependencies, building out the application and publishing the artifacts. It&#39;s a function. It&#39;s supposed to be like a TypeScript function. It&#39;s deploying that function to AWS Lambda and then it&#39;s going to do the same thing on Azure Functions as well.&lt;/p&gt;

&lt;p&gt;For pipelines, there&#39;s different things to consider. Do you want to have environment-specific pipelines? Do you want to separate your CI/CD pipelines for different environments? Maybe your development environment, your staging environment: if you have a pre-production or a testing in production, people call it different things, testing in production, integration, depending on the company that you&#39;re at. If you want to split your pipelines into environment-specific pipelines so that you can manage those things, I would always advocate for that because it allows you to actually control your deployments.&lt;/p&gt;

&lt;p&gt;It minimizes the risks of introducing bugs into production because you have it split and it&#39;s easier to manage. You also want to use cloud provider-specific tools. Whatever tool that you end up using, you want to make sure that the specific tools that are for your cloud providers and those features and integrations, you have them in your chosen CI/CD tool so that you can have the functionality to even do your deployment in a better way and have a better developer experience generally. You could also consider unifying your build and test phase because that necessarily shouldn&#39;t change.&lt;/p&gt;

&lt;p&gt;How you build the application is how the application is supposed to be built, and how you do unit testing, so this should be unifying your build and unit test phase. How you do that unit testing where you&#39;re just testing the different components in your application as individual units, and when you build the app, don&#39;t necessarily change regardless of the platform that you&#39;re on until you have to start deploying to different platforms and running end-to-end tests or other kinds of tests before you do your deployment. You want to also think of a way to unify that and then having the other multi-cloud things differently so that you have less things to worry about.&lt;/p&gt;

&lt;p&gt;This will be my recommendation, you could think about things differently if you&#39;d like, but I would always recommend a hybrid pipeline because it reduces complexity so you can easily set up and manage your pipeline at the very beginning, if you go the hybrid way. It&#39;s flexible so you can customize and tailor your deployments with phased rollouts and manage that entire process. It also offers easier maintenance compared to complex single pipelines that you have while enabling some code reuse through shared build stages. This is an example of what a hybrid pipeline looks like. Let&#39;s say I&#39;ve done my deploy to Azure in a different YAML file. I&#39;ve created that template.&lt;/p&gt;

&lt;p&gt;Then I have a deploy to AWS as a different file as well. I&#39;ve created that template. This is my main template. Now I can have the jobs that deploy to AWS in development, the jobs that deploy to AWS in staging, Azure to dev, Azure to staging, and a lot more. I can break it down further if I feel like I also want to split it more, maybe doing all my dev things only in a dev place and having those templates only address those things.&lt;/p&gt;

&lt;p&gt;Typically, this is what it would look like where your build and your test phase is the initial stage, like I said, and when I was recommending that your build and test phase should be unified, sort of. Then branches out depending on the many cloud providers that you are deploying to. You can manage each step across the different branches.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;The Phases for Multi-Cloud Migration&lt;/h2&gt;

&lt;p&gt;Finally, I&#39;m just going to be ending with the phases for multi-cloud migration because I&#39;ve talked about what you need to do, but I should also maybe talk about how you should do it. There are four things here. You should plan, prototype, pilot, and then go to production.&lt;/p&gt;

&lt;p&gt;When you&#39;re planning, the first thing you need to do is define your goals. What are your objectives for adopting this multi-cloud strategy? What business problems are you trying to solve? Are you aiming for cost optimization? Are you aiming for redundancy? Do you want to tap into new markets and you&#39;re thinking of a way to do that because you know that some providers are a better fit in some regions? What exactly is your goal? You want to evaluate the different cloud providers that fit into the goals that you&#39;ve set and consider factors to making a decision for the different cloud providers that you want to choose.&lt;/p&gt;

&lt;p&gt;Then you want to now, based on what we talked about earlier, do your whole architecture part of that. Design your application architecture, come up with a well-defined architecture for your service, while thinking about security as well, because that&#39;s very important. Once you&#39;ve done that plan, what follows is some proof of concepts. A small-scale prototype for that multi-cloud deployment so that you can test the new architecture that you have, the deployment methods that you&#39;ve chosen, and how you plan to simulate security monitoring and a bunch of all these other things as well. A small proof of concept that you can test and just understand what needs to be done.&lt;/p&gt;

&lt;p&gt;Then, when you are obviously prototyping, you encounter challenges. Most times things are not perfect at the first step, which is fine. This is the time for you to use the opportunity to identify the potential issues with performance, integration, security, whatever issues that you have, and address them before you roll out to wider audiences, or to everybody in fact. Then based on that, you want to refine your plans, like document your learnings from the prototype actually, because you would need to.&lt;/p&gt;

&lt;p&gt;Sometimes it might mean that you want to adjust your cloud provider choices. It might mean that you want to change your architecture. It might mean that you want to think of a new strategy for doing security. It might mean that your CI/CD is not as robust as you thought it was and you want to try something else. What you identify, refine, and go from there.&lt;/p&gt;

&lt;p&gt;Then the next thing is piloting. You want to do some limited deployment. You want to deploy a pilot project to some users within a specific area, and test your multi-cloud setup, beyond a simulated thing that we did before in the proof of concept, but test it in a more realistic setting and figure out what happens.&lt;/p&gt;

&lt;p&gt;You might run into issues again, but at least this time, because you are not doing it on a large scale that you can&#39;t control, you are able to minimize whatever disruption happens. You also want to monitor performance. You want to check metrics like the latency, the reliability. When I deploy to a new cloud service, if I have 1,000 API calls in 10 minutes, in terms of success or reliability, what&#39;s the 90th percentile, what&#39;s the 95th percentile? How many of these API calls actually succeed? How many of them actually fail? If your SLA that you&#39;ve promised your customers in the grand scheme of things is that you should be able to give them some kind of, maybe you said two nines, so you&#39;re 99% reliable.&lt;/p&gt;

&lt;p&gt;Somehow, when you are testing for performance, you realize that 50% of your API calls are failing because maybe there are some things you&#39;ve not set up correctly. This is a chance for you to fix that. Gather feedback from the users as well that are involved in that pilot. Whether it&#39;s your Canary customers or some private preview users that you have or different kinds of things, you want to get that feedback from them because this rollout obviously is a phased rollout. You don&#39;t just wake up and say you want to go multi-cloud.&lt;/p&gt;

&lt;p&gt;It might take a very long time, months to probably even a year or one year plus, depending on your staff strength and the scale you are going, and things like that. You want to also collect feedback from them and improve on that before you now say, I want to be generally available for everybody.&lt;/p&gt;

&lt;p&gt;Then, that&#39;s when you get to the production stage. Now, you are still doing your phased rollout, but you are gradually migrating your workloads slowly to a bigger audience because you can&#39;t obviously do everything in a day. You still do it in phases, but now with the goal of hitting that larger or global market like you&#39;ve planned. You want to make sure that you consistently and continuously monitor your multi-cloud environment for performance, for security, for availability, for cost optimization opportunities as well. Because you don&#39;t want to start doing multi-cloud and then you realize that you could have probably just stayed single cloud because the cost optimization you were hoping for, you didn&#39;t get. You want to make sure that happens.&lt;/p&gt;

&lt;p&gt;Then, you want to improve things as you go because nothing is always perfect in the first time. I think if things were perfect in the first time, maybe we wouldn&#39;t need the app store because we&#39;ll just go to the websites once and download the original version of the app and that would be it. There&#39;s always room for improvement. It&#39;s something to think about. Having documentation on the team for new engineers that join to know how things are, how things are running. Having clear processes laid out. These are things that happen in these improvement steps as well. Documenting things, learning things, and just further refining things as you go is something that is very important.&lt;/p&gt;

&lt;p&gt;When you follow these 4Ps, and you adopt a methodical approach, you can increase your chances of successfully integrating and managing a multi-cloud environment that meets the needs of your business. I just want to leave you with this final thing, which is that a well-planned and well-executed multi-cloud strategy can unlock benefits for you such as increased agility, stability, redundancy, and cost efficiency for your teams and for your organization.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant: Your background is in infrastructure as code, in Pulumi, in Microsoft, and platform engineering. From that time, what do you think is the hardest part when somebody is considering going through this journey to multi-cloud? Are there any aspects around, whether policy, security, identity?&lt;/p&gt;

&lt;p&gt;Nwodo: I think for me, I would think that the hardest part is actually the security part. Especially when you&#39;re dealing with identity and access management, and you have to do things like JIT access, for example. JIT access means like just in time access. I want to make sure that not everybody has access to production resources or any resources at all. They get access when they need it, for the specific thing they need it for, and for a stipulated time. When you have to do this thing across different cloud providers, it means you have to manage different kinds of permissions. You need to write different kinds of rules. It gets very complicated.&lt;/p&gt;

&lt;p&gt;If you don&#39;t manage it properly, it could be problematic as well. The thing for me that would probably be the hardest is the security side. This is just my opinion, because at the end of the day, somebody else might have a different experience. That might be because the part of security that I engage in mostly is the compliance side of things. Maybe that&#39;s why I would find it the most challenging. Somebody else, for them, it could be the fact that they have to think about managing different branches in their CI/CD pipeline.&lt;/p&gt;

&lt;p&gt;For me, it&#39;s the security thing mostly, because you have to think of all these things, managing access. Because, like I said, it&#39;s important to have the cloud provider&#39;s security specific service because it was built for those services, so it&#39;s better. When you have to manage different kinds of security services for different kinds of providers, it gets challenging.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/multiple-clouds/</link><guid isPermaLink="false">infoq-presentations/multiple-clouds</guid><pubDate>Mon, 10 Mar 2025 16:00:00 GMT</pubDate><author>Adora Nwodo</author><enclosure url="https://res.infoq.com/presentations/multiple-clouds/en/card_header_image/adora-nwodo-twitter-card-1740569205699.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-apr-borderlesscloud.mp4" type="video/mp4"></enclosure><itunes:duration>41:54</itunes:duration><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>.NET Core</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>QCon London 2024</category><category>Infrastructure as Code</category><category>Java9</category><category>Cloud</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>The (Not So) Hidden Social Drivers behind the Highest Performing Engineering Teams</title><description>&lt;figure&gt;&lt;img alt=&quot;The (Not So) Hidden Social Drivers behind the Highest Performing Engineering Teams&quot; src=&quot;https://res.infoq.com/presentations/trust-psychological-safety/en/card_header_image/lizziematusov-twitter-card-1740043411740.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s1_20250320073842_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-nov-hiddensocialdrivers.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-nov-hiddensocialdrivers.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-nov-hiddensocialdrivers.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Lizzie Matusov delves into how trust and psychological safety serve as powerful signals of team success with practical methods to evaluate and measure these key social dimensions.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Lizzie Matusov is co-founder and CEO of Quotient, a developer tool that surfaces the friction slowing down engineering teams and resolves it directly. She previously worked in various engineering roles at Red Hat and Invitae, and has an MS in Engineering Sciences and MBA from Harvard.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon San Francisco empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Matusov: My name is Lizzie Matusov. My goal is to make you walk away thinking a little bit differently about what drives the highest performing engineering teams. When I was at Red Hat, I had a pretty interesting job. I was a software engineer, but I worked in our consulting arm, which basically meant every six or so months, I would get together with a new group of engineers, and we would have a product that we were working to deliver. We would go through the forming, storming, norming, and performing process as we figured out what were the processes that worked best for us to deliver our products over the finish line. We would do really great work.&lt;/p&gt;

&lt;p&gt;Then, six months later, we would deliver our product, completely disband and start again with a new team. In running that process as many times as I did, I came to understand that social dynamics, they were driving our performance quite a bit. As I continued on in my career, I realized that those social drivers were critical towards understanding the performance of our engineering teams. To take a step back for a second, it&#39;s actually no surprise that social drivers impact how we work in general. We&#39;re human beings, and that means, by nature, we&#39;re quite social. Social interactions really govern how we see the world. It&#39;s no surprise to know that those social drivers also impact how we perform in situations like at work.&lt;/p&gt;

&lt;p&gt;One of my favorite examples away from the engineering world of just how important social drivers is, is through, &quot;The Bear&quot;. It&#39;s a show about a restaurant that goes through this transformation from a casual diner to a high-end restaurant. In one of the most famous scenes of the recent season, it&#39;s one of the first nights of operation of this high-end restaurant, and things are really moving. It&#39;s a very high stakes night. From the front of house, guests are being served these fantastic, high-quality dishes in a dimly lit setting, and it&#39;s really a great experience.&lt;/p&gt;

&lt;p&gt;From the back of house, the tension is starting to build with the people that work as the chefs. They&#39;re supposed to be working together as a team, and they&#39;re delivering good work, but the social dynamics between them is a little bit tense. Pressure is growing, and as it mounts over the course of the night, things start to amplify. Finally, the head chef gets locked in a freezer, is getting screamed at by his teammate from the other side, and is then left there for the rest of the night. Not how you want things to go on a team.&lt;/p&gt;

&lt;p&gt;It&#39;s actually a really important lesson here, because without knowing the social drivers that impact a team, we don&#39;t know how our team is going to perform under different circumstances. Those chefs, as an example, were incredible at their work, and they were outputting high-quality products, but this was a volatile situation because of their social dynamics, and it could really impact the performance of the restaurant. That same principle applies to engineering teams. I could show you a graph of speed versus quality, and you could look at this and say, this is an engineering team that&#39;s pretty performant. On the left-hand side, you&#39;ve got velocity, which is steadily improving over time.&lt;/p&gt;

&lt;p&gt;On the right-hand side you have quality metrics, like change failure rate, which is telling you that the team is deploying regularly with minimal impacts to production and their builds. This would be what we would call an elite performing team. If I showed you these graphs, you&#39;d suddenly think a little bit differently about this team. This is a team that has ever-decreasing psychological safety and heavily increased risk of burnout. This is a team that&#39;s on the edge of breaking. What you see on those top two graphs is not going to last if things don&#39;t change. You might be looking at these graphs and thinking, Lizzie, that&#39;s great, but capturing and analyzing this type of data, it&#39;s not an easy feat.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Confidence by Measurement Cadence&lt;/h2&gt;

&lt;p&gt;Also, let me tell you why this is important. We took a sample of responses from about 30 companies and asked people to tell us, how often do they measure social drivers and how confident are they in their understanding of their team&#39;s productivity? What you&#39;ll see is that the teams who measure social drivers on a quarterly or monthly basis are more likely to feel confident that they understand their team&#39;s productivity.&lt;/p&gt;

&lt;p&gt;It&#39;s not a guarantee, but what this shows us is that without doing some regular measurement of the social drivers on your team, you are just not likely to feel confident in your team&#39;s productivity. In this talk, I want to show you what matters, how to capture that data, and then the steps you can take today to get started.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Timeline&lt;/h2&gt;

&lt;p&gt;We&#39;re going to first introduce a framework that&#39;s called TAPPs, that covers the most important social dynamics that you should know to understand the performance of your engineering team. Then we&#39;re going to talk a little bit about the measurement. We&#39;ll get into the who and the how. Third, we&#39;ll talk about how to get started.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;The TAPPs Framework (Setting the Scene)&lt;/h2&gt;

&lt;p&gt;Before we get into those dimensions, I want to set the scene. This is probably a situation that many of you have been in before, so it might sound a little bit familiar. Let&#39;s say that we&#39;re thinking about a software engineering team, and they&#39;re working towards a big launch. This is a really important launch, and it&#39;s going to drive a lot of revenue for the company, so no pressure, but a lot of pressure. They&#39;re getting close to the finish line.&lt;/p&gt;

&lt;p&gt;The launch plan has been written. We&#39;re on to the final stages of development, and suddenly somebody discovers a bug. We don&#39;t know what the impact of that bug is. We don&#39;t even know really the severity. We just know that it exists. The question is, what does that teammate do? I&#39;m going to walk you through how you should think about that. I&#39;m going to continue the story using what I call the TAPPs framework, or the top four social drivers that you can use to understand the performance of your engineering team, which is trust, autonomy, purpose, and psychological safety.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;1. Trust&lt;/h2&gt;

&lt;p&gt;Let&#39;s start with trust. Trust is the belief that the people you work with are on the same page as you. It&#39;s knowing that the teammates that you work with are going to deliver on their commitments, share honest feedback, and support each other&#39;s work. How does that story go when you have low trust versus high trust? In a low trust setting, the teammate who discovers the bug, they&#39;ll share what they discover to their team, but they&#39;re just not sure that they believe the team is going to take it as seriously as they need to. They&#39;ll tell the team, but they&#39;re also going to go ahead and do their own discovery, their own analysis, their own research, and maybe form their own conclusion.&lt;/p&gt;

&lt;p&gt;The rest of the team is probably going to do the same thing. They&#39;re thinking, let me find out for myself, because I&#39;m not sure that I trust my team to have the right assessment of what&#39;s going on. What you just had was a lot of redundant work and a lot of wasted effort. If the bug is not high impact, then what that means is the team just wasted a ton of cycles all doing the same amount of research to come to that same conclusion. If the bug is significant, then all that time that was spent doing redundant work could have been spent helping find a solution. We&#39;re also just assuming as well that the team trusts that the individual has the right assessment of a bug.&lt;/p&gt;

&lt;p&gt;There just as well could be a case where everybody says, &quot;I&#39;m not sure that I even trust this as a bug, because you were the one that shared it&quot;. It&#39;s not a great situation. Let&#39;s play out a high trust scenario. In a high trust scenario, the teammate will share what they&#39;ve discovered and call together a meeting. They&#39;ll do some initial diagnosis work together, and then they will break up to do various pieces of the discovery, the analysis, and the triaging. Then the team comes together to knowledge share.&lt;/p&gt;

&lt;p&gt;Now they get to learn from one another, they get to pool their knowledge, and they&#39;re able to much more quickly identify and resolve the issue. If it&#39;s not an issue, then we probably found out pretty quickly, and if it was an issue, the team came together quickly in order to find a solution. Trust leads to open communication, faster problem solving, and less rework. It&#39;s what allows the team to break up, work independently, and then come together and share in what they&#39;ve learned, knowing that they understand the whole team is on the same page.&lt;/p&gt;

&lt;p&gt;What&#39;s some of the research behind it? In 2019, a team of researchers at Google asked over 600 developers across three different companies to identify what factors impacted their productivity most. What you&#39;ll see in this graph is the top factors, starting from F1 moving down to F10, that predicts engineers&#39; productivity. Highlighted in yellow are the factors that relate to how well they trust their team, which is 40% of the top 10 factors. These are things like, people on my project are supportive of new ideas, or people who write code for my software are highly capable. These findings show us that trust is imperative in the productivity of software engineers.&lt;/p&gt;

&lt;p&gt;The impact of trust is extremely important when it comes to the performance of a team. For team performance, you heard me share earlier about how improves collaboration on the team, which helps makes the team much more productive. From a product performance standpoint, you see increased product reliability and other downstream effects like decreased deployment time and decreased change failure rate. What&#39;s incredible is that the DORA report in 2022 and 2023 found that a high trust culture correlated with a 30% increase in organizational performance.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;2. Autonomy&lt;/h2&gt;

&lt;p&gt;Now that we know that trust is a massive driver of performance on engineering teams, let&#39;s talk about autonomy. Autonomy is the ability of software engineers to make decisions independently about their work. The whole team is going to have a clear sense of alignment on goals and boundaries, but within the team, teammates feel empowered to figure out what makes sense, how to prioritize their work, and how to achieve their team&#39;s goals independently and how they see fit. Let&#39;s consider the story here. In a low autonomy ending, the team is often inhibited by process or permissions, which impacts their own ability to quickly triage and determine the severity of the bug.&lt;/p&gt;

&lt;p&gt;This new engineer, or this engineer that comes in and discovers this bug might have some ideas about what it could be, but instead needs to pass it over to a formal channel, instead of independently taking those first steps to discover how severe and what the impact could be. That means there&#39;s a delay in determining how important this issue is. If it&#39;s not a big issue, then there was a small lag. If it is a big issue, then it&#39;s possible that that was just sitting in a queue waiting to be looked at by a formal process, when that engineer could have gotten started understanding what the impact is and come with a much richer discussion. Let&#39;s look at a high autonomy situation.&lt;/p&gt;

&lt;p&gt;In a high autonomy situation, the teammate who discovers the bug is going to decide that this is an important thing to look into to understand the potential impact. They&#39;ll probably prioritize it over another task that they&#39;re doing, because they think that the impact could be big. What happens is, if this is not an issue, then they just saved the team from spinning their wheels over something that wasn&#39;t a concern. If it is a big issue, then they&#39;re going to come into the conversation with a lot more information about the impact, the severity, and what they should do next, which allows the team to move much faster.&lt;/p&gt;

&lt;p&gt;Autonomy is what gives engineers the power to solve problems faster. As engineers, one of our strongest skills is in our ability to solve problems. Autonomy is really the foundation that allows us to do our best work. Let&#39;s talk about the research. I first want to bring back this study that we just talked about before. You remember how I told you that 40% of the top 10 factors related to trust? Another 30% of them relate to autonomy. Things like, my job allows me to make decisions about what methods I use to complete my work, or, my job allows me to use personal judgment in carrying out my work. There&#39;s another great study that we can look at too.&lt;/p&gt;

&lt;p&gt;In 2017, researchers asked software engineers and managers to identify and rank what makes a great manager of software engineers. On the left you&#39;ll see what&#39;s called a violin plot, which shows you the top ranked answers from top to bottom. For each attribute, you&#39;ll see the wavy line shows you the distribution of responses for engineers on top of the line and managers below the line. Then the thick horizontal lines will show you the interquartile range. The vertical line shows you the mean. The most important thing you should take away from this, though, is that enables autonomy was number three.&lt;/p&gt;

&lt;p&gt;More interesting thing is that it really had the highest consensus equated from both engineers and managers on being highly important. Basically, engineers and their managers agree that autonomy is foundational for the performance of engineering teams. The results really speak for themselves. Autonomy empowers engineering teams to increase collaboration and improve delivery cycles. In fact, high-performing teams are two times more likely to have high levels of autonomy than low-performing teams. Behaviors of autonomy also just lead the team to move faster, as we heard in our story above.&lt;/p&gt;

&lt;p&gt;High autonomy teams are 1.4 times more likely to achieve high deployment frequency and lower lead time compared to people with less autonomy. Perhaps most interesting is that when you give teams a shared goal and the teammates have the space to choose their way of aligning with that goal, it actually builds strategic alignment. You&#39;re giving people the space to say, this is the goal, and how you help us get there is your own way. That&#39;s really important for helping to build organizational alignment towards the same goals for the year.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;3. Purpose&lt;/h2&gt;

&lt;p&gt;We&#39;ve talked about autonomy. Now let&#39;s talk about purpose. I&#39;m sure we&#39;ve all felt the contrast of going from a place where you&#39;re just clocking in and clocking out, and it&#39;s a 9:00 to 5:00, and we&#39;d like to be done with the day, to a team and a company where you feel like what you&#39;re doing really matters. That feeling is purpose. It&#39;s that clear shared understanding of why a team&#39;s work matters and how it aligns with broader organizational goals. Let&#39;s talk about a low purpose ending. You guys have seen, &quot;Office Space&quot;. I think that is one of the most fantastic examples of a team that has very low sense of purpose. In this world, an engineer discovers a bug and they feel indifferent.&lt;/p&gt;

&lt;p&gt;Or worse, they&#39;re annoyed, because they&#39;re like, &quot;Now I&#39;m going to have to work late tonight, and I just really didn&#39;t want to be here in the first place&quot;. They&#39;ll probably communicate the issue, but it&#39;s not going to be with the same sense of urgency. The team is probably not going to find the most optimal solution to resolve it, because, quite frankly, if the team has a low sense of purpose, then they don&#39;t really care about finding the best possible outcome for their users. You can see in my description how product quality suffers.&lt;/p&gt;

&lt;p&gt;When a team cares a lot about the impact of their work, they&#39;re going to make sure that that work speaks for itself. In a high purpose environment, the engineer and the team will quickly come together to minimize the impact to their project and keep the team marching towards the deadline. They&#39;re going to come up with creative solutions to minimize impact to their end users, because they really care about delivering something for their customers. They&#39;re going to thoroughly report, advocate clearly, and resolve effectively, again, because they care.&lt;/p&gt;

&lt;p&gt;Purpose matters, because it aligns engineers&#39; work to the customers that they serve. There&#39;s actually some really interesting research about this. The 2024 DORA report found that with high user-centricity, delivery throughput did not correlate with product performance. Let me take a step back and talk about what that means. First off, I&#39;ve talked about the DORA report a couple of times, but I realized I didn&#39;t level set what it is.&lt;/p&gt;

&lt;p&gt;The context is, the DORA report is a study that&#39;s performed by the DevOps Research Assessment group at Google, where they collect data from over 30,000 engineering leaders across the world, and then they use that to understand what are the drivers and signals of high-performing engineering teams. This year, what they found is that actually in teams with higher user-centricity, which means higher alignment to their users, delivery throughput, or how fast features get delivered, actually doesn&#39;t correlate with product performance. This means that even if you deliver at half the throughput, it doesn&#39;t necessarily mean that your product is going to be better, as long as you have high user-centricity.&lt;/p&gt;

&lt;p&gt;That doesn&#39;t intuitively make sense, because we know that rapidly delivering features to your customers usually builds better products. Why does that happen? In the words of the researchers, there&#39;s no longer a disconnect between the software that&#39;s developed and the world that it lives. The researchers believe that it&#39;s because when a team feels a sense of purpose towards their work and the users that they serve, they&#39;re always going to be pointed in the right direction of delivering value. Purpose matters. It&#39;s what makes teams perform better. It makes them excited about their work. It improves both their collaboration and their job satisfaction.&lt;/p&gt;

&lt;p&gt;It&#39;s what improves both the stability, the reliability, and the performance of the product. Then, from an organizational standpoint, it&#39;s what keeps the organization together and reduces attrition. Teams want to work on products that they believe really matter.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;4. Psychological Safety&lt;/h2&gt;

&lt;p&gt;Finally, I want to talk about psychological safety. I want to talk a little bit about what it is as well as what it isn&#39;t, because it&#39;s a word that gets thrown out quite a bit, and I want to make sure that we&#39;re clear on what it means. What is psychological safety? It&#39;s the belief that a teammate can take interpersonal risks without fear of negative consequences. Things like speaking up, asking a question, admitting that something went wrong. It&#39;s what allows team members to do that in an environment where they feel safe and comfortable doing that. There are some really important nuances here that I just want to make sure that we get right. Let&#39;s talk about what it&#39;s not.&lt;/p&gt;

&lt;p&gt;Dr. Amy Edmondson, a Harvard professor who really brought this term into mainstream popularity, once said, this term implies to people a sense of coziness, that we&#39;re all just going to be nice to each other. That&#39;s not what it&#39;s really about. What it is about is candor. It&#39;s about being direct, taking risks, and being willing to say, I screwed that up. It&#39;s a common and very misguided understanding of the team when people say things like, psychological safety is comfort, or that it&#39;s a soft type of team. Let me walk through how it manifests on teams. I&#39;m going to continue our story here, so that way you can really see how it would play out.&lt;/p&gt;

&lt;p&gt;An important additional factor I want you to consider is accountability, because the way psychological safety manifests, depends on how much accountability is present on the team. Let&#39;s say we&#39;ve got a team, this team that we&#39;re talking about has both low psychological safety and a low sense of accountability. This is what we would call the apathy zone. There aren&#39;t really repercussions for mistakes. Teams don&#39;t really have that much support. What happens is people actually struggle to care about their work because they feel a little bit disengaged. It honestly sounds a lot like a low sense of purpose.&lt;/p&gt;

&lt;p&gt;It&#39;s actually true that if you have low psychological safety and low accountability, you&#39;re also likely to have a low sense of purpose. If the bug is identified, it&#39;s probably not going to be treated with much of a sense of urgency, but there&#39;s also not a lot of accountability in doing that. Let&#39;s say this team has a high sense of accountability and still, low psychological safety.&lt;/p&gt;

&lt;p&gt;This is what we would call the anxiety zone. It&#39;s that fear of humiliation or punishment or blame that keeps the team from working effectively. If you&#39;ve ever experienced a situation where you want to bring something up, but you really don&#39;t want to sound stupid, or you want to mention that this bug is happening, but you don&#39;t want to out your teammate and have them get in trouble for it, that sounds a lot like this anxiety zone.&lt;/p&gt;

&lt;p&gt;In the case of our team here, the individual who discovers it, they might try and scramble and solve it themselves, cleaning it up so that nobody else sees, mostly because they don&#39;t want to be associated with that bug. If they bring it up, they&#39;re worried it&#39;s going to be a blame game. They don&#39;t want whoever maybe caused the bug to get in trouble either. We&#39;re focusing a lot on who did it and why are you bringing this up, as opposed to, how do we actually solve for it? That&#39;s a huge communication failure that keeps the team from focusing on what matters, which is solving this bug before we have this launch.&lt;/p&gt;

&lt;p&gt;Let&#39;s talk about higher psychological safety. Let&#39;s say we&#39;ve got high psychological safety, but we have low accountability. This is often what people think about when they talk about psychological safety, and they use it in an incorrect context. In this case, the team does work really well together. They have comfort. They know that they can take risks and speak up, but they don&#39;t really have a push to be held accountable for it. They&#39;ll feel comfortable communicating that issue. They&#39;ll triage the impact, and they&#39;ll find a solution, but there&#39;s not really that sense of urgency placed on the team.&lt;/p&gt;

&lt;p&gt;The good news here is that the team does like working together, that&#39;s always great. They know that they can take risks. The bad news is they feel no reason. They&#39;re not compelled to take any risks. The real magic happens when you have both high accountability and psychological safety. The team moves fast. They&#39;re focusing their problems on what to do and how to solve, instead of looking back at who did this or why would you ask that. In our story, this would be the team that would work quickly to identify the impact, put out a solution, and would never put an individual to blame in the process.&lt;/p&gt;

&lt;p&gt;The team has the opportunity to learn from their experience moving forward, and they&#39;ll be compelled to do better next time from the lessons that they&#39;ve learned. These are the teams that move quickly, perform well, and learn a lot. Psychological safety is the driver behind taking calculated risks, learning quickly, iterating, and building the most innovative solutions.&lt;/p&gt;

&lt;p&gt;For the research here, I want to talk about an extremely impactful and one of my favorite research studies on workplaces, to really highlight just how critical psychological safety is. In 2012, Google endeavored to discover, what is the most performant engineering team? They looked at about 180 teams internal to Google to understand, why do some teams consistently perform better than others. They looked at factors like team size, composition, management style, individual skill sets, so many things.&lt;/p&gt;

&lt;p&gt;For a while it was just noise, until they started thinking, what if we measure the dynamics that happen within those teams and bring that in as a data point? Suddenly they started seeing correlations. What they actually found was that these individual factors, like team composition, individual skill set, managerial style, they were actually far less important than how the team worked together. They discovered five key elements in order of importance, from top to bottom, that set successful teams up.&lt;/p&gt;

&lt;p&gt;At the top, you have psychological safety, followed by dependability, structure and clarity, meaning, and impact. Psychological safety was so important that they basically immediately created working groups to figure out, how do they make every single team exhibit higher psychological safety so they could gain on their performance? What&#39;s also really nice here is that you&#39;ll see some of the items we talked about from the TAPPs framework are also covered here in Google&#39;s Project Aristotle.&lt;/p&gt;

&lt;p&gt;Another really awesome way of showcasing the value of psychological safety is the 2019 DORA report. They evaluated which factors contribute to organizational performance and productivity. What they found is that psychological safety is the only dimension that actually impacts both. Solving for psychological safety will improve not just the productivity of the team, but the performance of the organization, more broadly. The impact is profound. Teams with higher psychological safety are up to 20% more productive compared to teams without it.&lt;/p&gt;

&lt;p&gt;On the product side, psychological safety correlates with better product performance across the board, because teams feel comfortable taking risks and experimenting and trying new things, knowing that failure isn&#39;t going to be seen as a threat against their own selves. All these benefits give the organization a huge lift in their overall performance, but also in retention. People want to work on teams where they can take risks, try new things, and do so in an environment that supports them.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Measurement: The Who and How&lt;/h2&gt;

&lt;p&gt;We&#39;ve now covered the dimensions of the TAPPs framework. I want to spend a little bit time talking about measurement, the who and the how. Then we&#39;ll talk a little bit about how we can apply it to our teams today. Let&#39;s first talk about who&#39;s involved in this process. In an engineering organization, the groups that you can imagine are, of course, the engineers who are on the team, the management, and at the highest level, the executive. You need the team to contribute data, because they&#39;re the ones on the ground experiencing and contributing to the social dynamics.&lt;/p&gt;

&lt;p&gt;Of course, giving visibility to management and executives is important for building buy-in, for executing these larger projects. I want to focus on a really key line here that is often overlooked, and we should be talking about. The team should be involved in both viewing and analyzing the data. This might seem obvious to some of you, but it&#39;s actually not the status quo.&lt;/p&gt;

&lt;p&gt;As engineers, you can probably relate to the experience of someone sending you a survey to fill out that you spend your hard-earned time on, and then you finish it, and you submit it, and you never hear anything again about what happened. Does that sound like an experience some of you have had? Yes, happens all the time. It turns out that the research overwhelmingly shows that having engineering teams see the data and contribute to the analysis of the data is what&#39;s going to lead to actual change on the team. Why is that? There are two big reasons. One, to contribute the data, teams must trust the data.&lt;/p&gt;

&lt;p&gt;In the case of social drivers, the most powerful way to capture that data is to ask the engineers who are living that experience day to day. If an engineer just doesn&#39;t believe that this data is actually going to be used to improve their team, why are they going to spend the time on it? Even worse, if they believe that that data could be used in an adverse way against their team, then they&#39;re certainly not likely to give authentic and honest responses. This is something that was also highlighted in Nicole Forsgren and Gene Kim&#39;s book, &quot;Accelerate&quot;. The second reason is because building alignment is what drives results.&lt;/p&gt;

&lt;p&gt;When the team can weigh in on their perspective and build buy-in on what actions the team should take, they&#39;ll feel a greater sense of ownership over that work. When they feel greater sense of ownership over the changes, those changes will get executed faster. We just talked about how autonomy builds strategic alignment by giving the team the independence to achieve an organizational goal in the way that they see best. The same thing goes for driving these performance improvements more broadly. The more you can give the engineering team the opportunity to build alignment in their social drivers and how they work, the better the results will be.&lt;/p&gt;

&lt;p&gt;Now that we know who should be involved, let&#39;s talk about how to actually measure this data. In my work, I&#39;ve talked to thousands of engineering leaders, and sometimes I&#39;ll ask them questions like, how are you guys measuring these social drivers today? A very common answer that I get is, we do it in one-on-ones. If you guys have experience with one-on-ones, which I would venture to say at least 80% of the people do, then you probably know something about how those agendas might go.&lt;/p&gt;

&lt;p&gt;In a 30-minute discussion, let&#39;s say, you&#39;ve got to talk about your holiday PTO coming up. How&#39;s this upcoming project? Any challenges? Any blockers? What are our priorities for Q4? Let&#39;s make sure to have that performance and career conversation so you&#39;re set up for next year. We forgot to talk about the team social drivers, let&#39;s make sure to cover that too. I don&#39;t know how long you guys have for your one-on-one conversations, but this is a lot to pack in to 30 minutes. That&#39;s a problem, because what that means is that it&#39;s rushed, and the team might not be getting the space to really think about the social drivers that impact them.&lt;/p&gt;

&lt;p&gt;Worse, it&#39;s unstructured, biased data. It&#39;s unstructured because you&#39;re probably not going to be listening to people compare apples to apples. What you&#39;ll be hearing is, here&#39;s a story of how trust was impacted in my engineering team. As a manager, those stories are really important, but it&#39;s very difficult to understand how one story compares to another story as far as severity and impact.&lt;/p&gt;

&lt;p&gt;More importantly, we&#39;re asking teammates to exchange information on social drivers with another human being. You have to assume a certain level of social drivers being present in order to have that conversation. If you think back to psychological safety, if you have a team with low psych safety, a teammate is going to feel hesitant bringing up some of these situations because they don&#39;t want to get another teammate in trouble, or they don&#39;t want to look like the person who is bringing up all these issues and causing nuances in the team.&lt;/p&gt;

&lt;p&gt;In an honest search for information, the strategy of one-on-ones is actually not a very effective way to capture this data. The best way to measure social drivers on engineering teams is through anonymous, aggregated surveys. The biggest criticism we hear is that, and it&#39;s just so hard to set these surveys up so they&#39;re measuring the right thing and getting the right responses. I hear you. Let&#39;s talk about how we can actually do this. To design a survey well, you need a few ingredients. I&#39;m going to use trust as one of our TAPP social dimensions here as an example.&lt;/p&gt;

&lt;p&gt;First things first, you need to have a clear research-backed question. This question is actually pulled from one of the research papers we talked about, so it was designed by the researchers at Microsoft in one of their productivity and performance studies. Actually, I&#39;ll share a link so you guys can just get all of the TAPPs questions, the questions that you would want to ask on your team, so you don&#39;t have to even worry about designing this. One thing that I will point out is that it&#39;s a single barrel question, which means it&#39;s only asking one thing, that makes it clear and easy to map to the original dimension.&lt;/p&gt;

&lt;p&gt;The second thing is a survey scale. What you need is consistency. Here we&#39;re using a Likert scale, which is, from 1 to 5, strongly disagree to strongly agree. It&#39;s easy to understand. It creates space for outlier answers. Most importantly, you can start to do statistical analysis on this data and see how the mean or the ranges change over time.&lt;/p&gt;

&lt;p&gt;Finally, and perhaps most importantly, you want to design the survey to be anonymous and also aggregate your responses to the team level over time. Social dimensions are inherently about the interactions between teammates, and so your atomic unit should really be the team, not the individuals who are responding. To reinforce this and to also keep a high trust environment, you&#39;re going to want to make this data anonymous, and again, aggregated to the team level.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;How to Get Started&lt;/h2&gt;

&lt;p&gt;Now that we know what the social dimensions are, we can understand how best to measure them. Let&#39;s talk about what your teams can do today to go out and start capturing this data. To get started, you&#39;re going to want to focus on three key steps that I&#39;m going to break down. First, you&#39;re going to want to build a process that&#39;s going to allow you to capture this data over time, more reliably. Second, you&#39;re going to want to review the data regularly and with curiosity. We&#39;ll talk about what that means. Third, you want to drive actions and improvement, and create a consistent flywheel, from measurement to improvement.&lt;/p&gt;

&lt;p&gt;First things first, you want to build a process to reliably measure TAPPs over time. This means that you&#39;re regularly collecting data so you can analyze how it changes over time. There&#39;s nothing wrong with starting with one-on-ones, but the benefit of moving to a survey-based structure, especially an anonymized and aggregated survey, is that you can more easily establish a process that allows you to see that change over time. You can look at things like the range of responses. You can look at the means. You can see how this graph changes over time.&lt;/p&gt;

&lt;p&gt;Suddenly, you&#39;re getting a much better understanding of your team&#39;s psychological safety over a longer time horizon. I also recommend that you measure these dimensions at least quarterly. If you establish a flywheel and good trust with your engineering teams, capturing this data monthly is actually going to be your best bet, because it allows you to capture early signals of changes that need to be addressed, as well as crazy outliers that might be a conversation that you need to have.&lt;/p&gt;

&lt;p&gt;Now that we&#39;ve talked about how to set up that process, let&#39;s talk about how to review that data with curiosity in mind. For this, I have a few tips. When you&#39;re analyzing this data, think about changes over time over a single snapshot. Again, if there&#39;s a crazy swing, you&#39;ll want to know what happened there. In general, you want to look at how things are trending over time, because that&#39;ll give you a better sense of how does the team actually operate in their day to day and the social drivers that impact them, as opposed to maybe what happened on a particular Tuesday.&lt;/p&gt;

&lt;p&gt;The second thing you want to do is ask yourself, why? As teams we should understand the stories of what&#39;s going on. If you see a giant spike upwards, that could be after a team off-site, where the team really felt like they came together and really understood each other. Or if you see things suddenly shoot down, and you look and say, this correlates with when our deadlines got pushed up by six weeks, and pressure really mounted. Ask yourself those questions. Think about the why. Then, third, I touched on this a bit before, but don&#39;t over-index on a single data point, particularly when you&#39;re looking at ranges.&lt;/p&gt;

&lt;p&gt;You really want to think about, how are the averages, or how are the team&#39;s responses changing over time? Instead of, again, really over-indexing on a single data point. Now the team is capturing data and they&#39;re reviewing it regularly, you can now use that data to drive meaningful improvements. For example, if you see a consistent trend of the team&#39;s sense of purpose declining, you could consider bringing your team closer to their end users. Maybe that&#39;s having them listen to some customer success conversations, or having the product team come in and do a lunch and learn where they&#39;re able to talk about this last feature that was built and how it actually impacted the users.&lt;/p&gt;

&lt;p&gt;Then, those actions are going to drive changes in your team&#39;s sense of purpose, and hopefully, if you&#39;ve set up a consistent form of measurement, you can see those changes over time. Now you&#39;re able to create this process of continuing to collect the data, analyzing these data, taking the right actions, and seeing change on the other side.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Key Takeaways&lt;/h2&gt;

&lt;p&gt;First, to capture the top social drivers behind engineering team performance, use the TAPPs framework, that&#39;s trust, autonomy, purpose, and psychological safety. Why do these four things matter? Trust is what unlocks open communication, faster problem solving, and less rework. Autonomy empowers engineers to make decisions faster and solve their problems. Purpose is what aligns engineers&#39; work to the customers that they serve. Psychological safety enables risk taking, honest communication, and greater innovation. That is the TAPPs framework. How do you go about executing it?&lt;/p&gt;

&lt;p&gt;For the best results, engineers should have visibility into data collection and analysis. Of course, you want to make sure that management and executives are involved as well, but don&#39;t leave this line out. The best way to measure social drivers is through anonymized, aggregated surveys. One-on-ones are a great place to start, but if you really want to kick off that flywheel of moving from information to action to measurement, you&#39;re going to want to use a consistent measurement technique like surveys.&lt;/p&gt;

&lt;p&gt;To get started today, teams should build a consistent process, review the data regularly, and of course, with curiosity, and drive actions and improvements. When we know the top social drivers that impact a team, we can create a happier and higher-performing engineering culture.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;You can actually access the survey questions for the TAPPs framework, www.getquotient.com/qcon, so if you want to grab that, start measuring it on your team. You&#39;ll also be able to access research from this presentation, and more, and other topics on engineering leadership.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/trust-psychological-safety/</link><guid isPermaLink="false">infoq-presentations/trust-psychological-safety</guid><pubDate>Thu, 06 Mar 2025 16:00:00 GMT</pubDate><author>Lizzie Matusov</author><enclosure url="https://res.infoq.com/presentations/trust-psychological-safety/en/card_header_image/lizziematusov-twitter-card-1740043411740.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-nov-hiddensocialdrivers.mp4" type="video/mp4"></enclosure><itunes:duration>40:06</itunes:duration><category>Collaboration</category><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>Agile</category><category>QCon San Francisco 2024</category><category>.NET Core</category><category>Distributed Team</category><category>Teamwork</category><category>Psychological Safety</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>Team Collaboration</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>Java9</category><category>Trust</category><category>Success</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item><item><title>Powering User Experiences with Streaming Dataflow</title><description>&lt;figure&gt;&lt;img alt=&quot;Powering User Experiences with Streaming Dataflow&quot; src=&quot;https://res.infoq.com/presentations/readyset-dataflow/en/card_header_image/alana-marzoev-twitter-card-1738316345388.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/figure&gt;       
                    








                    







    

                    












                    

                    

                    

                    
                    

                        &lt;video poster=&quot;https://cdn.infoq.com/statics_s2_20250320073856_u1/styles/static/images/logo/logo_scrubber_16_9.jpg&quot; controls=&quot;&quot;&gt;&lt;source src=&quot;https://videoh.infoq.com/presentations/24-apr-streamingdataflow.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;object data=&quot;https://videoh.infoq.com/presentations/24-apr-streamingdataflow.mp4&quot;&gt;&lt;embed src=&quot;https://videoh.infoq.com/presentations/24-apr-streamingdataflow.mp4&quot;&gt;&lt;/object&gt;&lt;/video&gt;     

                    &lt;div id=&quot;slideContainer&quot; class=&quot;intro slides&quot;&gt;
                        &lt;div id=&quot;slide&quot;&gt;&lt;/div&gt;
                    &lt;/div&gt;

                    &lt;div class=&quot;summary&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Summary&lt;/h2&gt;
                        &lt;p&gt;Alana Marzoev discusses the fundamentals of streaming dataflow and the architecture of ReadySet, a streaming dataflow system designed specifically for operational workloads.&lt;/p&gt;
                    &lt;/div&gt;

                    &lt;div data-nosnippet=&quot;&quot; class=&quot;bio&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;Bio&lt;/h2&gt;
                        &lt;p&gt;Alana Marzoev is the founder and CEO of ReadySet, the company commercializing Noria, a transparent SQL caching solution for relational databases. Before starting ReadySet, Alana was a PhD student at MIT CSAIL where she worked on databases and distributed systems. Prior to MIT, she did research in the Systems &amp;amp; Networking group at Microsoft Research and worked on the Ray project at RISELab.&lt;/p&gt;
                    &lt;/div&gt;
                    &lt;div data-nosnippet=&quot;&quot; class=&quot;about-conference&quot;&gt;
                        &lt;h2 class=&quot;heading&quot;&gt;About the conference&lt;/h2&gt;
                        &lt;p&gt;Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.&lt;/p&gt;
                    &lt;/div&gt;
                    
                        
                            









    

                        
                    

                    &lt;div class=&quot;article__content&quot;&gt;
                        &lt;div class=&quot;article__data&quot;&gt;
                            
                                &lt;div id=&quot;presentationNotes&quot;&gt;
                                    &lt;h2 class=&quot;expanded&quot;&gt;Transcript&lt;/h2&gt;

&lt;p&gt;Marzoev: I&#39;m Alana Marzoev. I&#39;m the co-founder and CEO of Readyset. In this talk, I&#39;m going to be telling you about how you can use a computational model called partially stateful streaming dataflow to build a high-performance SQL cache. Partially stateful streaming dataflow is a relatively new concept that was introduced by the computer systems research community a few years ago.&lt;/p&gt;

&lt;p&gt;The main idea is that you can represent SQL queries as these long running computational graphs, and stream data change updates through the graph to incrementally update the result that the graph emits. At Readyset, we&#39;ve been using this computational model to build a transparent database cache that has a performance profile that&#39;s on par with a hand-optimized, custom caching system, but that helps to alleviate a lot of the burden that you typically see with caching, so specifically around having to come up with a cache invalidation policy, rewrite your application, all of that stuff.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;I&#39;ll start off by talking about the problem domain that we&#39;re going to be working within in greater depth, so you can get a sense for both the use cases and the workloads that we&#39;re ultimately solving for. Then from there, I&#39;m going to do a deep dive into what I call dataflow-based caching. The way I&#39;ll approach this is by starting off by showing you a demo of Readyset in particular. You can think of it as a standard replacement for any dataflow-based caching system.&lt;/p&gt;

&lt;p&gt;From there, I will describe what you just saw. I&#39;ll start off by going into the interface and the specific design decisions that we made there, and how they affect the usability and programmability of this caching system. From there, I&#39;m going to walk through the life of a query, and this will give you a better understanding of the system architecture and how requests flow through that system. Finally, I&#39;m going to do a deep dive into dataflow. I&#39;ll introduce what dataflow is.&lt;/p&gt;

&lt;p&gt;Then from there, I&#39;ll show how we can specifically apply these concepts to the land of relational databases, and how we can specifically solve cache staleness issues in an automated way using dataflow. Finally, I&#39;m going to more explicitly walk through some of the alternative approaches that people use today to address database scaling. I&#39;ll discuss how we see dataflow-based caching relative to these alternatives, so you can get a sense for when it&#39;s appropriate to use and when it might not be.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Problem Domain&lt;/h2&gt;

&lt;p&gt;Without further ado, problem domain. Imagine you&#39;re building a new web application, and at the moment, it doesn&#39;t really have any users. It&#39;s just you, so your goal is to get users. When you think about how you approach development in this context, the thing that you&#39;re going to be optimizing for is reducing iteration time. If your goal is just to try out a lot of features, see what sticks, you&#39;re not going to do anything particularly fancy with your tech stack to enable that. You&#39;re probably going to pick a programming language that you&#39;re already intimately familiar with. You&#39;ll use the web framework that&#39;s associated with that language.&lt;/p&gt;

&lt;p&gt;Then for your database, you&#39;re not going to do anything fancy. You&#39;re probably going to just pick a vanilla, standard relational database hosted by a hyperscaler like AWS. Let&#39;s say that some time passes and all of your hard work and efforts on feature development start to pay off, and suddenly you get this influx of users that you were looking for. Although this is objectively great, given your goals, it actually leads to a new set of technical challenges, most of which are at the database layer.&lt;/p&gt;

&lt;p&gt;Concretely what tends to happen in situations like these is you haven&#39;t spent a lot of upfront time optimizing your queries, adding indices, anything like that. When you start to see this influx of traffic, sometimes this mass of users will lead to a large number of queries being run simultaneously, so there&#39;s a lot of contention in your database. That could lead to both slow queries, aka slow page load times, which could lead to churn, and in the worst case, database outages, just because your database has not been provisioned appropriately to solve this problem.&lt;/p&gt;

&lt;p&gt;What specifically is happening here that makes it hard for your database to keep up? User facing applications as a broad group are really quite latency sensitive. Because there&#39;s humans on the other end of the screen that are waiting for results, and because modern day ORMs tend to autogenerate large numbers of queries, and also inefficient queries, then a single page load could require running sometimes dozens of SQL queries behind the scenes, and page load times will be bottlenecked by the slowest of the batch. In this context, it&#39;s really important that your tail latencies are always quite low, which is challenging with off-the-shelf databases. The other dynamic at play here is there tends to be a lot of variance in traffic.&lt;/p&gt;

&lt;p&gt;The canonical example of this in the U.S., at least, is Black Friday. It&#39;s the day after Thanksgiving. There&#39;s all of these big sales, and truly companies will spend the rest of the 11 months of the year just capacity planning for this day because it&#39;s such a big revenue generation day. You see the same dynamic at play at smaller scale. Let&#39;s say you&#39;re Airbnb, you could totally imagine it being the case that in certain seasons and specific regions, there&#39;s an uptick in bookings because it&#39;s summer vacation season, or something like that. Same could be the case with a B2B SaaS type of situation where you&#39;re going to see the most user sign-ins on Monday through Friday during business hours. There&#39;s a lot of fluctuations, and you have to plan accordingly.&lt;/p&gt;

&lt;p&gt;The last is that these types of workloads that are associated with user facing applications are both read-heavy and have quite skewed data access distributions, and this is something that we could actually leverage. Pay extra attention to this one. On the read-heavy side, that&#39;s exactly what it sounds like. There&#39;s more reads than writes. That makes sense with our mental model of the world. When you&#39;re going on a website, the vast majority of the time you&#39;re not going to be actively changing the data, you&#39;ll just be consuming it. That tends to be the case across this broader category. Similarly, the data distribution, or the data that we&#39;re accessing is not uniformly distributed.&lt;/p&gt;

&lt;p&gt;Normally, popularity is a dimension at play, and also how recent the content was created. If you think about a forum, for example, the front page of Hacker News, and it&#39;s the most popularly voted forum entries. The vast majority of people that visit Hacker News are going to go just to the front page, not to page 103. Then, moreover, the vast majority of things that are posted to Hacker News never see the light of day. They briefly show up on the new page and then no one ever sees them again.&lt;/p&gt;

&lt;p&gt;The way that people in this world like to describe that distribution is the Zipfian distribution, which is governed by Zipf&#39;s Law, which is that, generally speaking, the nth most common term in the set is 1 over n times as popular as the most common term. You can see a visualization of that here. In the context of Hacker News, it could be the case that the second most popular entry is half as popular as the top thing on the front page of Hacker News, and so forth.&lt;/p&gt;

&lt;p&gt;Let&#39;s say that with all of this context in mind, you&#39;re now starting to solution. You&#39;re like, my database is falling over. This is an urgent problem. How am I going to approach scaling things? The first thing you&#39;re going to try is probably just vertically scaling. You&#39;re going to get a bigger box to run your database on, and hopefully that gets you at least something, because there is some contention that there&#39;s a lot of queries being run simultaneously. It&#39;s a reasonable first step, but it tends to not fully solve the problem, because, as I mentioned, nowadays, the vast majority of companies are using ORMs, which specifically abstract away the database for you. That&#39;s kind of the point. It&#39;s really nice from a usability perspective. A lot of the queries that they generate tend to be quite unnatural, quite inefficient, and there tends to be more of them than what&#39;s necessary.&lt;/p&gt;

&lt;p&gt;Given that, oftentimes just adding more cores won&#39;t actually get you the latencies that you really need, given that page load times are bottlenecked by this. From there, you might think, now let&#39;s take a look at what the queries I&#39;m actually running are, and try to optimize them. That is also a perfectly reasonable solution. What we find is that a lot of people, you&#39;re not even aware of the specific queries that your application is generating due to this ORM obfuscating things, and then they tend to be more challenging to optimize if you even have that skill set. It&#39;s not a given that every application engineer is going to know how to optimize SQL queries. That&#39;s a pretty domain specific thing.&lt;/p&gt;

&lt;p&gt;Given that, one of the next solutions that you&#39;d go to is caching, because given that it&#39;s a read-heavy, skewed data access distribution, that has caching written all over it. Typically, what this look is you have a standalone in-memory key-value store off to the side of your database. You essentially will run the queries against your database and then store them in this in-memory key-value store, such that the next time a user comes along and requests that same entry, they&#39;ll be there in memory. You just do a key-value lookup, and it&#39;s really fast.&lt;/p&gt;

&lt;p&gt;There&#39;s a lot of different strategies for caching, but one of the most common ones, and the one that I&#39;m going to talk primarily about is read-through caching, which is quite simple. The main idea is that, you&#39;re always going to check the cache first to see if the thing that you&#39;re looking for is there. If it&#39;s not, the cache is going to trigger the query that would populate that result against the database, and then it would cache it locally before returning it to the end user, or the application that requested it. Let&#39;s think through what our code looks like before and after introducing read-through caching. For context, this is just a random code snippet I found from a Python, SQLAlchemy, and Redis caching tutorial, but it&#39;ll illustrate my point.&lt;/p&gt;

&lt;p&gt;This function here, is just fetching a user from the database that&#39;s opening up a session, using SQLAlchemy to generate the query, to pull the user info, and then just returning the user data. It&#39;s as simple, as simple can be. This is what your code starts to look like after caching. As you can see, there&#39;s twice as much of it for the same underlying function. The concrete new thing that&#39;s here is this get_user_from_cache function, which is implementing the read-through logic that I previously just described. You take a look at this, and you&#39;re like, there is another helper function. Obviously, this is just one of the places where I might want to start caching. If there&#39;s a lot of them in my app, it&#39;s annoying to add these helper functions. I don&#39;t want to do that. Maybe at the end of the day, it&#39;s not the end of the world. We can write helper functions.&lt;/p&gt;

&lt;p&gt;The question is like, is this it? Is your caching problem solved? Are you done with this code addition? The answer to that question is, unfortunately, no, simply because cache invalidation tends to get really messy. Concretely, in every point in the application where you want to introduce caching, you have to really sit down and think through what the user expects from you for that functionality, essentially. Then from there, you have to design a cache invalidation policy that aligns with those user expectations, and implement it. To make that a little bit more concrete, let&#39;s say you have these two settings.&lt;/p&gt;

&lt;p&gt;The first is a Facebook News Feed, and the other is a shopping cart in an e-commerce application. In the Facebook example, let&#39;s say that you are a user of Facebook, and so are your friends, and your friend goes to brunch and posts a photo from their brunch at some time point t. Let&#39;s say you don&#39;t see that until t plus 5. Five minutes have gone by, there&#39;s no update from your friends. In the vast majority of cases, you&#39;re going to be none the wiser. Unless you&#39;re truly sitting next to your friend and they asked you, do you see my Facebook post? You&#39;re not going to know that there&#39;s a 5-minute delay between when your friend posted the photo and when you saw it.&lt;/p&gt;

&lt;p&gt;In this case, it&#39;s easier to cache, because you don&#39;t have to think too hard about making sure the cache is up to date and returning fresh results to the user. Contrast that to the e-commerce example. Let&#39;s say that you are just trying to buy a t-shirt on the internet, and you&#39;re on an e-commerce site. You add it to your cart, and it takes 5 minutes for it to show up in your cart, and you&#39;re ready to check out. You&#39;re not going to wait 5 minutes under the hopes of like, I want this shirt to show up in the shopping cart. You&#39;re going to go on Amazon, or just buy it elsewhere. These are two examples where the expectations from the user are really quite different in terms of how fresh the data needs to be, and that is going to inform your cache invalidation policy.&lt;/p&gt;

&lt;p&gt;Right now, with caching, you have to write a lot of bespoke code for cache invalidation. There&#39;s no automated way of doing this in the general case. You could choose something as simple as a TTL. Maybe every 5 minutes your cache entries get purged and you have to re-request all the data. Or you could do something extremely complicated and sophisticated, where you could be tracking the specific tables that are referenced in the query that&#39;s being cached and trying to gauge, this one was updated, and therefore this cache entry likely needs to be recomputed and so forth. The world is your oyster there. There&#39;s this fundamental tradeoff that I think it&#39;s really important to highlight, where on one end of the spectrum, it&#39;s like, you&#39;re rarely ever evicting.&lt;/p&gt;

&lt;p&gt;This is good for your cache hit rate, because when your application checks to see if an item is in the cache, it&#39;s always going to be there because you&#39;re rarely evicting, but in all likelihood, your data is going to be extremely stale, and so it&#39;s not going to be appropriate. It&#39;s not going to align with expectations of your users. On the other hand, let&#39;s say that you&#39;re evicting all the time because you&#39;re like, I really need the data to be fresh. I want to provide the best user experience. Then, at that point you might as well not be using a cache at all, because you&#39;re never really leveraging it. Figuring out exactly where on this tradeoff curve you want to live is a hard problem.&lt;/p&gt;

&lt;p&gt;I&#39;m sure many of you have heard this quote, &quot;There are only two hard things in computer science, cache invalidation and naming things&quot;. This quote is referencing this general dynamic of, caching at a high level, it&#39;s super simple. It&#39;s like you&#39;re just precomputing a result set, storing it in memory. What is there to know about that? In reality, it&#39;s actually quite messy. It&#39;s really easy to introduce bugs. Let&#39;s say that you have sat down, you&#39;ve thought through the user expectations. You&#39;ve designed an invalidation policy, and now you just have to write the code to do that. That code itself could actually be quite nuanced.&lt;/p&gt;

&lt;p&gt;Concretely, you could accidentally introduce a distributed systems bug, like a thundering herd, where you are evicting a popular cache entry unintentionally, and that&#39;s triggering a storm of database queries against your database, which wasn&#39;t provisioned accordingly. Then that takes it down, which, again, at that point, why are you caching? You&#39;re making your own problem worse and shooting yourself in the foot. When I take a step back and think through a cost-benefit analysis of caching, this is what I would broadly come up with. On the pros side, you get the best-case query latencies when you get a cache hit rate. Because you are doing all of the work before, and all you&#39;re doing when you&#39;re doing a read is an O(1) lookup into an in-memory, lock-free data structure. Caching asymptotically doesn&#39;t get better than that.&lt;/p&gt;

&lt;p&gt;Similarly, there&#39;s a much more straightforward path to scaling, as opposed to sharding your database. Making your cache scale horizontally is a lot easier than making your source of truth scale horizontally. There&#39;s a lot of drawbacks as well. You have to rewrite your application, which is already not ideal. Caching is quite error prone. There&#39;s three different places at least we can get it wrong, like the user expectations level, the cache invalidation policy level, and the actual implementation. When you do make a mistake, bugs are quite obvious to the end user, or otherwise they could take down your database. The stakes are really high, and you don&#39;t have any isolation between the cache and the database. That would lead you to potentially pause and think, do I really need a cache? Only if you really needed it, would you proceed with this plan.&lt;/p&gt;

&lt;p&gt;If we were to wave a magic wand and elucidate specifically what we would want from an ideal caching system for this purpose, I think it would look something like this. One is, we wouldn&#39;t have to rewrite the application. Ideally, we&#39;re able to start caching in a way that is non-intrusive, doesn&#39;t take a lot of engineering time, and so forth. Two is that we wouldn&#39;t have to think so hard about cache invalidation and worry about staleness concerns, because, as I just described, it&#39;s a pretty fraught process, and it&#39;s really easy to make mistakes. Then, finally, the cache should absolutely never make things worse, because presumably you&#39;re adding this to your stack, because you&#39;re already in a bit of a pickle when it comes to databases falling over and so forth, and you really just don&#39;t want to be making it worse by trying to make it better.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Dataflow-Based Caching: Demo&lt;/h2&gt;

&lt;p&gt;With all of that in mind, I&#39;m going to introduce dataflow-based caching. I&#39;m going to start off with a demo, so you can get a sense for what it actually looks like and feels like, and we can go from there. For context, we are working with an IMDb dataset, which is a real dataset that&#39;s available on the internet, that&#39;s scraped from the IMDb website, the movie ratings website. We are going to try to cache a query, which the natural language interpretation of is, how many movies filmed after the year 2000 had over a 5-star rating on IMDb? Behind the scenes here we have a Postgres database in a Docker container, and we have a Readyset instance. A Readyset is like this example of a dataflow-based cache. Right now, we are going to connect to the cache, and we&#39;re going to connect to it using psql. You can see this connection string here, says Readyset in it. I&#39;m going to go ahead and do that.&lt;/p&gt;

&lt;p&gt;Then from there, I&#39;m going to turn on timing so that we can see how long queries take to run. The query that I just told you about, where we&#39;re trying to see how many movies had a rating of over 5 after 2000 is right here. I&#39;m going to start off by just running that against the underlying database. We can see that that took 173 milliseconds, and it returned the answer, 2418. There are 2418 movies that fit that criteria. You can see, I just typed the statement, show caches. There&#39;s currently no caches that are set up in the system. We&#39;re going to go ahead and create one by typing in, create cache from, and then I&#39;m going to copy and paste this query string. We can see now the cache presumably was created. We can see that that was created. Now we&#39;re going to run the same query again, and the first time we run it, it&#39;s going to be a cache miss.&lt;/p&gt;

&lt;p&gt;The cache started off empty, we&#39;re warming it up. We run it a few times, and then we can see, now we&#39;re in business. Now it&#39;s taking 0.75 milliseconds as opposed to 50. That&#39;s fairly consistent. That&#39;s pretty cool. Now what we&#39;re going to do is essentially issue a write that should change the result that&#39;s being returned. In this case, I&#39;m going to say, there&#39;s this one movie that I liked from 2003 that everyone else hated, so I think that one deserves a rating over 5. The public doesn&#39;t agree, but I&#39;m going to change it such that that&#39;s the case.&lt;/p&gt;

&lt;p&gt;Then that should presumably change the query result that we see. Let&#39;s see if I can find that update. Right now, the rating is 2.5. I&#39;m going to change it to 5.1. Now I&#39;m going to run the query again. We can see, all of a sudden, the result has been refreshed. Before was 2418, now it&#39;s 2419. Notably, the time that it took to get the state from the cache didn&#39;t change. We didn&#39;t just evict and recompute, because if we evicted and recomputed, it would take 50 milliseconds again. In this case, it&#39;s just the new result, but it&#39;s still a cache hit.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Dataflow-Based Caching: Interface&lt;/h2&gt;

&lt;p&gt;How does all of that work? That&#39;s going to be the rest of the talk. Let&#39;s start by talking through the interface, because that&#39;s the most tangible part of what we just saw. We saw how to interact with this. Let&#39;s make the decisions that we made there a tad more explicit. The first thing to know is this cache is set up as a read replica. When you start up Readyset, it is going to snapshot the tables in your database that are relevant for caching. Concretely, if you&#39;re trying to cache two queries and they&#39;re referencing tables A, B, and C through them, but you have 500 tables in your database that aren&#39;t A, B, and C, then it will only snapshot the tables that you actually need for caching, so A, B, and C. Then from there, it&#39;s going to maintain a replication slot on your primary database so that it can get the row changes that are being replicated from your database to automatically and incrementally update the cache state.&lt;/p&gt;

&lt;p&gt;The specific way that this works will vary depending on the database. Readyset in particular supports Postgres and MySQL. In Postgres, this is logical replication. MySQL is row-based replication. The next thing to know is that it&#39;s wire compatible with Postgres and MySQL. That&#39;s what we saw in the demo. I was using the psql client to connect to a Readyset connection string. The goal of doing that is to prevent you from having to make large changes to your application to start using a cache. This is an ergonomics related feature.&lt;/p&gt;

&lt;p&gt;Of course, under the hood, Readyset does not yet support all of the features of the underlying database, it just looks like one at the surface level, and it has a subtly different SQL dialect. By and large, you shouldn&#39;t have to worry about that, because for most of the things that you&#39;re caching, which are going to be SQL-92 type queries, like relational algebra type computations, that shouldn&#39;t be an issue. If in your application you&#39;re already using SQLAlchemy, or Django, and Rails, you shouldn&#39;t have to change your application code in any meaningful way.&lt;/p&gt;

&lt;p&gt;The next thing that I will point out, which you also saw in the demo, is that caches are specified explicitly. We have the DDL equivalents for cache management, so creation, deletion, and so forth. This wasn&#39;t obvious from the demo, but specifically, we&#39;re caching prepared statements, which you can think of as being parameterized SQL queries. If you have two queries that are structurally the same, they&#39;re operating over the same data, they&#39;re running the same computations, but the only difference is the literal in one. Let&#39;s say the user ID that you&#39;re asking for is 3 and the other is 5, then those are actually considered to be the same query within Readyset, and it&#39;s going to be handled by the same cache and the same dataflow graph, which I&#39;ll be explaining in a later section.&lt;/p&gt;

&lt;p&gt;The next thing and last thing is that, any queries that aren&#39;t explicitly cached will be, by default, proxied to the primary database. The same goes for writes. Readyset never tries to apply a write that you send it within the cache first, it&#39;ll just forward it along to your database and wait for that write to show up in the replication stream later on, and then from there, it will use that to incrementally update the cache state. The reason that we chose to do this is that, it gives you fine-grained control over what&#39;s cached or not, which is desirable, because sometimes you don&#39;t want to cache something. Sometimes you&#39;re running a bank transaction, and we know that we don&#39;t want to cache in bank transactions.&lt;/p&gt;

&lt;p&gt;Sometimes it&#39;s just not economical to cache because maybe you&#39;re just not running that query so much, but to get a high cache hit rate, you have to use a lot of memory. Maybe you just don&#39;t want to. At the same time, it&#39;s also not always desirable to have to manage multiple read-write connection strings. If you already have the infrastructure set up for read replicas, and you have reads and writes split out, then you can totally adopt it in that way. If you don&#39;t, then you don&#39;t have to. You can still just use a singular database connection and have the functionality be unchanged. Your app will still work, but you can still have this fine-grained control over what&#39;s cached or not.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Dataflow-Based Caching: Life of a Query&lt;/h2&gt;

&lt;p&gt;Now I&#39;m going to walk through the life of a query. The way I&#39;m going to do this is by starting off with a bird&#39;s eye view of the architecture. I&#39;ll quickly talk through what&#39;s happening here. Then we&#39;ll step through a bit more carefully all the different components for both setting up a cache and for actually serving traffic from the cache that we just set up. Here&#39;s architecture. Broadly speaking, there&#39;s the application server up on top, which is essentially unchanged. It&#39;s as though you&#39;re not using a cache at all. You have the business logic, which is presumably utilizing ORM. The ORM, in turn, is using a database client to actually connect to the database and issue the queries against it. In this case, instead of connecting to the relational database directly, it&#39;s connecting to the Readyset adapter. Within the Readyset adapter, there&#39;s different components.&lt;/p&gt;

&lt;p&gt;The first one that it speaks with is like the SQL shim, and it&#39;s responsible for decoding from the binary protocol of that database into just the text that we need to essentially create an internal representation of that query. Then once we have that internal representation, the Readyset client is going to be deciding whether or not that should be sent to the primary database or if it should be sent to the Readyset server. If it is sent to the Readyset server, then the query will be resolved within a component called the reader.&lt;/p&gt;

&lt;p&gt;Then, behind the scenes of all of this, your database is getting writes. Those data changes are being replicated to the Readyset server and being handled by a component called the replicator, which is updating the local copy of the base tables that I mentioned that we snapshotted before. Then emitting those row changes through the dataflow graph, which will be ultimately responsible for keeping the cache state in the readers up to date.&lt;/p&gt;

&lt;p&gt;I&#39;m going to talk through that in greater depth. We&#39;ll start by describing how we set up the cache. At this point in time, I&#39;m assuming that we&#39;ve already created a Readyset cluster or instance, and it gave us the database connection string that we can use. The first thing that we want to do in our application is swap out the database URL that was previously pointing at your primary database to now point to this Readyset instance that is connected to your primary database. Again, this is describing the approach of using Readyset as a full proxy as opposed to a read replica. Both are options. It&#39;s just a matter of what works best in your setting. The specific place in the code that you&#39;re going to actually change this URL is going to depend on the programming language you use, the framework you use, but typically it is just swapping a connection string.&lt;/p&gt;

&lt;p&gt;From there, you as developer need to decide which queries you want to cache. There are a lot of different ways that you can imagine doing this, either by looking at your database is slow query logs. If you&#39;re using an APM, like Datadog, or whatever, you can check that as well. In Readyset itself, we offer some basic heuristics to help you decide as well, so things like total CPU time that that query is taking up in your database, the count, how bad the tail latencies are, that sort of thing. You have to take a look at that, and then from there decide which queries you want to cache. Once you have a list in mind, you have to go ahead and actually create those caches. You do that via the DDL-esque SQL extensions that I showed you before, like the create cache from, and then the query string or query ID. Then that will trigger a migration within Readyset to construct the dataflow graph, which is going to be responsible for keeping the cache up to date.&lt;/p&gt;

&lt;p&gt;That process of creating the cache, it&#39;s not going to be instantaneous. It could be really fast. It could take a few minutes, depending on how big your tables are, what indices we have to create behind the scenes, and so forth. It will let you know once the cache is ready to start serving traffic. Now let&#39;s talk through the actual life of a query part of this. Let&#39;s say a user requests some data, which translates to an application server, or in app logic, a query is being generated.&lt;/p&gt;

&lt;p&gt;Again, the application is connected directly to the Readyset adapter. In this context, we&#39;re just continuing to use the ORM and proxy everything through Readyset. That&#39;s going to get sent to first the SQL shim, which is going to be responsible for decoding the binary representation and then converting that into this generic internal representation of the query that is the same across all database variants within Readyset. Think of this as the relational algebra representation of the query. Then that internal representation is going to be passed to a component called the Readyset client. From there, the Readyset client is going to be pattern matching this string or representation against the caches that it knows are stored and are created in Readyset server. If there&#39;s a match, and remember, we&#39;re doing this at the level of prepared statements, so we&#39;re ignoring any differences in literals. We&#39;re just looking at the queries in a structural way.&lt;/p&gt;

&lt;p&gt;Then that&#39;s going to be sent to the Readyset server, to the reader nodes, and if not, it&#39;s going to be proxied to the database. The database will compute the result and return it to the application. Then, in the background, the cache is continuously being refreshed in an eager way. Concretely, let&#39;s say you&#39;re sending writes to your database, those are being reflected in the replication stream. The replication stream is being sent to the component within the Readyset server called the replicator, and it&#39;s going to receive those data changes. It&#39;s going to update the local copy of the base tables to reflect those changes. Then it&#39;s going to propagate those changes through the dataflow graph. By doing that, the dataflow graph is able to compute an incremental update over the old cache state to reflect this new data change. Then it&#39;s going to swap out the old version in the reader, which is the cache, for all intents and purposes, with this new, updated version.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Dataflow-Based Caching: Solving Staleness with Dataflow&lt;/h2&gt;

&lt;p&gt;In this walkthrough, dataflow was a black box. I&#39;m going to spend the rest of the time talking through how this fancy incremental update mechanism actually works, because, in my mind, that&#39;s the cool part. Concretely, as I just alluded to, we&#39;re going to be figuring out how to solve cache staleness issues in an automated way using this mechanism. I&#39;ll start off by just explaining what dataflow computing is. Dataflow is just generally a very overloaded term in computer science, but the version that I&#39;m referring to is dataflow computing, which is also known as stream processing in a lot of cases.&lt;/p&gt;

&lt;p&gt;The main idea is that you can represent computations as directed graphs where the nodes of the graph are considered to be operators that perform those computations over incoming data. Then data is flowing through the edges into these operators. I have this little figure here, it&#39;s an arithmetic example. Let&#39;s say that our goal was to compute x times y plus z. What this would look like is, you have this graph where the nodes of the graph are the computations. There&#39;s the multiplication operator, and then there&#39;s the addition operator. Then the inputs of the graph are the data that we&#39;re inputting, so x, y, and z. You can see that x and y are inputs into the multiplication operator. They&#39;ll go to the multiplication operator. The multiplication operator will multiply those two together, and then it&#39;ll emit the result x times y out of its outgoing edge, which is leading into the addition operator.&lt;/p&gt;

&lt;p&gt;Then the addition operator will add x times y, which it got from its parent node, to z, which is the other input into the graph, and then emit the result. One way that I like to explain this idea is in contrast to batch processing. Where, typically, when at least I think about running a computation over a dataset, essentially, you have a dataset, and then you run some compute over it.&lt;/p&gt;

&lt;p&gt;Presumably, you&#39;re ingesting data for some time period. The data is accumulating. Then at some point in time you&#39;re like, let&#39;s run this computation. There might be some delay between when the last data point was updated, or the delay between the most recent version of the dataset and the response that you&#39;re giving. If you contrast that to stream processing, in stream processing or dataflow computing, you are continuously ingesting data, and you&#39;re continuously running some, in many times, like incremental computation over that data to compute a response that is eagerly made, up to date.&lt;/p&gt;

&lt;p&gt;Now let&#39;s talk about how we can represent SQL queries as dataflow, to bring this back to the domain that we were just talking about. In this case, the nodes of the graph which are again representing the computations, are just relational algebra operators. To name a few, like SELECTs, PROJECTs, JOINs, and so forth, it almost looks like a query plan, if you&#39;ve seen those. Then, the data that&#39;s flowing through the edges are these row updates coming from your primary database through the replication stream. Those updates are propagating through the graph. The relational operators are computing the changes over those and emitting the results.&lt;/p&gt;

&lt;p&gt;By the time you have the final output, that&#39;s the query result that we are looking for. I&#39;m going to make this more explicit with an example. I already talked a little bit about Hacker News. It&#39;s a forum for developers. People will post things. Most of the things that get uploaded make it to the top page. Let&#39;s say that, just for the sake of this example, we&#39;re only dealing with two tables. There&#39;s a stories table, which has the story ID, the author of the story, the title, and URL.&lt;/p&gt;

&lt;p&gt;Then there&#39;s the votes table, which has just a mapping of which users voted for which story IDs. Let&#39;s say, in an attempt to write a query that generates the content on the front page, we&#39;ll have this, and the natural language interpretation of this is essentially return the story content and metadata for that story, along with the vote count for the story with ID x. It&#39;s just doing a join and joining the story info with the vote count for that story, and it&#39;s parameterized on the story ID, so it&#39;s a prepared statement. The way that we would represent this as dataflow would look something like this, where you have the base tables which are stored locally on disk.&lt;/p&gt;

&lt;p&gt;Then, in the actual dataflow graph itself, you have a count group by node which is receiving input from the votes base table. Then there&#39;s the stories base table node which is feeding into the join node. You see the join node has another input coming in from the count group by. Finally, that&#39;s storing the cache results in the reader.&lt;/p&gt;

&lt;p&gt;Let&#39;s talk through how writes look in this world to make this a little bit more dynamic and complete. Let&#39;s say that Justin upvotes story with ID 2. That&#39;s going to be reflective of a row insert into the votes base table. Justin votes for story 2, that&#39;s going to propagate through the dataflow graph. As you can see, the outgoing edge from this base table is into the count node. The count node is going to receive that update, and it&#39;s going to take the prior vote count, which was 580, and increment it by 1 to reflect the fact that there&#39;s now one more person that voted for this story. Then it&#39;s going to emit that new vote count into the join node. The join node is going to be like, I have this new vote count. Let me update the result of the join for story ID 2 to have this new vote count, 581.&lt;/p&gt;

&lt;p&gt;Then, finally, let&#39;s make that available for consumption by users and update it in their reader node, which is where we&#39;re serving all of our traffic from. Now to walk through a read. Let&#39;s say that now we&#39;re trying to get all the story data and vote count for story with ID 2. That&#39;s going to just look like the query that we were just looking at before. Now, instead of being parameterized, we&#39;re passing in the parameter too and executing the prepared statement. To perform this read, all you have to do is a key-value lookup for ID 2 in this reader node, which is just an in-memory data structure that&#39;s storing all of the cached query results. It&#39;s pretty on par with normal caches in that regard.&lt;/p&gt;

&lt;p&gt;Let&#39;s talk about how efficient this approach is as currently described, because that&#39;s going to be really important. I&#39;ll start by comparing read efficiency. I just alluded to this. Essentially, in both types of caches, like in the one where you just have an in-memory key-value store, and you&#39;re evicting and recomputing and all of that. Then in this version, where it&#39;s like a dataflow-based cache, all you&#39;re doing is this very fast lookup into an in-memory, lock-free data structure. You get the same performance, slight code differences aside, in both contexts. Now let&#39;s compare cache update efficiency. Again, with &quot;traditional caches&quot;, like this in-memory, key-value store-based model where you&#39;re running queries against the database, storing it in the cache, and so forth. Whenever you want to invalidate the cache and update it, you are going to evict and then recompute the missing state. This recomputation could be pretty expensive. It won&#39;t necessarily be.&lt;/p&gt;

&lt;p&gt;Presumably, the reason you&#39;re introducing the cache is because you&#39;re running this computation all the time, or perhaps it&#39;s like a complex query, so the fact that you have to invalidate by removing the state from the cache and rerun against your database could take quite a while, and that&#39;s going to reduce your cache hit rate, because those are all going to be cache misses. With dataflow-based caches, things are different, because you aren&#39;t ever evicting and recomputing to update the cache. You&#39;re incrementally updating it, which tends to be a lot less compute intensive, because you&#39;re not throwing away the old value entirely. You&#39;re saying, I have the old value.&lt;/p&gt;

&lt;p&gt;Then, some small amount of the data has changed since we last looked at the old value, so let&#39;s just reuse that old computation and then just figure out how to update the old computation to reflect just these new data points, as opposed to starting from scratch. You&#39;re doing this all within the context of the cache. You&#39;re never running a query against your database to update the state and make it fresher, because we have the local copy of the base tables, all of that is just being resolved within the cache itself.&lt;/p&gt;

&lt;p&gt;Now let&#39;s talk about memory overhead. With traditional caches, the heuristic that people like to use is that you need to make sure that you have at least enough memory for your working set allocated to the cache. Because if you don&#39;t, you&#39;re going to have this thing called thrashing, where you&#39;re continuously swapping items in and out of the cache, because the application needs them but there&#39;s not enough space to hold all of them. The same dynamic is absolutely true with dataflow-based caches. You need to absolutely make sure you have enough memory allocated for the working set.&lt;/p&gt;

&lt;p&gt;There&#39;s this whole dataflow graph around that as well, because the working set, that&#39;s just the reader nodes. We have all these other nodes that are also stored in memory, like the dataflow graph ones. That could actually be pretty heavy, because it&#39;s going to depend, of course, on the base dataset sizes. It&#39;ll depend on how complex the query is, because the more complex the query, the more graph you have. The more computations you&#39;re going to be doing in the graph, and the more memory you&#39;re going to have to allocate to it. It also has to do with the data distribution in this interesting way. Because let&#39;s say you&#39;re filtering out some of the nodes responsible for filtering out the data, you could have instances where you have the largest base tables of all time, but you&#39;re very quickly filtering away most of that data.&lt;/p&gt;

&lt;p&gt;Really, there&#39;s a question of like, how much are you filtering out? How heavy are any of these nodes going to be along the way? It begs the question, is this memory overhead practical? The answer is, not always, at least as described. This is the main obstacle, as I see it, to really using this in a real-world setting.&lt;/p&gt;

&lt;p&gt;Now I&#39;m going to talk about how we approach solving this memory overhead blowup problem in Readyset, at least. The way we do this is via a mechanism called partial state, also known as partial materialization. The main insight here is coming back to the beginning of this talk, like we have a skewed data access distribution. It&#39;s not the case that everything is going to be equally as popular, and we have to make sure we&#39;re preemptively putting everything in the cache.&lt;/p&gt;

&lt;p&gt;Rather, it&#39;s very likely that there&#39;s going to be a relatively small subset of items that are really popular, really recent, or whatever. It&#39;s distributed via Zipf&#39;s Law, so it&#39;s going to be highly skewed. You can get a really high cache hit rate just by caching the bulk of those, as opposed to caching the full long tail of potential query results. In the context of these prepared statements, think of it this way. You don&#39;t have to preemptively compute the results of this query for any possible input parameter, like any possible story ID, because that would be a waste of time, because, as we discussed, most people don&#39;t go on page 103 of Hacker News.&lt;/p&gt;

&lt;p&gt;With partial materialization or state, the main idea is that when you first create this dataflow graph, it&#39;s going to start off entirely empty, and you&#39;re going to fill it in an entirely lazy way on-demand, via a mechanism called an upquery. The way upqueries work is that, again, cache is starting off empty. A user requests a value from the cache, it&#39;s going to recursively traverse up the dataflow graph to find the closest node that has the missing state and then run it through the rest of the graph to perform the update.&lt;/p&gt;

&lt;p&gt;I will just walk through a quick visual of this. Again, let&#39;s say we&#39;re running this query for story ID 2, the graph is starting off completely empty. The request is going to go into the reader node. The reader node is going to be like, &quot;I don&#39;t have it. Let&#39;s ask my parent node&quot;, which is the join node. The join node is also empty because we just got started. It&#39;s going to be like, let me ask my two parent nodes, see if we can find the missing state. This is actually happening in parallel. In this diagram, it&#39;s happening sequentially. It&#39;s going to ask the count node, count node is also empty. The count node is going to ask the votes base table. The votes base table is, again, this local copy of the tables on disk. It&#39;s going to necessarily have the state that we&#39;re looking for, and it&#39;s going to replay all of those row entries into the count node.&lt;/p&gt;

&lt;p&gt;Concretely, all of the rows that indicated somebody voted for story 2, that&#39;s going to be counted up by the count node, and it&#39;s going to then emit that to the join node. The join node had asked its stories base table parent to pull the story info for story 2, and it&#39;s going to join the results together and store it in the reader, and then return a response. The way we deal with memory pressure is the same as normal caches, where once we hit some memory threshold, maybe just like the memory on the machine, as opposed to simply being, we&#39;ll just evict state based on LRU, LFU, whatever. It&#39;s pretty standard.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Dataflow-Based Caching: Comparison to Alternatives&lt;/h2&gt;

&lt;p&gt;If I think through dataflow-based caching relative to traditional caching, you can avoid a lot of the usability challenges because it&#39;s wire compatible with the database. It has the same interface. You don&#39;t have to rewrite the app. You don&#39;t have to worry as much about staleness, because it&#39;s eagerly updating the cache entries behind the scenes. Then, the cache is never going to make anything worse, because we have a local copy of the base tables. We use that to deal with misses. You&#39;re not going to intentionally introduce distributed systems bug. The important thing to note is that this is effectively expanding the space of supported workloads. Because you&#39;re not doing this evict and recompute, you can tolerate comparatively write-heavy workloads.&lt;/p&gt;

&lt;p&gt;If you had a lot of writes in a normal caching setting, then you would have to continuously evict, and then you never get a high cache hit rate. Here, because we&#39;re just updating the cache directly in-memory, and it&#39;s not a cache miss, then you can tolerate a much larger number of writes while still getting a good cache hit rate and having the cache be worth it. Of course, you can more easily cache complex queries without having to track data provenance when it comes to doing invalidation. I compare this to query optimization, obviously, the barrier of entry is lower because you don&#39;t need to know anything about databases to be able to do this.&lt;/p&gt;

&lt;p&gt;Sometimes, even if you optimize a query to the max extent possible, it&#39;s still too slow, so caching still has a role there. When I compare this to read replicas, you can essentially set up this type of cache to have the same interface as a read replica, but you don&#39;t have to optimize queries, and because you&#39;re doing caching, you just get lower latencies.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Conclusion, and Resources&lt;/h2&gt;

&lt;p&gt;With dataflow-based caching, you can get on par performance to hand-optimized homegrown caching systems. You don&#39;t have to design a cache invalidation policy. That&#39;s all handled for you. It&#39;s useful in a wider range of settings in the database world, for example, when you have more writes. We have a GitHub. If you want to see the source code, it&#39;s available there, just look up Readyset. Then this is based on the Noria research project from MIT, so if you want to geek out further, then there&#39;s a whole paper on it that you can take a look at. If you just Google that, you can find it.&lt;/p&gt;

&lt;h2 class=&quot;expanded&quot;&gt;Questions and Answers&lt;/h2&gt;

&lt;p&gt;Participant 1: Do you have read-after-write consistency?&lt;/p&gt;

&lt;p&gt;Marzoev: We do not. It&#39;s actually technically possible in this model, but we haven&#39;t productionized it. By default, it&#39;s eventually consistent. The way that I like to think about this is, when you deal with normal caches, you have no guarantees whatsoever, and you don&#39;t expect any guarantees. We like to compare ourselves to the best alternatives, which are regular caches, as opposed to databases. Yes, CAP theorem.&lt;/p&gt;

&lt;p&gt;Participant 2: Actually, how big is the table size it can handle in memory? For example, there is lots of tables we are querying in. If my query has lots of large tables, and since it is going to replicate those tables into the memory, would that be efficient to keep the copy of those tables in the memory?&lt;/p&gt;

&lt;p&gt;Marzoev: If we have really large base tables, will it be efficient to store them in memory? We never store the full base tables in memory. We store them on disk in RocksDB. The only thing that&#39;s in memory is the computational nodes. It&#39;s possible that you might unintentionally bring those into memory if you&#39;re running a really hefty join, but that&#39;s what the whole partial materialization scheme is meant to defend against, where you don&#39;t have to materialize the whole join, you just materialize the result for queries that you&#39;re asking for.&lt;/p&gt;

&lt;p&gt;Participant 2: Isolations like when the read and writes happens at the same time on the tables, how is that being handled? There could be lots of cases when the dirty read would be possible, in some scenario, when one user is updating the data and another one is reading the data.&lt;/p&gt;

&lt;p&gt;Marzoev: Is there isolation between reads and writes? Yes. Essentially, the way that works is like, we are waiting to see the writes come in from your primary database, so they&#39;re already sorted. The reader node is separate from the dataflow graph, because we want to be able to both continuously write updates to it, while supporting reading this eventually consistent state. Then there&#39;s a point in time where we just do a pointer swap and we&#39;re like, we&#39;ve written to the new version of the cache for a while, so let&#39;s flip that to still service it to the user. They&#39;re completely separate, like threads.&lt;/p&gt;

&lt;p&gt;Participant 2: There is a possibility of a dirty read in between, then.&lt;/p&gt;

&lt;p&gt;Marzoev: It depends on how you define that dirty read.&lt;/p&gt;

&lt;p&gt;Participant 3: Can you share a little bit about spreading into other database support, rather than Postgres and MySQL?&lt;/p&gt;

&lt;p&gt;Marzoev: There&#39;s something fundamental about this that makes it specific to Postgres or MySQL. Obviously, there&#39;s a big assumption we&#39;re making that we&#39;re dealing with relational datasets. To support a new database, we just need to write a new replicator for that database and then write a new SQL shim for that database. It&#39;s possible. It&#39;s just engineering work. We&#39;re a startup so we are probably not going to expand past these two for quite some time.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;


&lt;p&gt;&lt;big&gt;&lt;strong&gt;See more &lt;a href=&quot;https://www.infoq.com/transcripts/presentations/&quot;&gt;presentations with transcripts&lt;/a&gt;&lt;/strong&gt;&lt;/big&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

                                &lt;/div&gt;
                            
                        &lt;/div&gt;
                        
                        
                        
                        
                            
                                
                                
                                
                                
                                    
                                    
                                
                                
                                    
                                
                            
                        
                        
                    &lt;/div&gt;

                    

                    
                    









                    

                    
                    
                    
                    
                    
                        















                    
                   </description><link>https://www.infoq.com/presentations/readyset-dataflow/</link><guid isPermaLink="false">infoq-presentations/readyset-dataflow</guid><pubDate>Wed, 05 Mar 2025 16:00:00 GMT</pubDate><author>Alana Marzoev</author><enclosure url="https://res.infoq.com/presentations/readyset-dataflow/en/card_header_image/alana-marzoev-twitter-card-1738316345388.jpg" type="image/jpeg"></enclosure><enclosure url="https://videoh.infoq.com/presentations/24-apr-streamingdataflow.mp4" type="video/mp4"></enclosure><itunes:duration>53:21</itunes:duration><category>Architecture &amp; Design</category><category>Culture &amp; Methods</category><category>.NET Core</category><category>InfoQ</category><category>Machine Learning</category><category>Microservices</category><category>Streaming</category><category>Database</category><category>Database Design</category><category>AI, ML &amp; Data Engineering</category><category>QCon Software Development Conference</category><category>QCon London 2024</category><category>Java9</category><category>DevOps</category><category>Reactive Programming</category><category>Transcripts</category><category>Development</category></item></channel></rss>