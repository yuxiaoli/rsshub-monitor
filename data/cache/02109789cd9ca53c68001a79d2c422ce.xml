<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>BLOG | Samsung Research</title><link>https://research.samsung.com/blog</link><atom:link href="http://rsshub.isrss.com/samsung/research/blog" rel="self" type="application/rss+xml"></atom:link><description>BLOG | Samsung Research - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Tue, 25 Mar 2025 00:44:20 GMT</lastBuildDate><ttl>5</ttl><item><title>CheapNVS: Real-Time On-Device Novel View Synthesis for Mobile Applications</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;
&lt;b&gt;Novel View Synthesis (NVS)—&lt;/b&gt;the task of generating new perspectives of a scene from a single image—has vast potential in augmented reality (AR), robotics, and immersive media. &lt;b&gt;NVS is a notoriously ill-posed problem&lt;/b&gt;, as it is not only comprised of reverse-projecting an image onto the 3D space, but it also requires the completion of missing data in occluded regions. &lt;b&gt;Despite the recent advances&lt;/b&gt; in the literature, &lt;b&gt;NVS still suffers from several critical bottlenecks&lt;/b&gt;:
&lt;/p&gt;

&lt;p&gt;
• &lt;b&gt;Computational Overhead&lt;/b&gt;: Traditional pipelines rely on explicit 3D reconstruction or per-scene optimization (e.g., NeRF-based methods (Mildenhall, 2021)), which makes them computationally complex, and consequently, lowers their feasibility to be deployed on-device for real-time applications. More recent multi-view diffusion model-based solutions (Bourigault, 2024), as accurate as they might be, are quite expensive to deploy as well. 
&lt;/p&gt;

&lt;p&gt;
• &lt;b&gt;Limited Generalization&lt;/b&gt;: Many approaches only work on the specific camera baselines they are trained on (Yang Zhou, 2023), or require scene-specific training (Kerbl, 2023) (Mildenhall, 2021), which limits both scalability and practicality. Having to train a new model for each scene is not desirable, and definitely not practical in real-time scenarios.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;CheapNVS tackles these challenges by reimagining NVS as an efficient, end-to-end task&lt;/b&gt;. It leverages &lt;b&gt;lightweight modules to approximate 3D warping and performs inpainting in parallel to deliver real-time performance on mobile hardware&lt;/b&gt;, while maintaining competitive accuracy.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Novel View Synthesis – Reimagined&lt;/h2&gt;

&lt;p&gt;
Single-view Novel View Synthesis aims to synthesize a new image &lt;b&gt;ӏ&lt;sub&gt;t&lt;/sub&gt;&lt;/b&gt; from a given target camera pose &lt;b&gt;T&lt;/b&gt; by using the input image &lt;b&gt;ӏ&lt;sub&gt;s&lt;/sub&gt;&lt;/b&gt; and depth map of the input image &lt;b&gt;D&lt;/b&gt;.  Formally, it can be written as 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsAhEKAOMAUGpw.png&quot; width=&quot;25%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
where w(⋅;θ&lt;sub&gt;w&lt;/sub&gt; ) is the function that implements image warping, &lt;b&gt;M&lt;/b&gt; is the occlusion mask indicating the areas to be inpainted in the warped image, and f(⋅;θ&lt;sub&gt;f&lt;/sub&gt; )  is the function that implements inpainting.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;The issue&lt;/b&gt; with the equation above is that f(⋅;θ&lt;sub&gt;f&lt;/sub&gt; )  requires the output of  w(⋅;θ&lt;sub&gt;w&lt;/sub&gt; ), which makes the &lt;b&gt;NVS process inherently sequential. This sequential nature of NVS creates a performance bottleneck&lt;/b&gt;, no matter how performant f(⋅;θ&lt;sub&gt;f&lt;/sub&gt; ) or w(⋅;θ&lt;sub&gt;w&lt;/sub&gt; ) might be.
&lt;/p&gt;

&lt;p&gt;
We hypothesize that we can resolve this bottleneck by performing inpainting and warping in parallel. We then propose our “reimagined” novel view synthesis formulation, which is written as 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsAmqKAOYAUGpw.png&quot; width=&quot;65%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
where  ϕ(⋅;θ&lt;sub&gt;ϕ&lt;/sub&gt; ) is the function that outputs the occlusion mask. &lt;b&gt;This formulation performs inpainting and warping in parallel, which alleviates the bottleneck&lt;/b&gt;. Furthermore, note that f(⋅;θ&lt;sub&gt;f&lt;/sub&gt; ), w(⋅;θ&lt;sub&gt;w&lt;/sub&gt; ) and ϕ(⋅;θ&lt;sub&gt;ϕ&lt;/sub&gt; ) take the same inputs, which can be exploited for further computational savings.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;CheapNVS&lt;/h2&gt;

&lt;p&gt;
Based on the reimagined NVS formulation, &lt;b&gt;we propose to implement&lt;/b&gt;
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsArpKAOkAUGpw.png&quot; width=&quot;40%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
where &lt;b&gt;S&lt;/b&gt;, &lt;b&gt;P&lt;/b&gt;, &lt;b&gt;F&lt;/b&gt; and  w ̂(⋅;&lt;b&gt;θ&lt;sub&gt;w&lt;/sub&gt; ̂ &lt;/b&gt;) are the predicted flow, inpainting, the shared embedding and the flow predictor, respectively. The novel view is then synthesized as 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsAw36AOwAUGpw.png&quot; width=&quot;30%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
where &lt;b&gt;gs&lt;/b&gt; is the grid sampling operation that shifts each pixel by the offset values in &lt;b&gt;S&lt;/b&gt;.  We implement δ(⋅),∇(⋅),&lt;b&gt;w ̂&lt;/b&gt;(⋅),f(⋅) and ϕ(⋅) with neural networks by learning their parameters θ&lt;sub&gt;δ,ϕ,∇,f,w ̂&lt;/sub&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;
&lt;b&gt;RGBD Encoder&lt;/b&gt;: We leverage an off-the-shelf depth estimation model (Rene Ranftl, 2021) to generate &lt;b&gt;D&lt;/b&gt;, and leverage a MobileNetv2 (Sandler, 2018) as the encoder. 
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Extrinsics Encoder&lt;/b&gt;: The goal of this module is to condition the warping on a target camera pose, and to generalize across different target camera poses. This encoder takes in a camera transformation matrix, and lifts it to a 256-dimensional latent vector using a lightweight, two-layer MLP.  
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsA1MaAO8AUGpw.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; CheapNVS embeds target camera pose and RGB information into a shared latent space, which is used by flow, mask and inpainting decoders to perform “learned” warping and inpainting, to produce the synthesized novel view.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The outputs of both encoders are concatenated into the shared latent &lt;b&gt;F&lt;/b&gt;, allowing decoders to jointly reason about scene content and camera geometry.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Flow Decoder&lt;/b&gt;: Flow decoder leverages the shared latent &lt;b&gt;F&lt;/b&gt; to predict a shift-map, which contains the offset values to be applied on each pixel to perform the warping. Once the shift map is applied to the input image, we get the warped image which we then multiply with the mask &lt;b&gt;M&lt;/b&gt; to contain only the pixels coming from the source image. &lt;b&gt;Unlike traditional 3D warping&lt;/b&gt; (which projects pixels using depth and camera matrices), &lt;b&gt;this decoder learns to approximate warping directly&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Mask Decoder&lt;/b&gt;: This decoder uses the shared latent &lt;b&gt;F&lt;/b&gt; and generates the mask &lt;b&gt;M&lt;/b&gt;, with which we perform the blending between the warped input image and the generated inpainting.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Inpainting Decoder&lt;/b&gt;: Inpainting decoder leverages the shared latent &lt;b&gt;F&lt;/b&gt; to generate the inpainting output &lt;b&gt;P&lt;/b&gt;, which is then blended with the inverse of mask &lt;b&gt;M&lt;/b&gt; to produce the final synthesized view.
&lt;/p&gt;

&lt;p&gt;
All these decoders run in parallel and share the same architecture, which is formed of several decoder blocks of bilinear sampling, convolution and ELU activations. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Multi-stage Training&lt;/h2&gt;

&lt;p&gt;
We train our model in &lt;b&gt;a two-stage regime&lt;/b&gt;, where &lt;b&gt;we first train all encoders, as well as flow and mask decoders&lt;/b&gt; for a few epochs, and &lt;b&gt;then activate the inpainting decoder and train the entire network&lt;/b&gt;. This phased approach helps inpainting decoder start from a point where the network has learned warping up to a degree. This stabilizes the overall training and improves the results. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Experimental Results &lt;/h2&gt;

&lt;p&gt;
We train our models with &lt;b&gt;a combination of several losses&lt;/b&gt;; &lt;b&gt;L1 loss for the flow decoder, cross-entropy loss for the mask decoder&lt;/b&gt; and &lt;b&gt;L1 loss for the inpainting decoder&lt;/b&gt;.  We &lt;b&gt;train CheapNVS on COCO&lt;/b&gt; (Lin, 2014) to be comparable against AdaMPI (Yuxuan Han, 2022), but also &lt;b&gt;propose to use a random 174K subset of OpenImages&lt;/b&gt; (Kuznetsova, 2020). We evaluate and compare our method using SSIM, PSNR and LPIPS metrics, and use DPT (Rene Ranftl, 2021) (COCO training/evaluation) and Marigold (Ke, 2024) (OpenImages training/evaluation) to generate depth maps.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Quantitative Results &lt;/h2&gt;

&lt;p&gt;
We compare CheapNVS against AdaMPI (Yuxuan Han, 2022), which is the leading 3D photography method. We perform comparisons using both COCO and OpenImages as the training set. 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsA77KAPIAUGpw.png&quot; width=&quot;80%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Results on Open Images. The first two rows are trained on OpenImages, whereas the last two are trained on COCO. † indicates training on OpenImages by us. AdaMPI performs the conventional 3D warping we use as ground-truth, so we do not report its warping results.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsBAAKAPUAUGpw.png&quot; width=&quot;80%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 2.&lt;/b&gt; Results on COCO. The first two rows are trained on OpenImages, whereas the last two are trained on COCO. † indicates training on OpenImages by us. AdaMPI performs the conventional 3D warping we use as ground-truth, so we do not report its warping results. &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Tables 1 and 2 show that &lt;b&gt;CheapNVS outperforms AdaMPI on inpainting&lt;/b&gt;, and also performs warping successfully.  Another finding is that &lt;b&gt;OpenImages training produces better models&lt;/b&gt; – based on COCO and OpenImages evaluation– which shows the value of OpenImages and its diversity. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Runtime performance&lt;/h2&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsBDTKAPgAUGpw.png&quot; width=&quot;80%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 3.&lt;/b&gt; Runtime analysis on a desktop GPU (RTX 3090), mobile GPU (Samsung Tab 9+) and memory consumption.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Table 3 shows the runtime and memory consumption performance of AdaMPI and CheapNVS. &lt;b&gt;CheapNVS runs 10 times faster than AdaMPI&lt;/b&gt; on a consumer GPU. Furthermore, while &lt;b&gt;AdaMPI can not be ported to mobile due to external dependencies&lt;/b&gt; (used for 3D warping) , we manage to &lt;b&gt;port CheapNVS to device&lt;/b&gt; and &lt;b&gt;achieve ~30FPS runtime&lt;/b&gt;. &lt;b&gt;We also consume slightly less memory&lt;/b&gt; during inference compared to AdaMPI, showing the value of CheapNVS.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Qualitative Results &lt;/h2&gt;

&lt;p&gt;
Figure 2 shows that &lt;b&gt;CheapNVS accurately mimics 3D warping&lt;/b&gt;, and &lt;b&gt;learns to smooth occlusion masks&lt;/b&gt;, which is necessary for better blending. &lt;b&gt;CheapNVS performs quite competitively in inpainting, even outperforming AdaMPI&lt;/b&gt; in &lt;b&gt;removal of object boundary artefacts&lt;/b&gt; (see person on 1&lt;sup&gt;st&lt;/sup&gt; and 3&lt;sup&gt;rd&lt;/sup&gt; rows.). 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWsBIHaAPsAUGpw.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Left to right: Input, ground-truth occlusion mask, CheapNVS occlusion mask, ground-truth warped RGB, CheapNVS warped RGB, AdaMPI inpainting and CheapNVS inpainting.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Future Directions&lt;/h2&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Larger baselines&lt;/b&gt;: CheapNVS is trained on narrow baselines, and naturally struggles to warp properly in larger camera baselines. We aim to train CheapNVS on a more diverse interval of camera transformation matrices, which should address the larger baseline issue.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Depth dependency&lt;/b&gt;: Our method relies on an external depth estimator, which might introduce errors in the pipeline. We aim to use higher quality depth estimators, or even integrate depth estimation itself into our pipeline.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Inpainting teacher&lt;/b&gt;: We aim to improve inpainting accuracy even further by using a diffusion-based inpainting teacher, potentially finetuned with warp-back data. 
&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;
CheapNVS, by replacing traditional 3D warping with learnable modules and executing tasks in parallel, achieves real-time single-image novel view synthesis on mobile devices, without sacrificing quality. Its success on Open Images highlights the value of scalable training data, while phased training ensures robust optimization.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Publication&lt;/h2&gt;

&lt;p&gt;
Our paper “CheapNVS: Real-Time On-Device Narrow-Baseline Novel View Synthesis” will appear at International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025.
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/10888972&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://ieeexplore.ieee.org/document/10888972&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
Bourigault, E. a. (2024). MVDiff: Scalable and Flexible Multi-View Diffusion for 3D Object Reconstruction from Single-View. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.
&lt;/p&gt;
&lt;p&gt;
Ke, B. O. (2024). Repurposing diffusion-based image generators for monocular depth estimation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
&lt;/p&gt;
&lt;p&gt;
Kerbl, B. G. (2023). 3D Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics .
&lt;/p&gt;
&lt;p&gt;
Kuznetsova, A. R.-T. (2020). The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision.
&lt;/p&gt;
&lt;p&gt;
Lin, T.-Y. M. (2014). Microsoft coco: Common objects in context. European Conference on Computer Vision .
&lt;/p&gt;
&lt;p&gt;
Mildenhall, B. a. (2021). Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 99-106.
&lt;/p&gt;
&lt;p&gt;
Rene Ranftl, A. B. (2021). Vision Transformers for Dense Prediction. Proceedings of the IEEE/CVF International Conference on Computer Vision.
&lt;/p&gt;
&lt;p&gt;
Sandler, M. A.-C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
&lt;/p&gt;
&lt;p&gt;
Yang Zhou, H. W. (2023). Single-view view synthesis with self-rectified pseudostereo. International Journal of Computer Vision, 2032-2043.
&lt;/p&gt;
&lt;p&gt;
Yuxuan Han, R. W. (2022). Single-view view synthesis in the wild with learned adaptive multiplane images. ACM SIGGRAPH .
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/CheapNVS-Real-Time-On-Device-Novel-View-Synthesis-for-Mobile-Applications</link><guid isPermaLink="false">https://research.samsung.com/blog/CheapNVS-Real-Time-On-Device-Novel-View-Synthesis-for-Mobile-Applications</guid><pubDate>Tue, 18 Mar 2025 16:00:00 GMT</pubDate><author>Mehmet Kerim Yucel|Albert Saa-Garriga</author><category>AI</category><category>3DReconstruction</category><category>VirtualReality</category></item><item><title>SRCB&#39;s Non-Uniform Constellation Scheme in GLOBECOM 2024 Conference</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;
The current 5G systems adopt conventional uniform quadrature-amplitude modulation (QAM) with signal points on the rectangular grid, which results in a theoretical 1.53 dB gap to the Shannon capacity. To address these challenges, the non-uniform constellations (NUCs) relax this restriction by employing constellation-shaping techniques to approach the Shannon capacity for extremely high spectral efficiency, emerging as one of the promising technologies for the physical layer communication evolution. The constellation shaping solutions for NUCs can be categorized into two types [1]: 1) probabilistic shaping by tackling the symbol probabilities with a shaping encoder; 2) geometrical shaping by optimizing the positions of the constellations. Probabilistic shaping solutions require a shaping decoder at receiver side, which increases the overall complexity and is not suitable for commercial deployment. Geometrical shaping solutions only require a new set of constellation points and the quantization in hardware implementations, which is superior
&lt;/p&gt;

&lt;p&gt;
To address these issues, we integrate the biological observation into the constellation design. The idea is quiet interesting that the constellation arrangement is regarded as the progress of the seed growth. In order to obtain the enough sunlight and energy, the adjacent seeds are moving away as far as from each other possible which naturally solves the key challenge in the constellation design: how to maximize the Euclidean distances among the constellations. Specifically, we transform the conventional constellation design problem into a seed arrangement problem within a circular region. The proposed constellations, called ‘CirNUC’, adopt the contact-pressure model and utilize the centroidal Voronoi tessellation (CVT) method to maximize the average constellation point distance, which results in robust performance. Next, the particle swarm optimization (PSO) algorithm is proposed to minimize averaged bit error probability (ABEP), which is regarded as the lower bound of the bit error rate (BER) performance.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;System Setup&lt;/h2&gt;

&lt;p&gt;
We consider a point-to-point communication system with single antenna. The transmit symbols are randomly selected from an M-point constellation set with the normalized power. The symbols are associated to the bits at the input of the modulator by a one-to-one mapping. We employ two metrics to design the constellations: symbol error probability (SEP) and the averaged bit error probability (ABEP). The goal of our work is to arrange the locations of constellations s = [s&lt;sub&gt;1&lt;/sub&gt;, s&lt;sub&gt;2&lt;/sub&gt;, …, s&lt;sub&gt;M&lt;/sub&gt;]&lt;sup&gt;T&lt;/sup&gt; for minimizing SEP and minimizing ABEP.
&lt;/p&gt;


&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Proposed CirNUC&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;A.SEP minimization &lt;/h2&gt;

&lt;p&gt;
We transform the conventional constellation design problem to a seed growth problem. The circle-shape constellations, called “CirNUC-SEP”, are proposed to enlarge the Euclidean distances among the constellations. The constellation initialization and the constellation optimization are corresponds to the seed birth and seed growth, which is introduced as follows: 
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Seed Birth&lt;/b&gt;: We select a circle with radius r and initialize {s&lt;sub&gt;m&lt;/sub&gt; }&lt;sup&gt;M&lt;/sup&gt;&lt;sub style=&quot;margin-left: -12px;&quot;&gt;m=1&lt;/sub&gt; in Ω, which can be regarded as the birth of the seeds and the generators of the Voronoi tessellation.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Seed Growth&lt;/b&gt;: Next, the seeds grow up to occupy more space for obtaining enough sunlight and energy. The CVT method is introduced to model this progress, where the mass centroid c&lt;sub&gt;m&lt;/sub&gt; of V&lt;sub&gt;m&lt;/sub&gt; is computed to represent the evolution of the growth, which is given by
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWmXDCKAL4AUGpw.png&quot; width=&quot;20%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
where ρ(v) is the density function of the region. For the uniform density, the Voronoi region can be viewed as the polygon and calculate its coordinate of the mass centroid. Then, we employ the centroid as the updated constellation and the new generator of the Voronoi region. By the iteration of the centroid and the Voronoi region, the adjacent points move away from each other resulting in the increase of the Euclidean distance until the convergence. In addition, the Voronoi regions can also be extended to weighted Voronoi regions, which can approximate a Gaussian distribution on the two-dimensional plane, which benefits the received signals with a Gaussian distribution to approach the Shannon capacity. An example of the proposed 64 points CirNUC with golden angle modulation (GAM) as the initialization is shown in Figure 1.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWmXJ0KAMEAUGpw.png&quot; width=&quot;30%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; The proposed 64-CirNUC-SEP and its Voronoi region in a circle.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;B.ABEP minimization&lt;/h2&gt;

&lt;p&gt;
We jointly consider the constellation Euclidean distance and hamming distance of its bit labelling to minimize the BER. Since the joint design of the bit labeling and constellation design is a NP-hard problem, we propose “CirNUC-ABEP” that employs the CVT method to enlarge the Euclidean distances among the constellations and then uses the PSO algorithm to further minimize ABEP in (4), which is described in the following: First, we utilize the QAM constellations and its gray labeling as the initialization, and perform the CVT method for K times and find the constellations with the minimum ABEP. Next, the PSO algorithm is applied to further optimize the constellations with the ABEP fitness function.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;C.MLP-based demodulation&lt;/h2&gt;

&lt;p&gt;
We propose a light-weight MLP-based demodulator to reduce the complexity caused by the asymmetric of constellations. The total framework of the proposed modulation and demodulation is shown in Figure 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWmXOnaAMQAUGpw.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; A simplified demo of the proposed modulation and demodulation.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;
The SER and BER performance is evaluated for the proposed CirNUC and benchmarks which is shown in Figure 3. (a) SER performance for M =256. The proposed CirNUC-SEP scheme outperforms the QAM and GAM schemes for approximately 0.6dB and 0.3dB, respectively. (b): BER performance for M = 32. The code rate is 0.5 with low-density parity-check code. The GAM employs the bit labeling method in [2] and the QAM employs the gray mapping. The proposed scheme achieves 0.7dB and 1.6dB gain compared to QAM and GAM due to the ABEP minimization in our solution.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3419ee9fbf/AZWmXVfKAMcAUGpw.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; The SER and BER performance of the proposed CirNUC-SEP and CirNUC-ABEP.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;
This paper investigated the modulation and demodulation in the wireless communication network and proposed a transceiver solution. We found the answer from the biology and contributed to enlarging the constellation distances, which provides a high spectral efficiency compared to the current QAM-based network. The ramification of this paper is that it provides a new perspective for the NUCs design to replace the current modulation, which performs a candidate for the standardization and the upcoming products.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/10901255&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://ieeexplore.ieee.org/document/10901255&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
1.J. Barrueco, J. Montalban, E. Iradier, and P. Angueira, “Constellation design for future communication systems: A comprehensive survey,” IEEE Access, vol. 9, pp. 89 778–89 797, Jun. 2021.
&lt;/p&gt;
&lt;p&gt;
2.L. Xiao, X. Zhai, Y. Liu, G. Liu, P. Xiao, and T. Jiang, “A unified bit-to-symbol mapping for generalized constellation modulation,” China Commun., vol. 20, no. 6, pp. 229–239, Jun. 2023
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/SRCB-s-Non-Uniform-Constellation-Scheme-in-GLOBECOM-2024-Conference</link><guid isPermaLink="false">https://research.samsung.com/blog/SRCB-s-Non-Uniform-Constellation-Scheme-in-GLOBECOM-2024-Conference</guid><pubDate>Sun, 16 Mar 2025 16:00:00 GMT</pubDate><author>Dong Wang|Shiwen Ma</author><category>Communications</category><category>Non-uniform constellations</category><category>Centroidal Voronoi tessellation</category></item><item><title>FaceMe: Robust Blind Face Restoration With Personal Identification</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1 Introduction&lt;/h2&gt;

&lt;p&gt;
Face restoration focuses on improving the quality of facial images by removing complex degradation and enhancing details. Face restoration is inherently a highly ill-posed task because a single low-quality input can correspond to many potential high-quality counterparts scattered throughout the high-quality image space. Most advanced face restoration methods cannot guarantee identity consistency.
&lt;/p&gt;

&lt;p&gt;
Recently, some studies have used high-quality reference images of the same identity to enhance identity consistency in restored facial images. However, the feature alignment-based methods would achieve low quality of the restoration if the features are not well aligned. The diffusion model based fine-tuning methods typically requires 5~20 reference images and can be significantly effected by the quality of them.
&lt;/p&gt;

&lt;p&gt;
In this paper, we proposed FaceMe, a fine-tuning-free personalized blind face restoration method based on the diffusion model. Given a low-quality input and either a single or a few high-quality reference images of the same identity, FaceMe restores high-quality facial images and maintains identity consistency within seconds. Remarkably, changing identities does not require fine-tuning, and the reference images can have any posture, expression, or illumination. Furthermore, the quality of the reference image does not significantly impact the quality of the restored image. To our knowledge, this is the first approach that leverages diffusion prior for personalized face restoration tasks, which does not require fine-tuning when changing identity.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2Method&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.1Problem Definition&lt;/h2&gt;

&lt;p&gt;
Let X,Y,D,and G denotes the degraded facial image, the corresponding high-quality facial image, the degradation function, and the generation function, respectively. The objective of personalized face restoration is to generate Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;=G(X│Ref), while ensuring the following 3 constraints are satisfied: 
Consistency: D(Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;)  ≡ X,Realness:  Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;∼q(Y),Identity Consistency: Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;∼ID(Y), 
where q(Y) denotes the distribution of high-quality facial images, Ref and ID(Y) denote the reference images and the distribution of high-quality facial images of the same identity as Y respectively.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.2Proposed Personal Face Restoration&lt;/h2&gt;

&lt;p&gt;
&lt;b&gt;Identity encoder&lt;/b&gt;. We combine the CLIP [2] image encoder ϵ and the ArcFace [1] facial recognition module ψ, to extract identity features from facial images. We employ MLPs to align them, then merge them using MLPs, resulting in s&lt;sub&gt;i&lt;/sub&gt;∈R&lt;sup&gt;d&lt;/sup&gt;, where i denotes the index of N reference images, d is the dimension of cross-attention in the diffusion model.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Combining and replacing&lt;/b&gt;. We concatenate all s&lt;sub&gt;i&lt;/sub&gt; as s. We use a simple prompt: “a photo of 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRsg-h6AE4AUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Overview of proposed FaceMe (left) and training data construction pipeline (right).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
face.”, during the training and test phases. Let c&lt;sub&gt;text&lt;/sub&gt;={e&lt;sub&gt;1&lt;/sub&gt;,…,e&lt;sub&gt;5&lt;/sub&gt;} represent the embedding of the text. Then we replace the embedding e&lt;sub&gt;4&lt;/sub&gt; corresponding to “face” with s as c&lt;sub&gt;id&lt;/sub&gt;={e&lt;sub&gt;1&lt;/sub&gt;,e&lt;sub&gt;2&lt;/sub&gt;,e&lt;sub&gt;3&lt;/sub&gt;,s,e&lt;sub&gt;5&lt;/sub&gt;}
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Training strategy&lt;/b&gt;. The model consists of two trainable modules, i.e., ControlNet and ID encoder (Identity Encoder in Fig. 1). In this work, we propose a two-stage training strategy. In training stage I, we simultaneously train ControlNet and ID encoder, but only save ID encoder’s weights. In training stage II, we fix the ID encoder and only train ControlNet. In this training process, we randomly replace identity embedding c&lt;sub&gt;id&lt;/sub&gt; with non-identity embedding c&lt;sub&gt;text&lt;/sub&gt; with a 50% probability.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Inference strategy&lt;/b&gt;. We embed the low-quality input directly into the initial random Gaussian noise according to the training noise scheduler. We use Classifier-free guidance (CFG) [3] for personalized guidance. In addition, to mitigate the possibility of color shift, we apply wavelet-based color correction to the final result.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.3 Training Data Pool Construction&lt;/h2&gt;

&lt;p&gt;
To our knowledge, no publicly available facial dataset can support diffusion models for training with multiple reference images of the same identity. In this study, we employ synthetic facial images as reference facial images to construct our training data pool. 
&lt;/p&gt;

&lt;p&gt;
We synthesize multiple reference facial images of the same identity as the given facial image using Arc2Face, equipped with Control-Net. Given a pair of reference and pose, Arc2Face can synthesize facial images that maintain the identity of reference image and the given pose.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Pose reference data pool&lt;/b&gt;. We extract the pose attribute and the expression attribute for each image in FFHQ. We conduct K-Means clusters on these images based on the two attributes to get c&lt;sub&gt;1&lt;/sub&gt; and c&lt;sub&gt;2&lt;/sub&gt; cluster centers, resulting in c&lt;sub&gt;1&lt;/sub&gt;*c&lt;sub&gt;2&lt;/sub&gt; disjoint subsets, forming the pool.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Same identity&lt;/b&gt;. For each image, we randomly sample an pose image from any subset in the pool, Using them as input for Arc2Face, we get one reference image. We then assess the identity similarity between the input image and the generated reference one. If the similarity falls below δ, we re-sample pose image for regeneration. If an acceptable result is not obtained after 3 attempts, we stop generating it. We name the synthesized reference images for the FFHQ dataset as FFHQRef.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRsirC6AFEAUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Quantitative comparison. The bold numbers represent the best performance.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3Results&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.1Experimental Settings&lt;/h2&gt;

&lt;p&gt;
&lt;b&gt;Training datasets&lt;/b&gt;. Our training dataset consists of FFHQ dataset and our synthesized FFHQRef dataset, with all images resized to 512×512. The corresponding degraded image is synthesized using a degradation model (see in our paper).
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Implementation details&lt;/b&gt;. We employ SDXL model stable-diffusion-xl-base-1.0 as our base diffusion model and the CLIP image encoder as part of our identity encoder, both of which are fine-tuned by PhotoMaker. We use the Adam optimizer to optimize the network parameters with a learning rate of 5×10&lt;sup&gt;-5&lt;/sup&gt; for two training stages. The training process is implemented using the PyTorch framework and is conducted on eight A40 GPUs, with a batch size of 4 per GPU. The two training stage are trained 130K and 210K iterations, respectively.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Testing datasets&lt;/b&gt;. We use one synthetic dataset CelebRef-HQ and three real-world datasets: LFW-Test, WebPhoto-Test, and WIDER-Test for test. For the synthetic dataset, we randomly select 150 identities and select one image per identity as the ground truth, using 1∼4 images of the same identity as reference images. For the real-world datasets, due to the lack of reference images with the same identity, we first use face restoration method, i.e., Codeformer, to restore low-quality input. The restored images are then used as input to Arc2Face to generate reference images.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.2Quantitative Results&lt;/h2&gt;

&lt;p&gt;
Tab. 1 shows the performance of FaceMe on the synthetic dataset CelebRef-HQ. As shown, our method achieves the best performance in PSNR, FID, LMD, and IDS, and the second-best performance in LPIPS. Additionally, it is worth noting that FaceMe has significantly improved in IDS, which demonstrates its ability in personalization.
&lt;/p&gt;

&lt;p&gt;
As presented in Tab.1, our FaceMe achieves the best FID score on the LFWTest and Wider-Test datasets. LFW-Test and Wider-Test are mildly degraded and heavily degraded real-world dataset, respectively. The excellent performance on both datasets indicates that FaceMe is capable of adapting to complex degradation scenarios in the real world, demonstrating exceptional robustness.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.3Quantitative Results&lt;/h2&gt;

&lt;p&gt;
The visualization results are shown in Fig.2. It can be observed that the compared methods either produce many artifacts in the restored images or restore high quality images but fail to 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRskD2aAFQAUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Qualitative comparison on CelebRef-HQ.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRskHA6AFcAUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Qualitative comparison on real-world faces.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
maintain identity consistency with the ground truth. In contrast, our FaceMe can restore high quality images while preserving identity consistency.
&lt;/p&gt;

&lt;p&gt;
Fig. 3 shows the visual comparisons of different methods. It is clear that, compared to state-of-the-art methods, our method can handle more complex scenes and restore high-quality images 
without introducing unpleasant artifacts.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4Conclusion&lt;/h2&gt;

&lt;p&gt;
We propose a method to address the issue of identity shift in blind facial image restoration. Based on diffusion model, we use identity-related features extracted by identity encoder to guide the diffusion model in recovering face images with consistent identities. Our method supports any number of reference images input through simple combine identity related features. In addition, the strong robustness of the identity encoder allows us to use synthetic images as reference images for training. Moreover, our method does not require fine-tuning the model when changing identities. The experimental results demonstrate the superiority and effectiveness of our method.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2501.05177&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://arxiv.org/abs/2501.05177&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;


&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] Deng, J.; Guo, J.; Xue, N.; and Zafeiriou, S. 2019. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 4690–4699..
&lt;/p&gt;
&lt;p&gt;
[2] Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In ICML, 8748–8763. PMLR.
&lt;/p&gt;
&lt;p&gt;
[3] Ho, J.; and Salimans, T. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/FaceMe-Robust-Blind-Face-Restoration-With-Personal-Identification</link><guid isPermaLink="false">https://research.samsung.com/blog/FaceMe-Robust-Blind-Face-Restoration-With-Personal-Identification</guid><pubDate>Mon, 24 Feb 2025 16:00:00 GMT</pubDate><author>Jia Ouyang|Hyunhee Park</author><category>AI</category><category>Face Restoration</category><category>AAAI</category></item><item><title>Deep Learning for CSI Feedback: One-Sided Model and Joint Multi-Module Learning Perspectives</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1.Introduction&lt;/h2&gt;

&lt;p&gt;
Massive multiple-input multiple-output (MIMO) technology plays a crucial role in delivering the potential of 5G and meeting its requirements. To fully exploit the potential of MIMO technology, it is crucial to obtain downlink channel state information (CSI) on the base station (BS) side by feeding the estimated downlink CSI from the user equipment (UE) back to the BS through the uplink channel, as shown in Fig. 1. The feedback of CSI leads to extra overhead, which escalates significantly with the increase in the number of service antennas. When the uplink channel resources are limited, the challenge of CSI feedback lies in maintaining the accuracy of the feedback while minimizing the feedback overhead.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQS1nKAHwAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; A communication framework with CSI feedback&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Traditional methods for reducing the CSI feedback overhead include techniques based on codebooks or compressed sensing (CS). In the past few years, artificial intelligence (AI) has been utilized in the CSI feedback space to improve the precision of CSI reconstruction; see, for instance, the AI for CSI feedback enhancement in the study item of the Third Generation Partnership Project (3GPP) Release 18 [1].
&lt;/p&gt;

&lt;p&gt;
In this article, we first introduce an AI-based one-sided CSI feedback method, wherein the DL model is only employed at the BS, and AI-based multi-module learning involving the CSI feedback. In addition, we articulate the prospects and challenges pertaining to AI-driven CSI feedback tasks, taking into account both the AI limitations and the real-world implementation of AI models. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2.AI-Based One-Sided CSI Feedback&lt;/h2&gt;

&lt;p&gt;
For one-sided CSI feedback, the AI model is implemented solely on the BS, to reduce transmission overhead and improve performance. In addition, BS-sided model can avoid multi-vendors collaboration effort and preserve model confidentiality and data privacy. One-sided CSI feedback architectures, such as CS-CsiNet [2], CFnet [3], and CSI-PPPNet [4], improve CSI reconstruction using deep learning (DL). CSI-PPPNet [4] uses a single DL model for arbitrary compression rates, with CSI compressed at the UE and iteratively recovered at the BS, a feature not achieved by the other two methods. This approach decouples training from compression, simplifying model maintenance for the network vendor. Fig. 2 shows the normalized mean square errors (NMSE) performance of AI-based one-sided CSI feedback (CS-CsiNet, CSI-PPPNet) and two-sided CSI feedback (CsiNet) for indoor and urban macrocell (UMa) scenarios, with 256 subcarriers. The BS uses a ULA with 32 omnidirectional antennas, and the UE uses a single omnidirectional antenna. 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQT1baAH8AUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Performance comparison of AI-based one-sided CSI feedback networks, i.e., CS-CsiNet and CSI-PPPNet, and AI-based two-sided CSI feedback network, i.e., CsiNet, for indoor and UMa scenarios&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3.Joint Multi-Module Learning with CSI Feedback&lt;/h2&gt;

&lt;p&gt;
To fully exploit MIMO technology and improve throughput, it is crucial to jointly design multiple modules, including channel coding, CE, PD, and precoding, alongside the CSI feedback task.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.1 Joint CSI Compression and Channel Coding&lt;/h2&gt;

&lt;p&gt;
The CSI compression can be viewed as a source compression task, which can be designed independently of channel coding, following the paradigm of separate source-channel coding (SSCC).
&lt;/p&gt;

&lt;p&gt;
Using AI-based source coding methods, the CSI is compressed by an encoder and reconstructed using the corresponding decoder. When the quality of the channel deteriorates to such an extent that it surpasses the processing capacity of the channel coding, the precision of the reconstructed CSI within the SSCC framework experiences a significant drop, a phenomenon referred to as the ````````cliff effect&#39;&#39;. The use of heavily distorted CSI for the creation of precoding vectors results in an undesirable reduction in the system throughput. While the hybrid automatic repeat request mechanism and other methods can alleviate this problem, they also cause a delay in the downlink CSI acquisition and an increase in the feedback overhead.
&lt;/p&gt;

&lt;p&gt;
To address the &#39;&#39;cliff effect&#39;&#39;, a new CSI feedback architecture, coined as DJSCC, was introduced in [5]. This approach leverages DL to integrate source and channel coding, training the system with a dataset that encompasses both the source and the wireless channel. The performance of the SSCC and DJSCC networks for CSI feedback with the same overhead is shown in Fig. 3. Both networks are trained and tested on the same dataset, which is generated by QuaDriGa in an FDD indoor scenario. The number of feedback symbols n is set to 16. We employ explicit feedback, where the system employs 256 subcarriers. The BS utilizes a ULA with half-wavelength antenna spacing and 32 omnidirectional antennas, while the UE deploys a single omnidirectional antenna. 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQU_SaAIIAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Performance comparison between SSCC, DJSCC, and ADJSCC with the same feedback overhead. The label &#39;&#39;SSCC_en16_Q2_n4_R_1/2&#39;&#39; refers to the SSCC network where the encoder has 16 outputs of real numbers, 2 quantization bits, a QAM modulation order of 4, and a coding rate of 1/2&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
From the experimental results shown in Fig. 3, we can observe that the SSCC network suffers from a severe &#39;&#39;cliff effect&#39;&#39;. DJSCC addresses this issue by yielding only a gradual rise in NMSE with the reduction of the SNR in the feedback channel, while enhancing the performance across all SNR levels. To address the generalization problem, an attention mechanism-based DJSCC network (ADJSCC) was proposed in [5] to dynamically adjust the ratio of source coding and channel coding outputs based on the uplink SNR.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.2 Joint CSI Compression, Channel Coding and Precoding &lt;/h2&gt;

&lt;p&gt;
In a multiuser MIMO scenario, the precoding vector for one UE affects interference to/from other UEs. To maximize the downlink sum-rate, precoding vectors are jointly designed after the BS aggregates CSI from all UEs. Joint feedback and precoding networks (JFPNet) have been proposed to optimize both feedback and precoding design [6]. Notably, [6] employs implicit feedback, where the feedback source is the eigenvector matrix rather than full CSI.
&lt;/p&gt;

&lt;p&gt;
Fig. 4 shows the downlink SE of different methods. Initially, the downlink CSI undergoes preprocessing to generate the eigenvalues and the eigenvector matrix, and the DJSCC network is then used to compress the eigenvector matrix for feedback. The uplink channel SNR during the training phase spans a range from -10 dB to 10 dB, following a uniform distribution and the feedback overhead is fixed to n=32 symbols. The BS and UE use ULA omnidirectional arrays with 32 and 4 elements, respectively. The full CSI consists of 624 subcarriers, i.e., 13 subbands. The JFPNet, which jointly implements CSI compression, channel coding and precoding, achieves the highest SE at all SNRs. The label &#39;&#39;SFPNet&#39;&#39; indicates that the precoding module is trained separately from the DJSCC module for CSI feedback, and &#39;&#39;JFPNet&#39;&#39; refers to the joint training of the CSI feedback and precoding modules. &#39;&#39;JFPNet’’ outperforms &#39;&#39;SFPNet&#39;&#39;, highlighting the benefits of multi-module learning for extracting task-related semantic information. &#39;&#39;DJSCC_BD_WF&#39;&#39; and &#39;&#39;PF_BD_WF&#39;&#39; refer to DJSCC CSI feedback and perfect CSI feedback, respectively, using traditional non-AI precoding methods (block diagonalization (BD) and water-filling (WF) algorithms). 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQVWB6AIUAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 4.&lt;/b&gt; Performance comparison: a joint training approach and a separate training approach are utilized for the DL-based CSI feedback module and precoding module, respectively&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4.Opportunities and Challenges &lt;/h2&gt;

&lt;p&gt;
DL-based CSI feedback in FDD massive MIMO systems reduces overhead and complexity. Using a one-sided model and joint multi-module learning enhances practical deployment and end-to-end performance. However, challenges remain in both AI technology and the real-world implementation of AI models for CSI feedback. Current datasets are mostly statistical, generated by simulators under the assumption that channels across cells are independent and identically distributed. However, this assumption doesn’t reflect real-world conditions, leading to mismatched data and reduced model performance, which can degrade system throughput. 
&lt;/p&gt;

&lt;p&gt;
To support AI/ML for CSI feedback in current 5G or future 6G systems, for standardization, data collection, performance monitoring, and feedback of inference results are the main considerations. Data collection is needed for model training and performance monitoring, including data for AI inputs and ground truth for labeling. Data omission or compression may be needed to reduce the overhead. Performance monitoring is another unique aspect compared to traditional non-AI communication systems. Metrics and procedures for performance monitoring also need to be standardized. Recall that new signaling for model inference results in feedback. For example, for joint multi-module learning for CSI feedback, and since channel coding and potential modulation are already included in AI encoders and the output of the encoder may not fit in the existing signaling processes, new CSI feedback types need to be defined in standards. Besides, two-sided models, wherein the encoder and decoder need to be pairwise trained, will require inter-vendor calibration. To avoid offline calibrations between different vendors, some proposals are under discussion in 3GPP in Release 19. For example, the training data set and the model structure can be standardized. Alternatively, a well-trained model or model parameters can be delivered over the air interface to improve the performance.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;5.Conclusion &lt;/h2&gt;

&lt;p&gt;
In this article, we investigated the recent developments of DL-based CSI feedback from the perspectives of one-sided model and joint multi-module learning. Firstly, we introduced a one-sided CSI feedback architecture that only replaces the traditional decoding module with a DL module. Subsequently, we delved into CSI feedback architectures, jointly learned with various modules, e.g., channel coding, and precoding. For example, the joint design of the CSI feedback and channel coding via DJSCC overcomes the “cliff effect” observed in the traditional SSCC-based CSI feedback methods. Unlike traditional approaches of designing each module independently, multi-module joint design leverages DL techniques to jointly optimize the various modules and exploit the interdependencies between different modules, and can consequently enhance the overall system performance and training efficiency.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10812964&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10812964&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1]J. Guo, C. -K. Wen, S. Jin and X. Li, &quot;AI for CSI Feedback Enhancement in 5G-Advanced,&quot; in IEEE Wireless Communications, vol. 31, no. 3, pp. 169-176, June 2024
&lt;/p&gt;
&lt;p&gt;
[2]C. -K. Wen, W. -T. Shih and S. Jin, &quot;Deep Learning for Massive MIMO CSI Feedback,&quot; in IEEE Wireless Communications Letters, vol. 7, no. 5, pp. 748-751, Oct. 2018, doi: 10.1109/LWC.2018.2818160.
&lt;/p&gt;
&lt;p&gt;
[3]J. Guo, C. -K. Wen, M. Chen and S. Jin, &quot;Environment Knowledge-Aided Massive MIMO Feedback Codebook Enhancement Using Artificial Intelligence,&quot; in IEEE Transactions on Communications, vol. 70, no. 7, pp. 4527-4542, July 2022, doi: 10.1109/TCOMM.2022.3180388.
&lt;/p&gt;
&lt;p&gt;
[4]W. Chen, W. Wan, S. Wang, P. Sun, G. Y. Li and B. Ai, &quot;CSI-PPPNet: A One-Sided One-for-All Deep Learning Framework for Massive MIMO CSI Feedback,&quot; in IEEE Transactions on Wireless Communications, vol. 23, no. 7, pp. 7599-7611, July 2024, doi: 10.1109/TWC.2023.3342735.
&lt;/p&gt;
&lt;p&gt;
[5]J. Xu, B. Ai, N. Wang and W. Chen, &quot;Deep Joint Source-Channel Coding for CSI Feedback: An End-to-End Approach,&quot; in IEEE Journal on Selected Areas in Communications, vol. 41, no. 1, pp. 260-273, Jan. 2023, doi: 10.1109/JSAC.2022.3221963.
&lt;/p&gt;
&lt;p&gt;
[6]Y. Guo, W. Chen, J. Xu, L. Li and B. Ai, &quot;Deep Joint CSI Feedback and Multiuser Precoding for MIMO OFDM Systems,&quot; in IEEE Transactions on Vehicular Technology, vol. 74, no. 1, pp. 1730-1735, Jan. 2025, doi: 10.1109/TVT.2024.3452409.
&lt;/p&gt;
&lt;p&gt;
[7]H. Jiang, M. Cui, D. W. K. Ng and L. Dai, &quot;Accurate Channel Prediction Based on Transformer: Making Mobility Negligible,&quot; in IEEE Journal on Selected Areas in Communications, vol. 40, no. 9, pp. 2717-2732, Sept. 2022, doi: 10.1109/JSAC.2022.3191334.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/Deep-Learning-for-CSI-Feedback-One-Sided-Model-and-Joint-Multi-Module-Learning-Perspectives</link><guid isPermaLink="false">https://research.samsung.com/blog/Deep-Learning-for-CSI-Feedback-One-Sided-Model-and-Joint-Multi-Module-Learning-Perspectives</guid><pubDate>Wed, 22 Jan 2025 16:00:00 GMT</pubDate><author>Feifei Sun</author><category>Communications</category><category>6G</category><category>CSI feedback</category></item><item><title>FiRa Consortium Release 3.0 - UWB Core Specification and Certification Program Launch</title><description>&lt;p&gt;
In December 2024, FiRa Consortium announced launching of Release 3.0 Core Certification Program and Release 3.0 of Core Specifications of ultra-wideband (UWB) MAC, PHY and Link Layer subsystems in devices. This is a major milestone for FiRa after the successful release of 2.0 Certification Program, which supports UWB features for localization and device-to-device service in, October 2023.
&lt;/p&gt;

&lt;p&gt;
While FiRa Core 1.0 Release focused on UWB technology for use cases such as hands- free access control service, FiRa 2.0 Release extended FiRa UWB technology to location based service and device-to-device service, this FiRa 3.0 Release adds support for public transport service, tap-free payment and car access digital key.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3a4bb21f3e/AZRIj4mqABcAUGo7.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Usecases Examples of FiRa 3.0 Core Spec &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
FiRa is introducing three major UWB features in its Core 3.0 Specifications:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Car Digital Key UWB&lt;/b&gt;: FiRa adds the Car Connectivity Consortium® (CCC) Digital Key UWB feature to its Certification Program, which measures the proximity of a mobile device to the car, employing a one-to-many double-sided two-way ranging protocol defined by CCC.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Hybrid UWB Scheduling (HUS)&lt;/b&gt;: Enables advanced UWB applications that require various combinations of FiRa features to work together in a deterministic way, ensuring optimized performance in complex environments such as public transport systems.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Dedicated Data Transfer&lt;/b&gt;: Designed to use UWB for dedicated data transfer, this advancement allocates entire time slots exclusively for data transfer, independent of ranging operations.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
The FiRa Public Transport usecase consists of three main phases, illustrated in Figure. 2 and described below:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
Phase 1: Discovery process at the station entrance. FiRa 2.0 technology such as Contention based ranging, BLE device discovery can be used at this stage. 
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
Phase 2: Untracked Navigation using DL-TDoA from FiRa 2.0 specification when walking to the gate or to the platform.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
Phase 3: Gate selection and fare transaction when approaching and walking through the gate. Hybrid UWB Scheduling features is used for supporting payment transaction while ranging with gates. 
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRxrF26AGoAUGo7.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Visualization of the three phases of the UWB based fare transaction system. One of the anchors is colored in red, to indicate its primary role. The anchor in this role is responsible for ensuring that all anchors operate in a synchronized manner.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
With the extended feature set available in &lt;b&gt;FiRa Certification Program 3.0&lt;/b&gt;, it enables devices to support an even wider and richer set of use cases and applications, in an interoperable fashion. Devices can select the specific features for the certification with all the features of Certification Release 2.0 also fully available in Release 3.0. Launch of FiRa Certification Program 3.0, will help in improving the adoption of UWB technology into more industry verticals.
&lt;/p&gt;

&lt;p&gt;
Samsung Galaxy S24 Plus models were adopted as Reference Devices for  FiRa 3.0 Certification Program to test the device’s interoperability. This would lead all the FiRa Certified Devices to become interoperable with Samsung Galaxy mobile phones featured with the UWB chipset. Those models will become the world’s first FiRa 3.0 certified devices in the upcoming month.
&lt;/p&gt;

&lt;p&gt;
FiRa Core 3.0 Core Specifications and Certification Program are exclusively available to FiRa Consortium members. For more information, visit www.firaconsortium.org
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Conclusion and Next Steps&lt;/h2&gt;

&lt;p&gt;
FiRa evolves its technology and certification program to develop more market attractive use-cases. Beyond customer centric use cases of FiRa 1.0, 2.0 and 3.0 releases, FiRa sees another opportunity at industrial area such as factory, warehouse and hospital where FiRa use cases such as asset tracking, foot traffic and shopping behavior analytics will be crucially important for the business. Enhancing FiRa UWB ranging technology for the upcoming release will enable this promising scenario successful in the market.
&lt;/p&gt;

&lt;p&gt;
Moreover, FiRa keeps trying to co-work with other consortiums to extend FiRa UWB ecosystem. Working with CCC (Car Connectivity Consortium®) is one such remarkable collaboration for the successful adoption of FiRa 3.0 in the market. FiRa also plans to provide conformance certification to Connectivity Standards Alliance® Aliro for the next release. By harmonizing standards and certification processes, FiRa seeks to create a more cohesive UWB ecosystem. Also this partnership will pave the way forward with reduced development and certification costs for industry stakeholders while promoting interoperability across various applications and strengthening the broader UWB ecosystem as a whole.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;About FiRa Consortium&lt;/h2&gt;

&lt;p&gt;
The FiRa Consortium is a member-driven organization dedicated to transforming the way we interact with our environment by enabling precise location awareness for people and devices using the secured fine-ranging and positioning capabilities of ultra-wideband (UWB) technology. FiRa does this by driving the development of technical specifications and certification, advocating for effective regulations, and by defining a broad set of use cases for UWB. 
&lt;/p&gt;

&lt;p&gt;
To learn more about UWB and the FiRa Consortium, visit &lt;a href=&quot;https://research.samsung.com/blog/www.firaconsortium.org&quot; target=&quot;_blank&quot;&gt;www.firaconsortium.org&lt;/a&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;About Connectivity Standards Alliance&lt;sup&gt;®&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;
The Connectivity Standards Alliance&lt;sup&gt;®&lt;/sup&gt; (CSA) is the foundation and future of the Internet of Things (IoT). Established in 2002, its wide-ranging global membership collaborates to create and evolve universal open standards for the products transforming the way we live, work, and play. With its members’ deep and diverse expertise, robust certification programs, and a full suite of open IoT solutions CSA is leading the movement toward a more intuitive, imaginative, and useful world. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;About Car Connectivity Consortium&lt;sup&gt;®&lt;/sup&gt;(CCC)&lt;/h2&gt;

&lt;p&gt;
The Car Connectivity Consortium&lt;sup&gt;®&lt;/sup&gt; (CCC) is a cross-industry organization advancing technologies for smartphone-to-car connectivity solutions. CCC represents the vast majority of the global automotive and smartphone industries, with over 170 member companies. The CCC member companies include smartphone and vehicle manufacturers, automotive tier-1 suppliers, silicon/chip vendors, and security product suppliers representing a comprehensive ecosystem for secure access. Digital Key allows consumers to use their mobile devices, regardless of manufacturer or operating system type, to easily and securely access their vehicles. Based on the IEEE 802.15.4z standard, UWB secure ranging is a core technology that enables Digital Key 3.0 by preventing car theft while preserving full user convenience.
&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
- FiRa Consortium, &lt;a href=&quot;https://www.firaconsortium.org/&quot; target=&quot;_blank&quot;&gt;https://www.firaconsortium.org/&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
- Car Connectivity Consortium, &lt;a href=&quot;https://carconnectivity.org/&quot; target=&quot;_blank&quot;&gt;https://carconnectivity.org/&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
- Connectivity Standards Alliance, &lt;a href=&quot;https://csa-iot.org/&quot; target=&quot;_blank&quot;&gt;https://csa-iot.org/&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/FiRa-Consortium-Release-3-0-UWB-Core-Specification-and-Certification-Program-Launch</link><guid isPermaLink="false">https://research.samsung.com/blog/FiRa-Consortium-Release-3-0-UWB-Core-Specification-and-Certification-Program-Launch</guid><pubDate>Mon, 20 Jan 2025 16:00:00 GMT</pubDate><author>Jieun Keum|Gyubong Oh|Ankur Bansal</author><category>Communications</category><category>UWB</category><category>FiRa</category></item><item><title>Off-Policy Selection for Optimizing Ad Display Timing in Mobile Games (Samsung Instant Plays)</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;
Off-Policy Selection (OPS) aims to select the best policy from a set of policies trained using offline Reinforcement Learning. In this work, we describe our custom OPS method and its successful application in Samsung Instant Plays for optimizing ad delivery timings. The motivation behind proposing our custom OPS method is the fact that traditional Off-Policy Evaluation (OPE) methods often exhibit enormous variance leading to unreliable results. We applied our OPS method to initialize policies for our custom pseudo-online training pipeline. The final policy resulted in a substantial 49% lift in the number of watched ads while maintaining similar retention rate.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1 System Overview&lt;/h2&gt;

&lt;p&gt;
Samsung’s Instant Plays is a mobile application which allows users to play various games without any fees. The application generates revenue by displaying ads to users during a game-play. There are two major types of ads in our platform:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Reward&lt;/b&gt;: user-triggered longer 15s ads, that award the user with some bonuses inside a game.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Interstitial&lt;/b&gt;: self-triggered shorter 5s ads, that are displayed between game stages.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
In our project we take control over the latter, that is, the interstitial ads. During a gaming session, the user plays for a number of stages. Between each stage the system decides whether to display an ad. If the system displays ads too often for a specific user, that user might churn. On the contrary, if a user doesn’t mind watching ads and we don’t display them, then we might lose some revenue.
&lt;/p&gt;

&lt;p&gt;
The decision process of displaying interstitial ads is sequential in nature. In each timestamp when there is a slot for an interstitial ad, the system must decide whether to display it or not. The system might withhold showing an ad in some particular moment for the sake of better future moments. Thus, the Reinforcement Learning (RL) paradigm fits for our use case. We aim to train RL agent, which utilizes contextual information (user embedding, user profile, game embedding, time from last ad, number of ads shown in the last 10 minutes, etc.) to select the best slots for displaying ads. It is worth mentioning that our system aims only to select the timing of an ad, not the specific content of it. Thus, our problem is orthogonal to the content recommendation, but equally important.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2 Proposed Methods&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.1Offline Pretraining, OPE and OPS&lt;/h2&gt;

&lt;p&gt;
In order to initialize Reinforcement Learning agents for our system, we used offline RL paradigm and conducted the initial training, experimenting with off-policy methods like Rainbow DQN [4] and MARWIL [7]. However, it is known that models naively pretrained on offline data may underperform when deployed into real-life [6, 8]. Thus, we would like to use a reliable off-policy evaluation (OPE) method, where the performance of trained policies is evaluated solely using offline data. Unfortunately, we tested some common OPE methods including Importance Sampling [2] and Doubly Robust [3], and it turned out that getting accurate point estimates from OPEs is nearly impossible due to the their unrealistic estimates and huge variance (more on that in section 4.1). In such cases, OPS tasks, such as selecting the best performing policy from a set of policies, become equally important and are often easier to realize (fig. 1).
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRc7lm6AEEAUGo7.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Off-policy selection (OPS) aims to select the best policy out of a pool of policies. This is a common scenario in the real-world recommender systems where running multiple policies on an environment may be impossible.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.2Our Custom Off-Policy Selection Method&lt;/h2&gt;

&lt;p&gt;
We propose an Off-Policy Selection method based on measuring similarities between trajectories from the offline dataset and those that the trained policy would take. Let’s say that we have an offline dataset D consisting of M trajectories and a trained policy π. For each trajectory τ=(o&lt;sub&gt;0&lt;/sub&gt;,a&lt;sub&gt;0&lt;/sub&gt;,r&lt;sub&gt;1&lt;/sub&gt;,o&lt;sub&gt;1&lt;/sub&gt;,a&lt;sub&gt;1&lt;/sub&gt;,…) from the offline dataset D we apply trained policy to the underlying observations and, as a result, generate a series of new actions π(o&lt;sub&gt;i&lt;/sub&gt; )= a ̅&lt;sub&gt;i&lt;/sub&gt;. We compare series of actions (a0,a1,...) with the newly created ones (a ̅&lt;sub&gt;0&lt;/sub&gt;,a ̅&lt;sub&gt;1&lt;/sub&gt;,…) by computing Euclidean distance between corresponding elements and assign a distance d&lt;sub&gt;τ&lt;/sub&gt;. Given all such distances, we sort them in an ascending order d&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;  &amp;lt; · · · &amp;lt; d&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;M&lt;/sub&gt; and assign a final score of a trained policy as an average return from p% of the most similar trajectories, i.e. 1/N ∑&lt;sup&gt;N&lt;/sup&gt;&lt;sub style=&quot;margin-left: -11px;&quot;&gt;(n=1)&lt;/sub&gt;r&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt; , where r&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt; is a return corresponding to the distance d&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3 Implementation&lt;/h2&gt;

&lt;p&gt;
In order to realize our ideas, we needed an RL framework which would support offline Reinforcement Learning and Off-Policy Evaluation methods. We started with ReAgent suite [1] to develop proof of concept. However, we eventually switched to Ray [5] because this framework is more developer-friendly, supports more off-policy methods, is better suited for heavy-duty applications, contains modules for serving RL models, and is overall very well maintained.
&lt;/p&gt;

&lt;p&gt;
Since we used off-policy methods, we needed to store data for training in a convenient way. We utilized replay buffer, we updated it using historical data and done the retraining of RL models every 4 hours. The RL models were hosted using Ray Serve module, which allowed us to provide response to Samsung Instant Plays application within 100ms. The workflow of our system is summarised in fig. 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRc8Dd6AEQAUGo7.png&quot; width=&quot;40%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; The workflow of our system. The response time is expected to be delivered under 100ms. Replay buffer update and retraining is done every 4 hours.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4 Results&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;4.1Offline Evaluation&lt;/h2&gt;

&lt;p&gt;
To validate our Off-Policy Selection method described in section 2.2, we computed estimates for our method and compared them with real online experiments. We trained an RL agent on historical data removing last week for OPE methods evaluation. In table 1, we summarise estimated lifts in the number of interstitial ads watched on the one-week test split.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRc8XEaAEcAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Estimated lift in the number of watched interstitial ads according to different OPE methods.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
We hypothesize that such extreme policy value estimates of the above popular OPE methods are due to the nature of our use case. The game sessions for some users might last for an extended period, resulting in long sequences of actions. OPE methods try to correct the estimate by using the product of inverse propensity scores, which leads to a compounding error.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;4.2A/B Tests&lt;/h2&gt;

&lt;p&gt;
We conducted A/B tests where we compared our RL models against rule-based system which was used at that time. We focused on two metrics during that online evaluation: lift of the absolute number of watched interstitial ads and the retention rate. We ran the A/B test for a month and, similar to the offline setting, we aggregated the results from the last week of our A/B test. For the final results we obtained 49% lift in the absolute number of watched ads while maintaining very similar retention rate which slightly rose by 2%.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1145/3640457.3688058&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://doi.org/10.1145/3640457.3688058&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;


&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1]Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Zhengxing Chen, Yuchen He, Zachary Kaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform. arXiv preprint arXiv:1811.00260 (2018).
&lt;/p&gt;
&lt;p&gt;
[2]Josiah P. Hanna, Scott Niekum, and Peter Stone. 2019. Importance Sampling Policy Evaluation with an Estimated Behavior Policy. arXiv:1806.01347 [cs.LG]
&lt;/p&gt;
&lt;p&gt;
[3]Nan Jiang and Lihong Li. 2015. Doubly Robust Off-policy Evaluation for Re- inforcement Learning. CoRR abs/1511.03722 (2015). arXiv:1511.03722 http://arxiv.org/abs/1511.03722
&lt;/p&gt;
&lt;p&gt;
[4]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602 [cs.LG]
&lt;/p&gt;
&lt;p&gt;
[5]Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. 2018. Ray: a distributed framework for emerging AI applications. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation. 561–577.
&lt;/p&gt;
&lt;p&gt;
[6]Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Joséphine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, et al. 2023. Jump- start reinforcement learning. In International Conference on Machine Learning. PMLR, 34556–34583.
&lt;/p&gt;
&lt;p&gt;
[7]Qing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, and Tong Zhang. 2018. Exponentially Weighted Imitation Learning for Batched Historical Data. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/ 4aec1b3435c52abbdf8334ea0e7141e0-Paper.pdf
&lt;/p&gt;
&lt;p&gt;
[8]Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Dong Yan, and Jun Zhu. 2023. On the Reuse Bias in Off-Policy Reinforcement Learning. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI- 23, Edith Elkind (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4513–4521. https://doi.org/10.24963/ijcai.2023/502 Main Track.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/Off-Policy-Selection-for-Optimizing-Ad-Display-Timing-in-Mobile-Games</link><guid isPermaLink="false">https://research.samsung.com/blog/Off-Policy-Selection-for-Optimizing-Ad-Display-Timing-in-Mobile-Games</guid><pubDate>Wed, 15 Jan 2025 16:00:00 GMT</pubDate><author>Katarzyna Siudek-Tkaczuk|Bartłomiej Swoboda|Michał Romaniuk</author><category>AI</category><category>Off-PolicyEvaluation</category><category>RecSys</category></item><item><title>Towards Building a Trusted Execution Environment on RISC-V Microcontrollers</title><description>&lt;p&gt;
In this article we share our experience of developing a secure system using RISC-V Micro-Controller Unit (MCU). If this topic resonates, we encourage you to join our &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;mTower&lt;/a&gt; project to explore the potential of RISC-V security.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;RISC-V Open Architecture&lt;/h2&gt;

&lt;p&gt;
RISC-V has rapidly gained popularity among the architectures used for MCU. Its open architecture with a customizable instruction set (ISA) allows developers to tailor processors to specific needs (including security), to reduce the costs and to accelerate innovation.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;RISC-V is a lucrative choice because of the following features:&lt;/h2&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;1.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Open Architecture without Royalties&lt;/b&gt;: The key reason for RISC-V’s rise in popularity is its openness. It allows companies to avoid the costs associated with proprietary architectures like ARM, offering greater flexibility for custom solutions, which is crucial in embedded systems where optimization and cost control are essential&lt;sup&gt;[1][2]&lt;/sup&gt;.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;2.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Flexibility and Scalability&lt;/b&gt;: RISC-V is well-suited for everything from MCU to high-performance processors (CPU), making it versatile across various industries, including robotics, Internet of Things (IoT), and industrial automation[2][3]. Its modular design allows adding custom specialized instructions, which is particularly advantageous for resource-constrained microcontrollers.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;3.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Community Support and Rapid Development&lt;/b&gt;: RISC-V is backed by an active international community, which enables fast improvements and innovations. Major companies such as Google and NVIDIA, as well as governments, support this standard, making it promising for various industries&lt;sup&gt;[2][3]&lt;/sup&gt;. 
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;4.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Platform for Innovation and Research&lt;/b&gt;: RISC-V has opened new possibilities for research and development. Its modifiable architecture makes it ideal for creating secure environments like Trusted Execution Environments (TEE). In industries ranging from robotics to automotive systems, RISC-V microcontrollers are driving advances in security and energy efficiency&lt;sup&gt;[2]&lt;/sup&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Importance of Trusted Execution Environment for RISC-V Microcontrollers&lt;/h2&gt;

&lt;p&gt;
With the development of IoT technologies, security has become one of the key issues. IoT devices collect, process and transmit large volumes of confidential data, making it essential to ensure secure execution of security-critical operations. A TEE creates an isolated environment within the processor, guaranteeing that security-sensitive functions, such as Trusted Identity, Trusted Boot, Trusted Update, Trusted Firmware, and Trusted Operation, have no risk of compromise (Fig.1).
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/73e4d27b45/AZQaB_BKA6MAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Security aspects for embedded system &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The open and modular architecture of RISC-V provides developers with the flexibility to integrate TEE and explore new approaches to create secure environments for various IoT systems. A distinctive feature of RISC-V-based MCUs is that their development can be done faster comparing to more complex CPUs. This is due to the relative simplicity and accessibility of microcontrollers, both in manufacturing and usage, making them ideal for implementing new features and quickly adapting to new security standards.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Hardware Features for Building TEE Based on RISC-V&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Privilege Levels&lt;/h2&gt;

&lt;p&gt;
Privileges management is a fundamental element of security for the Trusted Execution Environment (TEE) built in MCUs. RISC-V architecture allows multiple privilege levels, enabling effective separation of trusted and untrusted processes.
&lt;/p&gt;

&lt;p&gt;
RISC-V supports several execution modes, including Machine mode (M-mode/M), Supervisor mode (S-mode/S), and User mode (U-mode/U)&lt;sup&gt;[4][5]&lt;/sup&gt;.
&lt;/p&gt;

&lt;p&gt;
Combinations of modes:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
M (simple embedded systems)
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
M, U (embedded systems with security)
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
M, S, U (systems running Unix-like operating systems)
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
MCUs with both Machine mode and User mode already allow for certain aspects of privilege management, as they can restrict untrusted programs&#39; access to resources. This type of MCU is more prevalent in the industry. MCUs/CPUs that support three modes - Machine mode, User mode, and Supervisor mode - can implement more complex privilege management systems. The presence of Supervisor mode allows for a higher level of control over resources and access, which is critical for security within TEE. 
&lt;/p&gt;

&lt;p&gt;
In mTower project, presented below, we used MCUs with only M-mode and U-mode, since they are more widespread and support the required basic privilege management system.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Access Control Using Physical Memory Protection (PMP)&lt;/h2&gt;

&lt;p&gt;
In RISC-V MCUs, the PMP mechanism plays a crucial role in enhancing security and managing access to memory. PMP allows the implementation of access control policies for physical memory addresses, enabling developers to set rules for memory regions&lt;sup&gt;[4][5]&lt;/sup&gt;.
&lt;/p&gt;

&lt;p&gt;
Key Aspects of PMP:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;1.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Memory Region Configuration&lt;/b&gt;: PMP enables the definition of a limited number of memory regions that can be configured with different access levels. Each region can have various attributes, such as: &lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;oAccess Type: whether access is allowed for reading, writing, or executing&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;oAccess Mode: privilege level for the region (e.g., user or supervisor)
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;2.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Process Isolation&lt;/b&gt;: PMP can create isolated memory regions for trusted and untrusted programs. This isolation helps to prevent unauthorized access to critical system resources, which is essential for implementing TEE.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;3.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Flexibility in Configuration&lt;/b&gt;: PMP supports dynamic configuration, allowing developers to adjust access rules according to specific application or system requirements. This means that access attributes can be modified during program execution.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;4.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Testing and Verification&lt;/b&gt;: since PMP is a hardware mechanism, it&#39;s vital to test its configuration to ensure it operates as intended. Verifying access rules may involve creating scenarios that check whether the access restrictions are effective.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Majority of RISC-V MCUs lack support of Input/Output Physical Memory Protection (IOPMP). This can be a limitation for implementing comprehensive security systems, since, without IOPMP, access to peripheral devices cannot be controlled in the same way as memory access. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Other Security Enhancement Mechanisms&lt;/h2&gt;

&lt;p&gt;
In addition to access control, RISC-V-based MCUs can employ a number of other mechanisms to enhance security. Crypto accelerators, for example, are hardware components designed to perform encryption and decryption operations with high efficiency, providing significant speed advantages over software implementations. Many of these accelerators support essential cryptographic algorithms, making them critical for ensuring data confidentiality and integrity.
&lt;/p&gt;

&lt;p&gt;
Furthermore, True Random Number Generators (TRNGs) are utilized to enhance security by providing high-quality random numbers essential for cryptographic operations. TRNGs ensure reliability in key generation, which is vital for data protection.
&lt;/p&gt;

&lt;p&gt;
Thus, the RISC-V architecture offers a platform for implementing security measures through effective privilege management and memory protection mechanisms. The combination of multiple privilege levels and the PMP system enables the reliable separation of trusted and untrusted processes, which is critically important for creating a TEE. By utilizing these mechanisms in mTower project, we have made progress in establishing a secure system. Let&#39;s delve deeper into the technical details.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Implementation overview&lt;/h2&gt;

&lt;p&gt;
The &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;mTower&lt;/a&gt; project is an experimental industry standard-compliant implementation TEE based on ARM TrustZone for Cortex-M23/33/35p/55 MCUs. It has been expanded to support MCUs based on the RISC-V architecture. From the very beginning, mTower has been designed to have a tiny RAM footprint and to avoid the use of time-consuming operations. The source code of mTower is available at GitHub&lt;sup&gt;[6]&lt;/sup&gt;. 
&lt;/p&gt;

&lt;p&gt;
The mTower project allows for exploring and implementing protection mechanisms for creating TEE. After examining existing approaches to building secure systems, we were drawn to the method discussed at the RISC-V Summit&lt;sup&gt;[11]&lt;/sup&gt; and used the Si-Five open source code&lt;sup&gt;[7]&lt;/sup&gt;.
&lt;/p&gt;

&lt;p&gt;
The initial requirements for implementing TEE included support for M-mode and U-mode in the MCUs, as well as the presence of the PMP memory protection mechanism. For the base operating system for the MCU, we chose FreeRTOS&lt;sup&gt;[10]&lt;/sup&gt;, which provides a stable platform for managing trusted and untrusted processes and allows for the efficient utilization of available hardware resources to protect data.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Extension of FreeRTOS&lt;/h2&gt;

&lt;p&gt;
In FreeRTOS, the code can run in both modes: privileged (M) mode (kernel, interrupt handlers, system tasks) and unprivileged (U) mode (user tasks). This allows for the effective separation of security-critical operations from general tasks. The privileged mode provides access to all hardware resources, including memory management and peripheral devices, while unprivileged tasks have restricted access, preventing the possibility of system compromise through uncontrolled access to resources (Fig 2).
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/73e4d27b45/AZQaCKSaA6YAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Privilege and Non-privileged calls&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
To restrict access to memory between user tasks on microcontrollers based on the RISC-V architecture, the PMP mechanism is used. PMP allows for configuring access rules for different regions of physical memory depending on the execution mode, which helps to protect critical data and to isolate processes from each other.
&lt;/p&gt;

&lt;p&gt;
This mechanism serves as the foundation for creating more secure environments where each user task has its clearly defined memory access boundaries. In case of direct access to a memory area or resource that is restricted, an exception will be raised.
&lt;/p&gt;

&lt;p&gt;
Due to the limitation on the number of memory regions that can be controlled by the PMP, a mechanism for PMP controller dynamic configuration has been added to the FreeRTOS. It triggers at every task context switch (Fig. 3). This means that with each transition between tasks, the PMP configuration is updated to reflect the new memory access boundaries for the current task. This approach allows to use hardware resources effectively and provides reliable memory access distribution even in systems with limited number of PMP registers.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/73e4d27b45/AZQaCPnaA6kAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Processing PMP configuration&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The mTower project can run tasks in both privileged and unprivileged modes, which is fundamental feature to manage system resource access securely. The privileged mode grants full access to hardware resources, while unprivileged tasks are restricted, thereby reducing the risk of compromising the system through unauthorized resource access.
&lt;/p&gt;

&lt;p&gt;
Additionally, the mTower dynamically configures the PMP mechanism during each task switch. This allows the definition of memory regions that tasks can access, ensuring process isolation and preventing unauthorized access. Any attempt to violate access rules results in an exception, further strengthening memory protection.
&lt;/p&gt;

&lt;p&gt;
The following demo scenarios were prototyped with mTower:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;1.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Privileged Task&lt;/b&gt;: executes critical system functions and utilizes PMP to configure access to resources;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;2.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Unprivileged Isolated Tasks&lt;/b&gt;: the tasks interact using system calls but do not have direct access to critical system resources;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;3.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Unprivileged Tasks with Shared Memory&lt;/b&gt;: tasks are executed in their dedicated isolated environments with different access rights to the shared memory. Any attempt to breach security boundaries triggers hardware exceptions.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
These scenarios illustrate how privileged mode and PMP are employed to build secure systems based on RISC-V architecture using FreeRTOS.
&lt;/p&gt;

&lt;p&gt;
Currently, the mTower project supports:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;SparkFun RED-V RedBoard&lt;/b&gt; - SiFive RISC-V FE310&lt;sup&gt;[8]&lt;/sup&gt;. However, we faced memory limitations of 16 kB. 
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Pine Ox64&lt;/b&gt; - Bouffalo Lab BL808 RISC-V&lt;sup&gt;[9]&lt;/sup&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Note that there may be variations in some hardware implementations of the H/W cores, as different manufacturers have their own architectural visions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Future plans&lt;/h2&gt;

&lt;p&gt;
In the future, we plan to focus on the following areas:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Expanding Supported MCU Boards&lt;/b&gt;: we plan to support new RISC-V-based platforms, enabling researchers and developers to work with various hardware and adapt our solutions to their needs;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Expanding Supported MCU Boards&lt;/b&gt;: we plan to support new RISC-V-based platforms, enabling researchers and developers to work with various hardware and adapt our solutions to their needs;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;In-Depth Security Testing&lt;/b&gt;: we plan to conduct a more comprehensive testing to identify potential vulnerabilities in our system. This will include both static and dynamic security analysis, as well as evaluating the effectiveness of the implemented protection mechanisms;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Enhancing PMP Capabilities&lt;/b&gt;: future mTower versions will aim at expanding the use of the PMP mechanism to manage access to input/output (I/O) resources, allowing for more complex security policies;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Integrating Cryptographic Components&lt;/b&gt;: our plans include the integration of hardware cryptographic accelerators to speed up encryption and decryption operations, ensuring high data security while reducing processor load; 
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Trusted Bootloader&lt;/b&gt;: to ensure the secure loading of the operating system and applications, we plan to develop a trusted bootloader. This component will verify the authenticity of the code before execution, significantly enhancing the overall security of the system.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Contribution is Welcome&lt;/h2&gt;

&lt;p&gt;
We welcome contributions to the &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;mTower&lt;/a&gt; project, which is open-source. If you are a systems developer or an information security engineer, feel free to join our team and contribute to our research. Your participation would be greatly appreciated! You can find more information and get involved at the following link: &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;https://github.com/Samsung/mTower&lt;/a&gt; 
&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] https://www.mordorintelligence.com/industry-reports/risc-v-tech-market
&lt;/p&gt;
&lt;p&gt;
[2] https://runtimerec.com/wp-content/uploads/2024/06/The-Rise-of-RISC-V-Microcontrollers.pdf
&lt;/p&gt;
&lt;p&gt;
[3] https://octopart.com/pulse/p/10-top-trends-microcontrollers-2024
&lt;/p&gt;
&lt;p&gt;
[4] https://riscv.org/wp-content/uploads/2017/05/riscv-privileged-v1.10.pdf
&lt;/p&gt;
&lt;p&gt;
[5] https://riscv.org/wp-content/uploads/2018/05/riscv-privileged-BCN.v7-2.pdf
&lt;/p&gt;
&lt;p&gt;
[6] https://github.com/Samsung/mTower
&lt;/p&gt;
&lt;p&gt;
[7] https://github.com/sifive 
&lt;/p&gt;
&lt;p&gt;
[8] https://www.sparkfun.com/products/retired/15594
&lt;/p&gt;
&lt;p&gt;
[9] https://wiki.pine64.org/index.php?title=Ox64
&lt;/p&gt;
&lt;p&gt;
[10] https://www.freertos.org/
&lt;/p&gt;

&lt;/div&gt;

&lt;!-- 220526 --&gt;
&lt;div class=&quot;RA&quot;&gt;
&lt;!-- //220525 --&gt;

&lt;div class=&quot;link&quot;&gt;

&lt;a href=&quot;https://research.samsung.com/hashsearch?q=RISC-V&quot; class=&quot;hash&quot;&gt;#RISC-V&lt;/a&gt; &lt;!-- #! 제거 후 링크를 넣어주세요. --&gt;


&lt;a href=&quot;https://research.samsung.com/hashsearch?q=MCU&quot; class=&quot;hash&quot;&gt;#MCU&lt;/a&gt; &lt;!-- #! 제거 후 링크를 넣어주세요. --&gt;


&lt;/div&gt;

&lt;div class=&quot;RA_cont hash-result block&quot;&gt;
&lt;div class=&quot;slick-list&quot;&gt;
&lt;div class=&quot;slick-head&quot;&gt;
&lt;h3 class=&quot;slick-tit&quot;&gt;Related Stories&lt;/h3&gt;
&lt;div class=&quot;arr-box&quot;&gt;
&lt;div class=&quot;arr-l&quot;&gt;&lt;i class=&quot;ico-arr&quot;&gt;&lt;/i&gt;&lt;/div&gt;
&lt;div class=&quot;arr-r&quot;&gt;&lt;i class=&quot;ico-arr&quot;&gt;&lt;/i&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/RISC-V-and-Vectorization&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxioyqAlAAUGoJ.jpg&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;RISC-V and Vectorization&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On November 28, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Bringing-RVV-to-Life-Overcoming-Hardware-Gaps-in-RISC-V-Development&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/5471502d58/AZKOd5gKAfAAUGoJ.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Bringing RVV to Life: Overcoming Hardware Gaps in RISC-V Development&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On October 18, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Open-source-open-hardware-ground-truth-for-Visual-Odometry-and-SLAM-applications&quot;&gt;
&lt;img src=&quot;https://gcg-stage-cdn-v2.stage.codeground.org/resources/9149edb6c7/AZJ07-QqAAYAUF-f.jpg&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Open Source, Open Hardware Ground Truth for Visual Odometry and SLAM Applications&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On October 11, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Matter-SDK-Contribution-to-Tizen-platform-support&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/7e17d06ab3/AYmuU7XaAOoAUGrH.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Matter SDK: Contribution to Tizen Platform Support&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On August 8, 2023&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/mTower-for-Quick-Start-with-ARM-TrustZone-on-MCU&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG8ATE6AKsAUGoJ.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;mTower – for Quick Start with ARM TrustZone on MCU&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On December 28, 2022&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Road-to-Get-Certified-as-gold-Badge-From-OpenSSF-Best-Practices&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/601118af5e/AYI9c0gkAnUpy3UD.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Road to Get Certified as &quot;gold&quot; Badge From OpenSSF Best Practices&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On July 28, 2022&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;!-- //220525 --&gt;

&lt;div class=&quot;RA_service&quot;&gt;

&lt;div class=&quot;prev-blog&quot;&gt;
&lt;div class=&quot;service-wrap&quot;&gt;
&lt;div class=&quot;prev&quot;&gt;PREV&lt;i&gt;&lt;/i&gt;&lt;/div&gt;
&lt;a href=&quot;https://research.samsung.com/blog/FaceMe-Robust-Blind-Face-Restoration-With-Personal-Identification&quot;&gt;FaceMe: Robust Blind Face Restoration With Personal Identification&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;



&lt;div class=&quot;next-blog&quot;&gt;
&lt;div class=&quot;service-wrap&quot;&gt;
&lt;div class=&quot;next&quot;&gt;NEXT&lt;i&gt;&lt;/i&gt;&lt;/div&gt;
&lt;a href=&quot;https://research.samsung.com/blog/SOI-Scaling-Down-Computational-Complexity-by-Estimating-Partial-States-of-the-Model&quot;&gt;SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;/div&gt;

&lt;div class=&quot;btnBox&quot;&gt;
&lt;button class=&quot;blog-btn&quot; onclick=&quot;location.href=&#39;/blog&#39; &quot;&gt;LIST&lt;/button&gt;
&lt;/div&gt;
&lt;!-- //220503 --&gt;
&lt;/div&gt;
</description><link>https://research.samsung.com/blog/Towards-Building-a-Trusted-Execution-Environment-on-RISC-V-Microcontrollers</link><guid isPermaLink="false">https://research.samsung.com/blog/Towards-Building-a-Trusted-Execution-Environment-on-RISC-V-Microcontrollers</guid><pubDate>Sun, 05 Jan 2025 16:00:00 GMT</pubDate><author>Taras Drozdovskyi</author><category>Open Source</category><category>RISC-V</category><category>MCU</category></item><item><title>SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;
The rapid advancements in artificial intelligence (AI) have led to the development of increasingly sophisticated and powerful artificial neural networks (ANNs). While these models have achieved groundbreaking performance across various domains [1 3], their escalating size and computational demands render them impractical for resource-constrained environments. This issue is particularly concerning for real-time, energy-sensitive applications in consumer electronics, such as smartwatches, augmented reality (AR) glasses, and wireless earbuds.
&lt;/p&gt;

&lt;p&gt;
Despite the growing processing power of microcontroller units (MCUs), their ability to run state-of-the-art ANNs remains severely limited. This challenge is further compounded by the slowdown of Moore&#39;s Law [4], which traditionally predicted consistent improvements in hardware performance. The deceleration of this trend [5] underscores a widening gap between the computational requirements of cutting-edge neural networks and the capabilities of compact, low-power devices [6].
&lt;/p&gt;

&lt;p&gt;
Moreover, the pursuit of maximal model accuracy in the AI research community often comes at the expense of efficiency, resulting in solutions that are unsuitable for real-world, latency-sensitive systems. Current optimization techniques, such as pruning [7] and quantization [8], fall short of bridging this gap without introducing significant trade-offs in model performance.
&lt;/p&gt;

&lt;p&gt;
Our paper is motivated by the need to address these challenges directly. We aim to develop a method that balances computational efficiency with model accuracy, enabling the deployment of high-performance neural networks on energy-constrained devices. By leveraging the inherent continuity and predictability of time-series data, we introduce Scattered Online Inference (SOI), a novel approach that reduces computational complexity through partial state predictions and efficient compression. SOI aligns with the growing demand for environmentally sustainable and economically viable AI solutions, pushing the boundaries of what is possible in compact, real-time systems.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Scattered Online Inference&lt;/h2&gt;

&lt;p&gt;
The key principle of SOI is leveraging compression and extrapolation along the time axis. Instead of recalculating every layer of the network for each incoming data point, SOI compresses the data using strided convolutions and reconstructs missing states through extrapolation techniques, such as frame duplication. These operations are applied selectively to specific layers of the network, allowing portions of the computational graph to remain static across consecutive inferences. By caching and reusing partial network states, SOI significantly reduces the frequency of full model updates, thereby optimizing computational efficiency. To enhance understanding of the SOI algorithm in CNNs, Figure 1 defines three types of convolutional layers utilized in our method. For comparison, Figure 1 also includes standard convolution and strided convolution.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOJ_nXKA38AUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; SOI for convolutional operations. For visualization purposes we show data as frames in time domain. A) Standard convolution. B) Strided convolution. C) Strided-Cloned Convolution. D) Shifted convolution. E) Shifted Strided-Cloned Convolution.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
SOI operates in two primary modes: Partially Predictive (PP) and Fully Predictive (FP). In partially predictive mode, the model predicts the next state based on a combination of newly computed and cached partial states. This approach reduces the average computational cost without increasing the system&#39;s peak demand. In contrast, FP mode predicts multiple future states ahead of time, allowing entire sections of the model to be precomputed and eliminating the need for on-the-fly calculations. While FP mode is more complex to implement, it offers greater reductions in both latency and computational load, particularly for tasks with highly predictable patterns. The inference patterns for both modes are illustrated in Figure 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKAUV6A4MAUGoJ.png&quot; width=&quot;80%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Inference patterns of each type of SOI based on U-Net architecture. A) Unmodified causal U-Net. B) Partially predictive (PP) SOI. C) Even inference of PP. D) Odd inference of PP. E) Fully predictive (FP) SOI. F) Even inference of FP. G) Odd inference of FP.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Another defining feature of SOI is the integration of skip connections, which maintain the flow of information from compressed input data to deeper layers of the network. These connections bridge the outputs of strided convolution layers and their corresponding reconstruction layers, ensuring that the model preserves critical context and causality. Skip connections also mitigate the risk of performance degradation caused by partial state predictions by allowing the model to incorporate new data into the computation graph without fully recalculating intermediate layers.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;
To evaluate the effectiveness of SOI, we conducted experiments on three distinct tasks: speech separation, acoustic scene classification (ASC), and video action recognition. These tasks were selected to demonstrate SOI&#39;s ability to process time-series data across diverse domains. The performance of SOI was compared to baseline models, traditional optimization techniques such as pruning and resampling, and the Short-Term Memory Convolution (STMC) method [9], which served as a foundational technique for SOI. The results highlight the versatility of SOI in reducing computational complexity while maintaining acceptable levels of accuracy and efficiency.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Speech Separation&lt;/h2&gt;

&lt;p&gt;
For the speech separation task, the U-Net architecture was employed to separate clean speech from noisy backgrounds. The experiments compared the partially predictive and fully predictive SOI variants against the baseline and STMC models. Metrics included computational complexity (measured in MMAC/s) and scale-invariant signal-to-noise ratio improvement (SI-SNRi). The results are shown in Figure 3.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKA-xqA4cAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Results of speech separation experiment with A) the partially predictive SOI, and B) fully predictive SOI.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The partially predictive SOI model demonstrated an ability to reduce computational complexity by up to 64%, with only a minor impact on performance, measured as a decrease of 0.017 dB in SI-SNRi for every 1% complexity reduction. Similarly, the FP SOI model achieved a 50% reduction in complexity by precomputing 83.7% of the network&#39;s operations, making it particularly well-suited for latency-sensitive applications.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Acoustic Scene Classification (ASC)&lt;/h2&gt;

&lt;p&gt;
The ASC task utilized the GhostNet architecture, with the objective of classifying urban acoustic scenes. Models incorporating SOI were compared to baseline and STMC variants across seven model sizes. Top-1 accuracy and computational complexity were evaluated, with the results summarized in Table 1.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKBWu6A4oAUGoJ.png&quot; width=&quot;45%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Results of ASC experiment.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
SOI achieved an average complexity reduction of 16% compared to the STMC method. In some configurations, SOI also improved model accuracy, likely due to partial state predictions enhancing the network’s generalization capabilities. Although the introduction of skip connections in SOI slightly increased the number of model parameters, this adjustment significantly reduced the overall computational load.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Video Action Recognition&lt;/h2&gt;

&lt;p&gt;
To explore SOI&#39;s applicability to non-audio time-series data, the ResNet-10 architecture and MoViNets were tested on the HMDB-51 dataset for the action recognition task. SOI was applied to 3D convolutional layers and evaluated on regular, small, and tiny ResNet-10 variants, as well as the A0 and A1 MoViNet variants. The results are presented in Table 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKBw5KA40AUGoJ.png&quot; width=&quot;60%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 2.&lt;/b&gt; Results of video action recognition experiment.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
SOI achieved complexity reductions of 10% to 17%, with little to no loss in accuracy. In some cases, SOI even improved model accuracy by expanding the receptive field through the use of strided convolutions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Pruning&lt;/h2&gt;

&lt;p&gt;
The application of SOI combined with pruning in the STMC model surpasses the effect of pruning alone. The addition of SOI enabled a further reduction in computational complexity by approximately 300 MMAC/s for the same model performance, representing about 16% of the original model&#39;s complexity. Interestingly, the “SOI 2|6” model outperformed the “SOI 1” model at around 6 dB SI-SNRi. The experimental results are shown in Figure 4.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKCDhqA5AAUGoJ.png&quot; width=&quot;40%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 4.&lt;/b&gt; Pruning of STMC, SOI and 2xSOI models. Unpruned models are indicated by markers.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Inference Time and Memory Footprint&lt;/h2&gt;

&lt;p&gt;
One of the critical objectives of SOI is to enhance the efficiency of neural network inference by reducing not only computational complexity but also inference time and memory usage. These aspects are particularly crucial in real-time applications, where latency and hardware limitations significantly affect system performance. We measured these metrics for SOI applied to the speech separation task, using U-Net as the test architecture. The results are summarized in Table 3.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKCWw6A5MAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 3.&lt;/b&gt; Results from experiments with partially predictive SOI for speech separation. &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The experiments revealed that SOI models consistently achieved lower average inference times compared to baseline and STMC models. From the collected results, we observed a drop in inference time from 9.93 ms in the STMC model to as low as 5.28 ms, representing a reduction of nearly 47%. Additionally, the peak memory footprint of the models decreased significantly as computational complexity was reduced. The memory footprint dropped from 27.2 MB in the STMC model to 14.6 MB, a reduction of over 46%.
&lt;/p&gt;

&lt;p&gt;
The relationship between these efficiency improvements and the model’s performance metrics was also analyzed. As shown in Figure 5, the inference time scaled linearly with the complexity reduction factor, demonstrating that SOI efficiently balanced the computational workload across the network.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKCoeqA5cAUGoJ.png&quot; width=&quot;60%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 5.&lt;/b&gt; Average inference time and peak memory footprint.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;
In this work, we presented a method for reducing the computational cost of convolutional neural networks (CNNs) by reusing partial network states from previous inferences, allowing these states to generalize over longer time periods. We discussed the effects of partial state prediction imposed by our method on the neural model and demonstrated its gradual application to balance model quality metrics and computational cost.
&lt;/p&gt;

&lt;p&gt;
Our experiments highlight the significant potential for reducing the computational cost of CNNs, particularly for tasks where outputs remain relatively constant, such as event detection or classification. We achieved a 50% reduction in computational cost without any loss in metrics for the ASC task, and a 64.4% reduction in computational cost with a relatively small 9.8% decrease in metrics for the speech separation task. Additionally, we demonstrated SOI’s ability to control the trade-off between model quality and computational cost, enabling resource- and requirement-aware tuning.
&lt;/p&gt;

&lt;p&gt;
The presented method offers an alternative to the STMC solution for strided convolution. While SOI reduces network computational complexity at the expense of some performance, STMC maintains performance metrics but at the cost of exponentially increased memory consumption. SOI is similar to methods like network pruning but does not rely on special sparse kernels for inference optimization. Importantly, these methods are not mutually exclusive. The STMC strided convolution handler, SOI, and pruning can coexist within a neural network to achieve the desired balance of model performance and resource efficiency.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.03813&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://arxiv.org/abs/2410.03813&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] Thai Son Nguyen, Sebastian Stueker, and Alexander H. Waibel. Super-human performance in online low-latency recognition of conversational speech. In Interspeech, 2020.
&lt;/p&gt;
&lt;p&gt;
[2] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067–1074, 2022.
&lt;/p&gt;
&lt;p&gt;
[3] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, dec 2020.
&lt;/p&gt;
&lt;p&gt;
[4] Gordon E Moore. Cramming more components onto integrated circuits. Proceedings of the IEEE, 86 (1):82–85, 1998.
&lt;/p&gt;
&lt;p&gt;
[5] Charles E. Leiserson, Neil C. Thompson, Joel S. Emer, Bradley C. Kuszmaul, Butler W. Lampson, Daniel Sanchez, and Tao B. Schardl. There’s plenty of room at the top: What will drive computer performance after moore’s law? Science, 368(6495):eaam9744, 2020.
&lt;/p&gt;
&lt;p&gt;
[6] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu Hu, and Yiyu Shi. Scaling for edge inference of deep neural networks. Nature Electronics 2018 1:4, 1:216–222, 4 2018. ISSN 2520-1131. doi: 10.1038/s41928-018-0059-3.
&lt;/p&gt;
&lt;p&gt;
[7] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989.
&lt;/p&gt;
&lt;p&gt;
[8] R.M. Gray and D.L. Neuhoff. Quantization. IEEE Transactions on Information Theory, 44(6): 2325–2383, 1998.
&lt;/p&gt;
&lt;p&gt;
[9] Grzegorz Stefański, Krzysztof Arendt, Paweł Daniluk, Bartłomiej Jasik, and Artur Szumaczuk. Short-term memory convolutions. In The Eleventh International Conference on Learning Representations, 2023.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/SOI-Scaling-Down-Computational-Complexity-by-Estimating-Partial-States-of-the-Model</link><guid isPermaLink="false">https://research.samsung.com/blog/SOI-Scaling-Down-Computational-Complexity-by-Estimating-Partial-States-of-the-Model</guid><pubDate>Tue, 03 Dec 2024 16:00:00 GMT</pubDate><author>Grzegorz Stefański|Paweł Daniluk|Artur Szumaczuk|Jakub Tkaczuk</author><category>AI</category><category>NeurIPS</category><category>SignalProcessing</category></item><item><title>RISC-V and Vectorization</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;
In 2023 Samsung joined and became an official member of “RISE (RISC-V Software Ecosystem)” project. Since then our company is participating in the development of a variety of projects and porting them to RISC-V architecture. In this article, we will throw some light on one of them: Chromium. When porting software to a new architecture, the first step is always to make it start and run on hardware, the next step is stabilization, but as Amber Huffman, chairperson of the RISE project emphasized: “In order for RISC-V to be commercialized, it is important to secure software that has performance, security, reliability, and compatibility”. For the final users performance is very important when browsing web pages, viewing streamed content or running web applications. All of those mentioned activities have a common ground: showing multimedia content and vectorization is one of the possibilities for performance improvement.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;RISC-V and Instruction Set Architecture (ISA)&lt;/h2&gt;

&lt;p&gt;
RISC-V is an open standard instruction set architecture based on established reduced instruction set computer (RISC) principles. Moreover, RISC-V is offered under royalty-free open-source licenses, and documents defining its ISA are offered under a Creative Commons license or a BSD License. That is one of major differences between this and other commercial vendors of processors, such as Arm Ltd. and MIPS Technologies.
&lt;/p&gt;

&lt;p&gt;
And what is an instruction set architecture? It is a part of the abstract model of a computer that defines how the CPU is controlled by the software and acts as an interface between the hardware and the software, specifying both what the processor is capable of doing as well as how it gets done. The ISA defines the supported data types, the registers, how the hardware manages main memory, key features (such as virtual memory), which instructions a microprocessor can execute, and the input/output model of multiple ISA implementations. The ISA can be extended by adding instructions or other capabilities, or by adding support for larger addresses and data values. [&lt;a href=&quot;https://www.arm.com/glossary/isa&quot; target=&quot;_blank&quot;&gt;https://www.arm.com/glossary/isa&lt;/a&gt;]
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Short story of vector extensions&lt;/h2&gt;

&lt;p&gt;
The first successful implementation of vector extension was MultiMedia eXtension (MMX) introduced by Intel in 1997. MMX defines eight processor registers, named MM0 through MM7, and operations that are performed on them. Each register is 64 bits wide and can be used to hold either 64-bit integers, or multiple smaller integers in a &quot;packed&quot; format: one instruction can then be applied to two 32-bit integers, four 16-bit integers, or eight 8-bit integers at once. MMX could take small and only integer numbers, like those representing shades of color in a picture, and process many of them at the same time.
&lt;/p&gt;

&lt;p&gt;
The successor of MMX was Streaming SIMD Extensions (SSE) introduced by Intel in 1999. SSE contains 70 new instructions (65 unique mnemonics using 70 encodings), most of which work on single precision floating-point data. SIMD instructions can greatly increase the performance when exactly the same operations are to be performed on multiple data objects. SSE could work with floating-point numbers, which are numbers with decimals, like 3.14 or 2.718. This made it perfect for 3D graphics processing and scientific calculations. SSE was subsequently expanded by Intel to SSE2, SSE3, SSSE3 and SSE4. Because it supports floating-point math, it had a wider range of applications than MMX and became more popular.
&lt;/p&gt;

&lt;p&gt;
Next, in 2011 Advanced Vector Extensions (AVX) was introduced by Intel. AVX uses sixteen YMM registers to perform a single instruction on multiple pieces of data. Each YMM register can hold and do simultaneous operations (math) on: eight 32-bit single-precision floating point numbers or four 64-bit double-precision floating point numbers. It was designed to help with extremely demanding tasks like high-definition video processing, advanced gaming graphics, and heavy scientific simulations. AVX also had its own family, with AVX2 and AVX-512, each more powerful than the last.
&lt;/p&gt;

&lt;p&gt;
In the ARM family processors there also is implementation of vector extensions: The Advanced SIMD extension (also known as Neon or &quot;MPE&quot; Media Processing Engine) is a combined 64- and 128-bit SIMD instruction set that provides standardised acceleration for media and signal processing applications. Neon is included in all Cortex-A8 devices, but is optional in Cortex-A9 devices. Neon can accelerate signal processing algorithms and functions to speed up applications such as audio and video processing, voice and facial recognition, computer vision, and deep learning. Neon instructions allow up to: 16x8-bit, 8x16-bit, 4x32-bit, 2x64-bit integer operations and 8x16-bit, 4x32-bit, 2x64-bit floating-point operations. [&lt;a href=&quot;https://www.arm.com/technologies/neon&quot; target=&quot;_blank&quot;&gt;https://www.arm.com/technologies/neon&lt;/a&gt;]
&lt;/p&gt;

&lt;p&gt;
Next, ARM introduced Scalable Vector Extension (SVE) which is a vector extension for the A64 instruction set of the Armv8-A architecture. Armv9-A builds on SVE with the SVE2 extension. Unlike other SIMD architectures, SVE and SVE2 do not define the size of the vector registers, but constrain it to a range of possible values, from a minimum of 128 bits up to a maximum of 2048 in 128-bit wide units. Therefore, any CPU vendor can implement the extension by choosing the vector register size that better suits the workloads the CPU is targeting. The design of SVE and SVE2 guarantees that the same program can run on different implementations of the instruction set architecture without the need to recompile the code. [https://developer.arm.com/Architectures/Scalable Vector Extensions]
&lt;/p&gt;

&lt;p&gt;
It is worth to mention that RISC-V vector extension (RVV) has adopted the same style as ARM SVE and SVE2: scalable vector registers of size unknown at compile-time. This approach meant to address ISA fragmentation by different vector sizes in x86 extensions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Vector extensions in RISC-V &lt;/h2&gt;

&lt;p&gt;
How can the &quot;V&quot; Vector extension be used? The RISC-V &quot;V&quot; Vector Extension allows CPUs to perform many operations simultaneously, making them faster and more efficient. Let&#39;s dive into this with the key components, including the registers and a selection of important instructions that are part of this extension.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Registers &lt;/h2&gt;

&lt;p&gt;
The RISC-V Vector Extension (RVV) version 1.0 introduces a rich set of vector registers that can take multiple data structures as arguments. It also provides instructions designed to perform parallel processing efficiently.
&lt;/p&gt;

&lt;p&gt;
These registers are critical for performing vectorized operations.
&lt;/p&gt;

&lt;p&gt;
1.Vector Registers (v0 - v31): &lt;br&gt;
•There are 32 vector registers, each identified as v0 to v31. &lt;br&gt;
•Each vector register can hold multiple elements, and the size of these elements can vary (e.g., 8-bit, 16-bit, 32-bit, 64-bit). &lt;br&gt;
•The total number of elements that a vector register can hold is determined by the configured vector length.
&lt;/p&gt;

&lt;p&gt;
2.Vector Configuration Registers: &lt;br&gt;
•VLEN: The length of the vector registers in bits. &lt;br&gt;
•SEW (Standard Element Width): Specifies the width of individual elements in the vector register (e.g., 8-bit, 16-bit, 32-bit, 64-bit). &lt;br&gt;
•LMUL (Length Multiplier): Determines the grouping of vector registers to allow for different vector lengths.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Configuration Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions configure the vector processing unit (VPU) to set up the vector length and element width.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxhqa6AjUAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Load/Store Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions are used to load data from memory into vector registers and store data from vector registers into memory.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxhyIaAjgAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
The above operations can have multiple variants. Besides the simple load and store (le32/se32), it’s possible to use strides (lse32/sse32) or segments (lsseg4_32/sseg4_32) which makes it possible to load and store data not only in SoA (Structure of Arrays) configuration but also as AoS (Array of Structures).
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Arithmetic Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions perform arithmetic operations on vector registers.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxh1dqAjsAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Masking Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions enable conditional operations on vector elements.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxh53KAj4AUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Example: Adding multiple 64-bit Numbers &lt;/h2&gt;

&lt;p&gt;
Adding arrays with 64-bit numbers is a complex task because standard registers in CPUs are typically 32-bit or 64-bit wide and this task can&#39;t be done in one step. Using the RISC-V &quot;V&quot; Vector Extension, we can break down this task and perform it more efficiently.
&lt;/p&gt;

&lt;p&gt;
Here&#39;s a step-by-step guide to adding two arrays with 64-bit numbers using the RISC-V &quot;V&quot; Vector Extension:
&lt;/p&gt;

&lt;p&gt;
1.&lt;b&gt;Set up the Vector Registers&lt;/b&gt;: First, we need to configure the vector registers to handle arrays. &lt;br&gt;
2.&lt;b&gt;Load the Data&lt;/b&gt;: Load the arrays into the vector registers.&lt;br&gt;
3.&lt;b&gt;Perform the Addition&lt;/b&gt;: Use vector instructions to add the numbers.&lt;br&gt;
4.&lt;b&gt;Store the Result&lt;/b&gt;: Store the result back to memory.
&lt;/p&gt;

&lt;p&gt;
Here is the assembly code for this:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;.section .data&lt;br&gt;  
 num1: .quad 0x123456789ABCDEF0, 0x0FEDCBA987654321, 0x0011223344556677, 0x8899AABBCCDDEEFF&lt;br&gt;
 num2: .quad 0x1122334455667788, 0x99AABBCCDDEEFF00, 0x1234567890ABCDEF, 0xFEDCBA9876543210&lt;br&gt;
 result: .space 32  # Reserve 32 bytes (256 bits) for the result&lt;br&gt;&lt;br&gt;  
  
.section .text&lt;br&gt;  
.globl _start&lt;br&gt;  
_start:&lt;br&gt;  
 # Set up vector registers&lt;br&gt;  
 li t0, 4             # Set AVL (Application Vector Length) to 4 (256 bits / 64-bit elements)&lt;br&gt;  
 vsetvli t0, t0, e64  # Set vector length and element width to 64-bit&lt;br&gt;&lt;br&gt;  
  
 la a0, num1&lt;br&gt;  
 # Load 256-bit numbers into vector registers&lt;br&gt;  
 vle64.v v0, (a0)       # Load the first array into vector register v0&lt;br&gt;  
 la a0, num2&lt;br&gt;  
 vle64.v v1, (a0)   # Load the second array into vector register v1&lt;br&gt;&lt;br&gt;  
  
 # Perform the addition&lt;br&gt;  
 vadd.vv v2, v0, v1   # Add vector register v0 and v1, store the result in vector register v2&lt;br&gt;  
  
 # Store the result back to memory&lt;br&gt;  
 la a0, result&lt;br&gt;  
 vse64.v v2, (a0)  # Store the result from vector register v2 to the memory location result&lt;br&gt;&lt;br&gt;  
  
 # Exit (for demonstration purposes, assuming an environment that supports this)&lt;br&gt;  
 li a7, 93            # ECALL number for exit&lt;br&gt;  
 ecall&lt;/mark&gt;
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Explanation &lt;/h2&gt;

&lt;p&gt;
1.&lt;b&gt;Data Section&lt;/b&gt;: We define two 256-bit numbers (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num2&lt;/mark&gt;) as arrays of four 64-bit values. We also reserve space for the result.&lt;br&gt;
2.&lt;b&gt;Set up Vector Registers&lt;/b&gt;: We set the vector length to handle 64-bit elements and configure the vector registers accordingly.&lt;br&gt;
3.&lt;b&gt;Load Data&lt;/b&gt;: The &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vle64.v&lt;/mark&gt; instruction loads 64-bit elements from memory into vector registers &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v0&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v1&lt;/mark&gt;.&lt;br&gt;
4.&lt;b&gt;Perform Addition&lt;/b&gt;: The &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vadd.vv&lt;/mark&gt; instruction adds the elements in vector registers &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v0&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v1&lt;/mark&gt; and stores the result in &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v2&lt;/mark&gt;.&lt;br&gt;
5.&lt;b&gt;Store Result&lt;/b&gt;: The &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vse64.v&lt;/mark&gt; instruction stores the 64-bit elements from vector register &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v2&lt;/mark&gt; back into memory at the location &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;result&lt;/mark&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Comparing assembly code with and without Vector Extensions &lt;/h2&gt;

&lt;p&gt;
To truly appreciate the power of the RISC-V &quot;V&quot; Vector Extension, let&#39;s compare the assembly code for adding two arrays with (example above) and without (example below) the usage of vector extensions.
&lt;/p&gt;

&lt;p&gt;
When we don&#39;t use vector extensions, we need to handle each 64-bit part of the array individually. This requires multiple instructions and more steps to achieve the same result. Here is how we can write the assembly code without the usage of vector extensions:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;.section .data&lt;br&gt;
num1: .quad 0x123456789ABCDEF0, 0x0FEDCBA987654321, 0x0011223344556677, 0x8899AABBCCDDEEFF&lt;br&gt;
num2: .quad 0x1122334455667788, 0x99AABBCCDDEEFF00, 0x1234567890ABCDEF, 0xFEDCBA9876543210&lt;br&gt;
result: .space 32  # Reserve 32 bytes (256 bits) for the result&lt;br&gt;&lt;br&gt;

.section .text&lt;br&gt;
.globl _start&lt;br&gt;
_start:&lt;br&gt;&lt;br&gt;

# Load the first 64-bit part of the first number&lt;br&gt;
 lui     a1, %hi(num1)&lt;br&gt;  
 addi    a1, a1, %lo(num1)&lt;br&gt;  
# Load the first 64-bit part of the second number&lt;br&gt;  
 lui     a2, %hi(num2)&lt;br&gt;  
 addi    a2, a1, %lo(num2)&lt;br&gt;&lt;br&gt;  
  
 lui     a3, %hi(result)&lt;br&gt;  
 addi    a3, a3, %lo(result)&lt;br&gt;&lt;br&gt;  

# Add the third 64-bit parts&lt;br&gt;
 ld t1, 0(a1)&lt;br&gt;  
 ld t2, 0(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;&lt;br&gt; 
 
 # Store the result&lt;br&gt;
 sd t3, 0(a3)&lt;br&gt;&lt;br&gt;  

# do the same stuff for every 64 bit word&lt;br&gt;
 ld t1, 8(a1)&lt;br&gt;  
 ld t2, 8(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;  
 sd t3, 8(a3)&lt;br&gt;&lt;br&gt;  
  
 ld t1, 16(a1)&lt;br&gt;  
 ld t2, 16(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;  
 sd t3, 16(a3)&lt;br&gt;&lt;br&gt;  
  
 ld t1, 24(a1)&lt;br&gt;  
 ld t2, 24(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;  
 sd t3, 24(a3)&lt;br&gt;&lt;br&gt;


 # Exit (for demonstration purposes, assuming an environment that supports this)&lt;br&gt;
 li a0, 0&lt;br&gt;
   li a7, 93            # ECALL number for exit&lt;br&gt;
 ecall&lt;/mark&gt;
&lt;/p&gt;

&lt;p&gt;
Let&#39;s compare now two presented approaches:
&lt;/p&gt;

&lt;p&gt;
1.Number of Instructions:&lt;br&gt;
•&lt;b&gt;Without Vector Extensions&lt;/b&gt;: The code is much longer because each 64-bit part of the arrays is handled separately. For each part, we need to load the numbers, add them, and store the result, repeating this four times.&lt;br&gt;
•&lt;b&gt;With Vector Extensions&lt;/b&gt;: The code is more compact. We only need to set up the vector registers once and then use a single vector addition instruction to perform the addition of all parts simultaneously.&lt;br&gt;
2.Simplicity:&lt;br&gt;
•&lt;b&gt;Without Vector Extensions&lt;/b&gt;: The code is more complex and repetitive, making it harder to read and maintain.&lt;br&gt;
•&lt;b&gt;With Vector Extensions&lt;/b&gt;: The code is simpler and more elegant. It abstracts away the repetitive tasks, making it easier to understand and maintain.&lt;br&gt;
3.Performance:&lt;br&gt;
•&lt;b&gt;Without Vector Extensions&lt;/b&gt;: The CPU has to execute more instructions, which can slow down the process, especially for large data sets.&lt;br&gt;
•&lt;b&gt;With Vector Extensions&lt;/b&gt;: The CPU can process multiple data elements in parallel, significantly speeding up the computation.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Building the Assembly Code with RISC-V Tools &lt;/h2&gt;

&lt;p&gt;
To turn our assembly code into a program that the CPU can run, we need to use some special tools. These tools are &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld&lt;/mark&gt;. 
&lt;/p&gt;

&lt;p&gt;
Those commands correspond to the toolchain binaries used for cross-compiling for a different architecture (RISC-V in this case) on x64/x86 machines. If you are compiling on the RISC-V machine, you can just use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;as&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;ld&lt;/mark&gt; instead.
&lt;/p&gt;

&lt;p&gt;
To install toolchain, follow the manual from &lt;a href=&quot;https://github.com/riscv-collab/riscv-gnu-toolchain&quot; target=&quot;_blank&quot;&gt;https://github.com/riscv-collab/riscv-gnu-toolchain&lt;/a&gt;. But don&#39;t forget to add &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;--with-arch=rv64gcv&lt;/mark&gt; everywhere you can. Take a look also at Appendix A.
&lt;/p&gt;

&lt;p&gt;
Let&#39;s see how we can use them to build our assembly code for adding 256-bit numbers.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Step-by-Step Guide &lt;/h2&gt;

&lt;p&gt;
1.Write the Assembly Code:&lt;br&gt;
•First, write the assembly code in a file. Let&#39;s name it &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.s&lt;/mark&gt;.&lt;br&gt;
2.Assemble the Code:&lt;br&gt;
•Use the &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as&lt;/mark&gt; utility to assemble the code. This will convert the assembly code into an object file.&lt;br&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as -march=rv64gcv -o add_256bit.o add_256bit.s&lt;/mark&gt;&lt;br&gt;
•This command will create an object file named &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.o&lt;/mark&gt;.&lt;br&gt;
3.Link the Object File:&lt;br&gt;
•Use the &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld&lt;/mark&gt; utility to link the object file and create an executable. &lt;br&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld -o add_256bit add_256bit.o&lt;/mark&gt;&lt;br&gt;
•This command will create an executable file named &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit&lt;/mark&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Detailed Explanation &lt;/h2&gt;

&lt;p&gt;
1.Assembler (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as&lt;/mark&gt;):&lt;br&gt;
•The assembler reads the assembly code from &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.s&lt;/mark&gt; and converts it into machine code, generating an object file (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.o&lt;/mark&gt;). The object file contains the binary representation of the instructions, but it is not yet ready to be executed on its own.&lt;br&gt;
2.Linker (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld&lt;/mark&gt;):&lt;br&gt;
•The linker takes the object file (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.o&lt;/mark&gt;) and links it with any necessary libraries or other object files to produce an executable (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit&lt;/mark&gt;). The linker resolves references to external symbols and assigns final memory addresses to the program&#39;s instructions and data.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Running the Executable &lt;/h2&gt;

&lt;p&gt;
To run the executable on an RV64 processor or an emulator, you would typically use a simulator like Spike, QEMU, or run it directly on RISC-V hardware. Here is the example with Spike, a functional RISC-V ISA simulator:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;spike pk add_256bit&lt;/mark&gt; 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;RISC-V Vector Functions in GCC &lt;/h2&gt;

&lt;p&gt;
The RISC-V Vector Extension (RVV) provides a rich set of vector functions in GCC that allow developers to leverage the power of vector processing directly in C/C++ code. These functions are typically available through intrinsic functions, which are special functions provided by the compiler to generate specific machine instructions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;RISC-V Vector Intrinsics &lt;/h2&gt;

&lt;p&gt;
Vector intrinsics in GCC for RISC-V are designed to map directly to RVV instructions, providing a way to write vectorized code without resorting to assembly language. These intrinsics follow a naming convention that helps in understanding their functionality.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Naming Convention &lt;/h2&gt;

&lt;p&gt;
The naming convention for RISC-V vector intrinsics is generally as follows:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_&lt;operation&gt;_&lt;arg-types&gt;_&lt;type&gt;&lt;size&gt;_&lt;mask&gt;&lt;/mask&gt;&lt;/size&gt;&lt;/type&gt;&lt;/arg-types&gt;&lt;/operation&gt;&lt;/mark&gt;&lt;br&gt;
1.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;operation&gt;&lt;/operation&gt;&lt;/mark&gt;: Describes the vector operation (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vmseq&lt;/mark&gt; for vector mask equal).&lt;br&gt;
2.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;arg-types&gt;&lt;/arg-types&gt;&lt;/mark&gt;: Indicates the types of arguments (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v&lt;/mark&gt; for vector, &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;f&lt;/mark&gt; for float, &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;x&lt;/mark&gt; for integer).&lt;br&gt;
3.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;type&gt;&lt;/type&gt;&lt;/mark&gt;: Indicates the type of elements (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;u8&lt;/mark&gt; for unsigned 8-bit integers).&lt;br&gt;
4.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;size&gt;&lt;/size&gt;&lt;/mark&gt;: Specifies the vector register group multiplier (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;m2&lt;/mark&gt;).&lt;br&gt;
5.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;mask&gt;&lt;/mask&gt;&lt;/mark&gt;: Optionally indicates whether the operation is masked (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;b4&lt;/mark&gt; for 4-bit mask).
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Intrinsics Examples &lt;/h2&gt;

&lt;p&gt;
Let&#39;s briefly describe some common RISC-V vector intrinsics with examples:
&lt;/p&gt;

&lt;p&gt;
Vector Load/Store Instructions&lt;br&gt;
These sample instructions are used to load data from memory into vector registers and store data from vector registers into memory.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiBDKAkEAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
Vector Arithmetic Instructions&lt;br&gt;
These sample instructions perform arithmetic operations on vector registers.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiDzaAkQAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
Vector Masking Instructions&lt;br&gt;
These sample instructions enable conditional operations on vector elements.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiGYKAkcAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Understanding the Arguments &lt;/h2&gt;

&lt;p&gt;
1.base (const uint8_t *base): Pointer to the base address in memory for load/store operations.&lt;br&gt;
2.vl (size_t vl): Vector length, specifying the number of elements to process.&lt;br&gt;
3.op1, op2, acc: Vector registers or scalars involved in the operation.&lt;br&gt;
4.mask (vbool4_t mask): Optional mask register for conditional operations.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Example: Adding Two 256-bit Numbers Using RISC-V Vector Intrinsics &lt;/h2&gt;

&lt;p&gt;
Now, it’s time to write a C program that adds two 256-bit numbers using RISC-V vector intrinsics. Since 256-bit numbers are not natively supported by standard data types in C, we will treat them as arrays of smaller elements (e.g., 8-bit, 16-bit, or 32-bit integers). For this example, we will use 32-bit integers to represent the 256-bit numbers.
&lt;/p&gt;

&lt;p&gt;
Step-by-Step Implementation&lt;br&gt;
1.&lt;b&gt;Include the necessary headers&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv_vector.h&lt;/mark&gt; for vector intrinsics.&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;stdio.h&lt;/mark&gt; for input/output functions.&lt;br&gt;
2.&lt;b&gt;Define the 256-bit numbers as arrays of 32-bit integers&lt;/b&gt;:&lt;br&gt;
•Each 256-bit number will be represented by an array of 8 &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;uint32_t&lt;/mark&gt; elements.&lt;br&gt;
3.&lt;b&gt;Use vector intrinsics to load the numbers into vector registers&lt;/b&gt;:&lt;br&gt;
•Use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_vle32_v_u32m1&lt;/mark&gt; to load the 32-bit elements into vector registers.&lt;br&gt;
4.&lt;b&gt;Perform the addition using vector intrinsics&lt;/b&gt;:&lt;br&gt;
•Use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_vadd_vv_u32m1&lt;/mark&gt; to add the elements of the two vector registers.&lt;br&gt;
5.&lt;b&gt;Store the result back into an array&lt;/b&gt;:&lt;br&gt;
•Use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_vse32_v_u32m1&lt;/mark&gt; to store the result from the vector register back into memory.
&lt;/p&gt;

&lt;p&gt;
Complete Code&lt;br&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;#include &lt;riscv_vector.h&gt;&lt;br&gt;
#include &lt;stdio.h&gt;&lt;br&gt;
#define VECTOR_LENGTH 8&lt;br&gt;
// Number of 32-bit elements to represent a 256-bit number&lt;br&gt;
void add_256bit_numbers(const uint32_t *num1, const uint32_t *num2, uint32_t *result) {&lt;br&gt;
  // Set the vector length and element width&lt;br&gt;
  size_t vl = __riscv_vsetvl_e32m1(VECTOR_LENGTH);&lt;br&gt;
  // Load the 256-bit numbers into vector registers&lt;br&gt;
  vuint32m1_t vec_num1 = __riscv_vle32_v_u32m1(num1, vl);&lt;br&gt;
  vuint32m1_t vec_num2 = __riscv_vle32_v_u32m1(num2, vl);  &lt;br&gt;
  // Perform the addition     vuint32m1_t vec_result = __riscv_vadd_vv_u32m1(vec_num1, vec_num2, vl);  &lt;br&gt;
  // Store the result back to memory&lt;br&gt;
  __riscv_vse32_v_u32m1(result, vec_result, vl);&lt;br&gt;
}&lt;br&gt;&lt;br&gt;

int main() {&lt;br&gt;
  // Define two 256-bit numbers as arrays of 8 32-bit integers&lt;br&gt;    
  uint32_t num1[VECTOR_LENGTH] = {0x12345678, 0x9ABCDEF0, 0xFEDCBA98, 0x76543210, 0x0F1E2D3C, 0x4B5A6978, 0x11223344, 0x55667788};&lt;br&gt;
  uint32_t num2[VECTOR_LENGTH] = {0x87654321, 0x0FEDCBA9, 0x12345678, 0x9ABCDEF0, 0x89ABCDEF, 0x12345678, 0x90ABCDEF, 0x12345678};&lt;br&gt;
  uint32_t result[VECTOR_LENGTH] = {0};&lt;br&gt;  
  // Add the 256-bit numbers  add_256bit_numbers(num1, num2, result);&lt;br&gt;  
  // Print the result&lt;br&gt;
  printf(&quot;Result: &quot;);&lt;br&gt;
  for (int i = 0; i &amp;lt; VECTOR_LENGTH; i++) {&lt;br&gt;
    printf(&quot;%08x &quot;, result[i]);&lt;br&gt;
  }&lt;br&gt;
  printf(&quot;\n&quot;); &lt;br&gt; 
  return 0;&lt;br&gt;
}&lt;/stdio.h&gt;&lt;/riscv_vector.h&gt;&lt;/mark&gt;
&lt;/p&gt;

&lt;p&gt;
Explanation&lt;br&gt;
1.&lt;b&gt;Define Constants&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;VECTOR_LENGTH&lt;/mark&gt; is defined as 8 to represent the number of 32-bit elements in a 256-bit number.&lt;br&gt;
2.&lt;b&gt;add_256bit_numbers Function&lt;/b&gt;:&lt;br&gt;
•This function takes two 256-bit numbers (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num2&lt;/mark&gt;) and stores their sum in &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;result&lt;/mark&gt;.&lt;br&gt;
3.&lt;b&gt;Set Vector Length&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vsetvl_e32m1&lt;/mark&gt; sets the vector length to handle 32-bit elements, with a length multiplier of 1.&lt;br&gt;
4.&lt;b&gt;Load Vector Registers&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vle32_v_u32m1&lt;/mark&gt; loads the 32-bit elements of &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num2&lt;/mark&gt; into vector registers &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num2&lt;/mark&gt;.&lt;br&gt;
5.&lt;b&gt;Vector Addition&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vadd_vv_u32m1&lt;/mark&gt; adds the elements of &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num2&lt;/mark&gt; and stores the result in &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_result&lt;/mark&gt;.&lt;br&gt;
6.&lt;b&gt;Store Result&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vse32_v_u32m1&lt;/mark&gt; stores the result from &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_result&lt;/mark&gt; back into the &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;result&lt;/mark&gt; array.&lt;br&gt;
7.&lt;b&gt;Main Function&lt;/b&gt;:&lt;br&gt;
•Defines two 256-bit numbers and an array to store the result.&lt;br&gt;
•Calls &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit_numbers&lt;/mark&gt; to add the two numbers.&lt;br&gt;
•Prints the result.
&lt;/p&gt;

&lt;p&gt;
Compilation&lt;br&gt;
To compile this code with RISC-V vector extension support, use the following command:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-gcc -march=rv64gcv -mabi=lp64d -o add_256bit add_256bit.c&lt;/mark&gt;
&lt;/p&gt;

&lt;p&gt;
It is worth mentioning that this command is in fact cross compilation on x64/x86 machine and produces a binary for RISC-V architecture. However, remember that you do not necessarily need to do that at all if you have a system working on a RISC-V capable platform like Banana PI or VisionFive2. Then, the build command will not differ from any other compiler invocation and you can use regular gcc or clang.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Example: Adding two 256-bit numbers using GCC and clang support for RVV &lt;/h2&gt;

&lt;p&gt;
What is worth mentioning is that GCC vector extensions (also supported by Clang) support RVV, so this can also be done in a much simpler way by enabling portable code and other compiler optimizations:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;#include &lt;riscv_vector.h&gt;&lt;br&gt;  
#include &lt;stdio.h&gt;&lt;br&gt;  
  
#define VECTOR_LENGTH 8  // Number of 32-bit elements to represent a 256-bit number&lt;br&gt;
typedef uint32_t uint32x8_t __attribute__ ((vector_size (sizeof(uint32_t) * VECTOR_LENGTH)));&lt;br&gt;&lt;br&gt;

int main() {&lt;br&gt;
   uint32x8_t num1 = {0x12345678, 0x9ABCDEF0, 0xFEDCBA98, 0x76543210, 0x0F1E2D3C, 0x4B5A6978, 0x11223344, 0x55667788};&lt;br&gt; 
   uint32x8_t num2 = {0x87654321, 0x0FEDCBA9, 0x12345678, 0x9ABCDEF0, 0x89ABCDEF, 0x12345678, 0x90ABCDEF, 0x12345678};&lt;br&gt;&lt;br&gt;

   // Add the 256-bit numbers&lt;br&gt;  
   uint32x8_t result = num1 + num2;&lt;br&gt;&lt;br&gt;
  
   // Print the result&lt;br&gt;  
   printf(&quot;Result: &quot;);&lt;br&gt;&lt;br&gt;  
   for (int i = 0; i &amp;lt; VECTOR_LENGTH; i++) {&lt;br&gt;  
       printf(&quot;%08x &quot;, result[i]);&lt;br&gt; 
   }&lt;br&gt;  
   printf(&quot;\n&quot;);&lt;br&gt;&lt;br&gt;  

   return 0;&lt;br&gt;  
}&lt;/stdio.h&gt;&lt;/riscv_vector.h&gt;&lt;/mark&gt;
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Working on Chromium &lt;/h2&gt;

&lt;p&gt;
SRPOL is currently working on porting Chromium project, to be able to run on RISC-V architecture. This year we have been focusing on enabling full features for web/JavaScript engine, compatibility with other architectures, stability and performance optimizations, because as we mentioned before: for the final users performance is very important when browsing web pages, viewing streamed content or running web applications. We have used ARM as a reference and looked for NEON code in the whole Tizen Web Components code with its dependencies, as Chromium widely uses various libraries to run and to present multimedia. As a result, we have a long list of modules, libraries in which some optimizations for RISC-V in the case of vector extensions can be introduced – please see below examples.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiLQqAkoAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
The above list includes just some examples and is much longer than the one presented. This shows the complexity and also the amount of work which needs to be done in performance optimization not only in Chromium itself, but also in many libraries on which it depends.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Contributions and further plans &lt;/h2&gt;

&lt;p&gt;
In the near future, we need Chromium to work with RVV code. The project is dependent on dozens of Open Source libraries which are crucial for excellent multimedia processing in browser and operating system. Gaining performance increase requires many modifications in various different libraries, like ffmpeg, v8, libpng, pixman or libjpeg-turbo. SRPOL has some achievements in pixman, a library being used in Chromium for composing and overlaying different layers, mixing them using alpha channel, etc., as some algorithms have already been ported using RVV, e.g. RGB565 to RGB888 conversion algorithm which gained ~10 times speedup comparing vector to scalar one. However, the engineers working in the Chromium project cannot do all the work by themselves. You can also be a part in this initiative, so please check a guide on how to optimize RISC-V at: &lt;a href=&quot;https://gitlab.com/riseproject/riscv-optimization-guide/-/blob/main/riscv-optimization-guide.adoc&quot; target=&quot;_blank&quot;&gt;https://gitlab.com/riseproject/riscv-optimization-guide/-/blob/main/riscv-optimization-guide.adoc&lt;/a&gt;, as it is a huge job for the whole industry. This is the reason why Samsung joined the RISC-V Software Ecosystem (RISE) project as a premier member in 2023.
&lt;/p&gt;

&lt;p&gt;
Contributing to optimizations for RISC-V in terms of vector extensions is not an easy job to do. First of all, Chromium and its dependencies create a lot of issues as those are Open Source projects supporting many different architectures and having their own path of development where maintainers decide to merge or not to merge the committed patch sets. On the other hand, there are still not many RISC-V boards with CPU that supports RVV on the market, so it is more difficult for maintainers to test those patches.
&lt;/p&gt;

&lt;p&gt;
As we are responsible for Chromium, we are planning to keep an eye on RVV support in the libraries that are most important for us and our plans for the near future are to port reported ARM NEON usage in Chromium and its dependencies to corresponding RISC-V RVV instruction, and measure performance gain in those specific regions. We keep our fingers crossed that the actual numbers proving performance increase and that tests made on our side will convince maintainers to merge our changes. If this will not be possible, the other option could be to branch out from the main repository, or postpone merges in some libraries. We will see what the future of RISC-V will bring.
&lt;/p&gt;

</description><link>https://research.samsung.com/blog/RISC-V-and-Vectorization</link><guid isPermaLink="false">https://research.samsung.com/blog/RISC-V-and-Vectorization</guid><pubDate>Wed, 27 Nov 2024 16:00:00 GMT</pubDate><author>Bartłomiej Kobierzyński</author><category>Open Source</category><category>RISC-V</category><category>RVV</category></item></channel></rss>