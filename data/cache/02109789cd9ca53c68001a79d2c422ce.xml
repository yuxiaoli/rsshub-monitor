<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>BLOG | Samsung Research</title><link>https://research.samsung.com/blog</link><atom:link href="http://rsshub.thzu.xyz/samsung/research/blog" rel="self" type="application/rss+xml"></atom:link><description>BLOG | Samsung Research - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 19 Mar 2025 23:28:40 GMT</lastBuildDate><ttl>5</ttl><item><title>FaceMe: Robust Blind Face Restoration With Personal Identification</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1 Introduction&lt;/h2&gt;

&lt;p&gt;
Face restoration focuses on improving the quality of facial images by removing complex degradation and enhancing details. Face restoration is inherently a highly ill-posed task because a single low-quality input can correspond to many potential high-quality counterparts scattered throughout the high-quality image space. Most advanced face restoration methods cannot guarantee identity consistency.
&lt;/p&gt;

&lt;p&gt;
Recently, some studies have used high-quality reference images of the same identity to enhance identity consistency in restored facial images. However, the feature alignment-based methods would achieve low quality of the restoration if the features are not well aligned. The diffusion model based fine-tuning methods typically requires 5~20 reference images and can be significantly effected by the quality of them.
&lt;/p&gt;

&lt;p&gt;
In this paper, we proposed FaceMe, a fine-tuning-free personalized blind face restoration method based on the diffusion model. Given a low-quality input and either a single or a few high-quality reference images of the same identity, FaceMe restores high-quality facial images and maintains identity consistency within seconds. Remarkably, changing identities does not require fine-tuning, and the reference images can have any posture, expression, or illumination. Furthermore, the quality of the reference image does not significantly impact the quality of the restored image. To our knowledge, this is the first approach that leverages diffusion prior for personalized face restoration tasks, which does not require fine-tuning when changing identity.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2Method&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.1Problem Definition&lt;/h2&gt;

&lt;p&gt;
Let X,Y,D,and G denotes the degraded facial image, the corresponding high-quality facial image, the degradation function, and the generation function, respectively. The objective of personalized face restoration is to generate Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;=G(X│Ref), while ensuring the following 3 constraints are satisfied: 
Consistency: D(Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;)  ≡ X,Realness:  Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;∼q(Y),Identity Consistency: Y&lt;sup style=&quot;margin-left: -5px;&quot;&gt; ̂&lt;/sup&gt;∼ID(Y), 
where q(Y) denotes the distribution of high-quality facial images, Ref and ID(Y) denote the reference images and the distribution of high-quality facial images of the same identity as Y respectively.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.2Proposed Personal Face Restoration&lt;/h2&gt;

&lt;p&gt;
&lt;b&gt;Identity encoder&lt;/b&gt;. We combine the CLIP [2] image encoder ϵ and the ArcFace [1] facial recognition module ψ, to extract identity features from facial images. We employ MLPs to align them, then merge them using MLPs, resulting in s&lt;sub&gt;i&lt;/sub&gt;∈R&lt;sup&gt;d&lt;/sup&gt;, where i denotes the index of N reference images, d is the dimension of cross-attention in the diffusion model.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Combining and replacing&lt;/b&gt;. We concatenate all s&lt;sub&gt;i&lt;/sub&gt; as s. We use a simple prompt: “a photo of 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRsg-h6AE4AUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Overview of proposed FaceMe (left) and training data construction pipeline (right).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
face.”, during the training and test phases. Let c&lt;sub&gt;text&lt;/sub&gt;={e&lt;sub&gt;1&lt;/sub&gt;,…,e&lt;sub&gt;5&lt;/sub&gt;} represent the embedding of the text. Then we replace the embedding e&lt;sub&gt;4&lt;/sub&gt; corresponding to “face” with s as c&lt;sub&gt;id&lt;/sub&gt;={e&lt;sub&gt;1&lt;/sub&gt;,e&lt;sub&gt;2&lt;/sub&gt;,e&lt;sub&gt;3&lt;/sub&gt;,s,e&lt;sub&gt;5&lt;/sub&gt;}
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Training strategy&lt;/b&gt;. The model consists of two trainable modules, i.e., ControlNet and ID encoder (Identity Encoder in Fig. 1). In this work, we propose a two-stage training strategy. In training stage I, we simultaneously train ControlNet and ID encoder, but only save ID encoder’s weights. In training stage II, we fix the ID encoder and only train ControlNet. In this training process, we randomly replace identity embedding c&lt;sub&gt;id&lt;/sub&gt; with non-identity embedding c&lt;sub&gt;text&lt;/sub&gt; with a 50% probability.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Inference strategy&lt;/b&gt;. We embed the low-quality input directly into the initial random Gaussian noise according to the training noise scheduler. We use Classifier-free guidance (CFG) [3] for personalized guidance. In addition, to mitigate the possibility of color shift, we apply wavelet-based color correction to the final result.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.3 Training Data Pool Construction&lt;/h2&gt;

&lt;p&gt;
To our knowledge, no publicly available facial dataset can support diffusion models for training with multiple reference images of the same identity. In this study, we employ synthetic facial images as reference facial images to construct our training data pool. 
&lt;/p&gt;

&lt;p&gt;
We synthesize multiple reference facial images of the same identity as the given facial image using Arc2Face, equipped with Control-Net. Given a pair of reference and pose, Arc2Face can synthesize facial images that maintain the identity of reference image and the given pose.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Pose reference data pool&lt;/b&gt;. We extract the pose attribute and the expression attribute for each image in FFHQ. We conduct K-Means clusters on these images based on the two attributes to get c&lt;sub&gt;1&lt;/sub&gt; and c&lt;sub&gt;2&lt;/sub&gt; cluster centers, resulting in c&lt;sub&gt;1&lt;/sub&gt;*c&lt;sub&gt;2&lt;/sub&gt; disjoint subsets, forming the pool.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Same identity&lt;/b&gt;. For each image, we randomly sample an pose image from any subset in the pool, Using them as input for Arc2Face, we get one reference image. We then assess the identity similarity between the input image and the generated reference one. If the similarity falls below δ, we re-sample pose image for regeneration. If an acceptable result is not obtained after 3 attempts, we stop generating it. We name the synthesized reference images for the FFHQ dataset as FFHQRef.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRsirC6AFEAUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Quantitative comparison. The bold numbers represent the best performance.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3Results&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.1Experimental Settings&lt;/h2&gt;

&lt;p&gt;
&lt;b&gt;Training datasets&lt;/b&gt;. Our training dataset consists of FFHQ dataset and our synthesized FFHQRef dataset, with all images resized to 512×512. The corresponding degraded image is synthesized using a degradation model (see in our paper).
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Implementation details&lt;/b&gt;. We employ SDXL model stable-diffusion-xl-base-1.0 as our base diffusion model and the CLIP image encoder as part of our identity encoder, both of which are fine-tuned by PhotoMaker. We use the Adam optimizer to optimize the network parameters with a learning rate of 5×10&lt;sup&gt;-5&lt;/sup&gt; for two training stages. The training process is implemented using the PyTorch framework and is conducted on eight A40 GPUs, with a batch size of 4 per GPU. The two training stage are trained 130K and 210K iterations, respectively.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Testing datasets&lt;/b&gt;. We use one synthetic dataset CelebRef-HQ and three real-world datasets: LFW-Test, WebPhoto-Test, and WIDER-Test for test. For the synthetic dataset, we randomly select 150 identities and select one image per identity as the ground truth, using 1∼4 images of the same identity as reference images. For the real-world datasets, due to the lack of reference images with the same identity, we first use face restoration method, i.e., Codeformer, to restore low-quality input. The restored images are then used as input to Arc2Face to generate reference images.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.2Quantitative Results&lt;/h2&gt;

&lt;p&gt;
Tab. 1 shows the performance of FaceMe on the synthetic dataset CelebRef-HQ. As shown, our method achieves the best performance in PSNR, FID, LMD, and IDS, and the second-best performance in LPIPS. Additionally, it is worth noting that FaceMe has significantly improved in IDS, which demonstrates its ability in personalization.
&lt;/p&gt;

&lt;p&gt;
As presented in Tab.1, our FaceMe achieves the best FID score on the LFWTest and Wider-Test datasets. LFW-Test and Wider-Test are mildly degraded and heavily degraded real-world dataset, respectively. The excellent performance on both datasets indicates that FaceMe is capable of adapting to complex degradation scenarios in the real world, demonstrating exceptional robustness.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.3Quantitative Results&lt;/h2&gt;

&lt;p&gt;
The visualization results are shown in Fig.2. It can be observed that the compared methods either produce many artifacts in the restored images or restore high quality images but fail to 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRskD2aAFQAUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Qualitative comparison on CelebRef-HQ.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRskHA6AFcAUGo7.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Qualitative comparison on real-world faces.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
maintain identity consistency with the ground truth. In contrast, our FaceMe can restore high quality images while preserving identity consistency.
&lt;/p&gt;

&lt;p&gt;
Fig. 3 shows the visual comparisons of different methods. It is clear that, compared to state-of-the-art methods, our method can handle more complex scenes and restore high-quality images 
without introducing unpleasant artifacts.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4Conclusion&lt;/h2&gt;

&lt;p&gt;
We propose a method to address the issue of identity shift in blind facial image restoration. Based on diffusion model, we use identity-related features extracted by identity encoder to guide the diffusion model in recovering face images with consistent identities. Our method supports any number of reference images input through simple combine identity related features. In addition, the strong robustness of the identity encoder allows us to use synthetic images as reference images for training. Moreover, our method does not require fine-tuning the model when changing identities. The experimental results demonstrate the superiority and effectiveness of our method.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2501.05177&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://arxiv.org/abs/2501.05177&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;


&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] Deng, J.; Guo, J.; Xue, N.; and Zafeiriou, S. 2019. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 4690–4699..
&lt;/p&gt;
&lt;p&gt;
[2] Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In ICML, 8748–8763. PMLR.
&lt;/p&gt;
&lt;p&gt;
[3] Ho, J.; and Salimans, T. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/FaceMe-Robust-Blind-Face-Restoration-With-Personal-Identification</link><guid isPermaLink="false">https://research.samsung.com/blog/FaceMe-Robust-Blind-Face-Restoration-With-Personal-Identification</guid><pubDate>Invalid Date</pubDate><author>Jia Ouyang|Hyunhee Park</author><category>AI</category><category>Face Restoration</category><category>AAAI</category></item><item><title>Deep Learning for CSI Feedback: One-Sided Model and Joint Multi-Module Learning Perspectives</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1.Introduction&lt;/h2&gt;

&lt;p&gt;
Massive multiple-input multiple-output (MIMO) technology plays a crucial role in delivering the potential of 5G and meeting its requirements. To fully exploit the potential of MIMO technology, it is crucial to obtain downlink channel state information (CSI) on the base station (BS) side by feeding the estimated downlink CSI from the user equipment (UE) back to the BS through the uplink channel, as shown in Fig. 1. The feedback of CSI leads to extra overhead, which escalates significantly with the increase in the number of service antennas. When the uplink channel resources are limited, the challenge of CSI feedback lies in maintaining the accuracy of the feedback while minimizing the feedback overhead.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQS1nKAHwAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; A communication framework with CSI feedback&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Traditional methods for reducing the CSI feedback overhead include techniques based on codebooks or compressed sensing (CS). In the past few years, artificial intelligence (AI) has been utilized in the CSI feedback space to improve the precision of CSI reconstruction; see, for instance, the AI for CSI feedback enhancement in the study item of the Third Generation Partnership Project (3GPP) Release 18 [1].
&lt;/p&gt;

&lt;p&gt;
In this article, we first introduce an AI-based one-sided CSI feedback method, wherein the DL model is only employed at the BS, and AI-based multi-module learning involving the CSI feedback. In addition, we articulate the prospects and challenges pertaining to AI-driven CSI feedback tasks, taking into account both the AI limitations and the real-world implementation of AI models. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2.AI-Based One-Sided CSI Feedback&lt;/h2&gt;

&lt;p&gt;
For one-sided CSI feedback, the AI model is implemented solely on the BS, to reduce transmission overhead and improve performance. In addition, BS-sided model can avoid multi-vendors collaboration effort and preserve model confidentiality and data privacy. One-sided CSI feedback architectures, such as CS-CsiNet [2], CFnet [3], and CSI-PPPNet [4], improve CSI reconstruction using deep learning (DL). CSI-PPPNet [4] uses a single DL model for arbitrary compression rates, with CSI compressed at the UE and iteratively recovered at the BS, a feature not achieved by the other two methods. This approach decouples training from compression, simplifying model maintenance for the network vendor. Fig. 2 shows the normalized mean square errors (NMSE) performance of AI-based one-sided CSI feedback (CS-CsiNet, CSI-PPPNet) and two-sided CSI feedback (CsiNet) for indoor and urban macrocell (UMa) scenarios, with 256 subcarriers. The BS uses a ULA with 32 omnidirectional antennas, and the UE uses a single omnidirectional antenna. 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQT1baAH8AUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Performance comparison of AI-based one-sided CSI feedback networks, i.e., CS-CsiNet and CSI-PPPNet, and AI-based two-sided CSI feedback network, i.e., CsiNet, for indoor and UMa scenarios&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3.Joint Multi-Module Learning with CSI Feedback&lt;/h2&gt;

&lt;p&gt;
To fully exploit MIMO technology and improve throughput, it is crucial to jointly design multiple modules, including channel coding, CE, PD, and precoding, alongside the CSI feedback task.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.1 Joint CSI Compression and Channel Coding&lt;/h2&gt;

&lt;p&gt;
The CSI compression can be viewed as a source compression task, which can be designed independently of channel coding, following the paradigm of separate source-channel coding (SSCC).
&lt;/p&gt;

&lt;p&gt;
Using AI-based source coding methods, the CSI is compressed by an encoder and reconstructed using the corresponding decoder. When the quality of the channel deteriorates to such an extent that it surpasses the processing capacity of the channel coding, the precision of the reconstructed CSI within the SSCC framework experiences a significant drop, a phenomenon referred to as the ````````cliff effect&#39;&#39;. The use of heavily distorted CSI for the creation of precoding vectors results in an undesirable reduction in the system throughput. While the hybrid automatic repeat request mechanism and other methods can alleviate this problem, they also cause a delay in the downlink CSI acquisition and an increase in the feedback overhead.
&lt;/p&gt;

&lt;p&gt;
To address the &#39;&#39;cliff effect&#39;&#39;, a new CSI feedback architecture, coined as DJSCC, was introduced in [5]. This approach leverages DL to integrate source and channel coding, training the system with a dataset that encompasses both the source and the wireless channel. The performance of the SSCC and DJSCC networks for CSI feedback with the same overhead is shown in Fig. 3. Both networks are trained and tested on the same dataset, which is generated by QuaDriGa in an FDD indoor scenario. The number of feedback symbols n is set to 16. We employ explicit feedback, where the system employs 256 subcarriers. The BS utilizes a ULA with half-wavelength antenna spacing and 32 omnidirectional antennas, while the UE deploys a single omnidirectional antenna. 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQU_SaAIIAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Performance comparison between SSCC, DJSCC, and ADJSCC with the same feedback overhead. The label &#39;&#39;SSCC_en16_Q2_n4_R_1/2&#39;&#39; refers to the SSCC network where the encoder has 16 outputs of real numbers, 2 quantization bits, a QAM modulation order of 4, and a coding rate of 1/2&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
From the experimental results shown in Fig. 3, we can observe that the SSCC network suffers from a severe &#39;&#39;cliff effect&#39;&#39;. DJSCC addresses this issue by yielding only a gradual rise in NMSE with the reduction of the SNR in the feedback channel, while enhancing the performance across all SNR levels. To address the generalization problem, an attention mechanism-based DJSCC network (ADJSCC) was proposed in [5] to dynamically adjust the ratio of source coding and channel coding outputs based on the uplink SNR.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.2 Joint CSI Compression, Channel Coding and Precoding &lt;/h2&gt;

&lt;p&gt;
In a multiuser MIMO scenario, the precoding vector for one UE affects interference to/from other UEs. To maximize the downlink sum-rate, precoding vectors are jointly designed after the BS aggregates CSI from all UEs. Joint feedback and precoding networks (JFPNet) have been proposed to optimize both feedback and precoding design [6]. Notably, [6] employs implicit feedback, where the feedback source is the eigenvector matrix rather than full CSI.
&lt;/p&gt;

&lt;p&gt;
Fig. 4 shows the downlink SE of different methods. Initially, the downlink CSI undergoes preprocessing to generate the eigenvalues and the eigenvector matrix, and the DJSCC network is then used to compress the eigenvector matrix for feedback. The uplink channel SNR during the training phase spans a range from -10 dB to 10 dB, following a uniform distribution and the feedback overhead is fixed to n=32 symbols. The BS and UE use ULA omnidirectional arrays with 32 and 4 elements, respectively. The full CSI consists of 624 subcarriers, i.e., 13 subbands. The JFPNet, which jointly implements CSI compression, channel coding and precoding, achieves the highest SE at all SNRs. The label &#39;&#39;SFPNet&#39;&#39; indicates that the precoding module is trained separately from the DJSCC module for CSI feedback, and &#39;&#39;JFPNet&#39;&#39; refers to the joint training of the CSI feedback and precoding modules. &#39;&#39;JFPNet’’ outperforms &#39;&#39;SFPNet&#39;&#39;, highlighting the benefits of multi-module learning for extracting task-related semantic information. &#39;&#39;DJSCC_BD_WF&#39;&#39; and &#39;&#39;PF_BD_WF&#39;&#39; refer to DJSCC CSI feedback and perfect CSI feedback, respectively, using traditional non-AI precoding methods (block diagonalization (BD) and water-filling (WF) algorithms). 
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/50d2db752b/AZSQVWB6AIUAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 4.&lt;/b&gt; Performance comparison: a joint training approach and a separate training approach are utilized for the DL-based CSI feedback module and precoding module, respectively&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4.Opportunities and Challenges &lt;/h2&gt;

&lt;p&gt;
DL-based CSI feedback in FDD massive MIMO systems reduces overhead and complexity. Using a one-sided model and joint multi-module learning enhances practical deployment and end-to-end performance. However, challenges remain in both AI technology and the real-world implementation of AI models for CSI feedback. Current datasets are mostly statistical, generated by simulators under the assumption that channels across cells are independent and identically distributed. However, this assumption doesn’t reflect real-world conditions, leading to mismatched data and reduced model performance, which can degrade system throughput. 
&lt;/p&gt;

&lt;p&gt;
To support AI/ML for CSI feedback in current 5G or future 6G systems, for standardization, data collection, performance monitoring, and feedback of inference results are the main considerations. Data collection is needed for model training and performance monitoring, including data for AI inputs and ground truth for labeling. Data omission or compression may be needed to reduce the overhead. Performance monitoring is another unique aspect compared to traditional non-AI communication systems. Metrics and procedures for performance monitoring also need to be standardized. Recall that new signaling for model inference results in feedback. For example, for joint multi-module learning for CSI feedback, and since channel coding and potential modulation are already included in AI encoders and the output of the encoder may not fit in the existing signaling processes, new CSI feedback types need to be defined in standards. Besides, two-sided models, wherein the encoder and decoder need to be pairwise trained, will require inter-vendor calibration. To avoid offline calibrations between different vendors, some proposals are under discussion in 3GPP in Release 19. For example, the training data set and the model structure can be standardized. Alternatively, a well-trained model or model parameters can be delivered over the air interface to improve the performance.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;5.Conclusion &lt;/h2&gt;

&lt;p&gt;
In this article, we investigated the recent developments of DL-based CSI feedback from the perspectives of one-sided model and joint multi-module learning. Firstly, we introduced a one-sided CSI feedback architecture that only replaces the traditional decoding module with a DL module. Subsequently, we delved into CSI feedback architectures, jointly learned with various modules, e.g., channel coding, and precoding. For example, the joint design of the CSI feedback and channel coding via DJSCC overcomes the “cliff effect” observed in the traditional SSCC-based CSI feedback methods. Unlike traditional approaches of designing each module independently, multi-module joint design leverages DL techniques to jointly optimize the various modules and exploit the interdependencies between different modules, and can consequently enhance the overall system performance and training efficiency.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10812964&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10812964&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1]J. Guo, C. -K. Wen, S. Jin and X. Li, &quot;AI for CSI Feedback Enhancement in 5G-Advanced,&quot; in IEEE Wireless Communications, vol. 31, no. 3, pp. 169-176, June 2024
&lt;/p&gt;
&lt;p&gt;
[2]C. -K. Wen, W. -T. Shih and S. Jin, &quot;Deep Learning for Massive MIMO CSI Feedback,&quot; in IEEE Wireless Communications Letters, vol. 7, no. 5, pp. 748-751, Oct. 2018, doi: 10.1109/LWC.2018.2818160.
&lt;/p&gt;
&lt;p&gt;
[3]J. Guo, C. -K. Wen, M. Chen and S. Jin, &quot;Environment Knowledge-Aided Massive MIMO Feedback Codebook Enhancement Using Artificial Intelligence,&quot; in IEEE Transactions on Communications, vol. 70, no. 7, pp. 4527-4542, July 2022, doi: 10.1109/TCOMM.2022.3180388.
&lt;/p&gt;
&lt;p&gt;
[4]W. Chen, W. Wan, S. Wang, P. Sun, G. Y. Li and B. Ai, &quot;CSI-PPPNet: A One-Sided One-for-All Deep Learning Framework for Massive MIMO CSI Feedback,&quot; in IEEE Transactions on Wireless Communications, vol. 23, no. 7, pp. 7599-7611, July 2024, doi: 10.1109/TWC.2023.3342735.
&lt;/p&gt;
&lt;p&gt;
[5]J. Xu, B. Ai, N. Wang and W. Chen, &quot;Deep Joint Source-Channel Coding for CSI Feedback: An End-to-End Approach,&quot; in IEEE Journal on Selected Areas in Communications, vol. 41, no. 1, pp. 260-273, Jan. 2023, doi: 10.1109/JSAC.2022.3221963.
&lt;/p&gt;
&lt;p&gt;
[6]Y. Guo, W. Chen, J. Xu, L. Li and B. Ai, &quot;Deep Joint CSI Feedback and Multiuser Precoding for MIMO OFDM Systems,&quot; in IEEE Transactions on Vehicular Technology, vol. 74, no. 1, pp. 1730-1735, Jan. 2025, doi: 10.1109/TVT.2024.3452409.
&lt;/p&gt;
&lt;p&gt;
[7]H. Jiang, M. Cui, D. W. K. Ng and L. Dai, &quot;Accurate Channel Prediction Based on Transformer: Making Mobility Negligible,&quot; in IEEE Journal on Selected Areas in Communications, vol. 40, no. 9, pp. 2717-2732, Sept. 2022, doi: 10.1109/JSAC.2022.3191334.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/Deep-Learning-for-CSI-Feedback-One-Sided-Model-and-Joint-Multi-Module-Learning-Perspectives</link><guid isPermaLink="false">https://research.samsung.com/blog/Deep-Learning-for-CSI-Feedback-One-Sided-Model-and-Joint-Multi-Module-Learning-Perspectives</guid><pubDate>Invalid Date</pubDate><author>Feifei Sun</author><category>Communications</category><category>6G</category><category>CSI feedback</category></item><item><title>FiRa Consortium Release 3.0 - UWB Core Specification and Certification Program Launch</title><description>&lt;p&gt;
In December 2024, FiRa Consortium announced launching of Release 3.0 Core Certification Program and Release 3.0 of Core Specifications of ultra-wideband (UWB) MAC, PHY and Link Layer subsystems in devices. This is a major milestone for FiRa after the successful release of 2.0 Certification Program, which supports UWB features for localization and device-to-device service in, October 2023.
&lt;/p&gt;

&lt;p&gt;
While FiRa Core 1.0 Release focused on UWB technology for use cases such as hands- free access control service, FiRa 2.0 Release extended FiRa UWB technology to location based service and device-to-device service, this FiRa 3.0 Release adds support for public transport service, tap-free payment and car access digital key.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/3a4bb21f3e/AZRIj4mqABcAUGo7.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Usecases Examples of FiRa 3.0 Core Spec &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
FiRa is introducing three major UWB features in its Core 3.0 Specifications:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Car Digital Key UWB&lt;/b&gt;: FiRa adds the Car Connectivity Consortium® (CCC) Digital Key UWB feature to its Certification Program, which measures the proximity of a mobile device to the car, employing a one-to-many double-sided two-way ranging protocol defined by CCC.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Hybrid UWB Scheduling (HUS)&lt;/b&gt;: Enables advanced UWB applications that require various combinations of FiRa features to work together in a deterministic way, ensuring optimized performance in complex environments such as public transport systems.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Dedicated Data Transfer&lt;/b&gt;: Designed to use UWB for dedicated data transfer, this advancement allocates entire time slots exclusively for data transfer, independent of ranging operations.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
The FiRa Public Transport usecase consists of three main phases, illustrated in Figure. 2 and described below:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
Phase 1: Discovery process at the station entrance. FiRa 2.0 technology such as Contention based ranging, BLE device discovery can be used at this stage. 
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
Phase 2: Untracked Navigation using DL-TDoA from FiRa 2.0 specification when walking to the gate or to the platform.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
Phase 3: Gate selection and fare transaction when approaching and walking through the gate. Hybrid UWB Scheduling features is used for supporting payment transaction while ranging with gates. 
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRxrF26AGoAUGo7.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Visualization of the three phases of the UWB based fare transaction system. One of the anchors is colored in red, to indicate its primary role. The anchor in this role is responsible for ensuring that all anchors operate in a synchronized manner.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
With the extended feature set available in &lt;b&gt;FiRa Certification Program 3.0&lt;/b&gt;, it enables devices to support an even wider and richer set of use cases and applications, in an interoperable fashion. Devices can select the specific features for the certification with all the features of Certification Release 2.0 also fully available in Release 3.0. Launch of FiRa Certification Program 3.0, will help in improving the adoption of UWB technology into more industry verticals.
&lt;/p&gt;

&lt;p&gt;
Samsung Galaxy S24 Plus models were adopted as Reference Devices for  FiRa 3.0 Certification Program to test the device’s interoperability. This would lead all the FiRa Certified Devices to become interoperable with Samsung Galaxy mobile phones featured with the UWB chipset. Those models will become the world’s first FiRa 3.0 certified devices in the upcoming month.
&lt;/p&gt;

&lt;p&gt;
FiRa Core 3.0 Core Specifications and Certification Program are exclusively available to FiRa Consortium members. For more information, visit www.firaconsortium.org
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Conclusion and Next Steps&lt;/h2&gt;

&lt;p&gt;
FiRa evolves its technology and certification program to develop more market attractive use-cases. Beyond customer centric use cases of FiRa 1.0, 2.0 and 3.0 releases, FiRa sees another opportunity at industrial area such as factory, warehouse and hospital where FiRa use cases such as asset tracking, foot traffic and shopping behavior analytics will be crucially important for the business. Enhancing FiRa UWB ranging technology for the upcoming release will enable this promising scenario successful in the market.
&lt;/p&gt;

&lt;p&gt;
Moreover, FiRa keeps trying to co-work with other consortiums to extend FiRa UWB ecosystem. Working with CCC (Car Connectivity Consortium®) is one such remarkable collaboration for the successful adoption of FiRa 3.0 in the market. FiRa also plans to provide conformance certification to Connectivity Standards Alliance® Aliro for the next release. By harmonizing standards and certification processes, FiRa seeks to create a more cohesive UWB ecosystem. Also this partnership will pave the way forward with reduced development and certification costs for industry stakeholders while promoting interoperability across various applications and strengthening the broader UWB ecosystem as a whole.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;About FiRa Consortium&lt;/h2&gt;

&lt;p&gt;
The FiRa Consortium is a member-driven organization dedicated to transforming the way we interact with our environment by enabling precise location awareness for people and devices using the secured fine-ranging and positioning capabilities of ultra-wideband (UWB) technology. FiRa does this by driving the development of technical specifications and certification, advocating for effective regulations, and by defining a broad set of use cases for UWB. 
&lt;/p&gt;

&lt;p&gt;
To learn more about UWB and the FiRa Consortium, visit &lt;a href=&quot;https://research.samsung.com/blog/www.firaconsortium.org&quot; target=&quot;_blank&quot;&gt;www.firaconsortium.org&lt;/a&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;About Connectivity Standards Alliance&lt;sup&gt;®&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;
The Connectivity Standards Alliance&lt;sup&gt;®&lt;/sup&gt; (CSA) is the foundation and future of the Internet of Things (IoT). Established in 2002, its wide-ranging global membership collaborates to create and evolve universal open standards for the products transforming the way we live, work, and play. With its members’ deep and diverse expertise, robust certification programs, and a full suite of open IoT solutions CSA is leading the movement toward a more intuitive, imaginative, and useful world. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;About Car Connectivity Consortium&lt;sup&gt;®&lt;/sup&gt;(CCC)&lt;/h2&gt;

&lt;p&gt;
The Car Connectivity Consortium&lt;sup&gt;®&lt;/sup&gt; (CCC) is a cross-industry organization advancing technologies for smartphone-to-car connectivity solutions. CCC represents the vast majority of the global automotive and smartphone industries, with over 170 member companies. The CCC member companies include smartphone and vehicle manufacturers, automotive tier-1 suppliers, silicon/chip vendors, and security product suppliers representing a comprehensive ecosystem for secure access. Digital Key allows consumers to use their mobile devices, regardless of manufacturer or operating system type, to easily and securely access their vehicles. Based on the IEEE 802.15.4z standard, UWB secure ranging is a core technology that enables Digital Key 3.0 by preventing car theft while preserving full user convenience.
&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
- FiRa Consortium, &lt;a href=&quot;https://www.firaconsortium.org/&quot; target=&quot;_blank&quot;&gt;https://www.firaconsortium.org/&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
- Car Connectivity Consortium, &lt;a href=&quot;https://carconnectivity.org/&quot; target=&quot;_blank&quot;&gt;https://carconnectivity.org/&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
- Connectivity Standards Alliance, &lt;a href=&quot;https://csa-iot.org/&quot; target=&quot;_blank&quot;&gt;https://csa-iot.org/&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/FiRa-Consortium-Release-3-0-UWB-Core-Specification-and-Certification-Program-Launch</link><guid isPermaLink="false">https://research.samsung.com/blog/FiRa-Consortium-Release-3-0-UWB-Core-Specification-and-Certification-Program-Launch</guid><pubDate>Invalid Date</pubDate><author>Jieun Keum|Gyubong Oh|Ankur Bansal</author><category>Communications</category><category>UWB</category><category>FiRa</category></item><item><title>Off-Policy Selection for Optimizing Ad Display Timing in Mobile Games (Samsung Instant Plays)</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;
Off-Policy Selection (OPS) aims to select the best policy from a set of policies trained using offline Reinforcement Learning. In this work, we describe our custom OPS method and its successful application in Samsung Instant Plays for optimizing ad delivery timings. The motivation behind proposing our custom OPS method is the fact that traditional Off-Policy Evaluation (OPE) methods often exhibit enormous variance leading to unreliable results. We applied our OPS method to initialize policies for our custom pseudo-online training pipeline. The final policy resulted in a substantial 49% lift in the number of watched ads while maintaining similar retention rate.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1 System Overview&lt;/h2&gt;

&lt;p&gt;
Samsung’s Instant Plays is a mobile application which allows users to play various games without any fees. The application generates revenue by displaying ads to users during a game-play. There are two major types of ads in our platform:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Reward&lt;/b&gt;: user-triggered longer 15s ads, that award the user with some bonuses inside a game.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Interstitial&lt;/b&gt;: self-triggered shorter 5s ads, that are displayed between game stages.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
In our project we take control over the latter, that is, the interstitial ads. During a gaming session, the user plays for a number of stages. Between each stage the system decides whether to display an ad. If the system displays ads too often for a specific user, that user might churn. On the contrary, if a user doesn’t mind watching ads and we don’t display them, then we might lose some revenue.
&lt;/p&gt;

&lt;p&gt;
The decision process of displaying interstitial ads is sequential in nature. In each timestamp when there is a slot for an interstitial ad, the system must decide whether to display it or not. The system might withhold showing an ad in some particular moment for the sake of better future moments. Thus, the Reinforcement Learning (RL) paradigm fits for our use case. We aim to train RL agent, which utilizes contextual information (user embedding, user profile, game embedding, time from last ad, number of ads shown in the last 10 minutes, etc.) to select the best slots for displaying ads. It is worth mentioning that our system aims only to select the timing of an ad, not the specific content of it. Thus, our problem is orthogonal to the content recommendation, but equally important.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2 Proposed Methods&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.1Offline Pretraining, OPE and OPS&lt;/h2&gt;

&lt;p&gt;
In order to initialize Reinforcement Learning agents for our system, we used offline RL paradigm and conducted the initial training, experimenting with off-policy methods like Rainbow DQN [4] and MARWIL [7]. However, it is known that models naively pretrained on offline data may underperform when deployed into real-life [6, 8]. Thus, we would like to use a reliable off-policy evaluation (OPE) method, where the performance of trained policies is evaluated solely using offline data. Unfortunately, we tested some common OPE methods including Importance Sampling [2] and Doubly Robust [3], and it turned out that getting accurate point estimates from OPEs is nearly impossible due to the their unrealistic estimates and huge variance (more on that in section 4.1). In such cases, OPS tasks, such as selecting the best performing policy from a set of policies, become equally important and are often easier to realize (fig. 1).
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRc7lm6AEEAUGo7.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Off-policy selection (OPS) aims to select the best policy out of a pool of policies. This is a common scenario in the real-world recommender systems where running multiple policies on an environment may be impossible.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.2Our Custom Off-Policy Selection Method&lt;/h2&gt;

&lt;p&gt;
We propose an Off-Policy Selection method based on measuring similarities between trajectories from the offline dataset and those that the trained policy would take. Let’s say that we have an offline dataset D consisting of M trajectories and a trained policy π. For each trajectory τ=(o&lt;sub&gt;0&lt;/sub&gt;,a&lt;sub&gt;0&lt;/sub&gt;,r&lt;sub&gt;1&lt;/sub&gt;,o&lt;sub&gt;1&lt;/sub&gt;,a&lt;sub&gt;1&lt;/sub&gt;,…) from the offline dataset D we apply trained policy to the underlying observations and, as a result, generate a series of new actions π(o&lt;sub&gt;i&lt;/sub&gt; )= a ̅&lt;sub&gt;i&lt;/sub&gt;. We compare series of actions (a0,a1,...) with the newly created ones (a ̅&lt;sub&gt;0&lt;/sub&gt;,a ̅&lt;sub&gt;1&lt;/sub&gt;,…) by computing Euclidean distance between corresponding elements and assign a distance d&lt;sub&gt;τ&lt;/sub&gt;. Given all such distances, we sort them in an ascending order d&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;  &amp;lt; · · · &amp;lt; d&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;M&lt;/sub&gt; and assign a final score of a trained policy as an average return from p% of the most similar trajectories, i.e. 1/N ∑&lt;sup&gt;N&lt;/sup&gt;&lt;sub style=&quot;margin-left: -11px;&quot;&gt;(n=1)&lt;/sub&gt;r&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt; , where r&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt; is a return corresponding to the distance d&lt;sup&gt;&#39;&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3 Implementation&lt;/h2&gt;

&lt;p&gt;
In order to realize our ideas, we needed an RL framework which would support offline Reinforcement Learning and Off-Policy Evaluation methods. We started with ReAgent suite [1] to develop proof of concept. However, we eventually switched to Ray [5] because this framework is more developer-friendly, supports more off-policy methods, is better suited for heavy-duty applications, contains modules for serving RL models, and is overall very well maintained.
&lt;/p&gt;

&lt;p&gt;
Since we used off-policy methods, we needed to store data for training in a convenient way. We utilized replay buffer, we updated it using historical data and done the retraining of RL models every 4 hours. The RL models were hosted using Ray Serve module, which allowed us to provide response to Samsung Instant Plays application within 100ms. The workflow of our system is summarised in fig. 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRc8Dd6AEQAUGo7.png&quot; width=&quot;40%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; The workflow of our system. The response time is expected to be delivered under 100ms. Replay buffer update and retraining is done every 4 hours.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4 Results&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;4.1Offline Evaluation&lt;/h2&gt;

&lt;p&gt;
To validate our Off-Policy Selection method described in section 2.2, we computed estimates for our method and compared them with real online experiments. We trained an RL agent on historical data removing last week for OPE methods evaluation. In table 1, we summarise estimated lifts in the number of interstitial ads watched on the one-week test split.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/b0d82c4e77/AZRc8XEaAEcAUGo7.png&quot; width=&quot;50%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Estimated lift in the number of watched interstitial ads according to different OPE methods.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
We hypothesize that such extreme policy value estimates of the above popular OPE methods are due to the nature of our use case. The game sessions for some users might last for an extended period, resulting in long sequences of actions. OPE methods try to correct the estimate by using the product of inverse propensity scores, which leads to a compounding error.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;4.2A/B Tests&lt;/h2&gt;

&lt;p&gt;
We conducted A/B tests where we compared our RL models against rule-based system which was used at that time. We focused on two metrics during that online evaluation: lift of the absolute number of watched interstitial ads and the retention rate. We ran the A/B test for a month and, similar to the offline setting, we aggregated the results from the last week of our A/B test. For the final results we obtained 49% lift in the absolute number of watched ads while maintaining very similar retention rate which slightly rose by 2%.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1145/3640457.3688058&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://doi.org/10.1145/3640457.3688058&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;


&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1]Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Zhengxing Chen, Yuchen He, Zachary Kaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform. arXiv preprint arXiv:1811.00260 (2018).
&lt;/p&gt;
&lt;p&gt;
[2]Josiah P. Hanna, Scott Niekum, and Peter Stone. 2019. Importance Sampling Policy Evaluation with an Estimated Behavior Policy. arXiv:1806.01347 [cs.LG]
&lt;/p&gt;
&lt;p&gt;
[3]Nan Jiang and Lihong Li. 2015. Doubly Robust Off-policy Evaluation for Re- inforcement Learning. CoRR abs/1511.03722 (2015). arXiv:1511.03722 http://arxiv.org/abs/1511.03722
&lt;/p&gt;
&lt;p&gt;
[4]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602 [cs.LG]
&lt;/p&gt;
&lt;p&gt;
[5]Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. 2018. Ray: a distributed framework for emerging AI applications. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation. 561–577.
&lt;/p&gt;
&lt;p&gt;
[6]Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Joséphine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, et al. 2023. Jump- start reinforcement learning. In International Conference on Machine Learning. PMLR, 34556–34583.
&lt;/p&gt;
&lt;p&gt;
[7]Qing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, and Tong Zhang. 2018. Exponentially Weighted Imitation Learning for Batched Historical Data. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/ 4aec1b3435c52abbdf8334ea0e7141e0-Paper.pdf
&lt;/p&gt;
&lt;p&gt;
[8]Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Dong Yan, and Jun Zhu. 2023. On the Reuse Bias in Off-Policy Reinforcement Learning. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI- 23, Edith Elkind (Ed.). International Joint Conferences on Artificial Intelligence Organization, 4513–4521. https://doi.org/10.24963/ijcai.2023/502 Main Track.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/Off-Policy-Selection-for-Optimizing-Ad-Display-Timing-in-Mobile-Games</link><guid isPermaLink="false">https://research.samsung.com/blog/Off-Policy-Selection-for-Optimizing-Ad-Display-Timing-in-Mobile-Games</guid><pubDate>Invalid Date</pubDate><author>Katarzyna Siudek-Tkaczuk|Bartłomiej Swoboda|Michał Romaniuk</author><category>AI</category><category>Off-PolicyEvaluation</category><category>RecSys</category></item><item><title>Towards Building a Trusted Execution Environment on RISC-V Microcontrollers</title><description>&lt;p&gt;
In this article we share our experience of developing a secure system using RISC-V Micro-Controller Unit (MCU). If this topic resonates, we encourage you to join our &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;mTower&lt;/a&gt; project to explore the potential of RISC-V security.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;RISC-V Open Architecture&lt;/h2&gt;

&lt;p&gt;
RISC-V has rapidly gained popularity among the architectures used for MCU. Its open architecture with a customizable instruction set (ISA) allows developers to tailor processors to specific needs (including security), to reduce the costs and to accelerate innovation.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;RISC-V is a lucrative choice because of the following features:&lt;/h2&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;1.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Open Architecture without Royalties&lt;/b&gt;: The key reason for RISC-V’s rise in popularity is its openness. It allows companies to avoid the costs associated with proprietary architectures like ARM, offering greater flexibility for custom solutions, which is crucial in embedded systems where optimization and cost control are essential&lt;sup&gt;[1][2]&lt;/sup&gt;.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;2.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Flexibility and Scalability&lt;/b&gt;: RISC-V is well-suited for everything from MCU to high-performance processors (CPU), making it versatile across various industries, including robotics, Internet of Things (IoT), and industrial automation[2][3]. Its modular design allows adding custom specialized instructions, which is particularly advantageous for resource-constrained microcontrollers.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;3.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Community Support and Rapid Development&lt;/b&gt;: RISC-V is backed by an active international community, which enables fast improvements and innovations. Major companies such as Google and NVIDIA, as well as governments, support this standard, making it promising for various industries&lt;sup&gt;[2][3]&lt;/sup&gt;. 
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;4.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Platform for Innovation and Research&lt;/b&gt;: RISC-V has opened new possibilities for research and development. Its modifiable architecture makes it ideal for creating secure environments like Trusted Execution Environments (TEE). In industries ranging from robotics to automotive systems, RISC-V microcontrollers are driving advances in security and energy efficiency&lt;sup&gt;[2]&lt;/sup&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Importance of Trusted Execution Environment for RISC-V Microcontrollers&lt;/h2&gt;

&lt;p&gt;
With the development of IoT technologies, security has become one of the key issues. IoT devices collect, process and transmit large volumes of confidential data, making it essential to ensure secure execution of security-critical operations. A TEE creates an isolated environment within the processor, guaranteeing that security-sensitive functions, such as Trusted Identity, Trusted Boot, Trusted Update, Trusted Firmware, and Trusted Operation, have no risk of compromise (Fig.1).
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/73e4d27b45/AZQaB_BKA6MAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; Security aspects for embedded system &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The open and modular architecture of RISC-V provides developers with the flexibility to integrate TEE and explore new approaches to create secure environments for various IoT systems. A distinctive feature of RISC-V-based MCUs is that their development can be done faster comparing to more complex CPUs. This is due to the relative simplicity and accessibility of microcontrollers, both in manufacturing and usage, making them ideal for implementing new features and quickly adapting to new security standards.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Hardware Features for Building TEE Based on RISC-V&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Privilege Levels&lt;/h2&gt;

&lt;p&gt;
Privileges management is a fundamental element of security for the Trusted Execution Environment (TEE) built in MCUs. RISC-V architecture allows multiple privilege levels, enabling effective separation of trusted and untrusted processes.
&lt;/p&gt;

&lt;p&gt;
RISC-V supports several execution modes, including Machine mode (M-mode/M), Supervisor mode (S-mode/S), and User mode (U-mode/U)&lt;sup&gt;[4][5]&lt;/sup&gt;.
&lt;/p&gt;

&lt;p&gt;
Combinations of modes:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
M (simple embedded systems)
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
M, U (embedded systems with security)
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
M, S, U (systems running Unix-like operating systems)
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
MCUs with both Machine mode and User mode already allow for certain aspects of privilege management, as they can restrict untrusted programs&#39; access to resources. This type of MCU is more prevalent in the industry. MCUs/CPUs that support three modes - Machine mode, User mode, and Supervisor mode - can implement more complex privilege management systems. The presence of Supervisor mode allows for a higher level of control over resources and access, which is critical for security within TEE. 
&lt;/p&gt;

&lt;p&gt;
In mTower project, presented below, we used MCUs with only M-mode and U-mode, since they are more widespread and support the required basic privilege management system.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Access Control Using Physical Memory Protection (PMP)&lt;/h2&gt;

&lt;p&gt;
In RISC-V MCUs, the PMP mechanism plays a crucial role in enhancing security and managing access to memory. PMP allows the implementation of access control policies for physical memory addresses, enabling developers to set rules for memory regions&lt;sup&gt;[4][5]&lt;/sup&gt;.
&lt;/p&gt;

&lt;p&gt;
Key Aspects of PMP:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;1.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Memory Region Configuration&lt;/b&gt;: PMP enables the definition of a limited number of memory regions that can be configured with different access levels. Each region can have various attributes, such as: &lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;oAccess Type: whether access is allowed for reading, writing, or executing&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;oAccess Mode: privilege level for the region (e.g., user or supervisor)
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;2.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Process Isolation&lt;/b&gt;: PMP can create isolated memory regions for trusted and untrusted programs. This isolation helps to prevent unauthorized access to critical system resources, which is essential for implementing TEE.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;3.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Flexibility in Configuration&lt;/b&gt;: PMP supports dynamic configuration, allowing developers to adjust access rules according to specific application or system requirements. This means that access attributes can be modified during program execution.
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;4.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Testing and Verification&lt;/b&gt;: since PMP is a hardware mechanism, it&#39;s vital to test its configuration to ensure it operates as intended. Verifying access rules may involve creating scenarios that check whether the access restrictions are effective.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Majority of RISC-V MCUs lack support of Input/Output Physical Memory Protection (IOPMP). This can be a limitation for implementing comprehensive security systems, since, without IOPMP, access to peripheral devices cannot be controlled in the same way as memory access. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Other Security Enhancement Mechanisms&lt;/h2&gt;

&lt;p&gt;
In addition to access control, RISC-V-based MCUs can employ a number of other mechanisms to enhance security. Crypto accelerators, for example, are hardware components designed to perform encryption and decryption operations with high efficiency, providing significant speed advantages over software implementations. Many of these accelerators support essential cryptographic algorithms, making them critical for ensuring data confidentiality and integrity.
&lt;/p&gt;

&lt;p&gt;
Furthermore, True Random Number Generators (TRNGs) are utilized to enhance security by providing high-quality random numbers essential for cryptographic operations. TRNGs ensure reliability in key generation, which is vital for data protection.
&lt;/p&gt;

&lt;p&gt;
Thus, the RISC-V architecture offers a platform for implementing security measures through effective privilege management and memory protection mechanisms. The combination of multiple privilege levels and the PMP system enables the reliable separation of trusted and untrusted processes, which is critically important for creating a TEE. By utilizing these mechanisms in mTower project, we have made progress in establishing a secure system. Let&#39;s delve deeper into the technical details.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Implementation overview&lt;/h2&gt;

&lt;p&gt;
The &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;mTower&lt;/a&gt; project is an experimental industry standard-compliant implementation TEE based on ARM TrustZone for Cortex-M23/33/35p/55 MCUs. It has been expanded to support MCUs based on the RISC-V architecture. From the very beginning, mTower has been designed to have a tiny RAM footprint and to avoid the use of time-consuming operations. The source code of mTower is available at GitHub&lt;sup&gt;[6]&lt;/sup&gt;. 
&lt;/p&gt;

&lt;p&gt;
The mTower project allows for exploring and implementing protection mechanisms for creating TEE. After examining existing approaches to building secure systems, we were drawn to the method discussed at the RISC-V Summit&lt;sup&gt;[11]&lt;/sup&gt; and used the Si-Five open source code&lt;sup&gt;[7]&lt;/sup&gt;.
&lt;/p&gt;

&lt;p&gt;
The initial requirements for implementing TEE included support for M-mode and U-mode in the MCUs, as well as the presence of the PMP memory protection mechanism. For the base operating system for the MCU, we chose FreeRTOS&lt;sup&gt;[10]&lt;/sup&gt;, which provides a stable platform for managing trusted and untrusted processes and allows for the efficient utilization of available hardware resources to protect data.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Extension of FreeRTOS&lt;/h2&gt;

&lt;p&gt;
In FreeRTOS, the code can run in both modes: privileged (M) mode (kernel, interrupt handlers, system tasks) and unprivileged (U) mode (user tasks). This allows for the effective separation of security-critical operations from general tasks. The privileged mode provides access to all hardware resources, including memory management and peripheral devices, while unprivileged tasks have restricted access, preventing the possibility of system compromise through uncontrolled access to resources (Fig 2).
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/73e4d27b45/AZQaCKSaA6YAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Privilege and Non-privileged calls&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
To restrict access to memory between user tasks on microcontrollers based on the RISC-V architecture, the PMP mechanism is used. PMP allows for configuring access rules for different regions of physical memory depending on the execution mode, which helps to protect critical data and to isolate processes from each other.
&lt;/p&gt;

&lt;p&gt;
This mechanism serves as the foundation for creating more secure environments where each user task has its clearly defined memory access boundaries. In case of direct access to a memory area or resource that is restricted, an exception will be raised.
&lt;/p&gt;

&lt;p&gt;
Due to the limitation on the number of memory regions that can be controlled by the PMP, a mechanism for PMP controller dynamic configuration has been added to the FreeRTOS. It triggers at every task context switch (Fig. 3). This means that with each transition between tasks, the PMP configuration is updated to reflect the new memory access boundaries for the current task. This approach allows to use hardware resources effectively and provides reliable memory access distribution even in systems with limited number of PMP registers.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/73e4d27b45/AZQaCPnaA6kAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Processing PMP configuration&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The mTower project can run tasks in both privileged and unprivileged modes, which is fundamental feature to manage system resource access securely. The privileged mode grants full access to hardware resources, while unprivileged tasks are restricted, thereby reducing the risk of compromising the system through unauthorized resource access.
&lt;/p&gt;

&lt;p&gt;
Additionally, the mTower dynamically configures the PMP mechanism during each task switch. This allows the definition of memory regions that tasks can access, ensuring process isolation and preventing unauthorized access. Any attempt to violate access rules results in an exception, further strengthening memory protection.
&lt;/p&gt;

&lt;p&gt;
The following demo scenarios were prototyped with mTower:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;1.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Privileged Task&lt;/b&gt;: executes critical system functions and utilizes PMP to configure access to resources;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;2.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Unprivileged Isolated Tasks&lt;/b&gt;: the tasks interact using system calls but do not have direct access to critical system resources;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;3.&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Unprivileged Tasks with Shared Memory&lt;/b&gt;: tasks are executed in their dedicated isolated environments with different access rights to the shared memory. Any attempt to breach security boundaries triggers hardware exceptions.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
These scenarios illustrate how privileged mode and PMP are employed to build secure systems based on RISC-V architecture using FreeRTOS.
&lt;/p&gt;

&lt;p&gt;
Currently, the mTower project supports:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;SparkFun RED-V RedBoard&lt;/b&gt; - SiFive RISC-V FE310&lt;sup&gt;[8]&lt;/sup&gt;. However, we faced memory limitations of 16 kB. 
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Pine Ox64&lt;/b&gt; - Bouffalo Lab BL808 RISC-V&lt;sup&gt;[9]&lt;/sup&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
Note that there may be variations in some hardware implementations of the H/W cores, as different manufacturers have their own architectural visions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Future plans&lt;/h2&gt;

&lt;p&gt;
In the future, we plan to focus on the following areas:
&lt;/p&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Expanding Supported MCU Boards&lt;/b&gt;: we plan to support new RISC-V-based platforms, enabling researchers and developers to work with various hardware and adapt our solutions to their needs;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Expanding Supported MCU Boards&lt;/b&gt;: we plan to support new RISC-V-based platforms, enabling researchers and developers to work with various hardware and adapt our solutions to their needs;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;In-Depth Security Testing&lt;/b&gt;: we plan to conduct a more comprehensive testing to identify potential vulnerabilities in our system. This will include both static and dynamic security analysis, as well as evaluating the effectiveness of the implemented protection mechanisms;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Enhancing PMP Capabilities&lt;/b&gt;: future mTower versions will aim at expanding the use of the PMP mechanism to manage access to input/output (I/O) resources, allowing for more complex security policies;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Integrating Cryptographic Components&lt;/b&gt;: our plans include the integration of hardware cryptographic accelerators to speed up encryption and decryption operations, ensuring high data security while reducing processor load; 
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&quot;font-size: 1.47em;display: flex;flex-direction: row;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;div style=&quot;padding-right: 20px;&quot;&gt;•&lt;/div&gt;
&lt;div&gt;
&lt;b&gt;Trusted Bootloader&lt;/b&gt;: to ensure the secure loading of the operating system and applications, we plan to develop a trusted bootloader. This component will verify the authenticity of the code before execution, significantly enhancing the overall security of the system.
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Contribution is Welcome&lt;/h2&gt;

&lt;p&gt;
We welcome contributions to the &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;mTower&lt;/a&gt; project, which is open-source. If you are a systems developer or an information security engineer, feel free to join our team and contribute to our research. Your participation would be greatly appreciated! You can find more information and get involved at the following link: &lt;a href=&quot;https://github.com/Samsung/mTower&quot; target=&quot;_blank&quot;&gt;https://github.com/Samsung/mTower&lt;/a&gt; 
&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] https://www.mordorintelligence.com/industry-reports/risc-v-tech-market
&lt;/p&gt;
&lt;p&gt;
[2] https://runtimerec.com/wp-content/uploads/2024/06/The-Rise-of-RISC-V-Microcontrollers.pdf
&lt;/p&gt;
&lt;p&gt;
[3] https://octopart.com/pulse/p/10-top-trends-microcontrollers-2024
&lt;/p&gt;
&lt;p&gt;
[4] https://riscv.org/wp-content/uploads/2017/05/riscv-privileged-v1.10.pdf
&lt;/p&gt;
&lt;p&gt;
[5] https://riscv.org/wp-content/uploads/2018/05/riscv-privileged-BCN.v7-2.pdf
&lt;/p&gt;
&lt;p&gt;
[6] https://github.com/Samsung/mTower
&lt;/p&gt;
&lt;p&gt;
[7] https://github.com/sifive 
&lt;/p&gt;
&lt;p&gt;
[8] https://www.sparkfun.com/products/retired/15594
&lt;/p&gt;
&lt;p&gt;
[9] https://wiki.pine64.org/index.php?title=Ox64
&lt;/p&gt;
&lt;p&gt;
[10] https://www.freertos.org/
&lt;/p&gt;

&lt;/div&gt;

&lt;!-- 220526 --&gt;
&lt;div class=&quot;RA&quot;&gt;
&lt;!-- //220525 --&gt;

&lt;div class=&quot;link&quot;&gt;

&lt;a href=&quot;https://research.samsung.com/hashsearch?q=RISC-V&quot; class=&quot;hash&quot;&gt;#RISC-V&lt;/a&gt; &lt;!-- #! 제거 후 링크를 넣어주세요. --&gt;


&lt;a href=&quot;https://research.samsung.com/hashsearch?q=MCU&quot; class=&quot;hash&quot;&gt;#MCU&lt;/a&gt; &lt;!-- #! 제거 후 링크를 넣어주세요. --&gt;


&lt;/div&gt;

&lt;div class=&quot;RA_cont hash-result block&quot;&gt;
&lt;div class=&quot;slick-list&quot;&gt;
&lt;div class=&quot;slick-head&quot;&gt;
&lt;h3 class=&quot;slick-tit&quot;&gt;Related Stories&lt;/h3&gt;
&lt;div class=&quot;arr-box&quot;&gt;
&lt;div class=&quot;arr-l&quot;&gt;&lt;i class=&quot;ico-arr&quot;&gt;&lt;/i&gt;&lt;/div&gt;
&lt;div class=&quot;arr-r&quot;&gt;&lt;i class=&quot;ico-arr&quot;&gt;&lt;/i&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/RISC-V-and-Vectorization&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxioyqAlAAUGoJ.jpg&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;RISC-V and Vectorization&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On November 28, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Bringing-RVV-to-Life-Overcoming-Hardware-Gaps-in-RISC-V-Development&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/5471502d58/AZKOd5gKAfAAUGoJ.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Bringing RVV to Life: Overcoming Hardware Gaps in RISC-V Development&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On October 18, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Open-source-open-hardware-ground-truth-for-Visual-Odometry-and-SLAM-applications&quot;&gt;
&lt;img src=&quot;https://gcg-stage-cdn-v2.stage.codeground.org/resources/9149edb6c7/AZJ07-QqAAYAUF-f.jpg&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Open Source, Open Hardware Ground Truth for Visual Odometry and SLAM Applications&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On October 11, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Matter-SDK-Contribution-to-Tizen-platform-support&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/7e17d06ab3/AYmuU7XaAOoAUGrH.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Matter SDK: Contribution to Tizen Platform Support&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On August 8, 2023&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/mTower-for-Quick-Start-with-ARM-TrustZone-on-MCU&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG8ATE6AKsAUGoJ.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;mTower – for Quick Start with ARM TrustZone on MCU&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On December 28, 2022&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;slick-item&quot;&gt;
&lt;a href=&quot;https://research.samsung.com/blog/Road-to-Get-Certified-as-gold-Badge-From-OpenSSF-Best-Practices&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/601118af5e/AYI9c0gkAnUpy3UD.png&quot; class=&quot;img&quot; alt=&quot;&quot; title=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;div class=&quot;RA_cont&quot;&gt;
&lt;strong class=&quot;title&quot;&gt;Road to Get Certified as &quot;gold&quot; Badge From OpenSSF Best Practices&lt;/strong&gt;
&lt;span class=&quot;date&quot;&gt;On July 28, 2022&lt;/span&gt;
&lt;/div&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;!-- //220525 --&gt;

&lt;div class=&quot;RA_service&quot;&gt;

&lt;div class=&quot;prev-blog&quot;&gt;
&lt;div class=&quot;service-wrap&quot;&gt;
&lt;div class=&quot;prev&quot;&gt;PREV&lt;i&gt;&lt;/i&gt;&lt;/div&gt;
&lt;a href=&quot;https://research.samsung.com/blog/FaceMe-Robust-Blind-Face-Restoration-With-Personal-Identification&quot;&gt;FaceMe: Robust Blind Face Restoration With Personal Identification&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;



&lt;div class=&quot;next-blog&quot;&gt;
&lt;div class=&quot;service-wrap&quot;&gt;
&lt;div class=&quot;next&quot;&gt;NEXT&lt;i&gt;&lt;/i&gt;&lt;/div&gt;
&lt;a href=&quot;https://research.samsung.com/blog/SOI-Scaling-Down-Computational-Complexity-by-Estimating-Partial-States-of-the-Model&quot;&gt;SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;/div&gt;

&lt;div class=&quot;btnBox&quot;&gt;
&lt;button class=&quot;blog-btn&quot; onclick=&quot;location.href=&#39;/blog&#39; &quot;&gt;LIST&lt;/button&gt;
&lt;/div&gt;
&lt;!-- //220503 --&gt;
&lt;/div&gt;
</description><link>https://research.samsung.com/blog/Towards-Building-a-Trusted-Execution-Environment-on-RISC-V-Microcontrollers</link><guid isPermaLink="false">https://research.samsung.com/blog/Towards-Building-a-Trusted-Execution-Environment-on-RISC-V-Microcontrollers</guid><pubDate>Invalid Date</pubDate><author>Taras Drozdovskyi</author><category>Open Source</category><category>RISC-V</category><category>MCU</category></item><item><title>SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;
The rapid advancements in artificial intelligence (AI) have led to the development of increasingly sophisticated and powerful artificial neural networks (ANNs). While these models have achieved groundbreaking performance across various domains [1 3], their escalating size and computational demands render them impractical for resource-constrained environments. This issue is particularly concerning for real-time, energy-sensitive applications in consumer electronics, such as smartwatches, augmented reality (AR) glasses, and wireless earbuds.
&lt;/p&gt;

&lt;p&gt;
Despite the growing processing power of microcontroller units (MCUs), their ability to run state-of-the-art ANNs remains severely limited. This challenge is further compounded by the slowdown of Moore&#39;s Law [4], which traditionally predicted consistent improvements in hardware performance. The deceleration of this trend [5] underscores a widening gap between the computational requirements of cutting-edge neural networks and the capabilities of compact, low-power devices [6].
&lt;/p&gt;

&lt;p&gt;
Moreover, the pursuit of maximal model accuracy in the AI research community often comes at the expense of efficiency, resulting in solutions that are unsuitable for real-world, latency-sensitive systems. Current optimization techniques, such as pruning [7] and quantization [8], fall short of bridging this gap without introducing significant trade-offs in model performance.
&lt;/p&gt;

&lt;p&gt;
Our paper is motivated by the need to address these challenges directly. We aim to develop a method that balances computational efficiency with model accuracy, enabling the deployment of high-performance neural networks on energy-constrained devices. By leveraging the inherent continuity and predictability of time-series data, we introduce Scattered Online Inference (SOI), a novel approach that reduces computational complexity through partial state predictions and efficient compression. SOI aligns with the growing demand for environmentally sustainable and economically viable AI solutions, pushing the boundaries of what is possible in compact, real-time systems.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Scattered Online Inference&lt;/h2&gt;

&lt;p&gt;
The key principle of SOI is leveraging compression and extrapolation along the time axis. Instead of recalculating every layer of the network for each incoming data point, SOI compresses the data using strided convolutions and reconstructs missing states through extrapolation techniques, such as frame duplication. These operations are applied selectively to specific layers of the network, allowing portions of the computational graph to remain static across consecutive inferences. By caching and reusing partial network states, SOI significantly reduces the frequency of full model updates, thereby optimizing computational efficiency. To enhance understanding of the SOI algorithm in CNNs, Figure 1 defines three types of convolutional layers utilized in our method. For comparison, Figure 1 also includes standard convolution and strided convolution.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOJ_nXKA38AUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; SOI for convolutional operations. For visualization purposes we show data as frames in time domain. A) Standard convolution. B) Strided convolution. C) Strided-Cloned Convolution. D) Shifted convolution. E) Shifted Strided-Cloned Convolution.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
SOI operates in two primary modes: Partially Predictive (PP) and Fully Predictive (FP). In partially predictive mode, the model predicts the next state based on a combination of newly computed and cached partial states. This approach reduces the average computational cost without increasing the system&#39;s peak demand. In contrast, FP mode predicts multiple future states ahead of time, allowing entire sections of the model to be precomputed and eliminating the need for on-the-fly calculations. While FP mode is more complex to implement, it offers greater reductions in both latency and computational load, particularly for tasks with highly predictable patterns. The inference patterns for both modes are illustrated in Figure 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKAUV6A4MAUGoJ.png&quot; width=&quot;80%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Inference patterns of each type of SOI based on U-Net architecture. A) Unmodified causal U-Net. B) Partially predictive (PP) SOI. C) Even inference of PP. D) Odd inference of PP. E) Fully predictive (FP) SOI. F) Even inference of FP. G) Odd inference of FP.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Another defining feature of SOI is the integration of skip connections, which maintain the flow of information from compressed input data to deeper layers of the network. These connections bridge the outputs of strided convolution layers and their corresponding reconstruction layers, ensuring that the model preserves critical context and causality. Skip connections also mitigate the risk of performance degradation caused by partial state predictions by allowing the model to incorporate new data into the computation graph without fully recalculating intermediate layers.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;
To evaluate the effectiveness of SOI, we conducted experiments on three distinct tasks: speech separation, acoustic scene classification (ASC), and video action recognition. These tasks were selected to demonstrate SOI&#39;s ability to process time-series data across diverse domains. The performance of SOI was compared to baseline models, traditional optimization techniques such as pruning and resampling, and the Short-Term Memory Convolution (STMC) method [9], which served as a foundational technique for SOI. The results highlight the versatility of SOI in reducing computational complexity while maintaining acceptable levels of accuracy and efficiency.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Speech Separation&lt;/h2&gt;

&lt;p&gt;
For the speech separation task, the U-Net architecture was employed to separate clean speech from noisy backgrounds. The experiments compared the partially predictive and fully predictive SOI variants against the baseline and STMC models. Metrics included computational complexity (measured in MMAC/s) and scale-invariant signal-to-noise ratio improvement (SI-SNRi). The results are shown in Figure 3.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKA-xqA4cAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Results of speech separation experiment with A) the partially predictive SOI, and B) fully predictive SOI.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The partially predictive SOI model demonstrated an ability to reduce computational complexity by up to 64%, with only a minor impact on performance, measured as a decrease of 0.017 dB in SI-SNRi for every 1% complexity reduction. Similarly, the FP SOI model achieved a 50% reduction in complexity by precomputing 83.7% of the network&#39;s operations, making it particularly well-suited for latency-sensitive applications.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Acoustic Scene Classification (ASC)&lt;/h2&gt;

&lt;p&gt;
The ASC task utilized the GhostNet architecture, with the objective of classifying urban acoustic scenes. Models incorporating SOI were compared to baseline and STMC variants across seven model sizes. Top-1 accuracy and computational complexity were evaluated, with the results summarized in Table 1.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKBWu6A4oAUGoJ.png&quot; width=&quot;45%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Results of ASC experiment.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
SOI achieved an average complexity reduction of 16% compared to the STMC method. In some configurations, SOI also improved model accuracy, likely due to partial state predictions enhancing the network’s generalization capabilities. Although the introduction of skip connections in SOI slightly increased the number of model parameters, this adjustment significantly reduced the overall computational load.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Video Action Recognition&lt;/h2&gt;

&lt;p&gt;
To explore SOI&#39;s applicability to non-audio time-series data, the ResNet-10 architecture and MoViNets were tested on the HMDB-51 dataset for the action recognition task. SOI was applied to 3D convolutional layers and evaluated on regular, small, and tiny ResNet-10 variants, as well as the A0 and A1 MoViNet variants. The results are presented in Table 2.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKBw5KA40AUGoJ.png&quot; width=&quot;60%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 2.&lt;/b&gt; Results of video action recognition experiment.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
SOI achieved complexity reductions of 10% to 17%, with little to no loss in accuracy. In some cases, SOI even improved model accuracy by expanding the receptive field through the use of strided convolutions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Pruning&lt;/h2&gt;

&lt;p&gt;
The application of SOI combined with pruning in the STMC model surpasses the effect of pruning alone. The addition of SOI enabled a further reduction in computational complexity by approximately 300 MMAC/s for the same model performance, representing about 16% of the original model&#39;s complexity. Interestingly, the “SOI 2|6” model outperformed the “SOI 1” model at around 6 dB SI-SNRi. The experimental results are shown in Figure 4.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKCDhqA5AAUGoJ.png&quot; width=&quot;40%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 4.&lt;/b&gt; Pruning of STMC, SOI and 2xSOI models. Unpruned models are indicated by markers.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Inference Time and Memory Footprint&lt;/h2&gt;

&lt;p&gt;
One of the critical objectives of SOI is to enhance the efficiency of neural network inference by reducing not only computational complexity but also inference time and memory usage. These aspects are particularly crucial in real-time applications, where latency and hardware limitations significantly affect system performance. We measured these metrics for SOI applied to the speech separation task, using U-Net as the test architecture. The results are summarized in Table 3.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKCWw6A5MAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 3.&lt;/b&gt; Results from experiments with partially predictive SOI for speech separation. &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The experiments revealed that SOI models consistently achieved lower average inference times compared to baseline and STMC models. From the collected results, we observed a drop in inference time from 9.93 ms in the STMC model to as low as 5.28 ms, representing a reduction of nearly 47%. Additionally, the peak memory footprint of the models decreased significantly as computational complexity was reduced. The memory footprint dropped from 27.2 MB in the STMC model to 14.6 MB, a reduction of over 46%.
&lt;/p&gt;

&lt;p&gt;
The relationship between these efficiency improvements and the model’s performance metrics was also analyzed. As shown in Figure 5, the inference time scaled linearly with the complexity reduction factor, demonstrating that SOI efficiently balanced the computational workload across the network.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/0ab7f12ce6/AZOKCoeqA5cAUGoJ.png&quot; width=&quot;60%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 5.&lt;/b&gt; Average inference time and peak memory footprint.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;
In this work, we presented a method for reducing the computational cost of convolutional neural networks (CNNs) by reusing partial network states from previous inferences, allowing these states to generalize over longer time periods. We discussed the effects of partial state prediction imposed by our method on the neural model and demonstrated its gradual application to balance model quality metrics and computational cost.
&lt;/p&gt;

&lt;p&gt;
Our experiments highlight the significant potential for reducing the computational cost of CNNs, particularly for tasks where outputs remain relatively constant, such as event detection or classification. We achieved a 50% reduction in computational cost without any loss in metrics for the ASC task, and a 64.4% reduction in computational cost with a relatively small 9.8% decrease in metrics for the speech separation task. Additionally, we demonstrated SOI’s ability to control the trade-off between model quality and computational cost, enabling resource- and requirement-aware tuning.
&lt;/p&gt;

&lt;p&gt;
The presented method offers an alternative to the STMC solution for strided convolution. While SOI reduces network computational complexity at the expense of some performance, STMC maintains performance metrics but at the cost of exponentially increased memory consumption. SOI is similar to methods like network pruning but does not rely on special sparse kernels for inference optimization. Importantly, these methods are not mutually exclusive. The STMC strided convolution handler, SOI, and pruning can coexist within a neural network to achieve the desired balance of model performance and resource efficiency.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2410.03813&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://arxiv.org/abs/2410.03813&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] Thai Son Nguyen, Sebastian Stueker, and Alexander H. Waibel. Super-human performance in online low-latency recognition of conversational speech. In Interspeech, 2020.
&lt;/p&gt;
&lt;p&gt;
[2] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067–1074, 2022.
&lt;/p&gt;
&lt;p&gt;
[3] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, dec 2020.
&lt;/p&gt;
&lt;p&gt;
[4] Gordon E Moore. Cramming more components onto integrated circuits. Proceedings of the IEEE, 86 (1):82–85, 1998.
&lt;/p&gt;
&lt;p&gt;
[5] Charles E. Leiserson, Neil C. Thompson, Joel S. Emer, Bradley C. Kuszmaul, Butler W. Lampson, Daniel Sanchez, and Tao B. Schardl. There’s plenty of room at the top: What will drive computer performance after moore’s law? Science, 368(6495):eaam9744, 2020.
&lt;/p&gt;
&lt;p&gt;
[6] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu Hu, and Yiyu Shi. Scaling for edge inference of deep neural networks. Nature Electronics 2018 1:4, 1:216–222, 4 2018. ISSN 2520-1131. doi: 10.1038/s41928-018-0059-3.
&lt;/p&gt;
&lt;p&gt;
[7] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989.
&lt;/p&gt;
&lt;p&gt;
[8] R.M. Gray and D.L. Neuhoff. Quantization. IEEE Transactions on Information Theory, 44(6): 2325–2383, 1998.
&lt;/p&gt;
&lt;p&gt;
[9] Grzegorz Stefański, Krzysztof Arendt, Paweł Daniluk, Bartłomiej Jasik, and Artur Szumaczuk. Short-term memory convolutions. In The Eleventh International Conference on Learning Representations, 2023.
&lt;/p&gt;
&lt;/div&gt;

</description><link>https://research.samsung.com/blog/SOI-Scaling-Down-Computational-Complexity-by-Estimating-Partial-States-of-the-Model</link><guid isPermaLink="false">https://research.samsung.com/blog/SOI-Scaling-Down-Computational-Complexity-by-Estimating-Partial-States-of-the-Model</guid><pubDate>Invalid Date</pubDate><author>Grzegorz Stefański|Paweł Daniluk|Artur Szumaczuk|Jakub Tkaczuk</author><category>AI</category><category>NeurIPS</category><category>SignalProcessing</category></item><item><title>RISC-V and Vectorization</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;
In 2023 Samsung joined and became an official member of “RISE (RISC-V Software Ecosystem)” project. Since then our company is participating in the development of a variety of projects and porting them to RISC-V architecture. In this article, we will throw some light on one of them: Chromium. When porting software to a new architecture, the first step is always to make it start and run on hardware, the next step is stabilization, but as Amber Huffman, chairperson of the RISE project emphasized: “In order for RISC-V to be commercialized, it is important to secure software that has performance, security, reliability, and compatibility”. For the final users performance is very important when browsing web pages, viewing streamed content or running web applications. All of those mentioned activities have a common ground: showing multimedia content and vectorization is one of the possibilities for performance improvement.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;RISC-V and Instruction Set Architecture (ISA)&lt;/h2&gt;

&lt;p&gt;
RISC-V is an open standard instruction set architecture based on established reduced instruction set computer (RISC) principles. Moreover, RISC-V is offered under royalty-free open-source licenses, and documents defining its ISA are offered under a Creative Commons license or a BSD License. That is one of major differences between this and other commercial vendors of processors, such as Arm Ltd. and MIPS Technologies.
&lt;/p&gt;

&lt;p&gt;
And what is an instruction set architecture? It is a part of the abstract model of a computer that defines how the CPU is controlled by the software and acts as an interface between the hardware and the software, specifying both what the processor is capable of doing as well as how it gets done. The ISA defines the supported data types, the registers, how the hardware manages main memory, key features (such as virtual memory), which instructions a microprocessor can execute, and the input/output model of multiple ISA implementations. The ISA can be extended by adding instructions or other capabilities, or by adding support for larger addresses and data values. [&lt;a href=&quot;https://www.arm.com/glossary/isa&quot; target=&quot;_blank&quot;&gt;https://www.arm.com/glossary/isa&lt;/a&gt;]
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Short story of vector extensions&lt;/h2&gt;

&lt;p&gt;
The first successful implementation of vector extension was MultiMedia eXtension (MMX) introduced by Intel in 1997. MMX defines eight processor registers, named MM0 through MM7, and operations that are performed on them. Each register is 64 bits wide and can be used to hold either 64-bit integers, or multiple smaller integers in a &quot;packed&quot; format: one instruction can then be applied to two 32-bit integers, four 16-bit integers, or eight 8-bit integers at once. MMX could take small and only integer numbers, like those representing shades of color in a picture, and process many of them at the same time.
&lt;/p&gt;

&lt;p&gt;
The successor of MMX was Streaming SIMD Extensions (SSE) introduced by Intel in 1999. SSE contains 70 new instructions (65 unique mnemonics using 70 encodings), most of which work on single precision floating-point data. SIMD instructions can greatly increase the performance when exactly the same operations are to be performed on multiple data objects. SSE could work with floating-point numbers, which are numbers with decimals, like 3.14 or 2.718. This made it perfect for 3D graphics processing and scientific calculations. SSE was subsequently expanded by Intel to SSE2, SSE3, SSSE3 and SSE4. Because it supports floating-point math, it had a wider range of applications than MMX and became more popular.
&lt;/p&gt;

&lt;p&gt;
Next, in 2011 Advanced Vector Extensions (AVX) was introduced by Intel. AVX uses sixteen YMM registers to perform a single instruction on multiple pieces of data. Each YMM register can hold and do simultaneous operations (math) on: eight 32-bit single-precision floating point numbers or four 64-bit double-precision floating point numbers. It was designed to help with extremely demanding tasks like high-definition video processing, advanced gaming graphics, and heavy scientific simulations. AVX also had its own family, with AVX2 and AVX-512, each more powerful than the last.
&lt;/p&gt;

&lt;p&gt;
In the ARM family processors there also is implementation of vector extensions: The Advanced SIMD extension (also known as Neon or &quot;MPE&quot; Media Processing Engine) is a combined 64- and 128-bit SIMD instruction set that provides standardised acceleration for media and signal processing applications. Neon is included in all Cortex-A8 devices, but is optional in Cortex-A9 devices. Neon can accelerate signal processing algorithms and functions to speed up applications such as audio and video processing, voice and facial recognition, computer vision, and deep learning. Neon instructions allow up to: 16x8-bit, 8x16-bit, 4x32-bit, 2x64-bit integer operations and 8x16-bit, 4x32-bit, 2x64-bit floating-point operations. [&lt;a href=&quot;https://www.arm.com/technologies/neon&quot; target=&quot;_blank&quot;&gt;https://www.arm.com/technologies/neon&lt;/a&gt;]
&lt;/p&gt;

&lt;p&gt;
Next, ARM introduced Scalable Vector Extension (SVE) which is a vector extension for the A64 instruction set of the Armv8-A architecture. Armv9-A builds on SVE with the SVE2 extension. Unlike other SIMD architectures, SVE and SVE2 do not define the size of the vector registers, but constrain it to a range of possible values, from a minimum of 128 bits up to a maximum of 2048 in 128-bit wide units. Therefore, any CPU vendor can implement the extension by choosing the vector register size that better suits the workloads the CPU is targeting. The design of SVE and SVE2 guarantees that the same program can run on different implementations of the instruction set architecture without the need to recompile the code. [https://developer.arm.com/Architectures/Scalable Vector Extensions]
&lt;/p&gt;

&lt;p&gt;
It is worth to mention that RISC-V vector extension (RVV) has adopted the same style as ARM SVE and SVE2: scalable vector registers of size unknown at compile-time. This approach meant to address ISA fragmentation by different vector sizes in x86 extensions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Vector extensions in RISC-V &lt;/h2&gt;

&lt;p&gt;
How can the &quot;V&quot; Vector extension be used? The RISC-V &quot;V&quot; Vector Extension allows CPUs to perform many operations simultaneously, making them faster and more efficient. Let&#39;s dive into this with the key components, including the registers and a selection of important instructions that are part of this extension.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Registers &lt;/h2&gt;

&lt;p&gt;
The RISC-V Vector Extension (RVV) version 1.0 introduces a rich set of vector registers that can take multiple data structures as arguments. It also provides instructions designed to perform parallel processing efficiently.
&lt;/p&gt;

&lt;p&gt;
These registers are critical for performing vectorized operations.
&lt;/p&gt;

&lt;p&gt;
1.Vector Registers (v0 - v31): &lt;br&gt;
•There are 32 vector registers, each identified as v0 to v31. &lt;br&gt;
•Each vector register can hold multiple elements, and the size of these elements can vary (e.g., 8-bit, 16-bit, 32-bit, 64-bit). &lt;br&gt;
•The total number of elements that a vector register can hold is determined by the configured vector length.
&lt;/p&gt;

&lt;p&gt;
2.Vector Configuration Registers: &lt;br&gt;
•VLEN: The length of the vector registers in bits. &lt;br&gt;
•SEW (Standard Element Width): Specifies the width of individual elements in the vector register (e.g., 8-bit, 16-bit, 32-bit, 64-bit). &lt;br&gt;
•LMUL (Length Multiplier): Determines the grouping of vector registers to allow for different vector lengths.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Configuration Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions configure the vector processing unit (VPU) to set up the vector length and element width.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxhqa6AjUAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Load/Store Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions are used to load data from memory into vector registers and store data from vector registers into memory.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxhyIaAjgAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
The above operations can have multiple variants. Besides the simple load and store (le32/se32), it’s possible to use strides (lse32/sse32) or segments (lsseg4_32/sseg4_32) which makes it possible to load and store data not only in SoA (Structure of Arrays) configuration but also as AoS (Array of Structures).
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Arithmetic Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions perform arithmetic operations on vector registers.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxh1dqAjsAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Vector Masking Instructions &lt;/h2&gt;

&lt;p&gt;
These instructions enable conditional operations on vector elements.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxh53KAj4AUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Example: Adding multiple 64-bit Numbers &lt;/h2&gt;

&lt;p&gt;
Adding arrays with 64-bit numbers is a complex task because standard registers in CPUs are typically 32-bit or 64-bit wide and this task can&#39;t be done in one step. Using the RISC-V &quot;V&quot; Vector Extension, we can break down this task and perform it more efficiently.
&lt;/p&gt;

&lt;p&gt;
Here&#39;s a step-by-step guide to adding two arrays with 64-bit numbers using the RISC-V &quot;V&quot; Vector Extension:
&lt;/p&gt;

&lt;p&gt;
1.&lt;b&gt;Set up the Vector Registers&lt;/b&gt;: First, we need to configure the vector registers to handle arrays. &lt;br&gt;
2.&lt;b&gt;Load the Data&lt;/b&gt;: Load the arrays into the vector registers.&lt;br&gt;
3.&lt;b&gt;Perform the Addition&lt;/b&gt;: Use vector instructions to add the numbers.&lt;br&gt;
4.&lt;b&gt;Store the Result&lt;/b&gt;: Store the result back to memory.
&lt;/p&gt;

&lt;p&gt;
Here is the assembly code for this:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;.section .data&lt;br&gt;  
 num1: .quad 0x123456789ABCDEF0, 0x0FEDCBA987654321, 0x0011223344556677, 0x8899AABBCCDDEEFF&lt;br&gt;
 num2: .quad 0x1122334455667788, 0x99AABBCCDDEEFF00, 0x1234567890ABCDEF, 0xFEDCBA9876543210&lt;br&gt;
 result: .space 32  # Reserve 32 bytes (256 bits) for the result&lt;br&gt;&lt;br&gt;  
  
.section .text&lt;br&gt;  
.globl _start&lt;br&gt;  
_start:&lt;br&gt;  
 # Set up vector registers&lt;br&gt;  
 li t0, 4             # Set AVL (Application Vector Length) to 4 (256 bits / 64-bit elements)&lt;br&gt;  
 vsetvli t0, t0, e64  # Set vector length and element width to 64-bit&lt;br&gt;&lt;br&gt;  
  
 la a0, num1&lt;br&gt;  
 # Load 256-bit numbers into vector registers&lt;br&gt;  
 vle64.v v0, (a0)       # Load the first array into vector register v0&lt;br&gt;  
 la a0, num2&lt;br&gt;  
 vle64.v v1, (a0)   # Load the second array into vector register v1&lt;br&gt;&lt;br&gt;  
  
 # Perform the addition&lt;br&gt;  
 vadd.vv v2, v0, v1   # Add vector register v0 and v1, store the result in vector register v2&lt;br&gt;  
  
 # Store the result back to memory&lt;br&gt;  
 la a0, result&lt;br&gt;  
 vse64.v v2, (a0)  # Store the result from vector register v2 to the memory location result&lt;br&gt;&lt;br&gt;  
  
 # Exit (for demonstration purposes, assuming an environment that supports this)&lt;br&gt;  
 li a7, 93            # ECALL number for exit&lt;br&gt;  
 ecall&lt;/mark&gt;
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Explanation &lt;/h2&gt;

&lt;p&gt;
1.&lt;b&gt;Data Section&lt;/b&gt;: We define two 256-bit numbers (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num2&lt;/mark&gt;) as arrays of four 64-bit values. We also reserve space for the result.&lt;br&gt;
2.&lt;b&gt;Set up Vector Registers&lt;/b&gt;: We set the vector length to handle 64-bit elements and configure the vector registers accordingly.&lt;br&gt;
3.&lt;b&gt;Load Data&lt;/b&gt;: The &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vle64.v&lt;/mark&gt; instruction loads 64-bit elements from memory into vector registers &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v0&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v1&lt;/mark&gt;.&lt;br&gt;
4.&lt;b&gt;Perform Addition&lt;/b&gt;: The &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vadd.vv&lt;/mark&gt; instruction adds the elements in vector registers &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v0&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v1&lt;/mark&gt; and stores the result in &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v2&lt;/mark&gt;.&lt;br&gt;
5.&lt;b&gt;Store Result&lt;/b&gt;: The &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vse64.v&lt;/mark&gt; instruction stores the 64-bit elements from vector register &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v2&lt;/mark&gt; back into memory at the location &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;result&lt;/mark&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Comparing assembly code with and without Vector Extensions &lt;/h2&gt;

&lt;p&gt;
To truly appreciate the power of the RISC-V &quot;V&quot; Vector Extension, let&#39;s compare the assembly code for adding two arrays with (example above) and without (example below) the usage of vector extensions.
&lt;/p&gt;

&lt;p&gt;
When we don&#39;t use vector extensions, we need to handle each 64-bit part of the array individually. This requires multiple instructions and more steps to achieve the same result. Here is how we can write the assembly code without the usage of vector extensions:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;.section .data&lt;br&gt;
num1: .quad 0x123456789ABCDEF0, 0x0FEDCBA987654321, 0x0011223344556677, 0x8899AABBCCDDEEFF&lt;br&gt;
num2: .quad 0x1122334455667788, 0x99AABBCCDDEEFF00, 0x1234567890ABCDEF, 0xFEDCBA9876543210&lt;br&gt;
result: .space 32  # Reserve 32 bytes (256 bits) for the result&lt;br&gt;&lt;br&gt;

.section .text&lt;br&gt;
.globl _start&lt;br&gt;
_start:&lt;br&gt;&lt;br&gt;

# Load the first 64-bit part of the first number&lt;br&gt;
 lui     a1, %hi(num1)&lt;br&gt;  
 addi    a1, a1, %lo(num1)&lt;br&gt;  
# Load the first 64-bit part of the second number&lt;br&gt;  
 lui     a2, %hi(num2)&lt;br&gt;  
 addi    a2, a1, %lo(num2)&lt;br&gt;&lt;br&gt;  
  
 lui     a3, %hi(result)&lt;br&gt;  
 addi    a3, a3, %lo(result)&lt;br&gt;&lt;br&gt;  

# Add the third 64-bit parts&lt;br&gt;
 ld t1, 0(a1)&lt;br&gt;  
 ld t2, 0(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;&lt;br&gt; 
 
 # Store the result&lt;br&gt;
 sd t3, 0(a3)&lt;br&gt;&lt;br&gt;  

# do the same stuff for every 64 bit word&lt;br&gt;
 ld t1, 8(a1)&lt;br&gt;  
 ld t2, 8(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;  
 sd t3, 8(a3)&lt;br&gt;&lt;br&gt;  
  
 ld t1, 16(a1)&lt;br&gt;  
 ld t2, 16(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;  
 sd t3, 16(a3)&lt;br&gt;&lt;br&gt;  
  
 ld t1, 24(a1)&lt;br&gt;  
 ld t2, 24(a2)&lt;br&gt;  
 add t3, t2, t1&lt;br&gt;  
 sd t3, 24(a3)&lt;br&gt;&lt;br&gt;


 # Exit (for demonstration purposes, assuming an environment that supports this)&lt;br&gt;
 li a0, 0&lt;br&gt;
   li a7, 93            # ECALL number for exit&lt;br&gt;
 ecall&lt;/mark&gt;
&lt;/p&gt;

&lt;p&gt;
Let&#39;s compare now two presented approaches:
&lt;/p&gt;

&lt;p&gt;
1.Number of Instructions:&lt;br&gt;
•&lt;b&gt;Without Vector Extensions&lt;/b&gt;: The code is much longer because each 64-bit part of the arrays is handled separately. For each part, we need to load the numbers, add them, and store the result, repeating this four times.&lt;br&gt;
•&lt;b&gt;With Vector Extensions&lt;/b&gt;: The code is more compact. We only need to set up the vector registers once and then use a single vector addition instruction to perform the addition of all parts simultaneously.&lt;br&gt;
2.Simplicity:&lt;br&gt;
•&lt;b&gt;Without Vector Extensions&lt;/b&gt;: The code is more complex and repetitive, making it harder to read and maintain.&lt;br&gt;
•&lt;b&gt;With Vector Extensions&lt;/b&gt;: The code is simpler and more elegant. It abstracts away the repetitive tasks, making it easier to understand and maintain.&lt;br&gt;
3.Performance:&lt;br&gt;
•&lt;b&gt;Without Vector Extensions&lt;/b&gt;: The CPU has to execute more instructions, which can slow down the process, especially for large data sets.&lt;br&gt;
•&lt;b&gt;With Vector Extensions&lt;/b&gt;: The CPU can process multiple data elements in parallel, significantly speeding up the computation.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Building the Assembly Code with RISC-V Tools &lt;/h2&gt;

&lt;p&gt;
To turn our assembly code into a program that the CPU can run, we need to use some special tools. These tools are &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld&lt;/mark&gt;. 
&lt;/p&gt;

&lt;p&gt;
Those commands correspond to the toolchain binaries used for cross-compiling for a different architecture (RISC-V in this case) on x64/x86 machines. If you are compiling on the RISC-V machine, you can just use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;as&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;ld&lt;/mark&gt; instead.
&lt;/p&gt;

&lt;p&gt;
To install toolchain, follow the manual from &lt;a href=&quot;https://github.com/riscv-collab/riscv-gnu-toolchain&quot; target=&quot;_blank&quot;&gt;https://github.com/riscv-collab/riscv-gnu-toolchain&lt;/a&gt;. But don&#39;t forget to add &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;--with-arch=rv64gcv&lt;/mark&gt; everywhere you can. Take a look also at Appendix A.
&lt;/p&gt;

&lt;p&gt;
Let&#39;s see how we can use them to build our assembly code for adding 256-bit numbers.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Step-by-Step Guide &lt;/h2&gt;

&lt;p&gt;
1.Write the Assembly Code:&lt;br&gt;
•First, write the assembly code in a file. Let&#39;s name it &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.s&lt;/mark&gt;.&lt;br&gt;
2.Assemble the Code:&lt;br&gt;
•Use the &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as&lt;/mark&gt; utility to assemble the code. This will convert the assembly code into an object file.&lt;br&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as -march=rv64gcv -o add_256bit.o add_256bit.s&lt;/mark&gt;&lt;br&gt;
•This command will create an object file named &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.o&lt;/mark&gt;.&lt;br&gt;
3.Link the Object File:&lt;br&gt;
•Use the &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld&lt;/mark&gt; utility to link the object file and create an executable. &lt;br&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld -o add_256bit add_256bit.o&lt;/mark&gt;&lt;br&gt;
•This command will create an executable file named &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit&lt;/mark&gt;.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Detailed Explanation &lt;/h2&gt;

&lt;p&gt;
1.Assembler (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-as&lt;/mark&gt;):&lt;br&gt;
•The assembler reads the assembly code from &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.s&lt;/mark&gt; and converts it into machine code, generating an object file (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.o&lt;/mark&gt;). The object file contains the binary representation of the instructions, but it is not yet ready to be executed on its own.&lt;br&gt;
2.Linker (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-ld&lt;/mark&gt;):&lt;br&gt;
•The linker takes the object file (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit.o&lt;/mark&gt;) and links it with any necessary libraries or other object files to produce an executable (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit&lt;/mark&gt;). The linker resolves references to external symbols and assigns final memory addresses to the program&#39;s instructions and data.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Running the Executable &lt;/h2&gt;

&lt;p&gt;
To run the executable on an RV64 processor or an emulator, you would typically use a simulator like Spike, QEMU, or run it directly on RISC-V hardware. Here is the example with Spike, a functional RISC-V ISA simulator:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;spike pk add_256bit&lt;/mark&gt; 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;RISC-V Vector Functions in GCC &lt;/h2&gt;

&lt;p&gt;
The RISC-V Vector Extension (RVV) provides a rich set of vector functions in GCC that allow developers to leverage the power of vector processing directly in C/C++ code. These functions are typically available through intrinsic functions, which are special functions provided by the compiler to generate specific machine instructions.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;RISC-V Vector Intrinsics &lt;/h2&gt;

&lt;p&gt;
Vector intrinsics in GCC for RISC-V are designed to map directly to RVV instructions, providing a way to write vectorized code without resorting to assembly language. These intrinsics follow a naming convention that helps in understanding their functionality.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Naming Convention &lt;/h2&gt;

&lt;p&gt;
The naming convention for RISC-V vector intrinsics is generally as follows:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_&lt;operation&gt;_&lt;arg-types&gt;_&lt;type&gt;&lt;size&gt;_&lt;mask&gt;&lt;/mask&gt;&lt;/size&gt;&lt;/type&gt;&lt;/arg-types&gt;&lt;/operation&gt;&lt;/mark&gt;&lt;br&gt;
1.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;operation&gt;&lt;/operation&gt;&lt;/mark&gt;: Describes the vector operation (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vmseq&lt;/mark&gt; for vector mask equal).&lt;br&gt;
2.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;arg-types&gt;&lt;/arg-types&gt;&lt;/mark&gt;: Indicates the types of arguments (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;v&lt;/mark&gt; for vector, &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;f&lt;/mark&gt; for float, &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;x&lt;/mark&gt; for integer).&lt;br&gt;
3.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;type&gt;&lt;/type&gt;&lt;/mark&gt;: Indicates the type of elements (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;u8&lt;/mark&gt; for unsigned 8-bit integers).&lt;br&gt;
4.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;size&gt;&lt;/size&gt;&lt;/mark&gt;: Specifies the vector register group multiplier (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;m2&lt;/mark&gt;).&lt;br&gt;
5.&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;&lt;mask&gt;&lt;/mask&gt;&lt;/mark&gt;: Optionally indicates whether the operation is masked (e.g., &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;b4&lt;/mark&gt; for 4-bit mask).
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Intrinsics Examples &lt;/h2&gt;

&lt;p&gt;
Let&#39;s briefly describe some common RISC-V vector intrinsics with examples:
&lt;/p&gt;

&lt;p&gt;
Vector Load/Store Instructions&lt;br&gt;
These sample instructions are used to load data from memory into vector registers and store data from vector registers into memory.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiBDKAkEAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
Vector Arithmetic Instructions&lt;br&gt;
These sample instructions perform arithmetic operations on vector registers.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiDzaAkQAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
Vector Masking Instructions&lt;br&gt;
These sample instructions enable conditional operations on vector elements.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiGYKAkcAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Understanding the Arguments &lt;/h2&gt;

&lt;p&gt;
1.base (const uint8_t *base): Pointer to the base address in memory for load/store operations.&lt;br&gt;
2.vl (size_t vl): Vector length, specifying the number of elements to process.&lt;br&gt;
3.op1, op2, acc: Vector registers or scalars involved in the operation.&lt;br&gt;
4.mask (vbool4_t mask): Optional mask register for conditional operations.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Example: Adding Two 256-bit Numbers Using RISC-V Vector Intrinsics &lt;/h2&gt;

&lt;p&gt;
Now, it’s time to write a C program that adds two 256-bit numbers using RISC-V vector intrinsics. Since 256-bit numbers are not natively supported by standard data types in C, we will treat them as arrays of smaller elements (e.g., 8-bit, 16-bit, or 32-bit integers). For this example, we will use 32-bit integers to represent the 256-bit numbers.
&lt;/p&gt;

&lt;p&gt;
Step-by-Step Implementation&lt;br&gt;
1.&lt;b&gt;Include the necessary headers&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv_vector.h&lt;/mark&gt; for vector intrinsics.&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;stdio.h&lt;/mark&gt; for input/output functions.&lt;br&gt;
2.&lt;b&gt;Define the 256-bit numbers as arrays of 32-bit integers&lt;/b&gt;:&lt;br&gt;
•Each 256-bit number will be represented by an array of 8 &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;uint32_t&lt;/mark&gt; elements.&lt;br&gt;
3.&lt;b&gt;Use vector intrinsics to load the numbers into vector registers&lt;/b&gt;:&lt;br&gt;
•Use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_vle32_v_u32m1&lt;/mark&gt; to load the 32-bit elements into vector registers.&lt;br&gt;
4.&lt;b&gt;Perform the addition using vector intrinsics&lt;/b&gt;:&lt;br&gt;
•Use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_vadd_vv_u32m1&lt;/mark&gt; to add the elements of the two vector registers.&lt;br&gt;
5.&lt;b&gt;Store the result back into an array&lt;/b&gt;:&lt;br&gt;
•Use &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;__riscv_vse32_v_u32m1&lt;/mark&gt; to store the result from the vector register back into memory.
&lt;/p&gt;

&lt;p&gt;
Complete Code&lt;br&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;#include &lt;riscv_vector.h&gt;&lt;br&gt;
#include &lt;stdio.h&gt;&lt;br&gt;
#define VECTOR_LENGTH 8&lt;br&gt;
// Number of 32-bit elements to represent a 256-bit number&lt;br&gt;
void add_256bit_numbers(const uint32_t *num1, const uint32_t *num2, uint32_t *result) {&lt;br&gt;
  // Set the vector length and element width&lt;br&gt;
  size_t vl = __riscv_vsetvl_e32m1(VECTOR_LENGTH);&lt;br&gt;
  // Load the 256-bit numbers into vector registers&lt;br&gt;
  vuint32m1_t vec_num1 = __riscv_vle32_v_u32m1(num1, vl);&lt;br&gt;
  vuint32m1_t vec_num2 = __riscv_vle32_v_u32m1(num2, vl);  &lt;br&gt;
  // Perform the addition     vuint32m1_t vec_result = __riscv_vadd_vv_u32m1(vec_num1, vec_num2, vl);  &lt;br&gt;
  // Store the result back to memory&lt;br&gt;
  __riscv_vse32_v_u32m1(result, vec_result, vl);&lt;br&gt;
}&lt;br&gt;&lt;br&gt;

int main() {&lt;br&gt;
  // Define two 256-bit numbers as arrays of 8 32-bit integers&lt;br&gt;    
  uint32_t num1[VECTOR_LENGTH] = {0x12345678, 0x9ABCDEF0, 0xFEDCBA98, 0x76543210, 0x0F1E2D3C, 0x4B5A6978, 0x11223344, 0x55667788};&lt;br&gt;
  uint32_t num2[VECTOR_LENGTH] = {0x87654321, 0x0FEDCBA9, 0x12345678, 0x9ABCDEF0, 0x89ABCDEF, 0x12345678, 0x90ABCDEF, 0x12345678};&lt;br&gt;
  uint32_t result[VECTOR_LENGTH] = {0};&lt;br&gt;  
  // Add the 256-bit numbers  add_256bit_numbers(num1, num2, result);&lt;br&gt;  
  // Print the result&lt;br&gt;
  printf(&quot;Result: &quot;);&lt;br&gt;
  for (int i = 0; i &amp;lt; VECTOR_LENGTH; i++) {&lt;br&gt;
    printf(&quot;%08x &quot;, result[i]);&lt;br&gt;
  }&lt;br&gt;
  printf(&quot;\n&quot;); &lt;br&gt; 
  return 0;&lt;br&gt;
}&lt;/stdio.h&gt;&lt;/riscv_vector.h&gt;&lt;/mark&gt;
&lt;/p&gt;

&lt;p&gt;
Explanation&lt;br&gt;
1.&lt;b&gt;Define Constants&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;VECTOR_LENGTH&lt;/mark&gt; is defined as 8 to represent the number of 32-bit elements in a 256-bit number.&lt;br&gt;
2.&lt;b&gt;add_256bit_numbers Function&lt;/b&gt;:&lt;br&gt;
•This function takes two 256-bit numbers (&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num2&lt;/mark&gt;) and stores their sum in &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;result&lt;/mark&gt;.&lt;br&gt;
3.&lt;b&gt;Set Vector Length&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vsetvl_e32m1&lt;/mark&gt; sets the vector length to handle 32-bit elements, with a length multiplier of 1.&lt;br&gt;
4.&lt;b&gt;Load Vector Registers&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vle32_v_u32m1&lt;/mark&gt; loads the 32-bit elements of &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;num2&lt;/mark&gt; into vector registers &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num2&lt;/mark&gt;.&lt;br&gt;
5.&lt;b&gt;Vector Addition&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vadd_vv_u32m1&lt;/mark&gt; adds the elements of &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num1&lt;/mark&gt; and &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_num2&lt;/mark&gt; and stores the result in &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_result&lt;/mark&gt;.&lt;br&gt;
6.&lt;b&gt;Store Result&lt;/b&gt;:&lt;br&gt;
•&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vse32_v_u32m1&lt;/mark&gt; stores the result from &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;vec_result&lt;/mark&gt; back into the &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;result&lt;/mark&gt; array.&lt;br&gt;
7.&lt;b&gt;Main Function&lt;/b&gt;:&lt;br&gt;
•Defines two 256-bit numbers and an array to store the result.&lt;br&gt;
•Calls &lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;add_256bit_numbers&lt;/mark&gt; to add the two numbers.&lt;br&gt;
•Prints the result.
&lt;/p&gt;

&lt;p&gt;
Compilation&lt;br&gt;
To compile this code with RISC-V vector extension support, use the following command:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;riscv64-unknown-elf-gcc -march=rv64gcv -mabi=lp64d -o add_256bit add_256bit.c&lt;/mark&gt;
&lt;/p&gt;

&lt;p&gt;
It is worth mentioning that this command is in fact cross compilation on x64/x86 machine and produces a binary for RISC-V architecture. However, remember that you do not necessarily need to do that at all if you have a system working on a RISC-V capable platform like Banana PI or VisionFive2. Then, the build command will not differ from any other compiler invocation and you can use regular gcc or clang.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;Example: Adding two 256-bit numbers using GCC and clang support for RVV &lt;/h2&gt;

&lt;p&gt;
What is worth mentioning is that GCC vector extensions (also supported by Clang) support RVV, so this can also be done in a much simpler way by enabling portable code and other compiler optimizations:
&lt;/p&gt;

&lt;p&gt;
&lt;mark style=&quot;background-color:#e9e9e9; font-weight:lighter;&quot;&gt;#include &lt;riscv_vector.h&gt;&lt;br&gt;  
#include &lt;stdio.h&gt;&lt;br&gt;  
  
#define VECTOR_LENGTH 8  // Number of 32-bit elements to represent a 256-bit number&lt;br&gt;
typedef uint32_t uint32x8_t __attribute__ ((vector_size (sizeof(uint32_t) * VECTOR_LENGTH)));&lt;br&gt;&lt;br&gt;

int main() {&lt;br&gt;
   uint32x8_t num1 = {0x12345678, 0x9ABCDEF0, 0xFEDCBA98, 0x76543210, 0x0F1E2D3C, 0x4B5A6978, 0x11223344, 0x55667788};&lt;br&gt; 
   uint32x8_t num2 = {0x87654321, 0x0FEDCBA9, 0x12345678, 0x9ABCDEF0, 0x89ABCDEF, 0x12345678, 0x90ABCDEF, 0x12345678};&lt;br&gt;&lt;br&gt;

   // Add the 256-bit numbers&lt;br&gt;  
   uint32x8_t result = num1 + num2;&lt;br&gt;&lt;br&gt;
  
   // Print the result&lt;br&gt;  
   printf(&quot;Result: &quot;);&lt;br&gt;&lt;br&gt;  
   for (int i = 0; i &amp;lt; VECTOR_LENGTH; i++) {&lt;br&gt;  
       printf(&quot;%08x &quot;, result[i]);&lt;br&gt; 
   }&lt;br&gt;  
   printf(&quot;\n&quot;);&lt;br&gt;&lt;br&gt;  

   return 0;&lt;br&gt;  
}&lt;/stdio.h&gt;&lt;/riscv_vector.h&gt;&lt;/mark&gt;
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Working on Chromium &lt;/h2&gt;

&lt;p&gt;
SRPOL is currently working on porting Chromium project, to be able to run on RISC-V architecture. This year we have been focusing on enabling full features for web/JavaScript engine, compatibility with other architectures, stability and performance optimizations, because as we mentioned before: for the final users performance is very important when browsing web pages, viewing streamed content or running web applications. We have used ARM as a reference and looked for NEON code in the whole Tizen Web Components code with its dependencies, as Chromium widely uses various libraries to run and to present multimedia. As a result, we have a long list of modules, libraries in which some optimizations for RISC-V in the case of vector extensions can be introduced – please see below examples.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/aef0dbd388/AZKxiLQqAkoAUGoJ.png&quot; width=&quot;100%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;/div&gt;

&lt;p&gt;
The above list includes just some examples and is much longer than the one presented. This shows the complexity and also the amount of work which needs to be done in performance optimization not only in Chromium itself, but also in many libraries on which it depends.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Contributions and further plans &lt;/h2&gt;

&lt;p&gt;
In the near future, we need Chromium to work with RVV code. The project is dependent on dozens of Open Source libraries which are crucial for excellent multimedia processing in browser and operating system. Gaining performance increase requires many modifications in various different libraries, like ffmpeg, v8, libpng, pixman or libjpeg-turbo. SRPOL has some achievements in pixman, a library being used in Chromium for composing and overlaying different layers, mixing them using alpha channel, etc., as some algorithms have already been ported using RVV, e.g. RGB565 to RGB888 conversion algorithm which gained ~10 times speedup comparing vector to scalar one. However, the engineers working in the Chromium project cannot do all the work by themselves. You can also be a part in this initiative, so please check a guide on how to optimize RISC-V at: &lt;a href=&quot;https://gitlab.com/riseproject/riscv-optimization-guide/-/blob/main/riscv-optimization-guide.adoc&quot; target=&quot;_blank&quot;&gt;https://gitlab.com/riseproject/riscv-optimization-guide/-/blob/main/riscv-optimization-guide.adoc&lt;/a&gt;, as it is a huge job for the whole industry. This is the reason why Samsung joined the RISC-V Software Ecosystem (RISE) project as a premier member in 2023.
&lt;/p&gt;

&lt;p&gt;
Contributing to optimizations for RISC-V in terms of vector extensions is not an easy job to do. First of all, Chromium and its dependencies create a lot of issues as those are Open Source projects supporting many different architectures and having their own path of development where maintainers decide to merge or not to merge the committed patch sets. On the other hand, there are still not many RISC-V boards with CPU that supports RVV on the market, so it is more difficult for maintainers to test those patches.
&lt;/p&gt;

&lt;p&gt;
As we are responsible for Chromium, we are planning to keep an eye on RVV support in the libraries that are most important for us and our plans for the near future are to port reported ARM NEON usage in Chromium and its dependencies to corresponding RISC-V RVV instruction, and measure performance gain in those specific regions. We keep our fingers crossed that the actual numbers proving performance increase and that tests made on our side will convince maintainers to merge our changes. If this will not be possible, the other option could be to branch out from the main repository, or postpone merges in some libraries. We will see what the future of RISC-V will bring.
&lt;/p&gt;

</description><link>https://research.samsung.com/blog/RISC-V-and-Vectorization</link><guid isPermaLink="false">https://research.samsung.com/blog/RISC-V-and-Vectorization</guid><pubDate>Invalid Date</pubDate><author>Bartłomiej Kobierzyński</author><category>Open Source</category><category>RISC-V</category><category>RVV</category></item><item><title>EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1 Introduction&lt;/h2&gt;

&lt;p&gt;
Image restoration has witnessed significant progress over the decades, especially with the advent of various deep learning networks. However, single models with different architectures or random initialization states exhibit prediction deviations from ground-truths, resulting in sub-optimal restoration results. 
&lt;/p&gt;

&lt;p&gt;
To alleviate this problem, ensemble learning, a traditional but influential machine learning technique, has been applied to image restoration. However, most ensemble methods in image restoration focus on training-stage ensemble requiring the ensemble strategy to be determined while training multiple models, thus sacrificing flexibility of changing models and convenience for plug-and-play usage [1]. 
&lt;/p&gt;

&lt;p&gt;
Post-training ensemble methods are needed but challenging. Unlike classification and regression, image restoration predictions are matrices with each pixel is correlated with others and range from 0 to 255. As a result, traditional methods like bagging and boosting either require enormous computational resources for the restoration task or fail to generalize well due to the imbalance between candidate number and feature dimension. As an alternative, Jiang et al. propose a post-training ensemble algorithm for super-resolution by optimizing a maximum a posteriori problem with a reconstruction constraint [2]. However, this constraint requires an explicit expression of the degradation process, which is extremely difficult to define for other restoration tasks beyond super-resolution. These issues lead researchers in image restoration still suffer weighted averaging as their primary choice [3].
&lt;/p&gt;

&lt;p&gt;
To this end, we formulate the ensemble of restoration models using Gaussian mixture models (GMMs), where ensemble weights can be efficiently learned via the expectation maximization (EM) algorithm and stored in a lookup table (LUT) for subsequent inference. Our method does not require training or prior knowledge of the base models and degradation processes, making it applicable to various image restoration tasks.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2 Method&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.1Ensemble Formulation of Image Restoration&lt;/h2&gt;

&lt;p&gt;
Given a test set with numerous pairs of input images and ground-truths, suppose we have M pre-trained base models for image restoration. The widely-used averaging ensemble in image restoration assigns equal weights for all samples and pixels. Recent method in the NTIRE 2023 competition assigns weights inversely proportional to the mean squared error between the predictions and their average. However, they adopt globally constant weights for all pixels and samples, neglecting that the performances of base models may fluctuate for different patterns and samples. Alternatively, we start from the prospective of GMMs and assign range-specific weights based on the EM algorithm.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.2Restoration Ensemble as Gaussian Mixture Models&lt;/h2&gt;

&lt;p&gt;
Suppose we have a reference set with N pairs of input images and ground-truths. We assume the reference set and test set are sampled from the same data distribution. Based on Gaussian prior, it can be assumed that the estimation error of a model on an image follows a zero-mean Gaussian distribution, Then the observed ground-truth can be considered following a multivariate Gaussian with the mean equal to the prediction.
&lt;/p&gt;

&lt;p&gt;
We can consider the ensemble problem as the weighted averaging of Gaussian variables and estimate the weights by solving its maximum likelihood estimation. However, solving the sample-wise mixture of Gaussian is not feasible because the covariance matrices are sample-wise different and thus hard to estimate. Besides, the number of prediction samples is much fewer than feature dimension, resulting in the singularity of the covariance matrices.
&lt;/p&gt;

&lt;p&gt;
In contrast, we alternatively append the reference set into a single sample following Gaussian. Since data samples can be considered following i.i.d data distribution, the variance of the concatenated samples is diagonal.
&lt;/p&gt;

&lt;p&gt;
However, the covariance matrix is still singular due to the imbalance between prediction sample number and feature dimension. Thus, directly mixing the multivariate Gaussian is still infeasible to solve. We thus alternatively categorize pixels into various small bins of mutually exclusive ranges such that the pixels within each range can be considered following a univariate Gaussian distribution according to the central limit theorem.
&lt;/p&gt;

&lt;p&gt;
The reference set is therefore separated into a number of bin sets, and the ground-truth pixels
inside each of them form a solvable univariate GMM. We then introduce the latent variable z representing the probability of the pixel belonging to the m-th Gaussian component, which is equivalent to the role of the ensemble weight for the m-th base model. The value of ensemble weights can be estimated by the maximum likelihood estimates of the observed ground-truths. We formulated the expression of GMMs for estimating the range-specific ensemble weights. The complete and detailed formulars can be seen in the paper.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.3Restoration Ensemble via Expectation Maximization and Lookup Table&lt;/h2&gt;

&lt;p&gt;
For each bin set, we estimate ensemble weights by maximizing the log likelihood. We have an E-step to estimate the posterior distribution. After that, we have an M-step to obtain the maximum likelihood estimates. Thanks to the separation of bin sets, we have prior knowledge of the mean and variance of each model, which can be estimated.
&lt;/p&gt;

&lt;p&gt;
We store the range-specific weights estimated on the reference set into a LUT. During the inference stage for a test sample, we have the prediction of the m-th base model. For a bin set, we partition input pixels of multiple models into each bin. Then we retrieve the estimated range-wise weights from the LUT based on each key of the bin set and obtain the aggregated ensemble. The details of main algorithm can be found in the paper.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3 Results&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.1Experimental Settings&lt;/h2&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/06a5d35a29/AZM9nokaAzYAUGoJ.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; A visual comparison on an image from Manga109 for super resolution. “HR &amp;amp; LR” means high-resolution and bicubic-upscaled low-resolution images. The second line of (c)-(g) are error maps&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;b&gt;Benchmarks&lt;/b&gt;. We evaluate our ensemble method on 3 image restoration tasks including super resolution, deblurring, and deraining. For super-resolution, we use Set5, Set14, BSDS100, Urban100 and Manga109 as benchmarks. For deblurring, we use GoPro, HIDE, RealBlur-J and -R. For deraining, we adopt Rain100H, Rain100L, Test100, Test1200, and Test2800.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Base Models&lt;/b&gt;. To evaluate the generalization of ensemble methods against model choices, we employ a wide variety of base models, including CNNs, ViTs, MLPs and Mambas. For image super-resolution, we use SwinIR, SRFormer, and MambaIR. We choose MPRNet, DGUNet, and Restormer for deblurring, as well as MPRNet, MAXIM, and Restormer for deraining.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Baselines&lt;/b&gt;. We utilize regression algorithms including bagging, AdaBoost, random forests (RForest), gradient boosting decision tree (GBDT), histogram gradient boosting decision tree (HGBT) as baselines. Averaging is also a commonly used ensemble baseline. A recent method proposed by team ZZPM in the NTIRE 2023 competition is also included for comparison. Additionally, we adopt RefESR [2] for image super-resolution ensemble.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.2Quantitative Results&lt;/h2&gt;

&lt;p&gt;
We present quantitative comparisons with existing ensemble methods in Tab. 1 for super-resolution. In contrast, our method, which learns per-value weights, can recognize performance biases and alleviate and consistently performs well for all cases.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/06a5d35a29/AZM8sDyqAyMAUGoJ.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; The ensemble results on the task of image super-resolution. The categories of “Base”,
“Regr.” and “IR.” in the first column mean base models, regression-based ensemble methods, and those ensemble methods designed for image restoration.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
More details of quantitative comparisons for deblurring and de-raining can be seen in the paper.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.3Quantitative Results&lt;/h2&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/06a5d35a29/AZM80Ap6AyYAUGoJ.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; A visual comparison on an image from GoPro for the task of image deblurring. “GT &amp;amp; LQ” means ground-truth and low quality blurry images. The second line of (c)-(g) are error maps.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/06a5d35a29/AZM80EKaAykAUGoJ.jpg&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; A visual comparison of ensemble on an image from Test100 for image de-raining. “GT &amp;amp; LQ” means ground-truth and low quality rainy images. The second line of (c)-(g) are error maps&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
We also provide qualitative visual comparisons in Fig. 1 and 2. In Fig. 1, our method with the bin width b = 32 that learns fine-grained range-wise weights successfully recovers the pattern. In Fig. 2, only our method can effectively obtain a better ensemble with sharp edges and accurate colors. In Fig. 3, MPRNet removes all stripe patterns on the ground together with rain streaks while ours alleviates the issue.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4Conclusion&lt;/h2&gt;

&lt;p&gt;
In this paper, we propose an ensemble algorithm for image restoration based on GMMs. We partition the pixels of predictions and ground-truths into separate bins of exclusive ranges and formulate the ensemble problem using GMMs over each bin. The GMMs are solved on a reference set, and the estimated ensemble weights are stored in a lookup table for the ensemble inference on the test set. Our algorithm outperforms regression-based ensemble methods as well as commonly used averaging strategies. It is training-free, model-agnostic, and thus suitable for plug-and-play usage.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://neurips.cc/virtual/2024/poster/93407&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://neurips.cc/virtual/2024/poster/93407&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] Lingfeng Wang, Zehao Huang, Yongchao Gong, and Chunhong Pan. Ensemble based deep networks for image super-resolution. Pattern Recognition, 2017.
&lt;/p&gt;
&lt;p&gt;
[2] Junjun Jiang, Yi Yu, Zheng Wang, Suhua Tang, Ruimin Hu, and Jiayi Ma. Ensemble super-resolution with a reference dataset. IEEE TCYB, 2019
&lt;/p&gt;
&lt;p&gt;
[3] Zhang Z, Zhang S, Wu R, et al. NTIRE 2024 challenge on bracketing image restoration and enhancement: Datasets methods and results[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 6153-6166.
&lt;/p&gt;

&lt;/div&gt;

</description><link>https://research.samsung.com/blog/EnsIR-An-Ensemble-Algorithm-for-Image-Restoration-via-Gaussian-Mixture-Models</link><guid isPermaLink="false">https://research.samsung.com/blog/EnsIR-An-Ensemble-Algorithm-for-Image-Restoration-via-Gaussian-Mixture-Models</guid><pubDate>Invalid Date</pubDate><author>Zikun Liu|Hyunhee Park</author><category>AI</category><category>NeurIPS</category><category>Image Storation</category></item><item><title>Blind Face Video Restoration with Temporal Consistent Generative Prior and Degradation-Aware Prompt</title><description>&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;1 Introduction&lt;/h2&gt;

&lt;p&gt;
In real-world scenarios, both face images and videos may suffer from unknown and varied types of degradation, such as down-sampling, noise, blur, and compression. Blind Face Restoration (BFR) is a challenging task that aims at restoring low-quality faces suffering from unknown degradation. Existing BFR methods usually use facial priors such as reference prior, geometry prior, and generative prior in the network structure. Although these existing BFR methods work well in the blind face image restoration (BFIR) problem, they do not fully consider blind face videos. To the best of our knowledge, there is still no specialized method for restoring blind face videos.
&lt;/p&gt;

&lt;p&gt;
In this paper, we present a Stable Blind Face Video Restoration (StableBFVR). As shown in Fig. 1, we introduce temporal layers in the Stable Diffusion[1] preserve temporal consistency. First, we propose Shift-Resblock which implicitly captures global information for long-term aggregation. Second, we further improve restoration performance and temporal consistency by introducing Nearby-Frame Attention to aggregate short-term information. Moreover, to enable adaptive responses to complex and large-range blind degradation, we propose a degradation-aware prompt module to encode degradation-specific information as prompts to guide the restoration network.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG7lk96AJUAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 1.&lt;/b&gt; The architecture of the proposed StableBFVR.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;2 Methodology&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;2.1Preliminary: Latent Diffusion Models&lt;/h2&gt;

&lt;p&gt;
StableBFVR uses the pre-trained latent diffusion model (LDM) Stable Diffusion as facial prior. In this work, we start with the pre-trained Stable Diffusion and create a new video diffusion model for blind face video restoration. By adopting temporal strategies within the LDM framework, our method can achieve temporal consistency while leveraging the prior knowledge from Stable Diffusion.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em;&quot;&gt;2.2Temporal Layers in StableBFVR&lt;/h2&gt;

&lt;p&gt;
To maintain temporal consistency and use multiple frame information to improve the restoration performance, we introduce temporal layers to Stable Diffusion. As shown in Fig. 2, these temporal layers comprehensively consider both long-term and short-term information in the video. Specifically, we present Shift-ResBlock which uses the proposed forward temporal shift block and backward temporal shift block alternatively to achieve bi-directional aggregation. By using Shift-ResBlock repeatedly, the aggregation of long-term information is achieved. For short-term information aggregation, we introduce a Nearby-Frame Attention (NFA). By seeking complementary sharp information existing in neighboring frames, NFA can refine restoration details.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG7sSoqAJgAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 2.&lt;/b&gt; The structure of the proposed Shift-Resblock and Nearby-Frame Attention (NFA).&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em;&quot;&gt;2.3Degradation-Aware Prompt Module&lt;/h2&gt;

&lt;p&gt;
To further improve the restoration performance, we propose a Degradation-Aware Prompt Module (DAPM). As shown in Fig. 1, DAPM first extracts degradation-aware features from the input frames to predict prompt weights about different types of degradation. Then DAPM utilizes these weights to adjust the corresponding prompt corresponding to different types of degeneration and fuses these prompts to obtain degradation-aware prompts which encode discriminative information about various types of degradation.
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;3Results&lt;/h2&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em; margin-top: -0.1px;&quot;&gt;3.1Quantitative Comparison&lt;/h2&gt;

&lt;p&gt;
For the synthetic VFHQ-Test, the quantitative results are shown in Tab. 1. The results indicate that our method achieves state-of-the-art performance on all perceptual metrics. Specifically, our StableBFVR achieves the best performance regarding LPIPS, indicating that the perceptual quality of restored face videos is closest to ground truth. Moreover, StableBFVR also obtains the best results of NIQE, MUSIQ, and CLIP-IQA, showing that the outputs better align with human visual and perceptive systems.
To assess the generalization ability, we extend the evaluation of our model to the real-world dataset WebVideo-Test. StableBFVR exhibits superior performance across all three metrics NIQE, MUSIQ, and CLIP-IQA, showing its remarkable generalization capability. Furthermore, compared with video restoration methods, BFIR methods also show satisfactory performance, suggesting the importance of generative prior in the scenery of unknown degradations in the real world.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG7u9-6AJsAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Table 1.&lt;/b&gt; Quantitative comparison on VFHQ-Test and WebVideo-Test for blind face video restoration.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em;&quot;&gt;3.2Qualitative Comparison&lt;/h2&gt;

&lt;p&gt;
For the synthetic VFHQ-Test, as shown in Fig. 3. Compared with video restoration methods, our method recovers faithful details in the eyes, mouth, beard etc. Our method treats the input as a whole in restoration and performs well in all regions. Our method can aggregate information from other frames to improve performance.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG7vyDaAJ4AUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Visual comparison results of different methods on the VFHQ-Test.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
For WebVideo-Test, our method produces realistic facial textures in the case of complicated real-world degradation. As shown in the last column of Fig. 4, previous BRIR methods fail to restore the hair textures on the image boundary, while ours is successful. Compared with video restoration methods, our method produces significantly more texture detail.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG7v2aKAKEAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 4.&lt;/b&gt; Visual comparison results of different methods on the real-world dataset WebVideo-Test.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot; style=&quot;font-size: 1.47em;&quot;&gt;3.3Temporal Consistency&lt;/h2&gt;

&lt;p&gt;
To thoroughly verify our method, we visualize the consecutive frames generated by different methods in Fig. 5. It is observed that, although sequences restored by BFIR methods exhibit realistic texture, there are noticeable differences between the textures of continuous frames. Conversely, sequences restored by video restoration methods demonstrate commendable temporal consistency but tend to be excessively smooth, lacking textures. StableBFVR strikes a favorable balance, reconstructing more textures, while simultaneously preserving temporal consistency.
&lt;/p&gt;

&lt;div class=&quot;news-imgWrap&quot;&gt;
&lt;img src=&quot;https://cdn.codeground.org/resources/fb7ab3e5c2/AZG7wHNaAKQAUGoJ.png&quot; width=&quot;70%&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
&lt;p style=&quot;text-align:center;&quot;&gt;&lt;b&gt;Figure 5.&lt;/b&gt; Visual comparisons of the temporal consistency for restored videos. (a) GFP-GAN, (b) CodeFormer, (c) DiffBIR, (d) RestoreFormer, (e) BaiscVSR++[2], (f) RVRT, (g) DSTNet, (h) Ours.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;4Conclusion&lt;/h2&gt;

&lt;p&gt;
In this work, we tackle the BFVR problem for the first time. We propose StableBFVR leveraging the strong generative prior from the pre-trained generative model Stable Diffusion to restore face videos with realistic details. To ensure content consistency among frames and use multi-frame information for improved restoration, we develop Shift-Resblock and Nearby-Frame Attention to aggregate both long-term and short-term information. Additionally, we propose a Degradation-Aware Prompt Module to dynamically guide the restoration process and further enhance performance. Extensive experiments show that our StableBFVR achieves superior performance than video restoration methods and blind face image restoration methods. 
&lt;/p&gt;

&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;Link to the paper&lt;/h2&gt;
&lt;br&gt;&lt;br&gt;

&lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=qaIS3nvAem&quot; target=&quot;_blank&quot;&gt;&lt;u&gt;https://openreview.net/forum?id=qaIS3nvAem&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;


&lt;div class=&quot;referInfo&quot;&gt;
&lt;p&gt;&lt;/p&gt;&lt;h2 class=&quot;tit-newsSection blue-txt tit-big&quot;&gt;References&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition.
&lt;/p&gt;
&lt;p&gt;
[2] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. 2022. Basicvsr++: Improving video super-resolution with enhanced propagation and alignment. In IEEE Conference on Computer Vision and Pattern Recognition.
&lt;/p&gt;

&lt;/div&gt;

</description><link>https://research.samsung.com/blog/Blind-Face-Video-Restoration-with-Temporal-Consistent-Generative-Prior-and-Degradation-Aware-Prompt</link><guid isPermaLink="false">https://research.samsung.com/blog/Blind-Face-Video-Restoration-with-Temporal-Consistent-Generative-Prior-and-Degradation-Aware-Prompt</guid><pubDate>Invalid Date</pubDate><author>Ying Zhang|Hyunhee Park</author><category>AI</category><category>Video Restoration</category><category>Diffusion</category></item></channel></rss>