<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Google Research Blog</title><link>https://research.google/blog</link><atom:link href="http://rsshub.isrss.com/google/research" rel="self" type="application/rss+xml"></atom:link><description>Google Research Blog - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 19 Mar 2025 23:23:00 GMT</lastBuildDate><ttl>5</ttl><item><title>Generating synthetic data with differentially private LLM inference</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;jyti8&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Differential privacy&lt;/a&gt; (DP) provides mathematically rigorous guarantees that an algorithm will not reveal details about an individual’s data. However, endowing algorithms with DP guarantees can add complexity to an already complex machine learning (ML) pipeline. This is especially true at the scale of modern ML, where pipelines are maintained and used by many different groups in an organization.&lt;/p&gt;&lt;p data-block-key=&quot;b1cqt&quot;&gt;Differentially private &lt;a href=&quot;https://en.wikipedia.org/wiki/Synthetic_data#:~:text=Synthetic%20data%20are%20artificially%20generated,be%20seen%20as%20synthetic%20data.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;synthetic data&lt;/a&gt; can help deal with this scaling issue by serving as an &lt;a href=&quot;https://en.wikipedia.org/wiki/Interface_(computing)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;interface&lt;/a&gt; for model development teams to collaborate without downstream teams needing to understand DP. As described in a &lt;a href=&quot;https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/&quot;&gt;previous blog post&lt;/a&gt;, a common approach to generating DP synthetic data is to privately fine-tune large language models (LLMs), but this can be costly and has high minimum data requirements. The alternative to private training is &lt;a href=&quot;https://arxiv.org/abs/1803.10266&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;differentially private prediction&lt;/a&gt;, where only model outputs are released with DP, instead of the model itself. While there is a large fixed cost to conduct private fine-tuning, differentially private prediction trades off quantity for quality, preferring to release a limited number of high-quality outputs rather than an unlimited number of low-quality ones.&lt;/p&gt;&lt;p data-block-key=&quot;ev4q7&quot;&gt;In “&lt;a href=&quot;https://www.arxiv.org/abs/2407.12108&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Private prediction for large-scale synthetic text generation&lt;/a&gt;”, we present an inference-only approach for generating DP synthetic data. The approach works by prompting an off-the-shelf LLM with many sensitive examples in parallel, and aggregating their predictions with differential privacy. We address issues related to privacy budget and efficiency, which allow us to generate thousands of high-quality synthetic data points with DP guarantees and therefore greatly expand the set of potential applications.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Method&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Our approach builds on pioneering &lt;a href=&quot;https://arxiv.org/abs/2309.11765&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2305.15594&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;work&lt;/a&gt; that introduced a method for distributing sensitive examples over independent prompts, running LLM inference (i.e., the mechanism LLMs use to generate realistic responses) on these prompts, and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1250790.1250803&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;aggregating&lt;/a&gt; next-token &lt;a href=&quot;https://arxiv.org/abs/1610.05755&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;predictions&lt;/a&gt; with differential privacy. Specifically, various prompts (each containing a single piece of sensitive data) are fed into an LLM. Then, predictions from all the prompts are aggregated and the next token is decoded with differential privacy. This ensures the selected token is not influenced strongly by any single piece of sensitive data. Finally, the selected token is appended to all prompts and the process repeats.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- For GIFs, use a default width --&gt;
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-1-Overview.width-800.gif&quot; alt=&quot;SynthData-1-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-1-Overview.width-800.gif&quot; alt=&quot;SynthData-1-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;How the algorithm works.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;jyti8&quot;&gt;Due to challenges in generating text while maintaining DP and computational efficiency, &lt;a href=&quot;https://arxiv.org/abs/2309.11765&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior work&lt;/a&gt; focused on generating a small amount of data points (&amp;lt;10) to be used for in-context learning. We show that it’s possible to generate two to three orders of magnitude more data while preserving quality and privacy by solving issues related to the &lt;i&gt;privacy budget&lt;/i&gt; and &lt;i&gt;computational efficiency&lt;/i&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;c42g1&quot;&gt;The &lt;i&gt;privacy budget&lt;/i&gt; constrains the amount of output the model can release while maintaining a meaningful DP guarantee. DP operates by introducing randomness to mask the contribution of any single data point, enabling plausible deniability. We increase output while maintaining privacy by leveraging the inherent randomness in next-token sampling to ensure privacy.&lt;/p&gt;&lt;p data-block-key=&quot;bbsg9&quot;&gt;This connects next-token sampling in language models with a DP technique called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_mechanism&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;exponential mechanism&lt;/i&gt;&lt;/a&gt;. This mechanism is used to approximately choose the best token option from a set of options, with each option accompanied by a score computed from sensitive data. It does so by sampling an option with probability proportional to the exponential of its score – this introduces randomness crucial to the DP guarantee. This operation is the same as &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Applications&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;softmax sampling&lt;/a&gt; in language models when viewing the set of all tokens as the options from which the model chooses. Based on this connection, we design a DP token sampling algorithm that is strongly aligned with the standard generation process of large language models.&lt;/p&gt;&lt;p data-block-key=&quot;1pgso&quot;&gt;For &lt;i&gt;computational efficiency&lt;/i&gt;, we propose a new privacy analysis that lets us use the same contexts for each generation step and avoid recomputation. Our analysis uses a fixed batch of examples, whereas the DP guarantee of prior work required a fresh batch of sensitive examples to be generated for &lt;i&gt;each token&lt;/i&gt;. But using a fresh batch necessitates changing the input prompt for each sampled token, which is incompatible with standard inference efficiency techniques such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#KV_caching&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;KV caching&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;3ntvv&quot;&gt;Finally, we also introduce a &lt;i&gt;public drafter&lt;/i&gt;, a model that bases its next token predictions solely on already generated synthetic text, rather than sensitive data. Via the &lt;a href=&quot;https://www.cis.upenn.edu/~aaroth/courses/slides/Lecture11.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;sparse vector technique&lt;/a&gt;, we only pay a privacy cost when the drafter’s proposals disagree with predictions made from sensitive data. Otherwise, we accept the drafter’s suggestion and do not expend any privacy budget. We find this is particularly effective for structured data, where many formatting-related tokens can be predicted by the drafter without looking at sensitive data.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Results&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Treating benchmark ML datasets as stand-ins for sensitive data, we ran our algorithm with &lt;a href=&quot;https://blog.google/technology/developers/gemma-open-models/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemma&lt;/a&gt; models to generate synthetic versions of these datasets. We evaluated how useful the resulting synthetic data is for downstream tasks, namely in-context learning with &lt;a href=&quot;https://platform.openai.com/docs/models#gpt-base&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;GPT-3&lt;/a&gt;, and for fine-tuning &lt;a href=&quot;https://huggingface.co/docs/transformers/en/model_doc/bert&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BERT&lt;/a&gt; models. The aim is to understand to what extent we can &lt;i&gt;replace&lt;/i&gt; real sensitive datasets with our DP synthetic datasets in ML applications.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;GPT-3 in-context learning&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;We prompted GPT-3 with real and synthetic few-shot examples from benchmark datasets (&lt;a href=&quot;https://huggingface.co/datasets/fancyzhx/ag_news&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AGNews&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/datasets/fancyzhx/dbpedia_14&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DBPedia&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/datasets/CogComp/trec&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;TREC&lt;/a&gt;, &lt;a href=&quot;https://sls.csail.mit.edu/publications/2012/Liu-Interspeech12.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MIT-G&lt;/a&gt;, and &lt;a href=&quot;https://sls.csail.mit.edu/publications/2012/Liu-Interspeech12.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MIT-D&lt;/a&gt;) and evaluated test set accuracy.&lt;/p&gt;&lt;p data-block-key=&quot;1uo76&quot;&gt;We determine the accuracy of classification when GPT-3 is given &lt;i&gt;k&lt;/i&gt; examples from the data source as reference before being asked for the answer. Our improvements allow us to generate more synthetic data while preserving privacy and quality. We demonstrate improved in-context learning accuracy at the same privacy level, as a consequence of our method’s ability to generate more high-quality synthetic reference examples. Notably, our synthetic data at 64 shots improves over 4 shots of real data (an approximate upper bound on the performance of prior methods that limited themselves to generating 4 synthetic examples).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-2-InContext.width-1250.png&quot; alt=&quot;SynthData-2-InContext&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-2-InContext.width-1250.png&quot; alt=&quot;SynthData-2-InContext&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;GPT-3 in-context learning results. We compare the accuracy of GPT-3 when given few-shot examples from real and synthetic data. Synthetic data is comparable to real data, and we demonstrate improved accuracies when using more of our synthetic data.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;BERT fine-tuning on Yelp synthetic data&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Next, we fine-tuned BERT models on synthetic &lt;a href=&quot;https://huggingface.co/datasets/fancyzhx/yelp_polarity&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Yelp data&lt;/a&gt; generated from DP inference and compared them to the DP fine-tuning results we discussed in a &lt;a href=&quot;https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/&quot;&gt;previous blog post&lt;/a&gt;. While it’s possible to train reasonable models from data generated from DP inference, our results showed that there remains a large gap between inference and the best fine-tuning approaches.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-3-BERT.width-1250.png&quot; alt=&quot;SynthData-3-BERT&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-3-BERT.width-1250.png&quot; alt=&quot;SynthData-3-BERT&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;BERT fine-tuning results on Yelp synthetic data. We compared the accuracy of BERT models trained on real and synthetic data. DP inference can generate enough data to finetune BERT classifiers, but accuracy falls short of the best DP fine-tuning method.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Limited data regime&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;For limited data regimes, DP inference outperforms DP fine-tuning. On a 1K subset of &lt;a href=&quot;https://paperswithcode.com/dataset/ag-news&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AGNews&lt;/a&gt;, the 16-shot GPT-3 in-context learning accuracy was 68.1%, compared to 80.1% for DP inference on the same dataset and at the same privacy level.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-4-GPT-3.width-1250.png&quot; alt=&quot;SynthData-4-GPT-3&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-4-GPT-3.width-1250.png&quot; alt=&quot;SynthData-4-GPT-3&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;GPT-3 in-context learning results in the limited data regime. We compare the accuracy of GPT-3 when given few-shot examples generated from a small subset of AGNews. DP inference outperforms DP fine-tuning in the limited data regime.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;What’s next?&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Differentially private language model inference is a flexible and lightweight approach to generating DP synthetic data. Our work has shown that DP inference can be scaled up to generate large synthetic corpora.&lt;/p&gt;&lt;p data-block-key=&quot;cg2k3&quot;&gt;At scale, quality falls short of DP fine-tuning. However, analogous to tradeoffs between fine-tuning and prompt engineering, DP inference allows for fast iteration cycles — the time to first synthetic example is minutes as opposed to hours. Another important application is latency-constrained applications (e.g., agents) for which batch generation is unsuitable.&lt;/p&gt;&lt;p data-block-key=&quot;dk7o3&quot;&gt;We plan to explore further applications of DP inference and continue to make improvements to the quality and quantity of generated examples.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;&lt;i&gt;This work is the result of a collaboration between multiple people across Google Research and Google DeepMind, including (in alphabetical order by last name): Kareem Amin, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Andreas Terzis, Sergei Vassilvitskii.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/</link><guid isPermaLink="false">https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/</guid><pubDate>Mon, 17 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Machine Intelligence</category><category>Natural Language Processing</category><category>Security, Privacy and Abuse Prevention</category></item><item><title>From diagnosis to treatment: Advancing AMIE for longitudinal disease management</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;zirkj&quot;&gt;Effective clinical reasoning — the totality of all the decisions that go into patient care — is a cornerstone of healthcare. High quality clinical reasoning is a hallmark of expert clinicians and requires not only accurate diagnosis but also sophisticated reasoning about disease progression, therapeutic response, safe medication prescription, and the appropriate use of accepted guidelines or evidence in shared decision-making with patients. Even once a patient’s diagnosis has been established, an optimal management plan often requires monitoring of the patient’s trajectory and experience, personalized treatment plans with informed and shared decision-making, and proactive adjustments based on individual patient needs, preferences, and system constraints. While large language models (LLMs) &lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;have shown promise&lt;/a&gt; in capabilities underpinning diagnostic dialogue, their capabilities for clinical management reasoning over time remain under-explored.&lt;/p&gt;&lt;p data-block-key=&quot;4cto9&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2503.06074&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Towards Conversational AI for Disease Management&lt;/a&gt;”, we advance the previously-demonstrated diagnostic reasoning capabilities of the &lt;a href=&quot;https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/&quot;&gt;Articulate Medical Intelligence Explorer&lt;/a&gt; (AMIE) — our research AI system for medical reasoning and conversations — by integrating additional LLM agentic capabilities optimized specifically for clinical management reasoning and dialogue. This enhanced version of AMIE builds on the core strengths of the Gemini family of models, such as state-of-the-art long-context reasoning and lowest-in-class hallucination rates, to incorporate reasoning over the longitudinal (i.e., sequential over time) progression of disease, response to therapy, and information on safe medication prescription and clinical guidelines. It enables AMIE to go beyond diagnosis and towards the support of patients and clinicians in navigating the complexities of next steps. This latest evolution demonstrates how AMIE might engage in longitudinal interactions, ground its reasoning in an evolving body of authoritative clinical knowledge, and provide structured management plans aligned with accepted guidelines.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/AMIEMx-1-AMIEOverview.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE now supports longitudinal disease management, grounding its reasoning in clinical guidelines and adapting to patient needs across multiple visits.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;The challenge of disease management&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;Clinical care presents unique challenges that extend beyond the initial diagnostic process. Disease management requires consideration of a multitude of factors, including treatment side effects, patient adherence, lifestyle modifications, and the ever-changing landscape of medical research and clinical guidelines. The ability to perform management reasoning has remained an underexplored challenge for AI systems until now.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/AMIEMx-2-LongContext.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE leverages Gemini&#39;s long-context capabilities to access and reason over clinical guidelines, ensuring its recommendations are grounded in evidence-based medicine.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A two-agent architecture for enhanced reasoning&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;Our work addresses this challenge with a novel approach based on the interplay of two LLM-driven agents, which has similarities to &lt;a href=&quot;https://nap.nationalacademies.org/read/21794/chapter/4#59&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;how human clinicians tackle management problems&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;b4c24&quot;&gt;The &lt;i&gt;Dialogue Agent&lt;/i&gt; is user-facing and equipped to rapidly respond based on its current understanding of the patient. This agent handles the conversational aspects of the interaction, gathering information about the patient’s condition, addressing their concerns, and building rapport. By leveraging natural language processing and empathetic communication techniques, the Dialogue Agent ensures a seamless and engaging user experience.&lt;/p&gt;&lt;p data-block-key=&quot;a0pi1&quot;&gt;The &lt;i&gt;Mx Agent&lt;/i&gt; (Management Reasoning Agent) deliberately and continuously analyzes the available information, including clinical guidelines and patient-specific data, to optimize management of the patient. Leveraging Gemini’s state-of-the-art long-context capabilities, this agent synthesizes and reasons over large amounts of information — patient dialogues across several visits in addition to hundreds of pages of clinical guidelines — all at once. Using this approach, it produces structured plans for investigations, treatments, and follow-up care, taking into account the latest medical evidence, information gathered during previous visits, and individual patient preferences.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-3-2Agents.width-1250.png&quot; alt=&quot;AMIEMx-3-2Agents&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-3-2Agents.width-1250.png&quot; alt=&quot;AMIEMx-3-2Agents&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE&#39;s two-agent architecture: The Dialogue Agent interacts with the patient, while the Mx Agent creates structured management plans based on clinical guidelines. Management plans define the sequence of investigations and treatments recommended for that patient.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Grounding management in clinical guidelines&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;To ensure reliability and safety, AMIE’s management reasoning capabilities are primarily enabled by scaling test-time compute to perform deep reasoning with structural constraints while grounding recommendations in authoritative clinical knowledge. Here, too, AMIE relies on Gemini for long-context understanding to align its output with relevant and up-to-date clinical practice guidelines and drug &lt;a href=&quot;https://en.wikipedia.org/wiki/Formulary_(pharmacy)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;formularies&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;bjkci&quot;&gt;This involves selecting and processing documents from a comprehensive corpus of clinical guidelines that encompass trusted sources, such as the &lt;a href=&quot;https://www.nice.org.uk/guidance&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;UK National Institute for Health and Care Excellence Guidance&lt;/a&gt; and the &lt;a href=&quot;https://bestpractice.bmj.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BMJ Best Practice&lt;/a&gt; guidelines. The Mx Agent then uses these guidelines to inform its decision-making process, ensuring that its recommendations are evidence-based and aligned with community-established best practices.&lt;/p&gt;&lt;p data-block-key=&quot;f0c2r&quot;&gt;Intricate structured constraints help guide the model through specified reasoning strategies, while iterative drafting and merging of generated plans helps refine their quality. This allows AMIE to create personalized management plans that are both evidence-based and tailored to the individual patient&#39;s needs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-4-DeepReasoning.width-1250.png&quot; alt=&quot;AMIEMx-4-DeepReasoning&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-4-DeepReasoning.width-1250.png&quot; alt=&quot;AMIEMx-4-DeepReasoning&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE uses deep reasoning with structural constraints (A) to create structured management plans (B) grounded in a case analysis (C) and explicit management goals (D) that include in-visit investigations, ordered investigations, and treatment recommendations, all supported by citations (E). Here we present an example reasoning trace for a fictitious patient.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Evaluating AMIE&#39;s performance: The multi-visit OSCE study&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;To rigorously evaluate AMIE&#39;s ability to handle longitudinal disease management, we conducted a randomized, blinded virtual &lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structured_clinical_examination&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;objective structured clinical examination&lt;/a&gt; (OSCE) study of simulated text-chat consultations. In this study, AMIE was compared to 20 primary care physicians (PCPs) across 100 multi-visit case scenarios, allowing us to assess its performance in realistic clinical settings.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-5-OSCEStudy.width-1250.png&quot; alt=&quot;AMIEMx-5-OSCEStudy&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-5-OSCEStudy.width-1250.png&quot; alt=&quot;AMIEMx-5-OSCEStudy&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;Overview of randomized multi-visit OSCE study.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;zirkj&quot;&gt;The multi-visit design of the OSCE study allowed us to evaluate AMIE&#39;s ability to 1) remember and synthesize information from previous interactions, 2) adapt management plans based on evolving patient symptoms and test results, and 3) maintain consistent and empathetic communication with a patient throughout the course of treatment.&lt;/p&gt;&lt;p data-block-key=&quot;67mgf&quot;&gt;Specialist physicians evaluated the quality of AMIE&#39;s management plans across a range of criteria, including appropriateness, completeness, the use of clinical guidelines, and patient-centeredness.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-6-Management.width-1250.png&quot; alt=&quot;AMIEMx-6-Management&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-6-Management.width-1250.png&quot; alt=&quot;AMIEMx-6-Management&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;Specialist physicians (blinded to the source of the plans) rated AMIE&#39;s management plans as non-inferior to those of PCPs, with statistically significant improvements in treatment preciseness. Key measures here included selecting appropriate investigations and avoiding inappropriate investigations (i.e., doing tests that should be avoided given the information known). P-values are shown for statistically significant (p &amp;lt; 0.05) differences.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;zirkj&quot;&gt;Furthermore, both patient actors and specialist physicians also evaluated AMIE to determine whether its behaviors reflected clinical needs and priorities. We drew inspiration from &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/35830267/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior work determining a set of key features of management reasoning&lt;/a&gt; and created a pilot evaluation rubric based on these features, which we refer to as Management Reasoning Empirical Key Features (MXEKF). Key measures of MXEKF included prioritization of preferences, constraints and values, communication and shared decision making, contrasting and selection among different options, monitoring and adjustment of the management plan, and prognostication abilities.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-7-MXEKF.width-1250.png&quot; alt=&quot;AMIEMx-7-MXEKF&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-7-MXEKF.width-1250.png&quot; alt=&quot;AMIEMx-7-MXEKF&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE demonstrated consistent performance on key management reasoning metrics (MXEKF), receiving favorable ratings from both patient actors and specialist physicians.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;RxQA: Benchmarking medication reasoning&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;A critical aspect of disease management is the safe and effective use of medications. It is necessary, but not sufficient, to reliably recall medication-specific knowledge with appropriate factuality and topic-specific reasoning. To benchmark AMIE&#39;s capabilities in these axes, we contribute RxQA, a novel multiple-choice question set derived from national drug formularies, including the &lt;a href=&quot;https://www.fda.gov/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;US Food &amp;amp; Drug Administration&lt;/a&gt; and &lt;a href=&quot;https://www.nice.org.uk/bnf-uk-only&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;British National Formulary&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;5ratq&quot;&gt;RxQA comprises 600 questions designed to assess knowledge of medication indications, contraindications, dosages, side effects, and interactions. The questions were carefully validated by board-certified pharmacists to ensure their accuracy and relevance to clinical practice.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-8-RxQA.width-1250.png&quot; alt=&quot;AMIEMx-8-RxQA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-8-RxQA.width-1250.png&quot; alt=&quot;AMIEMx-8-RxQA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;Example question from the RxQA benchmark, designed to assess medication knowledge and reasoning. All data shown is synthetic (realistic but not real) patient data.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-9-RxQAResults.width-1250.png&quot; alt=&quot;AMIEMx-9-RxQAResults&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-9-RxQAResults.width-1250.png&quot; alt=&quot;AMIEMx-9-RxQAResults&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE achieved strong performance on the RxQA benchmark, demonstrating a robust understanding of medication information and guidelines. The dotted line represents accuracy achievable through random guessing.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Limitations&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;While these results showcase AMIE&#39;s potential in a new and important area for medical applications of AI, several limitations warrant consideration. The simulated OSCE scenarios, while valuable for standardized evaluation, intentionally simplify the complexities of real-world clinical practice, which includes chart review, interaction with an electronic health record, and a far broader range of patients and pathologies. In this evaluation, guidelines from a single health system were selected and no attempts were made to adapt them to local contexts, whereas that ability is one of the potential benefits of AMIE. The short intervals between simulated visits and a text-based interface, unlike the multimodal experience of real telehealth, likely underestimated real-world difficulty. The MXEKF scale, though promising as a pilot assessment rubric, requires further validation.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;AMIE&#39;s strong performance across these evaluations represents a significant step towards demonstrating the potential of conversational AI as a powerful tool to assist physicians in disease management. By combining longitudinal reasoning, clinical guideline grounding, and multi-agent system design, AMIE demonstrates the “art of the possible” for AI systems beyond differential diagnosis, towards longitudinal management.&lt;/p&gt;&lt;p data-block-key=&quot;867j9&quot;&gt;Further research is needed before real-world translation to better understand potential impacts of AMIE on clinical workflow and patient outcomes as well as the safety and reliability of the system under real-world constraints. &lt;a href=&quot;https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/&quot;&gt;We are already embarking on a prospective research study with our clinical partners&lt;/a&gt;. However, this work is an important milestone in the responsible development and the potential of AI to improve access to evidence-based care.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;&lt;i&gt;The research described here is joint work across many teams at Google Research and Google DeepMind. We are grateful to all our co-authors and would like to thank John Guilyard, Brian Gabriel and Jenn Sturgeon for contributions to the narratives and visuals. We are grateful to our partners at BMJ Best Practice, the UK National Institute for Health and Care Excellence, and the Royal Pharmaceutical Society. Finally, we thank Avinatan Hassidim, Yossi Matias, James Manyika, Ewa Dominowska, Juro Gottweis, Katherine Chou, Claire Cui, Ali Eslami, Greg S. Corrado, Michael Howell, Karen DeSalvo, Jeff Dean, Zoubin Ghahramani and Demis Hassabis for their support during the course of this project.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/</link><guid isPermaLink="false">https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/</guid><pubDate>Wed, 05 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Health &amp; Bioscience</category></item><item><title>Discovering new words with confidential federated analytics</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;4tnnv&quot;&gt;Introduced by Google Research in 2020, &lt;a href=&quot;https://research.google/blog/federated-analytics-collaborative-data-science-without-data-collection/&quot;&gt;federated analytics&lt;/a&gt; allows for the application of data science methods to the analysis of raw data stored locally on users’ devices so that only the aggregated results — and not data from any particular device — are made available to product engineers. This keeps users&#39; data private, while enabling applications from &lt;a href=&quot;https://security.googleblog.com/2021/09/introducing-androids-private-compute.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;on-device intelligence features&lt;/a&gt;, to &lt;a href=&quot;https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/&quot;&gt;keyboard modeling&lt;/a&gt;, to &lt;a href=&quot;https://arxiv.org/abs/2412.07962&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;estimating carbon emissions&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;aguo2&quot;&gt;In traditional federated analytics, devices respond to queries by sending &lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;minimized&lt;/i&gt; messages&lt;/a&gt;. Raw data stays on the device while updates that focus on a particular purpose are sent to the server for immediate aggregation (either ephemerally, where the raw data are immediately deleted after aggregation, or via cryptographic &lt;a href=&quot;https://research.google/pubs/practical-secure-aggregation-for-federated-learning-on-user-held-data/&quot;&gt;secure aggregation&lt;/a&gt;). But today, users cannot inspect how their data is being aggregated, which may undermine their confidence that their sensitive data will be managed securely.&lt;/p&gt;&lt;p data-block-key=&quot;5m4u0&quot;&gt;To address this, we created &lt;a href=&quot;https://arxiv.org/abs/2404.10764&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;c&lt;i&gt;onfidential federated analytics&lt;/i&gt;&lt;/a&gt;, a technique that leverages &lt;a href=&quot;https://en.wikipedia.org/wiki/Confidential_computing&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential computing&lt;/a&gt; to improve data privacy by publicizing the analyses that will be performed using device data &lt;i&gt;before&lt;/i&gt; the data are uploaded, and by restricting data access to &lt;i&gt;only&lt;/i&gt; those analyses. With this approach, no other analyses can be performed on the data and no human can access data from individual devices. The resulting aggregated output of the analyses can offer strong anonymization guarantees to individuals. Any subversion of these properties, accidental or malicious, would be discoverable by third parties. These properties are made possible by (and subject to the correctness of) &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted execution environments&lt;/a&gt; (TEEs) built into modern CPUs from AMD, Intel, and others.&lt;/p&gt;&lt;p data-block-key=&quot;ci8ss&quot;&gt;With confidential federated analytics, for the first time ever, all privacy-relevant server-side software is now inspectable. This means that any organization using confidential federated analytics can now be completely transparent about the privacy properties of their data processing. This approach is an application of &lt;a href=&quot;https://arxiv.org/abs/2404.10764&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential federated computations&lt;/a&gt;, a more general framework that also applies to machine learning. The source code for confidential federated analytics is available now as part of &lt;a href=&quot;https://research.google/blog/parfait-enabling-private-ai-with-research-tools/&quot;&gt;Google Parfait&lt;/a&gt;, in the &lt;a href=&quot;https://github.com/google-parfait/federated-compute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential federated compute&lt;/a&gt; and &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted computations platform&lt;/a&gt; repositories. In our recent paper, “&lt;a href=&quot;https://arxiv.org/abs/2410.08892&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Federated Learning in Practice: Reflections and Projections&lt;/a&gt;”, we describe the relationship between confidential federated computation and traditional federated learning and analytics.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;How confidential federated analytics works&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;To illustrate how confidential federated analytics works, we describe its successful implementation in &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gboard&lt;/a&gt;, the popular Android keyboard app. Language evolves continuously, with new words and phrases gaining popularity as technological, cultural, and other changes shape the world. Users of phone keyboards like Gboard expect to easily type common words, even new ones that were unknown just a few months ago. This means that Gboard must have a way of discovering new common words to incorporate them into the typing model, without revealing any uncommon private words. Further, these new words should be discoverable across the 900+ languages that people type in Gboard every day, a problem &lt;a href=&quot;https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/&quot;&gt;we’ve previously studied with federated analytics&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;c27oa&quot;&gt;Our previous use of federated analytics for the problem used the &lt;a href=&quot;https://arxiv.org/abs/2404.11607&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LDP-TrieHH&lt;/a&gt; algorithm, which uses local differential privacy (LDP) that applies noise before upload to protect against a server peeking at individual device data. While LDP-TrieHH achieved a strong anonymization guarantee if the server does not peek, it achieves only a weak local DP guarantee (ε= 10 per device, per day) against servers that do. It requires weeks to run, and often fails to discover new words in languages with low volumes of users.&lt;/p&gt;&lt;p data-block-key=&quot;d72at&quot;&gt;With confidential federated analytics we can reach more devices, more languages, and discover more words. For example, we discovered 3,600 previously missing Indonesian words in just two days. And, because this approach allows the DP algorithm to be externally verified, it also offers a substantially improved DP guarantee (&lt;i&gt;ε&lt;/i&gt; = &lt;i&gt;ln&lt;/i&gt;(&lt;i&gt;3&lt;/i&gt;) per device, per week).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/ConfidentialFederatedAnalytics-1-Overview.width-1250.png&quot; alt=&quot;ConfidentialFederatedAnalytics-1-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/ConfidentialFederatedAnalytics-1-Overview.width-1250.png&quot; alt=&quot;ConfidentialFederatedAnalytics-1-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;4tnnv&quot;&gt;Like other federated analytics applications, the first step in confidential federated analytics consists of devices securely storing locally potential new words that have been typed but are not present in the on-device dictionary. Devices then encrypt their data and upload it to the server along with a set of processing steps that the server is authorized to use for decryption. For this, the devices use encryption keys managed by a central service, called a &lt;i&gt;ledger&lt;/i&gt;. The ledger releases decryption keys &lt;i&gt;only&lt;/i&gt; to those device-approved processing steps that must also run in a properly configured TEE.&lt;/p&gt;&lt;p data-block-key=&quot;a1lnr&quot;&gt;In this case, the data processing steps implement a differentially private algorithm designed to identify the most frequently occurring items within a dataset while simultaneously protecting the privacy of individual data points by adding noise. The differential privacy (DP) guarantee ensures that the algorithm’s top words list cannot be too heavily influenced by any one device, and is considered a gold standard for formal data anonymization.&lt;/p&gt;&lt;p data-block-key=&quot;2iu97&quot;&gt;For a device to know that the decryption keys are protected by a ledger, it must leverage multiple TEE properties (in our case, via Google’s &lt;a href=&quot;https://github.com/project-oak/oak&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Project Oak&lt;/a&gt; attestation stack running on &lt;a href=&quot;https://www.amd.com/en/developer/sev.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AMD SEV-SNP&lt;/a&gt;). When an application runs in a TEE, it gains &lt;i&gt;confidentiality&lt;/i&gt; (i.e., its internal state is secret, even from system administrators) and &lt;i&gt;integrity&lt;/i&gt; (system administrators cannot force the application to deviate from its correct operation). The TEE also provides &lt;i&gt;cryptographic identity&lt;/i&gt; unique to each chip, which can be used to attest to the exact state of the firmware, operating system, and software running on the CPU. The ledger creates public–private key pairs, signing the public key with a signing key tied to the TEE&#39;s attestation. This signature, along with the TEE properties listed above, allows devices to verify that they are using a public key managed by &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute/tree/main/ledger_enclave_app&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;our ledger implementation&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;b213k&quot;&gt;The processing steps that a device approves at the time of upload are named in a structured &lt;i&gt;access policy&lt;/i&gt; associated with the uploaded data and enforced by the ledger. For our use case above, the goal is to run a differentially private discovery algorithm. Specifically, we use a &lt;a href=&quot;https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;stability-based histogram algorithm&lt;/a&gt; for the discovery of frequently typed words and their approximate counts. The algorithm works by adding noise to the counts of words in the aggregated histogram, keeping only the words with counts above a target threshold, illustrated below. The &lt;a href=&quot;https://github.com/google-parfait/tensorflow-federated/blob/b35e8d3eac8536534b6c56996001cf756d953b27/tensorflow_federated/cc/core/impl/aggregation/core/dp_group_by_factory.cc&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;algorithm implementation is published&lt;/a&gt; as open source software.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/Confidential_federatedAnalytics-2-Workflow.width-1250.png&quot; alt=&quot;Confidential federatedAnalytics-2-Workflow&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/Confidential_federatedAnalytics-2-Workflow.width-1250.png&quot; alt=&quot;Confidential federatedAnalytics-2-Workflow&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;4tnnv&quot;&gt;The &lt;i&gt;access policy&lt;/i&gt; that corresponds to the implementation of this algorithm authorizes a two-stage &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute/tree/bdbb8d167508160aa0d94991172dcf1c78ab777d/containers/fed_sql&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;data processing pipeline&lt;/a&gt;. The first stage pre-processes each client&#39;s data using per-client SQL queries to limit the number of words a client can contribute before performing a partial aggregation. The second performs final aggregation, DP noising, and thresholding. Only when the pipeline&#39;s differentially private output has been fully computed is an un-encrypted and fully anonymous result released to the data analyst.&lt;/p&gt;&lt;p data-block-key=&quot;aunf3&quot;&gt;Thus, with correctly implemented ledger and data processing steps, devices can upload data to be processed &lt;i&gt;only&lt;/i&gt; as part of a device-approved, privacy-preserving pipeline, and have confidence that it cannot be used for any other purpose. Of course, these guarantees are subject to the strengths and limitations of current generation TEEs, an area with &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3634737.3644993&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;risks&lt;/a&gt; that include side-channel attacks. Further, attacks and &lt;a href=&quot;https://eprint.iacr.org/2024/936&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;algorithmic defenses&lt;/a&gt; against them are ongoing areas of research.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Scaling confidential federated analytics&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;Applying confidential federated computations to many applications and many devices over long periods of time requires additional design considerations. For example, if the ledger TEE crashes, all datasets and intermediate results protected by that ledger will be lost (no decryption keys).&lt;/p&gt;&lt;p data-block-key=&quot;8b3e9&quot;&gt;To mitigate this single-point-of-failure, we implement our ledger as a &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform/blob/main/confidential_replicated_state_machines.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;replicated state machine&lt;/a&gt; via code hosted in our &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Trusted Computations Platform&lt;/a&gt; repository, using an industry-proven consensus protocol (&lt;a href=&quot;https://raft.github.io/raft.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Raft&lt;/a&gt;) for fault-tolerance. By modeling the ledger as a replicated state machine, we also guarantee that uploaded data cannot be analyzed repeatedly by the same data processing pipeline. Such re-analyses can undermine an algorithm’s differential privacy guarantee, for example, by averaging out noise across multiple runs.&lt;/p&gt;&lt;p data-block-key=&quot;3kvdd&quot;&gt;By publishing the access policies and the TEE binaries they reference, and by publishing the ledger binaries that enforce these policies, we hope to set a new standard for external verifiability for our privacy-preserving technologies. We publish signatures to a publicly readable, immutable, and tamper-resistant transparency log (&lt;a href=&quot;https://docs.sigstore.dev/logging/overview/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Rekor&lt;/a&gt;), and before uploading any data, devices check that the ledger attestation evidence and applicable access policy have a Rekor inclusion proof. This ensures that uploaded device data can only ever be processed by binaries for which the source code and configuration are externally inspectable. We&#39;ve published a &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute/blob/main/docs/inspecting_endorsements.md&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;step-by-step guide to inspecting attestation verification records&lt;/a&gt;, and we invite external researchers to join us in evaluating and improving these claims.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;We’re excited to have shared these initial steps toward scalable and privacy-preserving confidential federated computations by describing systems running in production on real devices today and providing more external verifiability of the end-to-end system than ever before.&lt;/p&gt;&lt;p data-block-key=&quot;7gq8d&quot;&gt;Going beyond analytics use cases, we expect to soon apply the confidential federated computations technique to Gboard&#39;s &lt;a href=&quot;https://arxiv.org/abs/2410.08892&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;model learning use cases&lt;/a&gt; as well. Furthermore, confidential federated computations promise to further strengthen the privacy guarantees offered by federated learning and analytics in many Google apps and systems, including those in Android &lt;a href=&quot;https://security.googleblog.com/2021/09/introducing-androids-private-compute.html#:~:text=We%20introduced%20Android&amp;amp;#x27;s%20Private%20Compute,re%20having%20in%20messaging%20apps&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Private Compute Core&lt;/a&gt; and &lt;a href=&quot;https://security.googleblog.com/2024/08/android-private-ai-approach.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Android’s approach to private AI&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;&lt;i&gt;The authors would like to thank Dzmitry Huba, Hubert Eichner, Kallista Bonawitz, Mark Simborg, Peter Kairouz, Prem Eruvbetine, Sarah de Haas for their extensive feedback and editing on the blog post itself, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. In particular, we would like to thank the collaborators who directly contributed to this effort: Adria Gascon, Albert Cheu, Allie Culp, Andri Saar, Artem Lagzdin, Brendan McMahan, Brett McLarnon, Chloé Kiddon, Chunxiang Zheng, Conrad Grobler, Edo Roth, Emily Glanz, Ernesto Ocampo, Grace Ni, Haicheng Sun, Ivan Petrov, Jeremy Gillula, Jianpeng Hou, Joe Woodworth, Juliette Pluto, Katharine Daly, Katsiaryna Naliuka, Marco Gruteser, Maya Spivak, Mira Holford, Nova Fallen, Octavian Suciu, Rakshita Tandon, Shumin Zhai, Stanislav Chiknavaryan, Stefan Dierauf, Steve He, Tiziano Santoro, Tom Binder, Ulyana Kurylo, Wei Huang, Yanxiang Zhang, Yu Xiao, Yuanbo Zhang, Zachary Charles, Zheng Xu, Zhimin Yao, and Zoe Gong. This work was supported by Corinna Cortes, Four Flynn, Blaise Aguera y Arcas, and Yossi Matias.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/</link><guid isPermaLink="false">https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/</guid><pubDate>Mon, 03 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Mobile Systems</category><category>Security, Privacy and Abuse Prevention</category><category>Software Systems &amp; Engineering</category></item><item><title>Mind the GAP: Geometry Aware Passthrough mitigates cybersickness</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;5k9m1&quot;&gt;Virtual reality (VR) headsets are becoming increasingly popular, especially in environments that benefit from immersion, such as gaming and education. In part, they accomplish this by restricting the user&#39;s perception to the virtual world that they are projecting. Yet many headsets, such as &lt;a href=&quot;https://www.meta.com/quest&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Meta Quest&lt;/a&gt;, &lt;a href=&quot;https://www.apple.com/apple-vision-pro&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Apple Vision Pro&lt;/a&gt;, and &lt;a href=&quot;https://news.samsung.com/global/unlock-the-infinite-possibilities-of-xr-with-galaxy-ai&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Samsung Moohan&lt;/a&gt;, also give the users the option to view their real-world surroundings. These video see-through (VST) headsets accomplish this with a passthrough system that uses world-facing cameras.&lt;/p&gt;&lt;p data-block-key=&quot;7k93l&quot;&gt;As the adoption of VST devices increases, so does the need to address the discomfort and cybersickness experienced by some users of VST technology. While motion sickness in VR has been extensively studied in the last few decades, there is limited work dedicated to enhancing comfort and safety with the use of VST headsets or other augmented reality (AR) devices. Insights from VR research can be informative, but the unique experience of VST, wherein users can see and interact with the physical world, warrants dedicated investigation to guide the design of VST head-mounted displays (HMD) that are comfortable for users.&lt;/p&gt;&lt;p data-block-key=&quot;2utai&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2502.11497&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Mind the GAP: Geometry Aware Passthrough Mitigates Cybersickness&lt;/a&gt;” (to be presented at &lt;a href=&quot;https://chi2025.acm.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CHI 2025&lt;/a&gt;), we demonstrate the potential of GAP systems to mitigate cybersickness through accurate depth perception. We propose a protocol to quantitatively measure cybersickness experienced by users in VST headsets. Using this protocol, we conduct a user study to compare DP and GAP systems. To the best of our knowledge, our study is the first one to reveal significantly reduced nausea, disorientation, and overall scores of cybersickness with GAP. It also uncovers several potential avenues to further mitigate visually-induced discomfort.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Geometry Aware Passthrough&lt;/h2&gt;&lt;p data-block-key=&quot;c83k9&quot;&gt;Due to inherent hardware limitations common in VST headsets, the camera&#39;s perspective deviates from the user&#39;s natural viewpoint. Direct passthrough (DP) delivers the raw camera feed to the display, which exaggerates distances and consequently the motion of objects. Compared to natural vision (NV), DP results in visual artifacts such as disocclusion, inaccurate perception of object positions, and exaggerated motion parallax. According to the &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/2178753/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;sensory-conflict theory&lt;/a&gt;, such a mismatch between visual and inertial cues can potentially cause discomfort.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap1-ComparisonHERORevised.width-1250.png&quot; alt=&quot;MindTheGap1-ComparisonHERORevised&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap1-ComparisonHERORevised.width-1250.png&quot; alt=&quot;MindTheGap1-ComparisonHERORevised&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;n8w9i&quot;&gt;&lt;i&gt;DP delivers mismatched visual cues compared to the natural vision.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;5k9m1&quot;&gt;To mitigate this mismatch between DP and natural vision, past research has focused on utilizing geometry information to reproject camera feeds into the natural view from the eyes. We introduce the term &lt;i&gt;geometry aware passthrough&lt;/i&gt; (GAP) to describe these passthrough systems. While previous work assumes that a GAP reduces discomfort compared to DP, to the best of our knowledge, no empirical studies have directly investigated or verified this assumption.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap2-OutputRevised.width-1250.jpg&quot; alt=&quot;MindTheGap2-OutputRevised&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap2-OutputRevised.width-1250.jpg&quot; alt=&quot;MindTheGap2-OutputRevised&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3djm5&quot;&gt;&lt;i&gt;DP versus GAP output images.Two images are shown above taken from the headset placed at the same point in the scene.We observe that DP enlarges all the objects, making the scene look closer to the user.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;3npxc&quot;&gt;Geometrical accuracy and distortion detection&lt;/h2&gt;&lt;p data-block-key=&quot;f4v8n&quot;&gt;Our analysis also involves a rigorous quantitative assessment of the geometrical accuracy and warping introduced by each system. This includes evaluating how accurately each system represents real-world geometry (position and scale of objects) within the AR environment and whether distortions get introduced in the passthrough video feed (as shown in the figure above). More details are in &lt;a href=&quot;https://arxiv.org/abs/2502.11497&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;the paper&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;User study design&lt;/h2&gt;&lt;p data-block-key=&quot;7ug87&quot;&gt;We introduce a comprehensive protocol focused on key VST use cases to holistically assess visually-induced discomfort and cybersickness in VST HMDs. We then use this to compare our GAP algorithm to DP.&lt;/p&gt;&lt;p data-block-key=&quot;jevh&quot;&gt;To achieve reproducibility, repeatability, and real-life relevance, we began with tasks identified in the literature, tested them in a pilot study, and iteratively refined the task nature and duration based on participant feedback. A total of 25 consenting participants with normal or corrected-to-normal vision completed the tasks for the study. Each participant experienced all conditions including NV, DP, and GAP, allowing for a direct comparison of each participant’s experience.&lt;/p&gt;&lt;p data-block-key=&quot;66h7e&quot;&gt;We devised our protocol focusing exclusively on passthrough-based real-world interactions and ensured no virtual elements were visible to participants. The tasks were inspired from fundamental real-world XR applications such as working with laptops for productivity, navigation in the physical world, and interaction with real-world objects. They emphasized user head motion while necessitating inspection and spatial awareness of the physical world. Specifically:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;50ghi&quot;&gt;&lt;i&gt;Typing&lt;/i&gt;: This task was chosen to reflect emerging applications in productivity and to effectively engage both visual and motor components. Participants typed on a physical &lt;a href=&quot;https://en.wikipedia.org/wiki/Dvorak_keyboard_layout&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Dvorak keyboard&lt;/a&gt;, chosen because it required frequent gaze shifts between the keyboard and a laptop screen.&lt;/li&gt;&lt;li data-block-key=&quot;7baga&quot;&gt;&lt;i&gt;Navigation&lt;/i&gt;: The realistic and holistic use of a VST HMD involves navigating physical spaces, avoiding obstacles, and interacting with real-world objects. We designed a navigation task where participants collected and dropped off 10 numbered cones, one at a time, into a designated drop zone. This task emphasized geometry perception and required multi-directional movements.&lt;/li&gt;&lt;li data-block-key=&quot;62rro&quot;&gt;&lt;i&gt;Interaction&lt;/i&gt;: This task was designed to simulate common assembly tasks requiring both motor and cognitive skills. Participants assembled large 24-piece jigsaw puzzles by retrieving and working on only one batch of 8 puzzle pieces at a time within a rectangular frame marked on the table. The large puzzle size was selected to accommodate head motion which is often associated with motion sickness.&lt;/li&gt;&lt;/ul&gt;&lt;p data-block-key=&quot;9ma8l&quot;&gt;To measure cybersickness, we used the &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1207/s15327108ijap0303_3&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Simulator Sickness Questionnaire&lt;/a&gt; (SSQ), which categorizes symptoms along four subscales: nausea, disorientation, oculomotor, and total severity. Participants filled out the SSQ before and after each task to isolate the cybersickness experienced in each mode. In addition to the SSQ, participants rated their general discomfort on a scale of 0 to 10 after completing each task. Finally, participants provided qualitative feedback on their experiences at the end of each task.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap3-SetupFinalRevised.width-1250.jpg&quot; alt=&quot;MindTheGap3-SetupFinalRevised&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap3-SetupFinalRevised.width-1250.jpg&quot; alt=&quot;MindTheGap3-SetupFinalRevised&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4txck&quot;&gt;&lt;i&gt;User study setup: Pictures of the lab setup for the three tasks completed by the participants while wearing the VST HMD.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Findings: GAP significantly reduces cybersickness&lt;/h2&gt;&lt;p data-block-key=&quot;88r54&quot;&gt;We found that GAP significantly reduced cybersickness compared to DP. Specifically, GAP led to lower scores in the nausea, disorientation, and total severity subscales of the SSQ (𝑝 &amp;lt; 0.05); lower discomfort scores across all tasks compared to DP, including typing (𝑝 = 0.046), navigation (𝑝 = 0.041), and interaction (𝑝 = 0.022); as well as significantly lower average discomfort scores (𝑝 = 0.016). In line with previous work on VST, our results revealed a VST symptom profile for cybersickness that is distinguished from other types of motion sickness. The most reported symptoms for both DP and GAP were sweating, eyestrain, general discomfort, and headache.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap4-Results1.width-1250.png&quot; alt=&quot;MindTheGap4-Results1&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap4-Results1.width-1250.png&quot; alt=&quot;MindTheGap4-Results1&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap5-Results2.width-1250.png&quot; alt=&quot;MindTheGap5-Results2&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap5-Results2.width-1250.png&quot; alt=&quot;MindTheGap5-Results2&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ohqit&quot;&gt;&lt;i&gt;SSQ results shown as box plots of nausea, oculomotor, disorientation, and total subscores of simulator sickness comparing all conditions.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap6-Results3.width-1250.png&quot; alt=&quot;MindTheGap6-Results3&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap6-Results3.width-1250.png&quot; alt=&quot;MindTheGap6-Results3&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap7-Results4.width-1250.png&quot; alt=&quot;MindTheGap7-Results4&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap7-Results4.width-1250.png&quot; alt=&quot;MindTheGap7-Results4&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ohqit&quot;&gt;&lt;i&gt;Subjective discomfort results shown as box plots of discomfort scores and preference for all conditions across the typing, navigation, and interaction tasks.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;5k9m1&quot;&gt;Participants also generally preferred GAP over DP subjectively. Several participants reported a mismatch between vision and motion with DP, aligning with the sensory-conflict theory. This was frequently mentioned in the context of head motion, particularly during the interaction task. DP caused impaired spatial awareness compared to GAP, and participants noted that DP caused them to move closer to objects due to inaccurate depth cues. Some users also experienced unstable gait and collisions with furniture while using DP. While most participants preferred GAP, some expressed a preference for DP for certain tasks such as typing due to the warping artifacts of GAP on the keyboard. However, some participants adapted to these artifacts over time.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Implications for future VST design&lt;/h2&gt;&lt;p data-block-key=&quot;70jig&quot;&gt;Our findings imply that VST HMD design should incorporate GAP to improve user comfort and facilitate the adoption of VST technology. However, GAP introduces additional computational demands compared to DP. Future design efforts should strive for a balance that maintains the benefits of GAP while minimizing the impact on system performance and latency.&lt;/p&gt;&lt;p data-block-key=&quot;2hiqv&quot;&gt;Participant feedback also helped identify several areas for future research into enhancing comfort in VST. Four issues emerged: frame drops, overexposed images, latency, and blurry vision. Several participants reported that slight delays when moving their heads caused nausea and discomfort. This suggests the need for more research to understand the impact of these factors on mitigating user discomfort and cybersickness.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Conclusion&lt;/h2&gt;&lt;p data-block-key=&quot;cune&quot;&gt;Mind the GAP is the first work that demonstrates that GAP significantly reduces nausea, disorientation, and total scores of cybersickness as well as subjective discomfort scores as compared to DP. We present a comprehensive protocol aimed at evaluating visually-induced discomfort and cybersickness in VST HMDs through key use cases. We hope that our comprehensive protocol sets a foundation for future studies aimed at refining these systems and enhancing user comfort in VST technologies.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Acknowledgments&lt;/h2&gt;&lt;p data-block-key=&quot;15h3f&quot;&gt;&lt;i&gt;This research was conducted by Trishia El Chemaly during her time at Google as a Student Researcher, Mohit Goyal, Tinglin Duan, Vrushank Phadnis, Sakar Khattar, Bjorn Vlaskamp, Achin Kulshrestha, Eric Lee Turner, Aveek Purohit, Gregory Neiswander, and Konstantine Tsotsos. We would also like to thank Abhishek Kar for his guidance and help during the ideation of this work.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/mind-the-gap-geometry-aware-passthrough-mitigates-cybersickness/</link><guid isPermaLink="false">https://research.google/blog/mind-the-gap-geometry-aware-passthrough-mitigates-cybersickness/</guid><pubDate>Thu, 27 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Human-Computer Interaction and Visualization</category><category>Machine Perception</category></item><item><title>Accelerating scientific breakthroughs with an AI co-scientist</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;In the pursuit of scientific advances, researchers combine ingenuity and creativity with insight and expertise grounded in literature to generate novel and viable research directions and to guide the exploration that follows. In many fields, this presents a breadth and depth conundrum, since it is challenging to navigate the rapid growth in the rate of scientific publications while integrating insights from unfamiliar domains. Yet overcoming such challenges is critical, as evidenced by the many modern breakthroughs that have emerged from transdisciplinary endeavors. For example, Emmanuelle Charpentier and Jennifer Doudna won the &lt;a href=&quot;https://www.nobelprize.org/uploads/2020/10/popular-chemistryprize2020.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;2020 Nobel Prize in Chemistry&lt;/a&gt; for their work on &lt;a href=&quot;https://en.wikipedia.org/wiki/CRISPR&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CRISPR&lt;/a&gt;, which combined expertise ranging from microbiology to genetics to molecular biology.&lt;/p&gt;&lt;p data-block-key=&quot;bngi7&quot;&gt;Motivated by unmet needs in the modern scientific discovery process and building on &lt;a href=&quot;https://arxiv.org/abs/2403.05530&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;recent AI advances&lt;/a&gt;, including the ability to synthesize across complex subjects and to perform &lt;a href=&quot;https://deepmind.google/technologies/gemini/flash-thinking/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;long-term planning and reasoning&lt;/a&gt;, we developed an &lt;a href=&quot;https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AI co-scientist system&lt;/a&gt;. The AI co-scientist is a multi-agent AI system that is intended to function as a collaborative tool for scientists. Built on &lt;a href=&quot;https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemini 2.0, AI co-scientist is&lt;/a&gt; designed to mirror the reasoning process underpinning the scientific method. Beyond standard literature review, summarization and “deep research” tools, the AI co-scientist system is intended to uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and tailored to specific research objectives.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Empowering scientists and accelerating discoveries with the AI co-scientist&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;Given a scientist’s research goal that has been specified in natural language, the AI co-scientist is designed to generate novel research hypotheses, a detailed research overview, and experimental protocols. To do so, it uses a coalition of specialized agents — &lt;i&gt;Generation&lt;/i&gt;, &lt;i&gt;Reflection&lt;/i&gt;, &lt;i&gt;Ranking&lt;/i&gt;, &lt;i&gt;Evolution&lt;/i&gt;, &lt;i&gt;Proximity&lt;/i&gt; and &lt;i&gt;Meta-review&lt;/i&gt; — that are inspired by the scientific method itself. These agents use automated feedback to iteratively generate, evaluate, and refine hypotheses, resulting in a self-improving cycle of increasingly high-quality and novel outputs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/AICoScientist-0-Hero.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;hvksh&quot;&gt;&lt;i&gt;AI co-scientist overview.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;j3yy8&quot;&gt;Purpose-built for collaboration, scientists can interact with the system in many ways, including by directly providing their own seed ideas for exploration or by providing feedback on generated outputs in natural language. The AI co-scientist also uses tools, like web-search and specialized AI models, to enhance the grounding and quality of generated hypotheses.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-1-Components.width-1250.png&quot; alt=&quot;AICoScientist-1-Components&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-1-Components.width-1250.png&quot; alt=&quot;AICoScientist-1-Components&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Illustration of the different components in the AI co-scientist multi-agent system and the interaction paradigm between the system and the scientist.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;The AI co-scientist parses the assigned goal into a research plan configuration, managed by a Supervisor agent. The Supervisor agent assigns the specialized agents to the worker queue and allocates resources. This design enables the system to flexibly scale compute and to iteratively improve its scientific reasoning towards the specified research goal.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-2-Overview.width-1250.png&quot; alt=&quot;AICoScientist-2-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-2-Overview.width-1250.png&quot; alt=&quot;AICoScientist-2-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;AI co-scientist system overview. Specialized agents (&lt;/i&gt;&lt;b&gt;&lt;i&gt;red boxes&lt;/i&gt;&lt;/b&gt;&lt;i&gt;, with unique roles and logic); scientist input and feedback (&lt;/i&gt;&lt;b&gt;&lt;i&gt;blue boxes&lt;/i&gt;&lt;/b&gt;&lt;i&gt;); system information flow (&lt;/i&gt;&lt;b&gt;&lt;i&gt;dark gray arrows&lt;/i&gt;&lt;/b&gt;&lt;i&gt;); inter-agent feedback (&lt;/i&gt;&lt;b&gt;&lt;i&gt;red arrows&lt;/i&gt;&lt;/b&gt;&lt;i&gt; within the agent section).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Scaling test-time compute for advanced scientific reasoning&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;The AI co-scientist leverages &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;test-time compute&lt;/a&gt; scaling to iteratively reason, evolve, and improve outputs. Key reasoning steps include &lt;a href=&quot;https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;self-play&lt;/a&gt;–based scientific debate for novel hypothesis generation, ranking tournaments for hypothesis comparison, and an &quot;evolution&quot; process for quality improvement. The system&#39;s agentic nature facilitates recursive self-critique, including tool use for feedback to refine hypotheses and proposals.&lt;/p&gt;&lt;p data-block-key=&quot;7ufde&quot;&gt;The system&#39;s self-improvement relies on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Elo_rating_system&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Elo&lt;/a&gt; auto-evaluation metric derived from its tournaments. Due to their core role, we assessed whether higher Elo ratings correlate with higher output quality. We analyzed the concordance between Elo auto-ratings and &lt;a href=&quot;https://arxiv.org/abs/2311.12022&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;GPQA benchmark&lt;/a&gt; accuracy on its diamond set of challenging questions, and we found that higher Elo ratings positively correlate with a higher probability of correct answers.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-3-Elo.width-1250.png&quot; alt=&quot;AICoScientist-3-Elo&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-3-Elo.width-1250.png&quot; alt=&quot;AICoScientist-3-Elo&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Average accuracy of the AI co-scientist (blue line) and reference Gemini 2.0 (red line) responses on GPQA diamond questions, grouped by Elo rating. The Elo is an auto-evaluation and is not based on an independent ground truth.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;Seven domain experts curated 15 open research goals and best guess solutions in their field of expertise. Using the automated Elo metric we observed that the AI co-scientist outperformed other state-of-the-art agentic and reasoning models for these complex problems. The analysis reproduced the benefits of scaling test-time compute using inductive biases derived from the scientific method. As the system spends more time reasoning and improving, the self-rated quality of results improve and surpass models and unassisted human experts.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-4-BestHypothesis.width-1250.png&quot; alt=&quot;AICoScientist-4-BestHypothesis&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-4-BestHypothesis.width-1250.png&quot; alt=&quot;AICoScientist-4-BestHypothesis&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-5-Top10Hypothesis.width-1250.png&quot; alt=&quot;AICoScientist-5-Top10Hypothesis&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-5-Top10Hypothesis.width-1250.png&quot; alt=&quot;AICoScientist-5-Top10Hypothesis&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Performance of the AI co-scientist improves as the system spends more time in computation. This can be seen in the automated Elo metric gradually improving over other baselines.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Top:&lt;/i&gt;&lt;/b&gt; &lt;i&gt;Elo progression of the best rated hypothesis.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Bottom:&lt;/i&gt;&lt;/b&gt;&lt;i&gt; Elo progression of the average of top-10 hypotheses.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;On a smaller subset of 11 research goals, experts assessed the novelty and impact of the AI co-scientist–generated results compared to other relevant baselines; they also provided overall preference. While the sample size was small, experts assessed the AI co-scientist to have higher potential for novelty and impact, and preferred its outputs compared to other models. Further, these human expert preferences also appeared to be concordant with the previously introduced Elo auto-evaluation metric.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-6-Novelty.width-1250.png&quot; alt=&quot;AICoScientist-6-Novelty&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-6-Novelty.width-1250.png&quot; alt=&quot;AICoScientist-6-Novelty&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-7-Ranking.width-1250.png&quot; alt=&quot;AICoScientist-7-Ranking&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-7-Ranking.width-1250.png&quot; alt=&quot;AICoScientist-7-Ranking&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Human experts assessed the AI co-scientist results to have higher potential for novelty and impact (&lt;/i&gt;&lt;b&gt;&lt;i&gt;left&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) and preferred it compared to other models (&lt;/i&gt;&lt;b&gt;&lt;i&gt;right&lt;/i&gt;&lt;/b&gt;&lt;i&gt;).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Validation of novel AI co-scientist hypotheses with real-world laboratory experiments&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;To assess the practical utility of the system’s novel predictions, we evaluated end-to-end laboratory experiments probing the AI co-scientist–generated hypotheses and research proposals in three key biomedical applications: drug repurposing, proposing novel treatment targets, and elucidating the mechanisms underlying antimicrobial resistance. These settings all involved expert-in-the-loop guidance and spanned an array of complexities:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block both --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-11-Table.width-1250.png&quot; alt=&quot;AICoScientist-11-Table&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-11-Table.width-1250.png&quot; alt=&quot;AICoScientist-11-Table&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Drug repurposing for acute myeloid leukaemia&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;Drug development is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Eroom%27s_law&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;increasingly time-consuming and expensive process&lt;/a&gt; in which new therapeutics require many aspects of the discovery and development process to be restarted for each indication or disease. Drug repurposing addresses this challenge by discovering new therapeutic applications for existing drugs beyond their original intended use. But, due to the complexity of the task, it demands extensive interdisciplinary expertise.&lt;/p&gt;&lt;p data-block-key=&quot;d494o&quot;&gt;We applied the AI co-scientist to assist with the prediction of drug repurposing opportunities and, with our partners, validated predictions through computational biology, expert clinician feedback, and &lt;i&gt;in vitro&lt;/i&gt; experiments.&lt;/p&gt;&lt;p data-block-key=&quot;18gc5&quot;&gt;Notably, the AI co-scientist proposed novel repurposing candidates for &lt;a href=&quot;https://en.wikipedia.org/wiki/Acute_myeloid_leukemia&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;acute myeloid leukemia&lt;/a&gt; (AML). Subsequent experiments validated these proposals, confirming that the suggested drugs inhibit tumor viability at clinically relevant concentrations in multiple AML cell lines.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-8-DoseResponse.width-1250.png&quot; alt=&quot;AICoScientist-8-DoseResponse&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-8-DoseResponse.width-1250.png&quot; alt=&quot;AICoScientist-8-DoseResponse&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;a href=&quot;https://www.merckmanuals.com/professional/clinical-pharmacology/pharmacodynamics/dose-response-relationships&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Dose-response curves&lt;/i&gt;&lt;/a&gt;&lt;i&gt; of one of the three novel AI co-scientist–predicted AML repurposing drugs. KIRA6 inhibits KG-1 (AML cell line) viability at clinically relevant concentrations. Being able to reduce cancer cell viability at lower drug concentrations is advantageous for multiple reasons, e.g., as it reduces the potential for off-target side effects.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Advancing target discovery for liver fibrosis&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;Identifying novel treatment targets is more complex than drug repurposing, and often leads to inefficient hypothesis selection and poor prioritization for &lt;i&gt;in vitro&lt;/i&gt; and &lt;i&gt;in vivo&lt;/i&gt; experiments. AI-assisted target discovery helps to streamline the process of experimental validation, potentially helping to reduce development time costs.&lt;/p&gt;&lt;p data-block-key=&quot;8k4au&quot;&gt;We probed the AI co-scientist system&#39;s ability to propose, rank, and generate hypotheses and experimental protocols for target discovery hypotheses, focusing on &lt;a href=&quot;https://pmc.ncbi.nlm.nih.gov/articles/PMC546435/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;liver fibrosis&lt;/a&gt;. The AI co-scientist demonstrated its potential by identifying epigenetic targets grounded in preclinical evidence with significant anti-fibrotic activity in &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/28878125/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;human hepatic organoids&lt;/a&gt; (3D, multicellular tissue cultures derived from human cells and designed to mimic the structure and function of the human liver). These findings will be detailed in an upcoming report led by collaborators at Stanford University.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-9a-LiverFibrosis.width-1250.png&quot; alt=&quot;AICoScientist-9a-LiverFibrosis&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-9a-LiverFibrosis.width-1250.png&quot; alt=&quot;AICoScientist-9a-LiverFibrosis&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Comparison of treatments derived from AI co-scientist–suggested liver fibrosis targets versus a fibrosis inducer (negative control) and an inhibitor (positive control). All treatments suggested by AI co-scientist show promising activity (p-values for all suggested drugs are &amp;lt;0.01), including candidates that possibly reverse a disease phenotype. Results are detailed in an upcoming report from our Stanford University collaborators.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Explaining mechanisms of antimicrobial resistance&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;As a third validation, we focused on generating hypotheses to explain bacterial gene transfer evolution mechanisms related to antimicrobial resistance (AMR) — microbes&#39; evolved mechanisms to resist infection-treating drugs. This is another complex challenge that involves understanding the molecular mechanisms of gene transfer (&lt;a href=&quot;https://www.nature.com/scitable/definition/conjugation-prokaryotes-290/#:~:text=Conjugation%20is%20the%20process%20by,factor%2C%20or%20F%2Dfactor.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;conjugation&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Transduction_(genetics)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;transduction&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_transformation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;transformation&lt;/a&gt;) alongside the ecological and evolutionary pressures that drive AMR genes to spread.&lt;/p&gt;&lt;p data-block-key=&quot;9a67g&quot;&gt;For this test, expert researchers instructed the AI co-scientist to explore a topic that had already been subject to novel discovery in their group, but had not yet been revealed in the public domain, namely, to explain how &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/36596306/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;capsid-forming phage-inducible chromosomal islands&lt;/a&gt; (cf-PICIs) exist across multiple bacterial species. The AI co-scientist system independently proposed that cf-PICIs interact with diverse phage tails to expand their host range. This&lt;i&gt; in silico&lt;/i&gt; discovery, which had been experimentally validated in the original novel laboratory experiments performed prior to use of the AI co-scientist system, are described in co-timed manuscripts (&lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2025.02.11.637232v1&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://storage.googleapis.com/coscientist_paper/penades2025ai.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;2&lt;/a&gt;) with our collaborators at the Fleming Initiative and Imperial College London. This illustrates the value of the AI co-scientist system as an assistive technology, as it was able to leverage decades of research comprising all prior open access literature on this topic.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-10-RediscoveryTimeline.width-1250.png&quot; alt=&quot;AICoScientist-10-RediscoveryTimeline&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-10-RediscoveryTimeline.width-1250.png&quot; alt=&quot;AICoScientist-10-RediscoveryTimeline&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Timeline of AI co-scientist re-discovery of a novel gene transfer mechanism.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Blue:&lt;/i&gt;&lt;/b&gt;&lt;i&gt; Experimental research pipeline timeline for cf-PICI mobilization discovery.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Red:&lt;/i&gt;&lt;/b&gt; &lt;i&gt;AI co-scientist development and recapitulation of these key findings (without prior knowledge).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Limitations and outlook&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;In our report we address several limitations of the system and opportunities for improvement, including enhanced literature reviews, factuality checking, cross-checks with external tools, auto-evaluation techniques, and larger-scale evaluation involving more subject matter experts with varied research goals. The AI co-scientist represents a promising advance toward AI-assisted technologies for scientists to help accelerate discovery. Its ability to generate novel, testable hypotheses across diverse scientific and biomedical domains — some already validated experimentally — and its capacity for recursive self-improvement with increased compute, demonstrate its potential to accelerate scientists&#39; efforts to address grand challenges in science and medicine. We look forward to responsible exploration of the potential of the AI co-scientist as an assistive tool for scientists. This project illustrates how collaborative and human-centred AI systems might be able to augment human ingenuity and accelerate scientific discovery.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Announcing Trusted Tester access to the AI co-scientist system&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;We are excited by the early promise of the AI co-scientist system and believe it is important to evaluate its strengths and limitations in science and biomedicine more broadly. To facilitate this responsibly we will be enabling access to the system for research organizations through a Trusted Tester Program. We encourage interested research organizations around the world to consider joining this program &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSdvw_8IPrc8O7ZM8FKF46i8BnOYMeSeyLeBNiuk_yGWIlnxYA/viewform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;&lt;i&gt;The research described here is a joint effort between many Google Research, Google Deepmind and Google Cloud AI teams. We thank our co-authors at Fleming Initiative and Imperial College London, Houston Methodist Hospital, Sequome, and Stanford University — José R Penadés, Tiago R D Costa, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Jacob Blum and Gary Peltz. We appreciate Subhashini Venugopalan and Yun Liu for their detailed feedback on the manuscripts described here. We are also grateful to the many incredible scientists across institutions providing detailed technical and expert feedback — please refer to our report to see the voices and minds that aided this work. We also thank our teammates Resham Parikh, Taylor Goddu, Siyi Kou, Rachelle Sico, Amanda Ferber, Cat Kozlowski, Alison Lentz, KK Walker, Roma Ruparel, Jenn Sturgeon, Lauren Winer, Juanita Bawagan, Tori Milner, MK Blake, Kalyan Pamarthy for their support&lt;/i&gt;.&lt;i&gt; Finally, we also thank John Platt, Michael Brenner, Zoubin Ghahramani, Dale Webster, Joelle Barral, Michael Howell, Susan Thomas, Jason Freidenfelds, Karen DeSalvo, Vladimir Vuskovic, Greg Corrado, Ronit Levavi Morad, Ali Eslami, Anna Koivuniemi, Royal Hansen, Andy Berndt, Noam Shazeer, Oriol Vinyals, Burak Gokturk, Amin Vahdat, Katherine Chou, Avinatan Hassidim, Koray Kavukcuoglu, Pushmeet Kohli, Yossi Matias, James Manyika, Jeff Dean and Demis Hassabis for their support.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</link><guid isPermaLink="false">https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</guid><pubDate>Tue, 18 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Health &amp; Bioscience</category><category>Human-Computer Interaction and Visualization</category></item><item><title>Mechanism design for large language models</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;yptit&quot;&gt;Generative AI and &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;large language models&lt;/a&gt; (LLMs) facilitate the automated generation of assets across a variety of domains. For many use cases, several LLM agents may need to collaborate to create a joint output. A potential example is in the context of Internet ads, where advertisers might be represented by LLM agents capable of producing ads in reply to a user query. Or it could be that the LLMs represent stakeholders of a company, working together to write a joint report.&lt;/p&gt;&lt;p data-block-key=&quot;30q39&quot;&gt;Consider an example situation where there is a single space on a webpage to be filled with an ad creative in reply to a user searching for “Vacations in Hawaii”. Suppose there are two advertisers interested in this search query, Alpha Airlines and Beta Resort, each represented by an LLM agent. Each LLM agent is capable of producing an ad creative in reply to the search query. For example: “Fly to Hawaii with Alpha Airlines” and “Enjoy the beauty of Hawaii at Beta Resort”. However, in this case, a suitable auction design would be flexible enough to enable the creation of a joint ad creative, such as “Alpha Airlines flies you to Hawaii where you can enjoy a magic weeklong experience at Beta Resort”.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/MechanismDesign-1-Example.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;cnp6n&quot;&gt;&lt;i&gt;An illustrative example, consider separate LLM agents representing two hypothetical advertisers, Alpha Airlines and Beta Resort, which are tasked to collaborate to produce a joint ad creative.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;yptit&quot;&gt;In applications like this, each LLM agent has (potentially diverging) preferences for the joint output. For example, advertisers prefer to have their products or services mentioned, while also caring about different aspects that they wish to mention more prominently. Problems in which multiple agents collaborate to select a joint output naturally call for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Mechanism_design&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;mechanism design&lt;/a&gt; (i.e., auction design) approach.&lt;/p&gt;&lt;p data-block-key=&quot;7arve&quot;&gt;In “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3589334.3645511&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Mechanism Design for Large Language Models&lt;/a&gt;”, which won the &lt;a href=&quot;https://www2024.thewebconf.org/program/awards/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;WWW 2024 Best Paper Award&lt;/a&gt;, we argue that the problem of joint output generation through multiple LLM agents comes with several unique challenges, and we present a new class of auction mechanisms tailored to address the key challenges of this novel application domain. We describe theoretical results that inform the design of auctions&lt;footnote id=&quot;ab829e67-fc88-45a8-8024-95d624053bf5&quot;&gt;[ab829e]&lt;/footnote&gt; from that class, and we show how these insights lead to practical auction designs that yield promising outcomes when deployed with real-world LLMs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;The token auction model&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;At a high-level, our approach — which we dub the &lt;i&gt;token auction model&lt;/i&gt; — mimics the mechanics of LLMs as closely as possible. The token auction operates on a token-by-token basis and functions like one giant LLM, while also defining some payment function, monetary or otherwise. We illustrate the key ideas and concepts in the context of text creation, but the same ideas apply to the creation of other media types (such as images or videos). In this context, a token is simply an individual word, a sequence of words, punctuation mark, etc. There is also a special “end” token signifying the end of the creation process.&lt;/p&gt;&lt;p data-block-key=&quot;d3qvr&quot;&gt;Let’s first look at how an individual LLM works. On an abstract level, an LLM defines for any sequence of input tokens, a distribution over tokens. For example, the input sequence could be “Mechanism Design for”, and the output distribution could be [(“Large”, 0.8), (“Generative”, 0.2)], meaning that the next token should be “Large” with 80% probability and “Generative” with 20%. This functionality can then be used for so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;auto-regressive text generation&lt;/a&gt;. The process starts with an initial prompt, which is just a sequence of tokens. Then a token is sampled from the distribution over tokens defined by the LLM. This token is appended to the sequence being generated. Then the process is repeated with the resulting expanded sequence of tokens until the special “end” token is sampled.&lt;/p&gt;&lt;p data-block-key=&quot;dvcaq&quot;&gt;The token auction performs two key tasks: it expands the shared token sequence and determines agent payments. Both of these tasks are achieved through functions, which we refer to as the &lt;i&gt;distribution aggregation function&lt;/i&gt; and the &lt;i&gt;payment function&lt;/i&gt;. Both functions take as input the distributions of the individual LLMs and a bid by each agent. The distribution aggregation function maps this to a distribution over tokens, while the output of the payment function is a vector of payments.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-2-Architecture.width-1250.png&quot; alt=&quot;MechanismDesign-2-Architecture&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-2-Architecture.width-1250.png&quot; alt=&quot;MechanismDesign-2-Architecture&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;cnp6n&quot;&gt;&lt;i&gt;The token auction model architecture.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;yptit&quot;&gt;For example, in the situation illustrated in the figure above, the shared sequence of tokens might be “Mechanism Design for”. The distributions might be [(“Large”, 0.8), (“Generative”, 0.2)] for the LLM of Agent 1, (“Large”, 1.0) for the LLM of Agent 2, and (“Generative”, 1.0) for the LLM of Agent 3. The bids might be 1, 2, and 2, respectively. A possible aggregated distribution would be the bid-weighted average of the distributions, namely [(“Large”, 0.56), (“Generative”, 0.44)]. A possible choice for the payments would be to ask each agent to pay their bid, which would have the agents commit 1, 2, and 2, respectively.&lt;/p&gt;&lt;p data-block-key=&quot;aap6&quot;&gt;For our theoretical analysis of this model (and possible choices of distribution aggregation functions and payment functions), we assume that the agents truthfully report their distributions, but may be strategic about their bids. We believe this is a realistic assumption, as LLMs encode preferences over output text in a succinct and non-obvious way. Moreover, in order for the token auction to be able to aggregate distributions, we need to have (at least) some (minimal) information about agent’s preferences away from their “preferred” distributions. Our approach here is to assume that the agents have (known) &lt;a href=&quot;https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_orders&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;partial preference orders&lt;/a&gt; over distributions. That is, we assume that agents may be able to rank some, but not all, pairs of distributions.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Main technical results&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;We present a suite of theoretical results for the token auction model, focusing on a single step in the generation process.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Design space reduction&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;The first result is a “design space reduction”, which confines the general space of distribution aggregation functions to a much smaller sub-space. Inspired by successful practical auction designs, we formulate two desirable properties that a token-auction mechanism should satisfy, namely “payment monotonicity” and “consistent aggregation”, and show that these are equivalent to requiring monotonicity of the distribution aggregation function. Thus, we can restrict attention to monotone distribution aggregation functions, meaning that if an agent weakly increases their bid, the aggregated distribution function would only change to a distribution weakly preferred by the agent (see &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3589334.3645511&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;paper&lt;/a&gt; for details).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;“Second-price” payments&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;Next, we explore the design of payment rules. An important insight of auction theory is that second-price payments provide good incentives. In the context of a single-item auction, the idea is to give the item to the bidder with the highest bid, but make that bidder only pay the second-highest bid. As our second result, we show that (under some additional, natural assumptions) any monotone distribution aggregation function admits an analog of such second-price payments. For this we show that any such rule admits a &lt;i&gt;stable sampling&lt;/i&gt;, i.e., an implementation of the distribution aggregation function based on an explicit random seed that, for each seed, outputs only one of two tokens depending on whether the bid is below or above a certain threshold.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Optimal aggregation rules&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;As our final set of results, we explore the design of &lt;i&gt;optimal&lt;/i&gt; distribution aggregation functions. Taking inspiration from state-of-the-art LLM training, we formulate aggregated loss functions that associate with each output distribution a total loss that we seek to minimize. We consider two different formulations of the aggregated loss functions, and show that these correspond to two simple distribution aggregation functions. The first is a linear distribution aggregation function, which outputs the bid-weighted average of the distributions. The second is a log-linear distribution aggregation function, which does the same in log space.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Demonstration&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;We also conducted a series of experiments with our proposed design. For the demonstrations we used prompt tuning on an off-the-shelf LLM. That is, rather than taking a specifically trained model, we instructed the LLM with a prompt to act on behalf of a hypothetical advertiser.&lt;/p&gt;&lt;p data-block-key=&quot;91s33&quot;&gt;In the example discussed here, there are two advertisers, “Alpha Airlines” and “Beta Resort”, which want to advertise a flight with the airline and a stay at the hotel, respectively. (See the paper for the prompts that we used). Below we show the output of the token auction (parameterized by λ, the relative weight of the bid of Alpha Airlines). The first column shows the output for the linear aggregation rule, and the second column shows the output for the log-linear aggregation rule. In both cases, the behavior is as intended with ad creatives shifting from mentioning only Alpha Airlines, to mentioning both Alpha Airlines and Beta Resort, to only mentioning Beta Resort.&lt;/p&gt;&lt;p data-block-key=&quot;79ipn&quot;&gt;Additional demonstrations with different prompts, including settings with competing advertisers can be found in the paper.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-3-Example.width-1250.png&quot; alt=&quot;MechanismDesign-3-Example&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-3-Example.width-1250.png&quot; alt=&quot;MechanismDesign-3-Example&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;cnp6n&quot;&gt;&lt;i&gt;Output generated by the two distribution aggregation functions, as a function of the relative weight of the bid by Alpha Airlines.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;In this research, we designed a mechanism for aggregating LLM output. Our proposed format allows the LLM agents to influence the output through single-dimensional bids. The mechanism’s design makes minimal assumptions about the agents’ preferences. Working under this paradigm, we showed that the natural requirements on the auction mechanism’s incentive properties called for monotone aggregation. We then showed that under robust preferences, any monotone aggregation function enables second-price–style payments. As a “proof of concept” for our designed mechanism, we demonstrated promising outcomes of our aggregation methods by implementing these aggregation functions in a real-world state-of-the-art LLM using prompt tuning.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;&lt;i&gt;The work described in this blog post is joint work with Vahab Mirroni, Renato Paes Leme, and Haifeng Xu. We thank Dirk Bergemann, Marina Halac, Philipp Strack, Elliot Lipnowski, Yang Cai, Vasilis Syrgkanis, Negin Gorezaei, Ido Cohen, Yoav Nagel, Yael Shemesh as well as the participants of the Yale Economics Seminar, the Stanford MS&amp;amp;E Seminar, the WWW 2024 conference, and the INFORMS Revenue and Management Pricing Section Conference for invaluable comments and suggestions to improve this manuscript. We are especially grateful to Yong Cheng from Google DeepMind for his expert guidance on the LLM-related details and literature.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/mechanism-design-for-large-language-models/</link><guid isPermaLink="false">https://research.google/blog/mechanism-design-for-large-language-models/</guid><pubDate>Wed, 12 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Algorithms &amp; Theory</category><category>Economics &amp; Electronic Commerce</category><category>Generative AI</category></item><item><title>Building AI for the pluralistic society</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Modern artificial intelligence (AI) systems rely on input from people. Human feedback helps train models to perform useful tasks, guides them toward safe and responsible behavior, and is used to assess their performance. While hailing the recent AI advancements, we should also ask: &lt;i&gt;which humans&lt;/i&gt; are we actually talking about? For AI to be most beneficial, it should &lt;i&gt;reflect&lt;/i&gt; and &lt;i&gt;respect&lt;/i&gt; the diverse tapestry of values, beliefs, and perspectives present in the pluralistic world in which we live, not just a single &quot;average&quot; or majority viewpoint. Diversity in perspectives is especially relevant when AI systems perform subjective tasks, such as deciding whether a response will be perceived as helpful, offensive, or unsafe. For instance, what one value system deems as offensive may be perfectly acceptable within another set of values.&lt;/p&gt;&lt;p data-block-key=&quot;aiqvo&quot;&gt;Since divergence in perspectives often aligns with socio-cultural and demographic lines, preferentially capturing certain groups’ perspectives over others in data may result in disparities in how well AI systems serve different social groups. For instance, we previously &lt;a href=&quot;https://aclanthology.org/2021.law-1.14/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;demonstrated&lt;/a&gt; that simply taking a majority vote from human annotations may obfuscate valid divergence in perspectives across social groups, inadvertently marginalizing minority perspectives, and consequently performing less reliably for groups marginalized in the data. How AI systems should deal with such diversity in perspectives depends on the context in which they are used. However, current models lack a systematic way to recognize and handle such contexts.&lt;/p&gt;&lt;p data-block-key=&quot;43t4g&quot;&gt;With this in mind, here we describe our ongoing efforts in pursuit of capturing diverse perspectives and building AI for the pluralistic society in which we live. We start with &lt;i&gt;understanding&lt;/i&gt; the varying perspectives in the world and, ultimately, we develop effective ways to &lt;i&gt;integrate&lt;/i&gt; these differences into the modeling pipeline. Each stage of the AI development pipeline — from conceptualization and data collection to training, evaluation, and deployment — offers unique opportunities to embed diverse perspectives, but also presents distinct challenges. A truly pluralistic AI cannot rely on isolated fixes or adjustments; it requires a holistic, layered approach that acknowledges and integrates complexity at every step. Having scalability in mind, we set out to (1) disentangle systematic differences in perspectives across social groups, (2) develop an in-depth understanding of the underlying causes for these differences, and (3) build effective ways to integrate meaningful differences into the machine learning (ML) modeling pipeline.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-1-Objectives.width-1250.png&quot; alt=&quot;PluralisticAI-1-Objectives&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-1-Objectives.width-1250.png&quot; alt=&quot;PluralisticAI-1-Objectives&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;her6d&quot;&gt;&lt;i&gt;A holistic, layered approach to (1)&lt;/i&gt; &lt;b&gt;&lt;i&gt;disentangle&lt;/i&gt;&lt;/b&gt;&lt;i&gt; systematic differences in perspectives across social groups, (2) develop an in-depth&lt;/i&gt; &lt;b&gt;&lt;i&gt;understanding&lt;/i&gt;&lt;/b&gt;&lt;i&gt; of the underlying causes for these differences, and (3) build effective ways to&lt;/i&gt; &lt;b&gt;&lt;i&gt;integrate&lt;/i&gt;&lt;/b&gt;&lt;i&gt; meaningful differences into the ML modeling pipeline. We use offensive language detection as an example task for demonstration, but the ideas presented here extend to any subjective task with varying human perspectives.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Over the last three years, we have made several advances along all of these three fronts:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Disentangling diverse perspectives&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Our efforts began with &lt;a href=&quot;https://aclanthology.org/2021.law-1.14/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;a call for action paper&lt;/a&gt; in 2021 in which we identified an issue in annotated datasets. Previous annotation methods, where only the majority vote of the annotator labels were made public, can lead to data that is inherently biased and unrepresentative of the breadth of users. Our research demonstrated that this reduction may unfairly disregard perspectives of certain annotators, and sometimes certain socio-demographic groups. We concluded that annotators are not interchangeable — that is, they draw from their socially embedded experiences and knowledge when making annotation judgments. As a result, retaining their perspectives separately in the datasets will enable dataset users to account for these differences according to their needs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-2-LabelAgreement.width-1250.png&quot; alt=&quot;PluralisticAI-2-LabelAgreement&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-2-LabelAgreement.width-1250.png&quot; alt=&quot;PluralisticAI-2-LabelAgreement&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;her6d&quot;&gt;&lt;i&gt;Agreement between annotators and the majority label. The figure on the left illustrates distribution of annotators’ agreement with the majority vore in a sentiment detection (&lt;/i&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3173574.3173986&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Díaz, &lt;i&gt;et al.&lt;/i&gt;, 2018&lt;/a&gt;&lt;i&gt;) The figures in the center and on the right present two boxplots of annotators’ agreement with the majority label, grouped by race and political orientation. A key finding is that majority label agreement was significantly lower among annotators who identified as Black.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Following these findings, our team took on the challenge of building more inclusive datasets. We collected and published large-scale datasets aimed at capturing diverse perspectives across various socio-demographic subgroups.&lt;/p&gt;&lt;p data-block-key=&quot;34bvm&quot;&gt;Two examples are &lt;a href=&quot;https://github.com/google-research-datasets/D3code&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;D3CODE&lt;/a&gt; and &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DICES&lt;/a&gt;, both of which centered on detecting offensive and harmful language. The &lt;a href=&quot;https://github.com/google-research-datasets/D3code&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;D3CODE dataset&lt;/a&gt; is a large-scale cross-cultural dataset that captures parallel annotations for offensive language in over 4.5k sentences annotated by a pool of over 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DICES dataset&lt;/a&gt; captures conversational safety ratings on around 1k sentences from a diverse pool of raters along gender, race, and age. The dataset is unique in providing a thorough set of labels for each individual post.&lt;/p&gt;&lt;p data-block-key=&quot;6alv&quot;&gt;Analyzing D3CODE and DICES revealed that annotator diversity is not just about individual differences, but also about the patterns of agreement and disagreement that emerge within and between groups. To quantify and identify patterns of agreement within groups, we introduced the &lt;a href=&quot;https://aclanthology.org/2024.naacl-long.190.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;GRASP metrics&lt;/a&gt;, a novel method for reliably measuring statistically significant group level associations in perspectives. When applied to an annotated dataset, the GRASP metric calculates a particular group’s congruency by comparing internal agreements with the level of agreement observed with outgroups.&lt;/p&gt;&lt;p data-block-key=&quot;6f0bg&quot;&gt;These datasets and metrics enable new lines of exploratory research to tease apart important distinctions in perspectives: e.g., &lt;a href=&quot;https://aclanthology.org/2024.nlperspectives-1.15.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;research on intersectionality in AI safety&lt;/a&gt; shows that South Asian women are 53% less likely than White men to rate a conversation safe, and &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3630106.3659021&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;a cross-cultural analysis of offensive language annotations&lt;/a&gt; shows that raters in the Global South tend to be more sensitive towards recognizing offensiveness in text.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Understanding the underlying factors&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;While detecting group-level differences is an important first step, meaningful interventions require us to develop an in-depth understanding of what factors contribute to these differences in perspectives and &lt;i&gt;why&lt;/i&gt; these perspectives diverge. This exploration often necessitates a multidisciplinary lens, drawing insights from various fields, such as psychology and sociology.&lt;/p&gt;&lt;p data-block-key=&quot;7crbs&quot;&gt;Consider the example of offensive language detection; research in moral and social psychology reveals that individuals’ judgments of offensive language can be rooted in their &lt;a href=&quot;https://books.google.com/books?id=U21BxGfm3RUC&amp;amp;printsec=frontcover&amp;amp;source=gbs_ge_summary_r&amp;amp;cad=0#v=onepage&amp;amp;q&amp;amp;f=false&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;personal values and beliefs&lt;/a&gt; as well as &lt;a href=&quot;https://www.stoppestennu.nl/sites/default/files/uploads/social_norms_and_the_expression_and_suppression_of_prejudice_the_struggle_for_internalization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;social norms of their society&lt;/a&gt; about what is right or wrong. By understanding these underlying sets of values, we can gain a more nuanced understanding of why different groups and individuals might disagree on what constitutes offensive language.&lt;/p&gt;&lt;p data-block-key=&quot;31udg&quot;&gt;In &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3630106.3659021&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;our recent research&lt;/a&gt;, we employed the social psychology framework of &lt;a href=&quot;https://moralfoundations.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;moral foundations theory&lt;/a&gt;, which posits moral beliefs across six foundations:&lt;i&gt; Care, Equality, Proportionality, Authority, Loyalty and Purity&lt;/i&gt;. We specifically relied on the moral foundations questionnaire (&lt;a href=&quot;https://psycnet.apa.org/record/2023-99083-001&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MFQ-2&lt;/a&gt;), because it was developed and validated through extensive cross-cultural assessments of moral judgments, making it a reliable tool for integrating a pluralistic definition of values into AI research.&lt;/p&gt;&lt;p data-block-key=&quot;bvjcn&quot;&gt;Our experiment demonstrated that cross-cultural differences in perceptions of offensiveness are significantly mediated by individual moral concerns, in particular, &lt;i&gt;Care&lt;/i&gt; and &lt;i&gt;Purity&lt;/i&gt;. In other words, cultures that place a higher weight on the value of &lt;i&gt;caring&lt;/i&gt; for other individuals and &lt;i&gt;avoiding&lt;/i&gt; impure thoughts are more sensitive to offensive language. These insights provide more meaningful theoretical grounding for data and model developers in their pursuit of aligning AI systems with human values.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
              --vertical-center
            
            
                glue-grid__col--span-6-md
            
          &quot;&gt;
            &lt;div class=&quot;--mobile-spacer&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-3-LabelDistribution.width-800.png&quot; alt=&quot;PluralisticAI-3-LabelDistribution&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-3-LabelDistribution.width-800.png&quot; alt=&quot;PluralisticAI-3-LabelDistribution&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
              --vertical-center
            
            
                glue-grid__col--span-6-md
            
          &quot;&gt;
            &lt;div class=&quot;--mobile-spacer&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-4-Perspectives.width-800.png&quot; alt=&quot;PluralisticAI-4-Perspectives&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-4-Perspectives.width-800.png&quot; alt=&quot;PluralisticAI-4-Perspectives&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;This research underscores the importance of the &lt;i&gt;why&lt;/i&gt; behind diverging human perspectives. By identifying moral values as the grounding factor in shaping different attitudes toward offensive language detection, we can move beyond acknowledging the presence of such disagreement to actually anticipating and accounting for them in the design of AI systems.&lt;/p&gt;&lt;p data-block-key=&quot;3qpd5&quot;&gt;This brings us to the next crucial step: &lt;i&gt;how&lt;/i&gt; can we integrate our understanding of pluralism into our AI models? How can we build models that are not only aware of diverse perspectives but also capable of effectively incorporating them into their decision-making process?&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Integrating pluralism in data and models&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;We have also spearheaded research on how to incorporate such diverse perspectives in ML data and model development and evaluation pipelines. For instance, we recently &lt;a href=&quot;https://aclanthology.org/2024.safety4convai-1.2.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;demonstrated&lt;/a&gt; how we can use our metrics to dynamically determine rater groups relevant for any given task. Then we can perform targeted diversification during data annotations by effectively identifying and involving annotators from specific social groups with unique perspectives to ensure representation of various viewpoints. Through simulation experiments, we observed that such an approach can efficiently increase &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;recall&lt;/a&gt; of safety issues flagged by minoritized rater groups without hurting overall &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;precision&lt;/a&gt;. We have also performed pioneering work on &lt;a href=&quot;https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00449/1986597/tacl_a_00449.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;multi-perspective modeling&lt;/a&gt; that efficiently learns a shared understanding of the problem at hand, while also catering to specific social groups with distinct perspectives in predictions. This approach also enables models to discern value-laden inputs that can have a variety of correct answers.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Our body of research reveals the importance of accounting for pluralism in AI model development and deployment, and builds tools and resources to further study and innovate on novel ways to incorporate pluralism in AI. It responds to the fact that global society is pluralistic. But the question of whether we want AI to be pluralistic — i.e., always adapting to local values, is a more nuanced question. For instance, in a paper published in 2022, we &lt;a href=&quot;https://arxiv.org/pdf/2210.02667&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discussed&lt;/a&gt; how certain universal baseline values, such as the principles of universal human rights, may sometimes be desirable. Ultimately, we aspire towards AI that is controllable — i.e., having the ability to control when, where, and how specific values and perspectives are adopted and extensible — is able to adapt to emergent perspectives as we encounter new cultural contexts, or societal change over time, and is transparent about what values guide a particular instantiation.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;&lt;i&gt;We would like to thank everyone on the team that contributed to the work presented in this blog post (in alphabetical order by last name): Lora Aroyo, Dylan Baker, Mark Díaz, Christopher Homan, Alicia Parrish, Charvi Rastogi, Greg Serapio-Garcia, Alex Taylor, Ding Wang, and Chris Welty.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/building-ai-for-the-pluralistic-society/</link><guid isPermaLink="false">https://research.google/blog/building-ai-for-the-pluralistic-society/</guid><pubDate>Wed, 12 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Natural Language Processing</category><category>Responsible AI</category></item><item><title>Urban mobility solutions: Calibrating digital twins at scale</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;In recent years, machine learning has enabled tremendous advances in urban planning and traffic management. However, as transportation systems become increasingly complex, due to factors like increased traveler and vehicle connectivity and the evolution of new services (e.g., ride-sharing, car-sharing, on-demand transit), finding solutions continues to be difficult. To better understand these challenges, cities are developing high-resolution urban mobility simulators, called “digital twins”, that can provide detailed descriptions of congestion patterns. These systems incorporate a variety of factors that might influence traffic flow, such as available mobility services, including on-demand rider-to-vehicle matching for ride-sharing services; network supply operations, such as traffic-responsive tolling or signal control; and sets of diverse traveler behaviors that govern driving style (e.g., risk-averse vs. aggressive), route preferences, and travel mode choices.&lt;/p&gt;&lt;p data-block-key=&quot;39ugv&quot;&gt;These simulators tackle a variety of use cases, such as the deployment of electric-vehicle charging stations, &lt;a href=&quot;https://research.google/blog/simulations-illuminate-the-path-to-post-event-traffic-flow/&quot;&gt;post-event traffic mitigation&lt;/a&gt;, &lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/trsc.2021.1043&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;congestion pricing and tolling&lt;/a&gt;, &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0191261514002240&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;sustainable traffic signal control&lt;/a&gt;, and public transportation expansions. However, it remains a challenge to estimate the inputs of these simulators, such as spatial and temporal distribution of travel demand, road attributes (e.g., number of lanes and geometry), prevailing traffic signal timings, etc., so that they can reliably replicate prevailing traffic patterns of congested, metropolitan-scale networks. The process of estimating these inputs is known as calibration.&lt;/p&gt;&lt;p data-block-key=&quot;5as4g&quot;&gt;The main goal of simulation calibration is to bridge the gap between simulated and observed traffic data. In other words, a well-calibrated simulator yields simulated congestion patterns that accurately reflect those observed in the field. Demand calibration (i.e., determining the demand for or popularity of a particular origin-to-destination trip) is the most important input to estimate, but also the most difficult. Traditionally, simulators have been calibrated using traffic sensors installed under the roadway. These sensors are present in most cities but costly to install and maintain. Also, their spatial sparsity limits the calibration quality because congestion patterns go largely unobserved. Moreover, most of the demand calibration work is based on single, typically small, road networks (e.g., an arterial).&lt;/p&gt;&lt;p data-block-key=&quot;bd9hn&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2501.04783&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Traffic Simulations: Multi-City Calibration of Metropolitan Highway Networks&lt;/a&gt;”, we showcase the ability to calibrate demand for the full metropolitan highway networks of six cities — Seattle, Denver, Philadelphia, Boston, Orlando, and Salt Lake City — for all congestion levels, from free-flowing to highly congested. To calibrate, we use non-sparse traffic data, namely aggregated and anonymized path travel times, yielding more accurate and reliable models. When compared to a standard benchmark, the proposed approach is able to replicate historical travel time data 44% better on average (and as much as 80% better in some cases).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Calibration approach&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;To calibrate travel demand, urban planners and traffic operators partition metropolitan areas into zones and use simulators to estimate travel demand between pairs of zones. However, high-resolution traffic simulators can be compute-costly to use. Our solution takes advantage of recent research into &lt;a href=&quot;https://research.google/pubs/active-sequential-posterior-estimation-for-sample-efficient-simulation-based-inference/&quot;&gt;sample-efficient algorithms&lt;/a&gt; that tackle the optimization problems in a computationally fast way, avoiding the need for large samples from a costly simulator. In particular, we use &lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/opre.2013.1226&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;metamodel methods&lt;/a&gt;, which leverage low-resolution, physics-informed traffic models to guide the search in the optimization process.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-1-Zones.width-1250.png&quot; alt=&quot;UrbanMobility-1-Zones&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-1-Zones.width-1250.png&quot; alt=&quot;UrbanMobility-1-Zones&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;Example of a spatial partition of a metro area into zones. Demand calibration estimates the number of trips that start in one zone and finish in another. (Source: Google Maps)&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;Historically, the most commonly used type of traffic data for modeling purposes has been spatially sparse &lt;a href=&quot;https://research.google/pubs/an-adversarial-variational-inference-approach-for-travel-demand-calibration-of-urban-traffic-simulators/&quot;&gt;vehicular count&lt;/a&gt; data. Recently, we have improved on that by incorporating &lt;a href=&quot;https://research.google/pubs/on-the-use-of-abundant-road-speed-data-for-travel-demand-calibration-of-urban-traffic-simulators/&quot;&gt;road speed data&lt;/a&gt;, which is spatially non-sparse. For our new calibration approach, we also use aggregated and anonymized path travel time data (i.e., the estimated average time it takes to travel along a specific route or path between two points within a road network), which provides a more spatially complete understanding of congestion patterns throughout the network.&lt;/p&gt;&lt;p data-block-key=&quot;cskah&quot;&gt;We calibrate traffic demand for six metropolitan areas (Seattle, Denver, Philadelphia, Boston, Orlando, and Salt Lake City) using the open-source &lt;a href=&quot;https://ieeexplore.ieee.org/document/8569938&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;SUMO simulation&lt;/a&gt; software to model their highway networks. Our calibration algorithms combine road network and historical traffic data with these SUMO simulated networks to optimize the input parameters of these simulators, yielding realistic simulated traffic statistics (e.g., travel times, speeds).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-2a-Overview.width-1250.png&quot; alt=&quot;UrbanMobility-2a-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-2a-Overview.width-1250.png&quot; alt=&quot;UrbanMobility-2a-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;Overview of our simulation calibration framework.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;The corresponding highway road networks are large scale, containing from 6,600 up to 18,600 modeled roads (or segments). The corresponding demand estimation optimization problems are high dimensional, ranging from 600 to 1,700.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-3a-Segments.width-1250.png&quot; alt=&quot;UrbanMobility-3a-Segments&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-3a-Segments.width-1250.png&quot; alt=&quot;UrbanMobility-3a-Segments&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;The number of segments (i.e., links or roads) in the network and the maximum number of origin-destination zone pairs.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;For each city, we calibrate the hourly demand from 2pm to 11pm on a typical weekday. This period includes the afternoon peak hours as well as the off-peak hours, allowing us to evaluate the algorithm’s performance under different congestion levels. For each scenario (i.e., city and hour combination), we use aggregated and anonymized path travel time data as the ground truth.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Results&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;Our baseline for comparison is the commonly used &lt;a href=&quot;https://www.jhuapl.edu/spsa/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;simultaneous perturbation stochastic approximation&lt;/a&gt; (SPSA) algorithm. The calibration quality of each approach is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Root_mean_square_deviation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;normalized root mean square error&lt;/a&gt; (nRMSE), the particular form of which is described in &lt;a href=&quot;https://arxiv.org/abs/2501.04783&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;our paper&lt;/a&gt;. The nRMSE can be interpreted as the relative error in the fit to the traffic data. For example, an nRMSE value of 0.2 means that there’s an average 20% error in the simulated travel time data compared to the historical traffic data. The plots compare the percentage improvement of the nRMSE of our metamodel calibration compared to the SPSA nRMSE.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-4-nRMSE.width-1250.png&quot; alt=&quot;UrbanMobility-4-nRMSE&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-4-nRMSE.width-1250.png&quot; alt=&quot;UrbanMobility-4-nRMSE&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;Path travel time nRMSE for off-peak (3pm–4pm) and peak (7pm–8pm) hours: SPSA vs. our metamodel method. Each curve considers a given hour and illustrates the performance as percentage improvement between path travel time nRMSE of the metamodel compared to SPSA.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;As the algorithm progresses, the metamodel consistently outperforms SPSA, rapidly establishing its advantage. The metamodel achieves a maximum 78% reduction in path travel time nRMSE during peak hour (Boston, 7pm) and a 76% reduction during a non-peak hour (Orlando, 3pm). Even across all cities and both hours (3pm and 7pm), the metamodel’s calibration quality is 52% better than the baseline SPSA. This sustained improvement ultimately leads to the metamodel generating higher-quality calibrated demand than SPSA, consistently across all cities and congestion levels.&lt;/p&gt;&lt;p data-block-key=&quot;btpmt&quot;&gt;Across all scenarios being calibrated, the metamodel yields a 44% average improvement. The most substantial improvement was observed for Salt Lake City during the afternoon peak traffic congestion period (i.e., 6pm–7pm) where the metamodel outperformed SPSA by 80%.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;What’s next?&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;Our work shows how non-sparse path travel times can be used to enhance the quality of urban mobility digital twins. This overcomes the main challenges of traditional calibration data: vehicular traffic counts, which rely on sparsely located sensors that are also costly to install and to maintain. Our work is shown to be scalable across metropolitan areas. Compared to SPSA, a commonly used algorithm, our method systematically and significantly improves demand calibration quality.&lt;/p&gt;&lt;p data-block-key=&quot;2cgq&quot;&gt;Nevertheless, challenges remain around the number of demand inputs leading to equivalent congestion patterns. As such, our ongoing work focuses on methods to account for this &lt;a href=&quot;https://research.google/pubs/active-sequential-posterior-estimation-for-sample-efficient-simulation-based-inference/&quot;&gt;input uncertainty&lt;/a&gt; and quantify its impact on &lt;a href=&quot;https://research.google/pubs/an-adversarial-variational-inference-approach-for-travel-demand-calibration-of-urban-traffic-simulators/&quot;&gt;output uncertainty&lt;/a&gt;. Accounting for these uncertainties helps improve &lt;a href=&quot;https://arxiv.org/abs/2402.01928&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;counterfactual robustness&lt;/a&gt;, which allows digital twins to better evaluate the impact of urban mobility policies, such as the deployment of congestion pricing or of new transit services. Other ways to improve counterfactual robustness include devising new problems and algorithmic formulations that exploit more information from traffic data, such as going beyond the use of &lt;a href=&quot;https://math.stackexchange.com/questions/628523/first-and-second-order-statistics&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;first-order sample statistics&lt;/a&gt;. This is another way to help us better understand and replicate congestion patterns, ultimately leading to less congested roads and improved quality of life.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;&lt;i&gt;This work was done in collaboration with Neha Arora, Yechen Li, and Damien Pierce. The authors also thank Andrew Tomkins, Yi-fan Chen, and Craig Boutilier for strategic guidance, Ivan Kuznetsov for product management, Iveel Tsogsuren, Martin Mladenov, and Chih-wei Hsu for general framework contributions, and Sheila de Guia for program management and coordination.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/urban-mobility-solutions-calibrating-digital-twins-at-scale/</link><guid isPermaLink="false">https://research.google/blog/urban-mobility-solutions-calibrating-digital-twins-at-scale/</guid><pubDate>Sun, 09 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Algorithms &amp; Theory</category><category>Climate &amp; Sustainability</category><category>General Science</category></item><item><title>Chain of Agents: Large language models collaborating on long-context tasks</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6uu2k&quot;&gt;Over the past few years large language models (LLMs) have shown remarkable capabilities on various tasks, such as reasoning, knowledge retrieval, and generation. However, it is still challenging for LLMs to solve tasks that require long inputs, because they typically have limitations on input length, and hence, cannot utilize the full context. This issue hinders long context tasks, such as long summarization, question answering, and code completion.&lt;/p&gt;&lt;p data-block-key=&quot;6b883&quot;&gt;To mitigate this, at &lt;a href=&quot;https://neurips.cc/Conferences/2024&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;NeurIPS 2024&lt;/a&gt; we introduced &lt;a href=&quot;https://openreview.net/pdf?id=LuCLf4BJsr&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Chain-of-Agents&lt;/a&gt; (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. We perform a comprehensive evaluation of CoA on a wide range of long-context tasks, including question answering, summarization, and code completion. We demonstrate significant improvements (up to 10%) over strong baselines: &lt;a href=&quot;https://cloud.google.com/use-cases/retrieval-augmented-generation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;retrieval augmented generation&lt;/a&gt; (RAG), multi-agent LLMs, and LLMs that have had their inputs truncated once the context window is full (called “full-context”).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A simple but effective approach to improve long-context understanding&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6uu2k&quot;&gt;Previous studies have mainly explored two major directions: &lt;i&gt;input reduction&lt;/i&gt; and &lt;i&gt;window extension&lt;/i&gt;. Input reduction reduces the length of the input context — for example, by directly truncating the input — before feeding to downstream LLMs. RAG extends this direction by breaking the input into chunks and then retrieving answers to the most relevant chunks based on embedding similarity. However, because of low retrieval accuracy, LLMs could receive an incomplete context for solving the task, hurting performance. Window extension extends the context window of LLMs via fine-tuning, training the model to consume longer inputs. For example, &lt;a href=&quot;https://gemini.google.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemini&lt;/a&gt; is able to directly process 2M tokens for each input. However, when the window becomes longer even than their extended input capacities, such LLMs still struggle to focus on the needed information to solve the task and suffer from ineffective context utilization. This long context approach is further complicated by the fact that the cost increases quadratically with length due to the design of the &lt;a href=&quot;https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/&quot;&gt;transformer architecture&lt;/a&gt; that underlies most LLMs.&lt;/p&gt;&lt;p data-block-key=&quot;1th7d&quot;&gt;Motivated by the aforementioned challenges, we designed CoA with inspiration from the way people interleave reading and processing of long contexts under our own limited working memory constraints. Whereas input reduction approaches need to start processing over shorter inputs (“read-then-process”), CoA breaks the input into chunks and then assigns workers to process each chunk sequentially before reading all of the input (“interleaved read-process”). Further, in contrast to context extension, CoA leverages the capacity of LLMs to communicate between agents rather than trying to feed a large number of tokens into the LLM. CoA is also compute cost–effective, significantly improving over full-context approaches, in particular, by reducing time complexity from &lt;i&gt;n&lt;/i&gt;2 to &lt;i&gt;nk&lt;/i&gt;, where &lt;i&gt;n&lt;/i&gt; is the number of input tokens and &lt;i&gt;k&lt;/i&gt; is the context limit of the LLM.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A novel approach to input processing&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6uu2k&quot;&gt;CoA contains two stages. In the first, a series of worker agents in charge of different chunks of long context collaborate and aggregate supporting data that can be used to answer the given query. To this end, the workers read and process sequentially, each receiving the message from the previous worker and transferring the useful updated information to the next. In the second stage, the manager agent receives the complete evidence from the last worker agent and generates the final response. Here is a motivating example:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    &lt;div class=&quot;rich-text --theme- --mode-&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 80px;&quot; data-offset-key=&quot;4r7lu-0-0&quot;&gt;&lt;em&gt;Question:&lt;/em&gt;&lt;span data-offset-key=&quot;4r7lu-0-1&quot;&gt; “Who is the grandchild of A?”&lt;/span&gt;&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;ac7s1-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;ac7s1-0-0&quot;&gt;&lt;em&gt;Source input, separated into chunks:&lt;/em&gt;&lt;span data-offset-key=&quot;ac7s1-0-1&quot;&gt; [1],[2],[3],[4]&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;em&gt;Supporting data from each chunk:&lt;/em&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[1] – A’s spouse is D&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[2] – A’s child is B&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[3] – No additional evidence&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[4] – B’s child is C&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;b6qsq-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;b6qsq-0-0&quot;&gt;&amp;nbsp;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;b6qsq-0-0&quot;&gt;&lt;strong&gt;Chain of Agents:&lt;/strong&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;2lfkb-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;2lfkb-0-0&quot;&gt;&lt;span data-offset-key=&quot;2lfkb-0-0&quot;&gt;&lt;em&gt;Question:&lt;/em&gt; &lt;/span&gt;&lt;span data-offset-key=&quot;2lfkb-0-1&quot;&gt;“Who is the grandchild of A?”&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;ffsl2-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;ffsl2-0-0&quot;&gt;&lt;em&gt;Workers assess their chunk and perform a relevant task:&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;fd9ae-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;fd9ae-0-0&quot;&gt;&lt;span data-offset-key=&quot;fd9ae-0-0&quot;&gt;[1] – topic exploration: A’s spouse is D &lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;6qvjk-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;6qvjk-0-0&quot;&gt;&lt;span data-offset-key=&quot;6qvjk-0-0&quot;&gt;[2] – answer first hop: A’s child is B &lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;eadtm-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;eadtm-0-0&quot;&gt;&lt;span data-offset-key=&quot;eadtm-0-0&quot;&gt;[3] – forward previous evidence: A’s child is B &lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;5n277-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;5n277-0-0&quot;&gt;&lt;span data-offset-key=&quot;5n277-0-0&quot;&gt;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C &lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;5n277-0-0&quot;&gt;&lt;em&gt;Manager:&lt;/em&gt;&lt;span data-offset-key=&quot;ae2pm-0-1&quot;&gt; “It is C.”&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

                    
                    
    


&lt;section class=&quot;component-as-block both --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Stage 1: Worker agent: Segment comprehension and chain-communication&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;gfrcw&quot;&gt;In Stage 1, CoA contains a sequence of worker agents. Each worker receives an heuristically concatenated portion from the source text, the query, instructions for a specific task assigned to that agent, and the message passed from the previous agent. This communication chain is unidirectional, passing from each worker to the next in sequential order. The worker agents process each concatenated block and outputs a message for the next worker.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Stage 2: Manager agent: Information integration and response generation&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;In Stage 2, after multiple steps of information extraction and comprehension by worker agents, the manager agent produces the final solution. While worker agents extract relevant information in a long-context source, the manager agent synthesizes relevant information accumulated by the end of ‘’worker–agent chain&#39;&#39; to generate the final answer. Specifically, given the instruction for manager and query, the manager agent assesses the accumulated knowledge from the last worker to generate the final answer.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/CoA-1-Overview.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;High-level illustration of Chain-of-Agents. It consists of multiple worker agents that sequentially communicate to handle different segmented portions of the text, followed by a manager agent that synthesizes these contributions into a coherent final output.&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Experiments&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;To illustrate the utility of this approach, we conduct intensive experiments on nine datasets, including question answering, summarization, and code completion tasks with six LLMs, &lt;a href=&quot;https://ai.google/discover/palm2/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;PaLM 2&lt;/a&gt; (Text Bison and Text Unicorn), &lt;a href=&quot;https://gemini.google.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemini&lt;/a&gt; (Ultra), and &lt;a href=&quot;https://claude.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Claude 3&lt;/a&gt; (Haiku, Sonnet, and Opus) models. We compare CoA with two strong baselines chosen from input reduction and window extension approaches, respectively: (i) RAG, which uses a state-of-the-art retriever to obtain the most relevant information to feed into the LLM, and (ii) Full-Context, which feeds all input into the LLM until reaching the window limit.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Comparison with a RAG model&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;The figures show the results on question answering, summarization, and code completion tasks for three models on eight different datasets, including HotpotQA, MuSiQue, RepoBench-P(RepoB) from &lt;a href=&quot;https://aclanthology.org/2024.acl-long.172.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LongBench&lt;/a&gt;, and NarrativeQA (NQA), Qasper, QuALITY, QMSum, GovReport from &lt;a href=&quot;https://www.scrolls-benchmark.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;SCROLLS&lt;/a&gt;. CoA (8k) (where “8k” refers to the length of input for the LLM) outperforms Full-Context (8k) by a large margin on all datasets. It also outperforms the RAG (8k) model for all eight datasets.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-2-Palm2TextBison.width-1250.png&quot; alt=&quot;CoA-2-Palm2TextBison&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-2-Palm2TextBison.width-1250.png&quot; alt=&quot;CoA-2-Palm2TextBison&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;Palm 2 Text Bison&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-3-Palm2TextUnicorn.width-1250.png&quot; alt=&quot;CoA-3-Palm2TextUnicorn&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-3-Palm2TextUnicorn.width-1250.png&quot; alt=&quot;CoA-3-Palm2TextUnicorn&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;Palm 2 Text Unicorn&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-4-GeminiUltra.width-1250.png&quot; alt=&quot;CoA-4-GeminiUltra&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-4-GeminiUltra.width-1250.png&quot; alt=&quot;CoA-4-GeminiUltra&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;Gemini Ultra&lt;/p&gt;&lt;p data-block-key=&quot;3pgpa&quot;&gt;Comparison of three LLMs with RAG and Full-Context baselines. Y-axis is the performance metric on each dataset.&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Multi-agent collaboration in CoA enables complex reasoning over long context&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;Below we present a comparison of outputs from RAG and CoA for a question on the &lt;a href=&quot;https://hotpotqa.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;HotpotQA dataset&lt;/a&gt;. To find the correct answer, RAG retrieves text chunks with high semantic similarity with the query. However, conducting multi-hop reasoning is challenging as the critical first-hop answer often lacks semantic relevance to the query. In contrast, CoA operates differently: the first agent explores related topics without knowing the query’s answer, aiding subsequent inference. The second agent, also unaware of the answer, broadens the topic scope by incorporating new information. The third agent finally discovers the answer, synthesizing information from earlier agents and new data to complete the reasoning chain. This collaborative approach highlights CoA’s ability to facilitate complex reasoning across long context tasks.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-5-HotpotQA.width-1250.png&quot; alt=&quot;CoA-5-HotpotQA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-5-HotpotQA.width-1250.png&quot; alt=&quot;CoA-5-HotpotQA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;A case study of RAG (&lt;/i&gt;&lt;b&gt;&lt;i&gt;left&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) and CoA (&lt;/i&gt;&lt;b&gt;&lt;i&gt;right&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) on HotpotQA. Sequential agent communication enables CoA to perform complex multi-hop reasoning over long contexts.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Comparison with long context LLMs&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;The figure below shows the comparison with long context LLMs on &lt;a href=&quot;https://arxiv.org/abs/1712.07040&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;NarrativeQA&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2105.08209&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BookSum&lt;/a&gt;. CoA (8k) significantly outperforms RAG (8k) and Full-Context (200k) baselines with three Claude 3 (Haiku, Sonnet, and Opus) models as backbones, even though the context limit of the latter is 200k.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-6-NarrativeQA.width-1250.png&quot; alt=&quot;CoA-6-NarrativeQA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-6-NarrativeQA.width-1250.png&quot; alt=&quot;CoA-6-NarrativeQA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;NarrativeQA&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-7-BookSum.width-1250.png&quot; alt=&quot;CoA-7-BookSum&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-7-BookSum.width-1250.png&quot; alt=&quot;CoA-7-BookSum&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;BookSum&lt;/i&gt;&lt;/p&gt;&lt;p data-block-key=&quot;4kiep&quot;&gt;&lt;i&gt;Comparison with long context LLMs: Claude 3 Haiku, Claude 3 Sonnet and Claude 3 Opus. The number on the bar is the performance. “W” / ”w/o Trun.” indicates the source text in the sample is more/less than 200k tokens, which needs/does not need truncation for the full-context (200k) baseline. “Avg.” is the mean value across all samples.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Greater improvement for long context models with longer inputs&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;We compare the performance of CoA and Full-Context with &lt;a href=&quot;https://claude.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Claude 3&lt;/a&gt; on &lt;a href=&quot;https://arxiv.org/abs/2105.08209&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BookSum&lt;/a&gt;. As shown in Figure, CoA can outperform the Full-Context baseline by a large margin on various source lengths. It is worth noting that, when the length of the sample increases, the performance even increases for CoA, and the improvement over Full-Context (200k) baseline becomes more significant. The improvement of CoA reaches around 100% when the length is larger than 400k. Thus, we can conclude that 1) CoA can still enhance the LLM performance even though the model has a very long context window limit; and 2) CoA delivers more performance gains when the input is longer.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-8-Claude3.width-1250.png&quot; alt=&quot;CoA-8-Claude3&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-8-Claude3.width-1250.png&quot; alt=&quot;CoA-8-Claude3&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;Performance of&lt;/i&gt; &lt;a href=&quot;https://claude.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Claude 3&lt;/i&gt;&lt;/a&gt;&lt;i&gt; on&lt;/i&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.08209&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;BookSum&lt;/i&gt;&lt;/a&gt;&lt;i&gt;. Improvement is more obvious for longer inputs.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;In this paper, we propose Chain-of-Agents (CoA), a multi-agent LLM collaboration framework for solving long context tasks. It is a training-free, task- and length-agnostic, interpretable, and cost-effective framework. Experiments show that Chain-of-Agents outperforms RAG and long context LLMs by a large margin, despite its simple design. Analysis shows that by integrating information aggregation and context reasoning, CoA performs better on longer samples.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/</link><guid isPermaLink="false">https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/</guid><pubDate>Wed, 22 Jan 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Machine Intelligence</category><category>Natural Language Processing</category></item><item><title>Parfait: Enabling private AI with research tools</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;xkhfv&quot;&gt;As generative AI advances, the need for more &lt;a href=&quot;https://developers.googleblog.com/en/enabling-more-private-gen-ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;private AI&lt;/a&gt; systems that protect users&#39; data and give them control throughout the AI lifecycle remains a top priority. At Google, we bake &lt;a href=&quot;https://ai.google/responsibility/principles/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;privacy&lt;/a&gt; into our AI development and use. With some AI systems relying on user data to perform helpful tasks — like understanding the user’s surroundings or acting on personal information — advancing privacy-preserving technologies remains essential in an era of innovation to safeguard personal data while fostering trust in the technologies that drive progress.&lt;/p&gt;&lt;p data-block-key=&quot;fqqia&quot;&gt;We’re excited to announce &lt;a href=&quot;https://github.com/google-parfait&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Parfait&lt;/a&gt; —&amp;nbsp;which stands for “private aggregation &amp;amp; retrieval, federated, analytics, inference, &amp;amp; training” — a GitHub organization (i.e., a shared account where businesses and open-source projects can securely collaborate across many projects at once) that we have developed at Google to demonstrate our state-of-the-art methods across four &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3494834.3500240&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;privacy pillars&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;fjmcr&quot;&gt;&lt;a href=&quot;https://transparency.google/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Transparency&lt;/i&gt;&lt;/a&gt;&lt;i&gt;,&lt;/i&gt; which shows the data that are being used and how&lt;/li&gt;&lt;li data-block-key=&quot;bdvv7&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_minimization&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Data minimization&lt;/i&gt;&lt;/a&gt;, which includes &lt;a href=&quot;https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/&quot;&gt;federated learning&lt;/a&gt;, &lt;a href=&quot;https://research.google/blog/federated-analytics-collaborative-data-science-without-data-collection/&quot;&gt;federated analytics&lt;/a&gt;, and &lt;a href=&quot;https://research.google/pubs/practical-secure-aggregation-for-privacy-preserving-machine-learning/&quot;&gt;secure aggregation&lt;/a&gt;&lt;/li&gt;&lt;li data-block-key=&quot;da1mp&quot;&gt;&lt;a href=&quot;https://policies.google.com/technologies/anonymization?hl=en-US&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Data anonymization&lt;/i&gt;&lt;/a&gt;, which includes &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;differential privacy&lt;/a&gt; algorithms for model training, model fine-tuning, &lt;a href=&quot;https://research.google/pubs/federated-heavy-hitters-with-differential-privacy/&quot;&gt;heavy hitter discovery&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/2412.07962&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;histogram estimation&lt;/a&gt;&lt;/li&gt;&lt;li data-block-key=&quot;3e3o4&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.10764&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;External verifiability&lt;/i&gt;&lt;/a&gt;, which uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted execution environment&lt;/a&gt; (TEE) workflows that allow users or other external parties to verify privacy claims&lt;/li&gt;&lt;/ul&gt;&lt;p data-block-key=&quot;buqt0&quot;&gt;Parfait has been used to provide research and production code for Google deployments of federated learning and analytics from &lt;a href=&quot;https://support.google.com/gboard/answer/12373137?hl=en#zippy=%2Cfederated-learning&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gboard&lt;/a&gt; to Android’s &lt;a href=&quot;https://arxiv.org/abs/2209.10317&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Private Compute Core&lt;/a&gt;, to &lt;a href=&quot;https://arxiv.org/abs/2412.07962&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Google Maps.&lt;/a&gt; We are releasing Parfait open-source repositories to advance private AI by helping define and execute machine learning (ML) and analytics computations and workflows under a variety of settings that enable strong privacy claims consistent with users’ privacy expectations. This blog post explains how and why Parfait was created, the repositories, and real-world Parfait use cases.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;xkhfv&quot;&gt;The Parfait journey&lt;/h2&gt;&lt;p data-block-key=&quot;50hjd&quot;&gt;Parfait evolved from technologies for federated learning and analytics. &lt;a href=&quot;https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/&quot;&gt;Federated learning&lt;/a&gt;, which Google &lt;a href=&quot;https://arxiv.org/abs/1602.05629&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;introduced in 2016&lt;/a&gt;, is an innovative, privacy-enhancing approach that enables developers to train ML models across many devices without centralized data collection, ensuring that only the user has a copy of their data. Since then, Federated Learning has been used to enhance the privacy of many experiences like &lt;a href=&quot;https://arxiv.org/abs/1812.02903&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;expressions&lt;/a&gt; in &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&amp;amp;pli=1&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gboard&lt;/a&gt; and &lt;a href=&quot;https://support.google.com/messages/answer/9327902?hl=en&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;improving the quality of smart replies&lt;/a&gt; in &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.apps.messaging&amp;amp;hl=en_US&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Android Messages&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;1vs8j&quot;&gt;In 2020, Google introduced &lt;a href=&quot;https://research.google/blog/federated-analytics-collaborative-data-science-without-data-collection/&quot;&gt;federated analytics&lt;/a&gt;, which was built on federated learning by applying data science methods to perform analysis of raw data stored locally on users’ devices in private and secure ways.&lt;/p&gt;&lt;p data-block-key=&quot;cluj9&quot;&gt;Since then, Google has opened the door for greater collaboration with the external privacy community by &lt;a href=&quot;https://blog.tensorflow.org/2019/03/introducing-tensorflow-federated.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;open sourcing TensorFlow Federated&lt;/a&gt;, a framework for federated computation and libraries and a reference architecture for cross-device federated computation, as well as collaborating on &lt;a href=&quot;https://www.tensorflow.org/responsible_ai/privacy/guide&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;TensorFlow Privacy&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;10ic5&quot;&gt;These building blocks and references led to the development of a Google Cloud architecture for &lt;a href=&quot;https://cloud.google.com/architecture/cross-silo-cross-device-federated-learning-google-cloud&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;cross-silo and cross-device federated learning&lt;/a&gt; and &lt;a href=&quot;https://privacysandbox.com/intl/en_us/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Privacy Sandbox&lt;/a&gt;’s &lt;a href=&quot;https://github.com/privacysandbox/odp-federatedcompute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Federated Compute server&lt;/a&gt; for &lt;a href=&quot;https://developers.google.com/privacy-sandbox/protections/on-device-personalization&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;on-device-personalization&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;1hc93&quot;&gt;Recognizing the increased value Google technologies provide when interconnected and evolved in unison, we created Parfait to support these deployments and the research advances driving them.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;xkhfv&quot;&gt;Parfait repositories&lt;/h2&gt;&lt;p data-block-key=&quot;5oa0&quot;&gt;Parfait repositories, which have more than 100 contributors, show some of Google’s key privacy-preserving technologies in practice.&lt;/p&gt;&lt;p data-block-key=&quot;fcjfq&quot;&gt;The repositories include:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;76ckg&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/federated-language&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;federated-language&lt;/a&gt;: Our core language for concisely expressing novel federated algorithms with distributed communication operators within a strongly-typed functional programming environment. This is also usable with any ML framework (e.g., &lt;a href=&quot;https://github.com/jax-ml/jax&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;JAX&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;TensorFlow&lt;/a&gt;). Previously part of our TensorFlow Federated framework, this foundational piece, upon which rests our open-sourced learning and analytics algorithms, has been fully decoupled from TensorFlow, making it truly platform independent.&lt;/li&gt;&lt;li data-block-key=&quot;eugjb&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/tensorflow-federated&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;tensorflow-federated&lt;/a&gt;: A set of high-level interfaces, algorithms, and ML platform integrations that allow developers to apply the included implementations of federated learning or federated analytics to their existing TensorFlow or JAX models.&lt;/li&gt;&lt;li data-block-key=&quot;7sf5j&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/federated-compute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;federated-compute&lt;/a&gt;: Code for executing cross-device federated programs and computations, including Android client libraries, as well as a reference end-to-end demo that lays out the core pieces of a cross-device architecture for federated compute. Check out our &lt;a href=&quot;https://arxiv.org/abs/1902.01046&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;federated learning at scale whitepaper&lt;/a&gt;.&lt;/li&gt;&lt;li data-block-key=&quot;b6p3i&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential-federated-compute&lt;/a&gt;: Publicly verifiable components that run within TEEs and interact with user data to enable federated learning and analytics using confidential computing. Check out our &lt;a href=&quot;https://arxiv.org/abs/2404.10764&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Confidential Federated Computations white paper&lt;/a&gt; for more information.&lt;/li&gt;&lt;li data-block-key=&quot;59fad&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted-computations-platform&lt;/a&gt;: Publicly verifiable components that run within secure enclaves and interact with user data to enable stateful rollback protected replicated computations.&lt;/li&gt;&lt;li data-block-key=&quot;du2ee&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/raft-rs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;raft-rs&lt;/a&gt;: &lt;a href=&quot;https://raft.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Raft distributed consensus algorithm&lt;/a&gt; implemented in &lt;a href=&quot;https://opensource.googleblog.com/2023/06/rust-fact-vs-fiction-5-insights-from-googles-rust-journey-2022.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Rust&lt;/a&gt;, used by trusted-computations-platform.&lt;/li&gt;&lt;li data-block-key=&quot;90mkd&quot;&gt;&lt;a href=&quot;https://github.com/google-parfait/dataset_grouper&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;dataset_grouper&lt;/a&gt;: Libraries for efficient and scalable group-structured dataset pipelines.&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;xkhfv&quot;&gt;Parfait in action&lt;/h2&gt;&lt;p data-block-key=&quot;f1m14&quot;&gt;While Parfait remains an evergreen space for research advancements to be driven into products (at Google and beyond), Google product teams are using it in real-world deployments. For example, Gboard has used technologies in Parfait to improve user experiences, launching the &lt;a href=&quot;https://research.google/blog/federated-learning-with-formal-differential-privacy-guarantees/&quot;&gt;first neural-net models&lt;/a&gt; trained using federated learning with formal differential privacy and &lt;a href=&quot;https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/&quot;&gt;expanding&lt;/a&gt; its use. They also continue to use federated analytics to &lt;a href=&quot;https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/&quot;&gt;advance&lt;/a&gt; Gboard’s out-of-vocab words for less common languages.&lt;/p&gt;&lt;p data-block-key=&quot;9qte7&quot;&gt;The &lt;a href=&quot;https://developers.google.com/privacy-sandbox/protections/on-device-personalization&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;on-device personalization module&lt;/a&gt;, which is in a limited testing phase as part of &lt;a href=&quot;https://developers.google.com/privacy-sandbox/overview/android&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;the Privacy Sandbox initiative on Android&lt;/a&gt;, helps to protect user information from businesses with whom they haven’t interacted. It provides an open-source, federated compute platform to coordinate cross-device ML and statistical analysis for its adopters. The module’s team, referencing and depending on different parts of Parfait, has launched a preview version of an open-source &lt;a href=&quot;https://github.com/privacysandbox/odp-federatedcompute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;federated compute service&lt;/a&gt; that can be deployed on a TEE-based cloud service.&lt;/p&gt;&lt;p data-block-key=&quot;9fru0&quot;&gt;More recently we &lt;a href=&quot;https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/&quot;&gt;previewed our novel approach to using CPU TEEs&lt;/a&gt; to enable Android devices to verify the exact version of the server-side software that may decrypt uploaded messages. This approach builds on &lt;a href=&quot;https://github.com/project-oak/oak&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Project Oak&lt;/a&gt; and a software keystore hosted in our new &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted computations platform&lt;/a&gt;. This new platform guarantees that uploaded data can be decrypted only by the expected server side workflow (anonymizing aggregation) in an expected virtual machine, running in a TEE backed by a CPU’s cryptographic attestation (e.g., AMD or Intel). Parfait’s &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential federated computations repository&lt;/a&gt; implements this code, leveraging state-of-the-art differential privacy aggregation primitives in the &lt;a href=&quot;https://github.com/google-parfait/tensorflow-federated&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;TensorFlow Federated repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;xkhfv&quot;&gt;Conclusion&lt;/h2&gt;&lt;p data-block-key=&quot;bkr22&quot;&gt;As part of our commitment to privacy-preserving technology, our hope is that Parfait makes it easier for researchers and developers to see how some of these key techniques work in practice, and we hope they inspire future collaborations and advances in other frameworks.&lt;/p&gt;&lt;p data-block-key=&quot;9vogg&quot;&gt;We believe strong, formal privacy guarantees are increasingly practical in real-world deployments, and we are committed to making our approaches and innovations open to the public. We encourage privacy engineers and researchers outside of Google to make their approaches public, too, and are excited about the potential for continued and further collaborations across industry and with academia.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;xkhfv&quot;&gt;Acknowledgements&lt;/h2&gt;&lt;p data-block-key=&quot;78l7j&quot;&gt;&lt;i&gt;Special thanks to Michael Reneer for his critical contributions in setting up Parfait. Direct contributors to work on Parfait repositories include Galen Andrew, Isha Arkatkar, Sean Augenstein, Kallista Bonawitz, Amlan Chakraborty, Zachary Charles, Stanislav Chiknavaryan, DeWitt Clinton, Taylor Cramer, Katharine Daly, Stefan Dierauf, Randy Dodgen, Hubert Eichner, Nova Fallen, Ken Franko, Zachary Garrett, Emily Glanz, Zoe Gong, Suxin Guo, Wolfgang Grieskamp, Mira Holford, Dzmitry Huba, Vladimir Ivanov, Peter Kairouz, Yash Katariya, Jakub Konečný, Artem Lagzdin, Hui Li, Stefano Mazzocchi, Brett McLarnon, Sania Nagpal, Krzysztof Ostrowski, Michael Reneer, Jason Roselander, Keith Rush, Karan Singhal, Maya Spivak, Rakshita Tandon, Hardik Vala, Timon Van Overveldt, Scott Wegner, Shanshan Wu, Yu Xiao, Zheng Xu, Ren Yi, Chunxiang Zheng, and Wennan Zhu. We would also like to thank external contributors and collaborators on TensorFlow Federated throughout the years. The private AI research program represented in these repositories is steered by Daniel Ramage and Brendan McMahan, with sponsorship from Corinna Cortes, Blaise Aguera y Arcas, and Yossi Matias.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/parfait-enabling-private-ai-with-research-tools/</link><guid isPermaLink="false">https://research.google/blog/parfait-enabling-private-ai-with-research-tools/</guid><pubDate>Tue, 21 Jan 2025 16:00:00 GMT</pubDate><author>Google</author><category>Distributed Systems &amp; Parallel Computing</category><category>Generative AI</category><category>Responsible AI</category><category>Security, Privacy and Abuse Prevention</category></item><item><title>Zero-shot mono-to-binaural speech synthesis</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6xg42&quot;&gt;Humans possess a remarkable ability to localize sound sources and perceive the surrounding environment through auditory cues alone. This sensory ability, known as &lt;i&gt;spatial hearing&lt;/i&gt;, plays a critical role in numerous everyday tasks, including identifying speakers in crowded conversations and navigating complex environments. Hence, emulating a coherent sense of space via listening devices like headphones becomes paramount to creating truly immersive artificial experiences. Due to the lack of multi-channel and positional data for most acoustic and room conditions, the robust and low- or zero-resource synthesis of &lt;a href=&quot;https://en.wikipedia.org/wiki/Binaural_recording&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;binaural audio&lt;/a&gt; from single-source, single-channel (mono) recordings is a crucial step towards advancing augmented reality (AR) and virtual reality (VR) technologies.&lt;/p&gt;&lt;p data-block-key=&quot;8e4n4&quot;&gt;Conventional mono-to-binaural synthesis techniques leverage a &lt;a href=&quot;https://en.wikipedia.org/wiki/Digital_signal_processing&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;digital signal processing&lt;/a&gt; (DSP) framework. Within this framework, the way sound is scattered across the room to the listener’s ears is formally described by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Head-related_transfer_function#:~:text=HRTF%20describes%20how%20a%20given,ear%20(see%20auditory%20system).&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;head-related transfer function&lt;/a&gt; and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Impulse_response&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;room impulse response&lt;/a&gt;. These functions, along with the ambient noise, are modeled as &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_time-invariant_system&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;linear time-invariant&lt;/a&gt; systems and are obtained in a meticulous process for each simulated room. Such DSP-based approaches are prevalent in commercial applications due to their established theoretical foundation and their ability to generate perceptually realistic audio experiences.&lt;/p&gt;&lt;p data-block-key=&quot;7o7nr&quot;&gt;Considering these limitations in conventional approaches, the possibility of using machine learning to synthesize binaural audio from monophonic sources is very appealing. However, doing so using standard supervised learning models is still very difficult. This is due to two primary challenges: (1) the scarcity of position-annotated binaural audio datasets, and (2) the inherent variability of real-world environments, characterized by diverse room acoustics and background noise conditions. Moreover, supervised models are susceptible to overfitting to the specific rooms, speaker characteristics, and languages in the training data, especially when their training dataset is small.&lt;/p&gt;&lt;p data-block-key=&quot;8i970&quot;&gt;To address these limitations, we present &lt;a href=&quot;https://arxiv.org/abs/2412.08356&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;ZeroBAS&lt;/a&gt;, the first zero-shot method for neural mono-to-binaural audio synthesis, which leverages geometric time warping, amplitude scaling, and a (monaural) denoising vocoder. Notably, we achieve natural binaural audio generation that is perceptually on par with existing supervised methods, despite never seeing binaural data. We further present a novel dataset-building approach and dataset, TUT Mono-to-Binaural, derived from the location-annotated ambisonic recordings of speech events in the &lt;a href=&quot;https://zenodo.org/records/1237703&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;TUT Sound Events 2018 dataset&lt;/a&gt;. When evaluated on this out-of-distribution data, prior supervised methods exhibit degraded performance, while ZeroBAS continues to perform well.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Architecture&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;Our proposed zero-shot mono-to-binaural synthesis method utilizes a three-stage architecture. The first stage follows &lt;a href=&quot;https://openreview.net/pdf?id=uAX8q61EVRu&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;previous work&lt;/a&gt;, performing geometric time warping (GTW) to manipulate the input mono waveform into two channels based on the provided position of the speaker relative to the listener. Subsequently, our proposed amplitude scaling (AS) module adjusts the amplitude of the warped signal according to the relative position information. Finally, an existing denoising vocoder iteratively refines the processed signal to generate the binaural output composed of two channels.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Geometric time warping&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;GTW aims to estimate a warpfield that separates the left and right binaural signals by applying the &lt;a href=&quot;https://en.wikipedia.org/wiki/Interaural_time_difference&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;interaural time delay&lt;/a&gt; based on the relative positions of the sound source and the listener&#39;s ears. GTW generates an initial estimate of the perceived signals by using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Speed_of_sound&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;speed of sound&lt;/a&gt;, the distances between the listener’s ears and the speaker. This approach offers a simple and parameter-free solution for a warpfield which can be applied to the mono signal.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Amplitude scaling&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;In addition to manipulating the time-delay of the signal, we also manipulate the amplitude of the signal based on the position of the speaker. Human spatial perception of sound relies on various factors, including the interaural time delay, the &lt;a href=&quot;https://pressbooks.umn.edu/sensationandperception/chapter/interaural-level-difference-draft/#:~:text=Interaural%20level%20difference%20refers%20to,it%20passes%20through%20your%20head.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;interaural level difference&lt;/a&gt;, and spectral cues due to head-related transfer functions. While &lt;a href=&quot;https://www.acoustics.asn.au/conference_proceedings/ICA2010/cdrom-ICA2010/papers/p45.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior works&lt;/a&gt; suggest that the interaural level difference is mostly caused by scattering off of the head and is dominant in human spatial perception for sounds with high frequencies, we find that amplitude scaling based on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse-square_law&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;inverse square law&lt;/a&gt; has a positive effect on the perceived spatial accuracy of the processed signal. Our approach aims to leverage this amplitude manipulation to enhance the spatial realism of the generated binaural audio.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Denoising vocoder&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;GTW and AS are simple, parameter-free operations that only roughly approximate binaural audio; using the warped and scaled speech signals as-is results in acoustic artifacts and inconsistencies. Hence, there is a need for further refinement to generate natural-sounding binaural audio. To this end, we propose that a sufficiently well-trained denoising vocoder could be used on each signal — we use a &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/10022496&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;WaveFit&lt;/a&gt; neural vocoder. It is a fixed-point iteration vocoder that takes the denoising perspective of Denoising Diffusion Probabilistic Models (&lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DDPMs&lt;/a&gt;) and the discriminator of &lt;a href=&quot;https://arxiv.org/abs/1910.06711&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;generative adversarial networks&lt;/a&gt; to learn to generate natural speech from a degraded input speech signal. As a vocoder, it takes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mel-frequency_cepstrum&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;log-mel spectrogram&lt;/a&gt; features and noise as input and produces clean waveform output. The WaveFit variant used was trained on the &lt;a href=&quot;https://github.com/facebookresearch/libri-light?tab=readme-ov-file&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LibriLight&lt;/a&gt; dataset.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-1-Overview.width-1250.jpg&quot; alt=&quot;BinauralAudio-1-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-1-Overview.width-1250.jpg&quot; alt=&quot;BinauralAudio-1-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;i&gt;Our proposed ZeroBAS method. Mono waveform is binauralized with geometric time warping conditional on the speaker’s position, then the two channels’ amplitudes are scaled. Each channel is then denoised N = 3 times by a low noise-level step of a monaural denoising vocoder.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Audio samples&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Binaural speech dataset&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    &lt;div class=&quot;rich-text --theme- --mode-&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Subject 1&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Subject 2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ground Truth&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject1-GroundTruth.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject2-GroundTruth.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Mono&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject1-mono.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject2-mono.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ZeroBAS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject1-ZeroBAS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinuaralAudio-subject2-ZeroBAS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;WarpNet&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject1-WarpNet.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject2-WarpNet.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BinauralGrad&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject1-BinauralGrad.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject2-BinauralGrad.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;NFS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject1-NFS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject2-NFS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block both --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;TUT mono to binaural dataset&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    &lt;div class=&quot;rich-text --theme- --mode-&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Subject 50&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Subject 73&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ground Truth&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject50-GroundTruth.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject73-GroundTruth.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Mono&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject50-Mono.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject73-Mono.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ZeroBAS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject50-ZeroBAS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject73-ZeroBAS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;WarpNet&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject50-WarpNet.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject73-WarpNet.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BinauralGrad&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinuaralAudio-subject50-BinauralGrad.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject73-BinauralGrad.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;NFS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject50-NFS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/BinauralAudio-subject73-NFS.wav&quot; controls=&quot;controls&quot;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

                    
                    
    


&lt;section class=&quot;component-as-block both --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6xg42&quot;&gt;Additional samples can be heard on our &lt;a href=&quot;https://alonlevko.github.io/zero-bas/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;demo page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Performance&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;We evaluated the performance of ZeroBAS on two datasets. The first is the &lt;a href=&quot;https://github.com/facebookresearch/BinauralSpeechSynthesis/releases/tag/v1.0&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;binaural speech dataset, which contains&lt;/a&gt; paired mono and binaural audio with tracking information, jointly collected in an echo-free (i.e., &lt;i&gt;non-anechoic&lt;/i&gt;) room. The second dataset is an adapted version of TUT Sound Events 2018, which we name &quot;TUT Mono-to-Binaural&quot;. Adaptation was done by converting coordinates from azimuth, distance and elevation to cartesian format, extracting monaural audio and speech segments by using available metadata, and rendering ambisonic recordings into binaural recordings.&lt;br&gt;&lt;/p&gt;&lt;p data-block-key=&quot;sbpg&quot;&gt;For evaluations, similar to prior work, we use:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;4jmrp&quot;&gt;&lt;i&gt;Wave L2&lt;/i&gt;: The &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;mean squared error&lt;/a&gt; (MSE) between the ground truth and synthesized per-channel waveforms (scaled by 103).&lt;/li&gt;&lt;li data-block-key=&quot;825rj&quot;&gt;&lt;i&gt;Amplitude L2&lt;/i&gt;: MSE between the &lt;a href=&quot;https://en.wikipedia.org/wiki/Short-time_Fourier_transform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;short-time Fourier transforms&lt;/a&gt; (STFT) of the ground truth and synthesized audio, with respect to amplitude (multiplied by 10).&lt;/li&gt;&lt;li data-block-key=&quot;fmevu&quot;&gt;&lt;i&gt;Phase L2&lt;/i&gt;: MSE between the left–right phase angle of the ground truth and synthesized audio computed from the STFT.&lt;/li&gt;&lt;li data-block-key=&quot;4hdfm&quot;&gt;&lt;i&gt;MRSTFT&lt;/i&gt;: The multi-resolution spectral loss.&lt;/li&gt;&lt;li data-block-key=&quot;eniqc&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Mean opinion score&lt;/i&gt;&lt;/a&gt;&lt;i&gt; (MOS)&lt;/i&gt;: Human evaluators are tasked with assigning a rating on a five-point scale to denote the perceived naturalness of a given speech utterance.&lt;/li&gt;&lt;li data-block-key=&quot;2fvsd&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/MUSHRA#:~:text=MUSHRA%20stands%20for%20Multiple%20Stimuli,by%20ITU%2DR%20recommendation%20BS.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Multiple Stimuli with Hidden Reference and Anchor&lt;/i&gt;&lt;/a&gt;&lt;i&gt; score (MUSHRA)&lt;/i&gt;: Human raters are asked to rate how similar each model output is to the reference on a scale from 0 to 100.&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Binaural speech dataset&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;Subjective evaluation results show that ZeroBAS sounds slightly more natural to human raters than the supervised methods while being on par in quantitative evaluations. In MOS, ZeroBAS improves over &lt;a href=&quot;https://openreview.net/forum?id=uAX8q61EVRu&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;WarpNet&lt;/a&gt;, and is comparable to &lt;a href=&quot;https://arxiv.org/abs/2205.14807&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BinauralGrad&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2211.00878&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;NFS&lt;/a&gt;, while MUSHRA results show that human raters do not have a statistically significant preference for any of the methods.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-2a-MOS.width-1250.png&quot; alt=&quot;BinauralAudio-2a-MOS&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-2a-MOS.width-1250.png&quot; alt=&quot;BinauralAudio-2a-MOS&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;i&gt;Mean opinion score (MOS) on the Binaural Speech dataset computed by ratings from human evaluators.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-3b-MUSHRA.width-1250.png&quot; alt=&quot;BinauralAudio-3b-MUSHRA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-3b-MUSHRA.width-1250.png&quot; alt=&quot;BinauralAudio-3b-MUSHRA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/MUSHRA#:~:text=MUSHRA%20stands%20for%20Multiple%20Stimuli,by%20ITU%2DR%20recommendation%20BS.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Multiple Stimuli with Hidden Reference and Anchor&lt;/i&gt;&lt;/a&gt;&lt;i&gt; score (MUSHRA) results on the Binaural Speech dataset. Human raters are asked to rate how similar each model output is to the reference.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6xg42&quot;&gt;In the objective evaluation, we observe that ZeroBAS achieves significant objective improvements over the DSP baseline, despite not modeling additional interactions between the two generated channel streams or the room impulse response and head-related transfer function. Furthermore, the performance of the ZeroBAS method approaches that of the supervised methods, even though ZeroBAS has not been trained on the binaural speech dataset.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-4-ObjectiveMetrics.width-1250.png&quot; alt=&quot;BinauralAudio-4-ObjectiveMetrics&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-4-ObjectiveMetrics.width-1250.png&quot; alt=&quot;BinauralAudio-4-ObjectiveMetrics&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;i&gt;Objective metrics evaluating the similarity to the reference waveform on the Binaural Speech dataset, which include Wave L2, Amplitude L2, Phase L2 and MRSTFT.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;TUT Mono-to-Binaural&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;Although the zero-shot method underperforms compared to supervised methods in the objective evaluation on the Binaural Speech dataset, we argue that the supervised methods are more sensitive to the room and recording conditions&lt;i&gt; of that dataset&lt;/i&gt;. But in a real-world scenario, systems would be expected to perform well across a wide range of acoustic conditions, even ones on which they were not originally trained. To demonstrate that ZeroBAS fulfills this criteria, we evaluated all methods on our newly constructed TUT Mono-to-Binaural dataset.&lt;br&gt;&lt;/p&gt;&lt;p data-block-key=&quot;nltl&quot;&gt;The subjective evaluation further demonstrates that ZeroBAS exhibits better performance in terms of perceived naturalness compared to the supervised methods WarpNet, BinauralGrad, and NFS. As evidenced by the MOS, ZeroBAS surpasses these methods by notable margins. Furthermore, MUSHRA evaluations reveal a statistically significant preference for the proposed ZeroBAS method compared to supervised approaches. This suggests that human listeners perceive the spatial quality of binaural signals generated by ZeroBAS to best align with the reference.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-5a-MUSHRAforTUT.width-1250.png&quot; alt=&quot;BinauralAudio-5a-MUSHRAforTUT&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-5a-MUSHRAforTUT.width-1250.png&quot; alt=&quot;BinauralAudio-5a-MUSHRAforTUT&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;i&gt;MUSHRA scores on the TUT Mono To Binaural dataset, where human raters are asked to rate how similar each model output is to the reference.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-6a-MOSforTUT.width-1250.png&quot; alt=&quot;BinauralAudio-6a-MOSforTUT&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-6a-MOSforTUT.width-1250.png&quot; alt=&quot;BinauralAudio-6a-MOSforTUT&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;i&gt;MOS on the TUT Mono to Binaural dataset computed by ratings from human evaluators.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6xg42&quot;&gt;Objective evaluation demonstrates that our zero-shot method, ZeroBAS, significantly outperforms all supervised methods on the TUT Mono-to-Binaural dataset. Yet, both ZeroBAS and the supervised methods struggle to capture accurate phase information, as evidenced by the Phase L2 metric, which indicates that there is still additional work to be done.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-7-ObjMetricsForTUT.width-1250.png&quot; alt=&quot;BinauralAudio-7-ObjMetricsForTUT&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/BinauralAudio-7-ObjMetricsForTUT.width-1250.png&quot; alt=&quot;BinauralAudio-7-ObjMetricsForTUT&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;tisrr&quot;&gt;&lt;i&gt;Objective metrics for the TUT Mono to Binaural dataset, which includes Wave L2, amplitude L2, Phase L2 and MRSTFT. These evaluate the similarity to the reference waveform.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;In this work, we presented a room-agnostic zero-shot method for binaural speech synthesis from monaural audio. Our results demonstrate that the method achieves perceptual performance comparable to supervised approaches on their in-distribution datasets. Furthermore, we introduce a novel dataset specifically designed to evaluate the generalization capabilities of monaural-to-binaural synthesis methods for out-of-distribution scenarios.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6xg42&quot;&gt;&lt;i&gt;The direct contributors to this work include Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn and Eliya Nachmani. We also want to thank Heiga Zhen, Yuma Koizumi and Benny Schlesinger.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/zero-shot-mono-to-binaural-speech-synthesis/</link><guid isPermaLink="false">https://research.google/blog/zero-shot-mono-to-binaural-speech-synthesis/</guid><pubDate>Wed, 15 Jan 2025 16:00:00 GMT</pubDate><author>Google</author><category>Sound &amp; Accoustics</category><category>Speech Processing</category></item><item><title>Understanding Transformer reasoning capabilities via graph algorithms</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;46zps&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Transformers&lt;/a&gt; are&amp;nbsp;a general purpose sequential deep learning architecture &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;introduced by Google in 2017&lt;/a&gt;. They have recently, and surprisingly, &lt;a href=&quot;https://sites.research.google/greenlight/&quot;&gt;hinted at being good at learning graph-based algorithmic tasks&lt;/a&gt; like connectivity (e.g., “Is there a path from node A to node B?”), shortest path (“How many edges must one take to get from node A to node B?”), and cycle checks (“Is there a cycle in the graph, e.g., edges from A → B, B → C, C → A?”).&lt;/p&gt;&lt;p data-block-key=&quot;3p4v0&quot;&gt;Why is this surprising? Because unlike &lt;a href=&quot;https://paperswithcode.com/method/mpnn&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;message passing neural networks&lt;/a&gt; (MPNNs), whose parameters are explicitly encoded with the structure of the input graphs, transformers are a general purpose architecture best-known for language tasks. Their backbone is a computationally expensive &lt;a href=&quot;https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;self-attention mechanism&lt;/a&gt; that encodes associations between input words/tokens and is not explicitly designed for graphs. So, while transformer-based neural networks have made tremendous empirical advances, we clearly still lack a theoretical understanding of their algorithmic reasoning capabilities in realistic scenarios. Knowing which architecture is best for which real-world tasks means spending a lot less time defining and evaluating new architectures via hand-picked tasks and a much better understanding of architectural decision trade-offs.&lt;/p&gt;&lt;p data-block-key=&quot;6sfap&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/pdf/2405.18512&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Understanding Transformer Reasoning Capabilities via Graph Algorithms&lt;/a&gt;”, we conduct an extensive study of graph reasoning problems with transformers from both theoretical and empirical perspectives. We find that transformers can solve parallelizable tasks (like connectivity) in a highly parameter-efficient manner and outperform &lt;a href=&quot;https://research.google/blog/robust-graph-neural-networks/&quot;&gt;graph neural networks&lt;/a&gt; (GNNs) on tasks that require the analysis of long-range dependencies. We also introduce a novel representational hierarchy of graph reasoning tasks that formalizes reasoning capabilities of transformers in several realistic parameter scaling regimes.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;46zps&quot;&gt;Experiment&lt;/h2&gt;&lt;p data-block-key=&quot;6ebg5&quot;&gt;MPNNs are a bespoke neural architecture explicitly designed to learn functions that respect graph structures, while transformers are much more generic. But transformers and MPNNs can be trained to solve graph algorithmic problems using similar ML frameworks.&lt;/p&gt;&lt;p data-block-key=&quot;6juso&quot;&gt;For the empirical part of the experiment, we investigated if a transformer could encode graph tasks the way MPNNs do, and if so, do it more efficiently.&lt;/p&gt;&lt;p data-block-key=&quot;elm1d&quot;&gt;To test this, we encoded a graph task as a transformer input. Given a graph instance (with nodes and edges) and a query (determine whether nodes &lt;i&gt;𝑣&lt;/i&gt;2 and &lt;i&gt;𝑣&lt;/i&gt;4 are connected), we encoded each node ID as a discrete token (similarly to how LLMs map words to tokens) and each edge as a pair of tokens. We then encoded the problem as a list of node tokens, a list of edge tokens, and a token encoding of the query.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/TFGraphs1.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;8laoe&quot;&gt;&lt;i&gt;How a graph reasoning task instance (here, graph connectivity) is embedded as a transformer input.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;bkrsc&quot;&gt;Seeing as transformers and MPNNs are not the only ML approaches for the structural analysis of graphs, we also compared the analytical capabilities of a wide variety of other GNN- and transformer-based architectures. For GNNs, we compared both transformers and MPNNs to models like graph &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;convolutional networks&lt;/a&gt; (GCNs) and graph &lt;a href=&quot;https://wandb.ai/graph-neural-networks/GIN/reports/What-are-Graph-Isomorphism-Networks---Vmlldzo1MTExMTg5&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;isomorphism networks&lt;/a&gt; (GINs).&lt;/p&gt;&lt;p data-block-key=&quot;fbr91&quot;&gt;Additionally, we compared our transformers with much larger language models. Language models are transformers as well, but with many orders of magnitude more parameters. We compared transformers to the language modeling approach described in &lt;a href=&quot;https://arxiv.org/abs/2310.04560&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Talk Like a Graph&lt;/a&gt;, which encodes the graph as text, using natural language to describe relationships instead of treating an input graph as a collection of abstract tokens.&lt;/p&gt;&lt;p data-block-key=&quot;37dhi&quot;&gt;We asked a trained language model to solve various retrieval tasks with a variety of prompting approaches:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;f2fgm&quot;&gt;&lt;i&gt;Zero-shot&lt;/i&gt;, which provides only a single prompt and asks for the solution without further hints.&lt;/li&gt;&lt;li data-block-key=&quot;fpvc2&quot;&gt;&lt;i&gt;Few-shot&lt;/i&gt;, which provides several examples of solved prompt–response pairs before asking the model to solve a task.&lt;/li&gt;&lt;li data-block-key=&quot;3v6pg&quot;&gt;&lt;a href=&quot;https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/&quot;&gt;C&lt;i&gt;hain-of-thought&lt;/i&gt;&lt;/a&gt;&lt;i&gt; (CoT)&lt;/i&gt;, which provides a collection of examples (similar to few-shot), each of which contains a prompt, a response, and an explanation before asking the model to solve a task.&lt;/li&gt;&lt;li data-block-key=&quot;4oa5p&quot;&gt;&lt;i&gt;Zero-shot CoT&lt;/i&gt;, which asks the model to show its work, without including additional worked-out examples as context.&lt;/li&gt;&lt;li data-block-key=&quot;apjm3&quot;&gt;&lt;i&gt;CoT-bag&lt;/i&gt;, which asks the LLM to construct a graph before being provided with relevant information.&lt;/li&gt;&lt;/ul&gt;&lt;p data-block-key=&quot;7e61l&quot;&gt;For the theoretical part of the experiment, we created a task difficulty hierarchy to assess which tasks transformers can solve with small models.&lt;/p&gt;&lt;p data-block-key=&quot;af7u8&quot;&gt;We only considered graph reasoning tasks that apply to undirected and unweighted graphs of bounded size: node count, edge count, edge existence, &lt;a href=&quot;https://en.wikipedia.org/wiki/Degree_(graph_theory)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;node degree&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Connectivity_(graph_theory)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;connectivity&lt;/a&gt;, node connectivity (for undirected graphs), &lt;a href=&quot;https://en.wikipedia.org/wiki/Cycle_(graph_theory)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;cycle check&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Shortest_path_problem&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;shortest path&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;p data-block-key=&quot;r23f&quot;&gt;In this hierarchy, we categorized graph task difficulty based on depth (the number of self-attention layers in the transformer, computed sequentially), width (the dimension of the vectors used for each graph token), number of blank tokens, and three different types:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;ctcrd&quot;&gt;&lt;i&gt;Retrieval tasks&lt;/i&gt;: easy, local aggregation tasks.&lt;/li&gt;&lt;li data-block-key=&quot;4leef&quot;&gt;&lt;i&gt;Parallelizable tasks&lt;/i&gt;: tasks that benefit greatly from parallel operations.&lt;/li&gt;&lt;li data-block-key=&quot;11lmu&quot;&gt;&lt;i&gt;Search&lt;/i&gt;: tasks with limited benefits from parallel operations.&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;bkrsc&quot;&gt;Results&lt;/h2&gt;&lt;h3 data-block-key=&quot;1eqti&quot;&gt;Empirical&lt;/h3&gt;&lt;p data-block-key=&quot;agsee&quot;&gt;For small models (~60M parameters) trained on specialized data, transformers outperform much larger LLMs without targeted training (as shown in the bar chart below). Also, their algorithmic reasoning capabilities can be enhanced with few samples and few parameters.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/TFGraphs2-Results.width-1250.png&quot; alt=&quot;TFGraphs2-Results&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/TFGraphs2-Results.width-1250.png&quot; alt=&quot;TFGraphs2-Results&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3j89g&quot;&gt;&lt;i&gt;Accuracy on all retrieval tasks (&lt;/i&gt;&lt;b&gt;&lt;i&gt;top&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) and parallelizable and search tasks (&lt;/i&gt;&lt;b&gt;&lt;i&gt;bottom&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) by mode of training, contrasting prompted PaLM2 LLMs (&lt;/i&gt;&lt;b&gt;&lt;i&gt;left&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) and trained transformers (&lt;/i&gt;&lt;b&gt;&lt;i&gt;right&lt;/i&gt;&lt;/b&gt;&lt;i&gt;).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;bkrsc&quot;&gt;As shown below, MPNNs perform better on “local” tasks, where we aggregate information about neighboring nodes without needing to make many “hops” of reasoning. Whether trained on 1K or 100K samples, MPNNs have higher accuracies on local tasks like node degree (the maximum number of edges adjacent to a node) and cycle check.&lt;/p&gt;&lt;p data-block-key=&quot;5efk4&quot;&gt;Moreover, transformers perform better on “global” tasks, where information needs to be propagated across the graph instances. An example of a global task would be graph connectivity, because any two nodes might be far apart in a graph.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/TFGraphs3-Architecture.width-1250.png&quot; alt=&quot;TFGraphs3-Architecture&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/TFGraphs3-Architecture.width-1250.png&quot; alt=&quot;TFGraphs3-Architecture&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3j89g&quot;&gt;&lt;i&gt;Task performance by architecture, comparing node degree to cycle check to connectivity.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;bkrsc&quot;&gt;These results show that the difference between the small sample regime and the large sample regime is much greater for transformers, and that GNNs don&#39;t come close to transformer performance when trained on 100K samples.&lt;/p&gt;&lt;p data-block-key=&quot;fsov8&quot;&gt;There are two key dynamics at play when training neural networks: capacity and learnability. Capacity represents the fundamental limit of a neural network architecture. That is, capacity governs what kinds of mathematical functions can be represented by a neural architecture. Learnability asks whether these functions can actually be learned in an ML setting with a relatively small number of training samples. We say that an architecture has a positive &lt;i&gt;inductive bias&lt;/i&gt; for certain kinds of solutions if it tends to gravitate towards them with few samples.&lt;/p&gt;&lt;p data-block-key=&quot;83clb&quot;&gt;If we try to understand our transformer and MPNN results under this framework, we see that GNNs indeed have better inductive biases for graph tasks, as is evident by their superior performance in less data-rich environments. On the other hand, the success of the transformers on larger datasets indicates that GNNs may be limited by &lt;i&gt;capacity&lt;/i&gt;, since those reflect fundamental model shortcomings that cannot be overcome with more data.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h3 data-block-key=&quot;bkrsc&quot;&gt;Theoretical&lt;/h3&gt;&lt;p data-block-key=&quot;c93pv&quot;&gt;We supported the empirical results with theoretical results that contrast the capacity, or expressivity, of the two architectures. &lt;a href=&quot;https://arxiv.org/abs/2402.09268&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Prior work&lt;/a&gt; has shown that transformers can simulate parallel algorithms, while other models like MPNNs are &lt;a href=&quot;https://arxiv.org/abs/1907.03199&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;limited&lt;/a&gt; by their lack of global connections. Using our task difficulty hierarchy, we showed that transformers have a much greater expressivity for parallelizable tasks under the &lt;a href=&quot;https://arxiv.org/abs/1805.03055&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MPC model&lt;/a&gt; and can solve them using logarithmic depth and bounded width (that is, the width of the model is at most some quantity &lt;i&gt;m&lt;/i&gt; that grows much slower than the number of graph tokens &lt;i&gt;N&lt;/i&gt;, i.e. &lt;i&gt;m ≤ N0.1&lt;/i&gt;). On the other hand, no MPNN can solve these tasks unless they are much larger both computationally and in terms of parameter count.&lt;/p&gt;&lt;p data-block-key=&quot;6gf7u&quot;&gt;We also found that &lt;i&gt;retrieval&lt;/i&gt; tasks are solvable with a single-layer transformer model, and &lt;i&gt;parallelizable&lt;/i&gt; tasks can be done easily with log depth (i.e., the depth of the transformer scales logarithmically with number of nodes in the graph). &lt;i&gt;Search&lt;/i&gt; tasks can also be done with log depth, but not in a parameter-efficient way.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/TFGraphs4-Hierarchy.width-1250.png&quot; alt=&quot;TFGraphs4-Hierarchy&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/TFGraphs4-Hierarchy.width-1250.png&quot; alt=&quot;TFGraphs4-Hierarchy&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3j89g&quot;&gt;&lt;i&gt;A visualization of the task difficulty hierarchy, where retrieval, parallel, and search tasks are organized based on the complexity of the transformers needed to solve them.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;bkrsc&quot;&gt;These results suggest why transformers perform so much better on connectivity — unlike GNNs, the size of the model (in terms of depth and number of parameters) does not need to grow rapidly with the size of the input graph.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;bkrsc&quot;&gt;Conclusion&lt;/h2&gt;&lt;p data-block-key=&quot;ctnof&quot;&gt;We provide a comprehensive evaluation of transformer models’ graph reasoning capabilities, shedding light on their effectiveness across diverse graph reasoning tasks. By introducing a novel representational hierarchy, the study distinguishes between retrieval, parallelizable, and search reasoning tasks and offers insights into the performance of transformers at varying levels of granularity. The empirical investigation reveals that transformers exhibit strong performance in graph-based reasoning problems, often matching or surpassing specialized graph models. Furthermore, the study highlights transformers’ exceptional ability to capture global graph patterns effectively, showcasing their capability in understanding long-range dependencies, a critical factor in solving tasks involving global graph structures. Overall, this work crystallizes precise representational trade-offs that reflect the fundamental reasoning capabilities of transformers and demonstrates that the tasks used to quantify those capabilities are indeed learnable in a sample-efficient and parameter-efficient manner.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;bkrsc&quot;&gt;Acknowledgments&lt;/h2&gt;&lt;p data-block-key=&quot;lm2b&quot;&gt;&lt;i&gt;This work was done in collaboration with colleagues across Google, including Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Bryan Perozzi, Jonathan Halcrow, and Vahab Mirrokni.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/understanding-transformer-reasoning-capabilities-via-graph-algorithms/</link><guid isPermaLink="false">https://research.google/blog/understanding-transformer-reasoning-capabilities-via-graph-algorithms/</guid><pubDate>Thu, 19 Dec 2024 16:00:00 GMT</pubDate><author>Google</author><category>Algorithms &amp; Theory</category><category>Data Mining &amp; Modeling</category></item></channel></rss>