<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Google Research Blog</title><link>https://research.google/blog</link><atom:link href="http://rss.datuan.dev/google/research" rel="self" type="application/rss+xml"></atom:link><description>Google Research Blog - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Tue, 25 Mar 2025 00:32:50 GMT</lastBuildDate><ttl>5</ttl><item><title>Deciphering language processing in the human brain through LLM representations</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6iix5&quot;&gt;How does the human brain process natural language during everyday conversations? Theoretically, large language models (LLMs) and symbolic psycholinguistic models of human language provide a fundamentally different computational framework for coding natural language. Large language models do not depend on symbolic parts of speech or syntactic rules. Instead, they utilize simple self-supervised objectives, such as next-word prediction and generation enhanced by &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;reinforcement learning&lt;/a&gt;. This allows them to produce context-specific linguistic outputs drawn from real-world text corpora, effectively encoding the statistical structure of natural speech (sounds) and language (words) into a multidimensional embedding space.&lt;/p&gt;&lt;p data-block-key=&quot;dc141&quot;&gt;Inspired by the success of LLMs, our team at Google Research, in collaboration with &lt;a href=&quot;https://hassonlab.princeton.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Princeton University&lt;/a&gt;, &lt;a href=&quot;https://nyulangone.org/locations/comprehensive-epilepsy-center&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;NYU&lt;/a&gt;, and &lt;a href=&quot;https://www.deepcognitionlab.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;HUJI&lt;/a&gt;, sought to explore the similarities and differences in how the human brain and deep language models process natural language to achieve their remarkable capabilities. Through a series of studies over the past five years, we explored the similarity between the internal representations (embeddings) of specific deep learning models and human brain neural activity during natural free-flowing conversations, demonstrating the power of deep language model’s embeddings to act as a framework for understanding how the human brain processes language. We demonstrate that the word-level internal embeddings generated by deep language models align with the neural activity patterns in established brain regions associated with speech comprehension and production in the human brain.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;6iix5&quot;&gt;Similar embedding-based representations of language.&lt;/h2&gt;&lt;p data-block-key=&quot;5u3tk&quot;&gt;Our most &lt;a href=&quot;https://www.nature.com/articles/s41562-025-02105-9&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;recent study&lt;/a&gt;, published in &lt;a href=&quot;https://www.nature.com/nathumbehav/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Nature Human Behaviour&lt;/i&gt;&lt;/a&gt;, investigated the alignment between the internal representations in a Transformer-based speech-to-text model and the neural processing sequence in the human brain during real-life conversations. In the study, we analyzed neural activity recorded using &lt;a href=&quot;https://ecog.med.nyu.edu/intracranial-electrodes/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;intracranial electrodes&lt;/a&gt; during spontaneous conversations. We compared patterns of neural activity with the internal representations — embeddings — generated by the &lt;a href=&quot;https://arxiv.org/abs/2212.04356&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Whisper&lt;/a&gt; speech-to-text model, focusing on how the model&#39;s linguistic features aligned with the brain&#39;s natural speech processing.&lt;/p&gt;&lt;p data-block-key=&quot;ae2d8&quot;&gt;For every word heard (during speech comprehension) or spoken (during speech production), two types of embeddings were extracted from the speech-to-text model — &lt;i&gt;speech&lt;/i&gt; embeddings from the model’s speech encoder and word-based &lt;i&gt;language&lt;/i&gt; embeddings from the model&#39;s decoder. A linear transformation was estimated to predict the brain’s neural signals from the speech-to-text embeddings for each word in each conversation. The study revealed a remarkable alignment between the neural activity in the human brain&#39;s speech areas and the model&#39;s speech embeddings and between the neural activity in the brain’s language area and the model&#39;s language embeddings. The alignment is illustrated in the following animation, modeling the sequence of the brain’s neural responses to subjects’ language comprehension:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/PITOM1_Comprehension.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1eu9y&quot;&gt;&lt;i&gt;Sequence of the brain’s neural responses to subjects&#39; language comprehension as they listen to the sentence “How are you doing?”.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6iix5&quot;&gt;As the listener processes the incoming spoken words, we observe a sequence of neural responses: Initially, as each word is articulated, speech embeddings enable us to predict cortical activity in speech areas along the &lt;a href=&quot;https://en.wikipedia.org/wiki/Superior_temporal_gyrus&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;superior temporal gyrus&lt;/a&gt; (STG). A few hundred milliseconds later, when the listener starts to decode the meaning of the words, language embeddings predict cortical activity in &lt;a href=&quot;https://en.wikipedia.org/wiki/Broca%27s_area&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Broca’s area&lt;/a&gt; (located in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Inferior_frontal_gyrus&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;inferior frontal gyrus&lt;/a&gt;; IFG).&lt;/p&gt;&lt;p data-block-key=&quot;8nte6&quot;&gt;Turning to participants&#39; production, we observe a different (reversed!) sequence of neural responses:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/PITOM2_Production.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1eu9y&quot;&gt;&lt;i&gt;Sequence of neural responses to subjects’ language production as they answer “feeling fantastic&quot;.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6iix5&quot;&gt;Looking at this alignment more closely, about 500 milliseconds before articulating the word (as the subject prepares to articulate the next word), language embeddings (depicted in blue) predict cortical activity in &lt;a href=&quot;https://en.wikipedia.org/wiki/Broca%27s_area&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Broca’s area&lt;/a&gt;. A few hundred milliseconds later (still before word onset), speech embeddings (depicted in red) predict neural activity in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Motor_cortex&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;motor cortex&lt;/a&gt; (MC) as the speaker plans the articulatory speech sequence. Finally, after the speaker articulates the word, speech embeddings predict the neural activity in the STG auditory areas as the listener listens to their own voice. This dynamic reflects the sequence of neural processing, starting with planning what to say in language areas, then how to articulate it in motor areas, and finally monitoring what was spoken in perceptual speech areas.&lt;/p&gt;&lt;p data-block-key=&quot;ct6ei&quot;&gt;The quantitative results of the whole-brain analysis are illustrated in figure below: for each word, given its speech embeddings (red) and language embedding (blue), we predicted the neural response in each electrode at time lags ranging from -2 seconds before to +2 seconds after the word onset (&lt;i&gt;x&lt;/i&gt;-axis value of 0 in the figure). This was done during speech production (left panel) and speech comprehension (right panel). The related graphs illustrate the accuracy of our predictions of neural activity (correlation) for all words as a function of the lag in the electrodes across various brain regions.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PITOM3_ResultsHERO.width-1250.jpg&quot; alt=&quot;PITOM3_ResultsHERO&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PITOM3_ResultsHERO.width-1250.jpg&quot; alt=&quot;PITOM3_ResultsHERO&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1eu9y&quot;&gt;&lt;i&gt;Fitting speech and language embeddings to human brain signals at production and comprehension.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6iix5&quot;&gt;During speech production, it is evident that language embeddings (blue) in the IFG peaked before speech embeddings (red) peaked in the sensorimotor area, followed by the peak of speech encoding in the STG. In contrast, during speech comprehension, the peak encoding shifted to after the word onset, with speech embeddings (red) in the STG peaking significantly before language encoding (blue) in the IFG.&lt;/p&gt;&lt;p data-block-key=&quot;638ar&quot;&gt;All in all, our findings suggest that the speech-to-text model embeddings provide a cohesive framework for understanding the neural basis of processing language during natural conversations. Surprisingly, while Whisper was developed solely for speech recognition, without considering how the brain processes language, we found that its internal representations align with neural activity during natural conversations. This alignment was not guaranteed — a negative result would have shown little to no correspondence between the embeddings and neural signals, indicating that the model&#39;s representations did not capture the brain&#39;s language processing mechanisms.&lt;/p&gt;&lt;p data-block-key=&quot;do4mr&quot;&gt;A particularly intriguing concept revealed by the alignment between LLMs and the human brain is the notion of a &quot;soft hierarchy&quot; in neural processing. Although regions of the brain involved in language, such as the IFG, tend to prioritize word-level semantic and syntactic information — as indicated by stronger alignment with language embeddings (blue) — they also capture lower-level auditory features, which is evident from the lower yet significant alignment with speech embeddings (red). Conversely, lower-order speech areas such as the STG tend to prioritize acoustic and phonemic processing — as indicated by stronger alignment with speech embeddings (red) — they also capture word-level information, evident from the lower yet significant alignment with language embeddings (blue).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;6iix5&quot;&gt;Shared objectives and geometry between LLMs and the human brain&lt;/h2&gt;&lt;p data-block-key=&quot;6j9ts&quot;&gt;LLMs are trained to process natural language by using a simple objective: predicting the next word in a sequence. In a &lt;a href=&quot;https://www.nature.com/articles/s41593-022-01026-4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;paper&lt;/a&gt; published in &lt;a href=&quot;https://www.nature.com/neuro/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Nature Neuroscience&lt;/i&gt;&lt;/a&gt;, we discovered that, similar to LLMs, the language areas of a listener’s brain attempt to predict the next word before it is spoken. Furthermore, like LLMs, listeners&#39; confidence in their predictions before the word’s onset modifies their surprise level (prediction error) after the word is articulated. These findings provide compelling new evidence for fundamental computational principles of pre-onset prediction, post-onset surprise, and embedding-based contextual representation shared by autoregressive LLMs and the human brain. In another &lt;a href=&quot;https://www.nature.com/articles/s41467-024-46631-y&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;paper&lt;/a&gt; published in &lt;a href=&quot;https://www.nature.com/ncomms/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Nature Communications&lt;/i&gt;&lt;/a&gt;, the team also discovered that the relation among words in natural language, as captured by the geometry of the embedding space of an LLM, is aligned with the geometry of the representation induced by the brain (i.e., brain embeddings) in language areas.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;6iix5&quot;&gt;Differences between how LLMs and the human brain process natural language&lt;/h2&gt;&lt;p data-block-key=&quot;abajj&quot;&gt;While the human brain and Transformer-based LLMs share fundamental computational principles in processing natural language, their underlying neural circuit architectures are markedly different. For example, in a &lt;a href=&quot;https://arxiv.org/abs/2310.07106&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;follow-up study&lt;/a&gt;, we investigated how information is processed across layers in Transformer-based LLMs compared to the human brain. The team found that while the non-linear transformations across layers are similar in LLMs and language areas in the human brain, the implementations differ significantly. Unlike the Transformer architecture, which processes hundreds to thousands of words simultaneously, the language areas appear to analyze language serially, word by word, recurrently, and temporally.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;6iix5&quot;&gt;Summary and future directions&lt;/h2&gt;&lt;p data-block-key=&quot;bajqj&quot;&gt;The accumulated evidence from the team’s work uncovered several shared computational principles between how the human brain and deep learning models process natural language. These findings indicate that deep learning models could offer a new computational framework for understanding the brain&#39;s neural code for processing natural language based on principles of statistical learning, blind optimization, and a &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S089662731931044X&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;direct fit to nature&lt;/a&gt;. At the same time, there are significant differences between the neural architecture, types and scale of linguistic data, the training protocols of Transformer-based language models, and the biological structure and developmental stages through which the human brain naturally acquires language in social settings environments. Moving forward, our goal is to create innovative, biologically inspired artificial neural networks that have improved capabilities for processing information and functioning in the real world. We plan to achieve this by adapting neural architecture, learning protocols, and training data that better match human experiences.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;6iix5&quot;&gt;Acknowledgments&lt;/h2&gt;&lt;p data-block-key=&quot;3cmiq&quot;&gt;&lt;i&gt;The work described is the result of Google Research&#39;s long-term collaboration with&lt;/i&gt; &lt;i&gt;the&lt;/i&gt; &lt;a href=&quot;https://hassonlab.princeton.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Hasson Lab&lt;/i&gt;&lt;/a&gt;&lt;i&gt; at the Neuroscience Institute and the Psychology Department at Princeton University, the&lt;/i&gt; &lt;a href=&quot;https://www.deepcognitionlab.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;DeepCognitionLab&lt;/i&gt;&lt;/a&gt; &lt;i&gt;at the Hebrew University Business School and Cognitive Department, and researchers from the&lt;/i&gt; &lt;a href=&quot;https://nyulangone.org/locations/comprehensive-epilepsy-center&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;NYU Langone Comprehensive Epilepsy Center.&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/</link><guid isPermaLink="false">https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/</guid><pubDate>Thu, 20 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>General Science</category><category>Health &amp; Bioscience</category><category>Natural Language Processing</category><category>Speech Processing</category></item><item><title>Load balancing with random job arrivals</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;i8n7v&quot;&gt;Cluster management systems, such as Google’s &lt;a href=&quot;https://research.google.com/pubs/pub43438.html&quot;&gt;Borg&lt;/a&gt;, run hundreds of thousands of jobs across tens of thousands of machines with the goal of achieving high utilization via effective load balancing, efficient task placement, and machine sharing. &lt;a href=&quot;https://www.usenix.org/conference/nsdi24/presentation/wydrowski&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Load balancing&lt;/a&gt; is the process of distributing network traffic or computational workloads across multiple servers or computing resources, and it is one of the most critical components of a modern cluster management system. Effective load balancing is critical to improving the performance, robustness and scalability of the system.&lt;/p&gt;&lt;p data-block-key=&quot;9o1vj&quot;&gt;In the &lt;a href=&quot;https://dl.acm.org/doi/10.5555/139404.139450&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;classical formulation&lt;/a&gt; of the online load balancing problem, computational jobs arrive one-by-one and, as soon as a job arrives, it must be assigned to one of several machines. Each job may impose different processing loads on different machines, and the load incurred by a machine depends on the jobs that are assigned to it. The goal of a load balancing algorithm is to minimize the maximum load on any machine. &lt;a href=&quot;https://en.wikipedia.org/wiki/Online_algorithm&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Online algorithms&lt;/a&gt; are those designed for situations where the input to the system is revealed to the algorithm piece by piece.&lt;/p&gt;&lt;p data-block-key=&quot;64dh0&quot;&gt;Online problems are common in decision-making scenarios that have uncertainty, including the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ski_rental_problem&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;ski-rental problem&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Secretary_problem&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;secretary problem&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_(computing)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;caching&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Job-shop_scheduling&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;scheduling&lt;/a&gt; problems, and many others. Scheduling and load balancing questions are prevalent in resource management for large-scale systems leading to research into many real-world scheduling problems, including &lt;a href=&quot;https://research.google/blog/consistent-hashing-with-bounded-loads/&quot;&gt;maintaining consistent allocation of clients to servers&lt;/a&gt; and, more recently, &lt;a href=&quot;https://cloud.google.com/blog/products/compute/introducing-dynamic-workload-scheduler&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;platforms for AI workloads&lt;/a&gt;. Traditionally, online algorithms for scheduling and load balancing are studied through the lens of competitive analysis. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Competitive_analysis_(online_algorithm)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;competitive ratio&lt;/a&gt; of an online algorithm quantifies the worst-case performance of the algorithm relative to an optimal offline algorithm that knows future jobs, specifically by determining the worst-case ratio of the cost incurred by the two algorithms over all possible sequences of jobs.&lt;/p&gt;&lt;p data-block-key=&quot;7rt03&quot;&gt;In “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3626183.3659983&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Online Load and Graph Balancing for Random Order Inputs&lt;/a&gt;”, presented at &lt;a href=&quot;https://spaa.acm.org/spaa-2024-2/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;SPAA 2024&lt;/a&gt;, we study the competitive ratio of online load balancing problems when jobs arrive in uniformly random order (i.e., when each possible permutation of job arrival sequences is equally likely). We show new limitations on how well deterministic online algorithms can perform in this setting.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A tree balancing game&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;i8n7v&quot;&gt;Consider the following game between an adversary and an algorithm: the adversary selects a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tree_(graph_theory)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;tree&lt;/a&gt; (a simple graph with no cycles, i.e., no path that can connect back to itself) whose nodes are labeled and presents to the algorithm the edges of the tree, one at a time, in an order it chooses. When the algorithm receives an undirected edge (&lt;i&gt;u&lt;/i&gt; — &lt;i&gt;v&lt;/i&gt;), it must choose the orientation, either &lt;i&gt;u&lt;/i&gt; → &lt;i&gt;v&lt;/i&gt; or &lt;i&gt;u&lt;/i&gt; ← &lt;i&gt;v&lt;/i&gt;. The goal of the algorithm is to minimize the maximum number of edges that are oriented towards any particular node, i.e., to minimize the maximum &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_graph#Indegree_and_outdegree&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;indegree&lt;/a&gt; of the tree. This simple game is a special instance of online load balancing. Indeed, each node of the tree corresponds to a machine and each edge corresponds to a job.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- For GIFs, use a default width --&gt;
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LoadBalancing-1-TreeBalancing.width-800.gif&quot; alt=&quot;LoadBalancing-1-TreeBalancing&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LoadBalancing-1-TreeBalancing.width-800.gif&quot; alt=&quot;LoadBalancing-1-TreeBalancing&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3hvn1&quot;&gt;&lt;i&gt;Illustration of the tree balancing game. Each dashed line denotes an undirected edge presented by the adversary. The animation shows the outcome of a&lt;/i&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Greedy_algorithm&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;greedy algorithm&lt;/i&gt;&lt;/a&gt;&lt;i&gt; that orients the incoming edge towards the node with the smaller indegree.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;i8n7v&quot;&gt;Since the 1990s, it has been known that no deterministic online algorithm can guarantee that the indegree of the tree will always be less than log &lt;i&gt;n&lt;/i&gt;, where &lt;i&gt;n&lt;/i&gt; is the number of nodes in the tree. Since one can always orient the edges of a tree so that each node has indegree at most 1, this shows that any deterministic algorithm must have a competitive ratio of at least log &lt;i&gt;n&lt;/i&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;52o8j&quot;&gt;The following animation illustrates such a hard instance. At the first step, a batch of edges arrive that all look exactly identical. Since there is no way of distinguishing the endpoints of any edge, the algorithm can do no better than orienting the edge arbitrarily (in the animation, each edge gets oriented towards the yellow node). The second batch of edges is then incident only on the yellow nodes forcing the algorithm to increase the indegree of the tree. This process then continues until no more batches can be constructed.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- For GIFs, use a default width --&gt;
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LoadBalancing-2-Deterministic.width-800.gif&quot; alt=&quot;LoadBalancing-2-Deterministic&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LoadBalancing-2-Deterministic.width-800.gif&quot; alt=&quot;LoadBalancing-2-Deterministic&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3hvn1&quot;&gt;&lt;i&gt;Any deterministic algorithm must incur a maximum load of at least log&lt;/i&gt; n&lt;i&gt;. Both end points of each arriving edge look identical to any algorithm. In each batch of edges, the maximum indegree of the graph increases, eventually leading to a node with degree log&lt;/i&gt; n&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;i8n7v&quot;&gt;In the example above, the adversary crucially needs to choose the arrival order of edges to ensure that the algorithm makes the wrong decision for each edge. But what happens when the tree edges arrive in random order?&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Random order arrivals&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;i8n7v&quot;&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/002001909500123T&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Previous work&lt;/a&gt; showed that the greedy algorithm, which orients each incoming edge towards the node with the smaller indegree, performs only slightly better in the random order model. However, the possibility that other, smarter algorithms could perform much better remained open and left a large gap in our understanding of the problem. In our work described below, we construct a new instance that improves the lower bound exponentially and show that no online algorithm can guarantee a competitive ratio significantly better than √(log &lt;i&gt;n&lt;/i&gt;).&lt;/p&gt;&lt;p data-block-key=&quot;ba30c&quot;&gt;The lower bound instance we construct is recursive and, as in the adversarial arrival order case, the intuition is to force the algorithm to face an impossible choice: deciding between two endpoints that look identical. If we let &lt;i&gt;D&lt;/i&gt; be a fixed integer, then the lower bound instance is a tree of height &lt;i&gt;D&lt;/i&gt; that is constructed recursively as follows: We start with a single root node r with label &lt;i&gt;l&lt;/i&gt;(&lt;i&gt;r&lt;/i&gt;)=&lt;i&gt;D&lt;/i&gt;. We then repeatedly perform the following process for each newly created node — for any node v with label &lt;i&gt;l&lt;/i&gt;(&lt;i&gt;v&lt;/i&gt;), we add 2&lt;i&gt;&lt;sup class=&quot;superscript&quot;&gt;D&lt;/sup&gt;&lt;/i&gt;&lt;sup class=&quot;superscript&quot;&gt;-&lt;/sup&gt;&lt;i&gt;&lt;sup class=&quot;superscript&quot;&gt;d&lt;/sup&gt;&lt;/i&gt; new nodes with label d (where 0 ≤ &lt;i&gt;d&lt;/i&gt; &amp;lt; &lt;i&gt;l&lt;/i&gt;(&lt;i&gt;v&lt;/i&gt;)), and connect them to v. For example, in the figure below, with &lt;i&gt;D&lt;/i&gt;=2, the root node with a label of &lt;i&gt;2&lt;/i&gt; has two children labeled &lt;i&gt;1&lt;/i&gt; and four children labeled &lt;i&gt;0&lt;/i&gt;. Each node with label &lt;i&gt;1&lt;/i&gt; then has four additional children with label &lt;i&gt;0&lt;/i&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LoadBalancing-3-LowerBounds.width-1250.png&quot; alt=&quot;LoadBalancing-3-LowerBounds&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LoadBalancing-3-LowerBounds.width-1250.png&quot; alt=&quot;LoadBalancing-3-LowerBounds&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3hvn1&quot;&gt;&lt;i&gt;Lower bound instance where the height of the tree is given by&lt;/i&gt; D&lt;i&gt;=2. The key observation is that when the first edge connecting a node of label&lt;/i&gt; 1&lt;i&gt; with node 2 arrives, no algorithm can distinguish between the end points. That is, an algorithm is equally likely to connect 1 → 2 as 1 ← 2.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;i8n7v&quot;&gt;We provide an intuition behind the lower bound. As a thought experiment, when the edges arrive at random, suppose that an edge (&lt;i&gt;r&lt;/i&gt;, &lt;i&gt;v&lt;/i&gt;) appears before any edge (&lt;i&gt;r&lt;/i&gt;, &lt;i&gt;w&lt;/i&gt;) with labels &lt;i&gt;l&lt;/i&gt;(&lt;i&gt;w&lt;/i&gt;) ≥ &lt;i&gt;l&lt;/i&gt;(&lt;i&gt;v&lt;/i&gt;). Now, conditioned on this event, the nodes &lt;i&gt;r&lt;/i&gt; and &lt;i&gt;v&lt;/i&gt; appear identical from the perspective of the algorithm. The algorithm, faced with an impossible choice, can do no better than a random guess, and hence orients the edge towards root &lt;i&gt;r&lt;/i&gt; with probability ½. But since by construction, the root has far more edges leading to nodes with smaller labels, the event above happens with probability at least ½ as well. Hence, we expect the root node &lt;i&gt;r&lt;/i&gt; to get a load of at least &lt;i&gt;D&lt;/i&gt;/4. Analyzing the construction, we have &lt;i&gt;D&lt;/i&gt; ~ √(log &lt;i&gt;n&lt;/i&gt;), where &lt;i&gt;n&lt;/i&gt; denotes the number of nodes in the tree.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Job scheduling on unrelated machines&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;i8n7v&quot;&gt;Moving beyond the simple tree balancing problem, we consider online load balancing in a general setting called unrelated machines, which effectively models load balancing in cluster management systems. Assume that we have a set of jobs that are queued for processing on &lt;i&gt;m&lt;/i&gt; machines. Jobs arrive in the system one-by-one and an algorithm needs to irrevocably assign each job to a machine when it arrives. The goal of the algorithm is to minimize the load of the most loaded machine.&lt;/p&gt;&lt;p data-block-key=&quot;6nr8k&quot;&gt;Just as in the tree balancing game, we show that a natural variant of the greedy algorithm yields an improved competitive ratio of log &lt;i&gt;m&lt;/i&gt; / log (log &lt;i&gt;m&lt;/i&gt;) when the jobs arrive in random order.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;i8n7v&quot;&gt;We study job scheduling on unrelated machines and obtain an improved competitive ratio for random job arrivals. Although we significantly improve the previous lower bound for the tree balancing game with random edge arrivals, there is still a significant gap between the lower bound and the upper bound. Closing this gap is an interesting open question.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;i8n7v&quot;&gt;&lt;i&gt;This represents joint work with Sungjin Im (UC Santa Cruz), Shi Li (Nanjing University) and Aditya Petety (UC Merced) and was a co-winner of the&lt;/i&gt; &lt;a href=&quot;https://spaa.acm.org/best-paper-award/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Outstanding Paper Award&lt;/i&gt;&lt;/a&gt;&lt;i&gt; at the 2024 ACM Symposium on Parallelism in Algorithms and Architectures (SPAA).&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/load-balancing-with-random-job-arrivals/</link><guid isPermaLink="false">https://research.google/blog/load-balancing-with-random-job-arrivals/</guid><pubDate>Wed, 19 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Algorithms &amp; Theory</category><category>Distributed Systems &amp; Parallel Computing</category></item><item><title>Loss of Pulse Detection on the Google Pixel Watch 3</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8ntp6&quot;&gt;Unwitnessed out-of-hospital cardiac arrest (OHCA), when an individual experiences a pulseless &lt;a href=&quot;https://en.wikipedia.org/wiki/Arrhythmia&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;arrhythmia&lt;/a&gt; without nearby bystanders who could activate an emergency response on their behalf, is a seemingly intractable public health challenge. OHCA leads to &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32087741/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;millions&lt;/a&gt; of deaths &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/20828914/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;worldwide&lt;/a&gt;, and it is estimated that one &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/20123673/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;half to three quarters&lt;/a&gt; of victims experience these events in &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/21796098/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;unwitnessed&lt;/a&gt; settings. About &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/28624594/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;half&lt;/a&gt; of unwitnessed OHCA victims receive no resuscitation &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34920017/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;because&lt;/a&gt; they are found too late and attempted resuscitation is determined to be futile. The time-sensitivity of these events is reflected by the tremendous societal investments we make to minimize the time from collapse to resuscitation as part of the “Chain of Survival” standard of care, for example through cardiopulmonary resuscitation (CPR) training, public access defibrillation, and community responder programs. All of these investments, however, do not address the fundamental challenge of unwitnessed OHCA — they are predicated on bystanders observing events and activating emergency responses.&lt;/p&gt;&lt;p data-block-key=&quot;282na&quot;&gt;In “&lt;a href=&quot;https://www.nature.com/articles/s41586-025-08810-9&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Automated loss of pulse detection on a consumer smartwatch&lt;/a&gt;”, recently published in &lt;i&gt;Nature,&lt;/i&gt; we provide evidence for a new paradigm of OHCA detection deployable on a mass market wearable device, creating an opportunity to transform unwitnessed events into functionally witnessed ones. We show that a smartwatch algorithm can opportunistically detect the cardinal sign of OHCA, loss of pulse, with a performance profile that enables societal implementation, with appropriate sensitivity and a low frequency of errant calls. In this blog post, we describe how we built and tested the algorithm on a consumer smartwatch.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LossOfPulse1_ChainOfSurvival.width-1250.png&quot; alt=&quot;LossOfPulse1_ChainOfSurvival&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LossOfPulse1_ChainOfSurvival.width-1250.png&quot; alt=&quot;LossOfPulse1_ChainOfSurvival&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;23j8g&quot;&gt;&lt;i&gt;The standard of care, the “Chain of Survival,” relies on timely identification, which until now required a human witness.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;8ntp6&quot;&gt;The power of witness status&lt;/h2&gt;&lt;p data-block-key=&quot;711pa&quot;&gt;Witnessed OHCA events have a &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/37453691/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;substantially higher&lt;/a&gt; (7.7×) survival rate than unwitnessed events, &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/37453691/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;attributed&lt;/a&gt; mainly to bystander presence. Bystanders play a crucial role in improving outcomes by recognizing the emergency, contacting emergency medical services (EMS) promptly and, in some instances, providing earlier resuscitation. For every minute people go without resuscitation, chances of survival &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/8214853/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;decrease&lt;/a&gt; by 7–10%, underscoring the enormous prognostic role of time. As a concrete &lt;a href=&quot;https://professional.heart.org/en/meetings/resuscitation-science-symposium/programming&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;example&lt;/a&gt;, in Seattle/King County, WA, witnessed OHCA has a 20% survival rate whereas unwitnessed events have a 4% survival rate. From an epidemiologic perspective, an intervention that reduces mortality by a &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/37453691/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;significant 16%&lt;/a&gt; (absolute difference) presents a considerable opportunity to improve health outcomes. Conceptually, viewing witness status as a theoretical intervention, converting an unwitnessed event to a witnessed one could equate to a &lt;a href=&quot;https://thennt.com/thennt-explained/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;number needed to treat&lt;/a&gt; (NNT) of 6 people to save 1 life — a highly favorable potential benefit profile from a public health intervention standpoint as long as false detections can be minimized.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;8ntp6&quot;&gt;How Loss of Pulse Detection works&lt;/h2&gt;&lt;p data-block-key=&quot;3s2vn&quot;&gt;The Loss of Pulse Detection feature has a multimodal algorithm that runs passively on a consumer wearable, consuming data from onboard &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;photoplethysmography&lt;/a&gt; (PPG) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Accelerometer&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;accelerometer&lt;/a&gt; sensors&lt;footnote id=&quot;541d22c0-f9cb-4db2-978d-5c2e14fb7fe6&quot;&gt;[541d22]&lt;/footnote&gt;. It is intended for opportunistic detection in a broad population without pre-existing conditions that are contraindicated in the user instructions. The data is &lt;a href=&quot;https://support.google.com/fitbit/answer/15250403?hl=en#zippy=%2Cwhere-can-i-find-additional-regulatory-information%2Chow-is-my-data-handled&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;processed&lt;/a&gt; entirely on the watch, &lt;a href=&quot;https://blog.google/products/platforms-devices/fitbit-acquisition/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;consistent&lt;/a&gt; with our privacy &lt;a href=&quot;https://policies.google.com/privacy?hl=en&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;policies&lt;/a&gt;. PPG is a technique that measures changes in pulse (i.e., pulsatility) by detecting photons backscattered by blood and tissue at green and infrared wavelengths.&lt;/p&gt;&lt;p data-block-key=&quot;73v06&quot;&gt;Given the rarity of loss of pulse events and the consequential impact of errant calls from false positives, the algorithm consists of multiple gates that must be passed before a classification is made:&lt;/p&gt;&lt;ol&gt;&lt;li data-block-key=&quot;dfkh6&quot;&gt;A sudden large drop in the &lt;a href=&quot;https://pmc.ncbi.nlm.nih.gov/articles/PMC6426305/#:~:text=The%20AC%20component%20is%20provided%20by%20the%20cardiac%20synchronous%20variations%20in%20blood%20volume%20that%20arise%20from%20heartbeats.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;alternating current&lt;/a&gt; (AC) component of the green PPG, suggestive of a transition from pulsatile to pulseless state, accompanied by concomitant stillness in the accelerometer.&lt;/li&gt;&lt;li data-block-key=&quot;350u1&quot;&gt;A machine learning algorithm, trained on a diverse set of users, that uses data from the PPG and accelerometer that quantifies the probability of a pulsatile to pulseless transition (more below).&lt;/li&gt;&lt;li data-block-key=&quot;e1siq&quot;&gt;Additional sensor checks to confirm that a weak pulse is absent by using additional &lt;a href=&quot;https://pmc.ncbi.nlm.nih.gov/articles/PMC6426305/#:~:text=light%20emitting%20diode&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LEDs&lt;/a&gt; and photodiodes with different geometries, wavelengths, LED currents, and photodiode gain settings.&lt;/li&gt;&lt;/ol&gt;&lt;p data-block-key=&quot;c2hmc&quot;&gt;If all three sequential gates are met over a short period of time (designed to detect events in less than a minute), then the feature will check for responsiveness. The feature does this with two check-in prompts that consist of visual, &lt;a href=&quot;https://en.wikipedia.org/wiki/Haptic_technology#Electronic_devices&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;haptic&lt;/a&gt;, and audio notifications. If the user remains persistently unresponsive, suggesting a potential true loss of pulse event, then the watch will attempt to contact emergency services, providing information on the situation and location of the user. To minimize the risk of errant calls, the algorithm provides the user with an opportunity to de-escalate if they move their arm purposefully, under the assumption that responsiveness is inconsistent with the physiological state of true pulselessness.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LossOfPulse2_TimelineHERO.width-1250.png&quot; alt=&quot;LossOfPulse2_TimelineHERO&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LossOfPulse2_TimelineHERO.width-1250.png&quot; alt=&quot;LossOfPulse2_TimelineHERO&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;23j8g&quot;&gt;&lt;i&gt;A timeline of the Loss of Pulse algorithm detection sequence illustrates the algorithm stages that can lead to the placement of an emergency call.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/LossOfPulse3_Example.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;23j8g&quot;&gt;&lt;i&gt;Example of the user flow for the Loss of Pulse Detection feature. An animation of the Loss of Pulse Detection feature in action on Pixel Watch 3. First, the watch face shows text saying the watch is checking for a pulse, above a button the user can tap that says I’m OK. If the user doesn’t respond, the watch shows a countdown and places a call to emergency services.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h3 data-block-key=&quot;8ntp6&quot;&gt;Algorithm development&lt;/h3&gt;&lt;p data-block-key=&quot;ful68&quot;&gt;Building a loss of pulse detection algorithm faces several challenges. These events are life-threatening and rare, meaning we cannot assemble a cohort waiting for events to occur, since it would take millions of person-years to capture hundreds of events prospectively. We overcame this challenge by partnering with &lt;a href=&quot;https://en.wikipedia.org/wiki/Cardiac_electrophysiology&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;cardiac electrophysiologists&lt;/a&gt; and their patients. Specifically, we had people who had previously scheduled &lt;a href=&quot;https://www.innovationsincrm.com/cardiac-rhythm-management/2012/special-fellows-edition-supplement/251-defibrillation-threshold-testing-a-primer&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;testing&lt;/a&gt; of their &lt;a href=&quot;https://www.mayoclinic.org/tests-procedures/implantable-cardioverter-defibrillators/about/pac-20384692&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;implanted cardiac defibrillator&lt;/a&gt; wear a watch while their heart was temporarily stopped during this defibrillator test. This allowed us to safely collect data from people with a pulseless arrhythmia, &lt;a href=&quot;https://www.mayoclinic.org/diseases-conditions/ventricular-fibrillation/symptoms-causes/syc-20364523&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;ventricular fibrillation&lt;/a&gt;, prior to their defibrillator shocking them back into a pulsatile rhythm. Our key insight was that the pulse signal at the wrist looks similar if the heart stops (like in the electrophysiology lab) or if you occlude the arterial blood supply to the wrist using a pneumatic tourniquet (which is more efficient and safer to reproduce), shown in the figure below. This foundational insight enabled us to scale data collection using arterial occlusion with diverse participants and build an algorithm that we could then test prospectively.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LossOfPulse4_BoxPlot.width-1250.png&quot; alt=&quot;LossOfPulse4_BoxPlot&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/LossOfPulse4_BoxPlot.width-1250.png&quot; alt=&quot;LossOfPulse4_BoxPlot&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;23j8g&quot;&gt;&lt;i&gt;A box plot overlaid with a jitter plot that compares pulselessness observed in a cardiac electrophysiology lab and with arterial occlusion. There is no significant difference in the maximum power of the power spectrum at typical pulse frequencies between central pulselessness observed in a cardiac electrophysiology lab and peripheral pulselessness induced with arterial occlusion. This foundational observation enabled us to develop loss of pulse detection algorithms on arterial occlusion pulseless data.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8ntp6&quot;&gt;After we implemented the Loss of Pulse Detection algorithm on a watch, we conducted multiple studies to test the algorithm. First, we performed two studies that verified that the algorithm had minimal false positives. We had hundreds of participants wear the watch during their everyday activities. We found that there were minimal false positives: only 1 errant call in over 21 person-years of wear time. Next, we verified that the algorithm detected loss of pulse events in two additional studies. We found that the algorithm was sensitive in participants who simulated common loss of pulse event scenarios, such as those that occur during sleep or are accompanied by collapses. All four studies included participants of diverse age, sex, and skin tone, demonstrating that the algorithm could work for a broad representation of users.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;8ntp6&quot;&gt;How Loss of Pulse Detection is built responsibly&lt;/h2&gt;&lt;p data-block-key=&quot;6bph0&quot;&gt;Our foremost priority when designing Loss of Pulse Detection was to ensure that the feature had clinically meaningful performance, contextualized for the status quo of unwitnessed events, which are nearly unsurvivable, and was appropriate for deployment at societal-level scale.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h3 data-block-key=&quot;8ntp6&quot;&gt;Building responsibly to minimize false positives&lt;/h3&gt;&lt;p data-block-key=&quot;8b87n&quot;&gt;The two post-algorithmic, pre-phone call check-in gates help minimize unintentional calls by giving users an opportunity to de-escalate if they are indeed responsive. In one prospective clinical validation study, we found that in the 16 instances in which the pre-phone call check-in gates were shown, all but one (94%) were cleared by responsive users. The design of the first-check-in stage (which consists of a visual notification and strong haptics) facilitates de-escalation by clearing via button tap or purposeful motion, such as by rotating the wrist. Individuals who are unresponsive to the first check-in are shown a second, more noticeable notification, which is accompanied by an audible sound in addition to haptics. Only individuals who are persistently unresponsive for 35 seconds proceed to the next stage where the watch attempts to place an emergency call.&lt;/p&gt;&lt;p data-block-key=&quot;472ca&quot;&gt;In many countries, emergency calls are first triaged by public service answering points (PSAPs), which involves trained individuals who forward time-sensitive emergencies to appropriate professionals. For example, suspected cardiac arrest cases often warrant emergency medical services (EMS) dispatch. The Loss of Pulse Detection feature provides essential context to PSAPs and EMS by describing the nature of the potential emergency (suspected loss of pulse and unresponsiveness), location, and the detection method (by a wearable). The nature and content of this message was informed by our conversations with EMS stakeholders across North America.&lt;/p&gt;&lt;p data-block-key=&quot;6p797&quot;&gt;We didn&#39;t want skin tone to be a barrier to using the feature and we worked hard to ensure it is not. We ensured our training and validation data were representative of the broader population and designed the algorithm deliberately to help ensure the sensitivity of the algorithm did not appreciably differ by skin tone.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;8ntp6&quot;&gt;Practical implementation on a wearable&lt;/h2&gt;&lt;p data-block-key=&quot;f14tv&quot;&gt;Smartwatches constrain the memory, power, and network connectivity available to passive algorithms, mainly to maximize battery life. These constraints informed the design of the Loss of the Pulse Detection feature: the first algorithm stage uses data from sensors that are typically already activated, including the green PPG for heart rate and accelerometer for step counts. Subsequent algorithm stages opportunistically activate additional PPG channels if a potential loss of pulse event is detected. This multistage, multimodal design ensures that the feature minimally impacts the watch’s battery life even though the end-to-end algorithm runs entirely on the watch.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;8ntp6&quot;&gt;Conclusions&lt;/h2&gt;&lt;p data-block-key=&quot;7khd6&quot;&gt;We intend the Loss of Pulse Detection feature to provide a new paradigm for opportunistic detection of unwitnessed loss of pulse events. Given the enormous difference in survival rates between witnessed and unwitnessed cardiac arrests, deployed at scale, such a system may present a significant opportunity to help improve outcomes in unwitnessed circumstances. We hope that Loss of Pulse Detection on consumer wearables can provide public health benefits for individuals who experience unwitnessed loss of pulse events and help get them evidence-based resuscitation faster.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;8ntp6&quot;&gt;Acknowledgements&lt;/h2&gt;&lt;p data-block-key=&quot;9fqqe&quot;&gt;&lt;i&gt;The research described here is joint work across Google Research, Google Health, Google DeepMind, and partnering teams, including Consumer Health Research, Personal Safety, quality, regulatory, and clinical operations. The following researchers contributed to this work: Kamal Shah, Anran Wang, Yiwen Chen, Jitender Munjal, Sumeet Chhabra, Anthony Stange, Enxun Wei, Tuan Phan, Tracy Giest, Beszel Hawkins, Dinesh Puppala, Elsina Silver, Lawrence Cai, Shruti Rajagopalan, Edward Shi, Yun-Ling Lee, Matt Wimmer, Pramod Rudrapatna, Thomas Rea, Shelten Yuen, Anupam Pathak, Shwetak Patel, Mark Malhotra, Marc Stogaitis, Jeanie Phan, Bakul Patel, Adam Vasquez, Luke Walcher, Christina Fox, Alistair Connell, Jim Taylor, Jacqueline Shreibati, David Miller, Daniel McDuff, Pushmeet Kohli, Tajinder Gadh, and Jake Sunshine. We are grateful to Flo Thng, John Hernandez, Michael Howell, and Karen DeSalvo. We are also grateful to the study participants who made this research possible.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/loss-of-pulse-detection-on-the-google-pixel-watch-3/</link><guid isPermaLink="false">https://research.google/blog/loss-of-pulse-detection-on-the-google-pixel-watch-3/</guid><pubDate>Wed, 19 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Health &amp; Bioscience</category><category>Mobile Systems</category><category>Product</category></item><item><title>Generating synthetic data with differentially private LLM inference</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;jyti8&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Differential privacy&lt;/a&gt; (DP) provides mathematically rigorous guarantees that an algorithm will not reveal details about an individual’s data. However, endowing algorithms with DP guarantees can add complexity to an already complex machine learning (ML) pipeline. This is especially true at the scale of modern ML, where pipelines are maintained and used by many different groups in an organization.&lt;/p&gt;&lt;p data-block-key=&quot;b1cqt&quot;&gt;Differentially private &lt;a href=&quot;https://en.wikipedia.org/wiki/Synthetic_data#:~:text=Synthetic%20data%20are%20artificially%20generated,be%20seen%20as%20synthetic%20data.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;synthetic data&lt;/a&gt; can help deal with this scaling issue by serving as an &lt;a href=&quot;https://en.wikipedia.org/wiki/Interface_(computing)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;interface&lt;/a&gt; for model development teams to collaborate without downstream teams needing to understand DP. As described in a &lt;a href=&quot;https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/&quot;&gt;previous blog post&lt;/a&gt;, a common approach to generating DP synthetic data is to privately fine-tune large language models (LLMs), but this can be costly and has high minimum data requirements. The alternative to private training is &lt;a href=&quot;https://arxiv.org/abs/1803.10266&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;differentially private prediction&lt;/a&gt;, where only model outputs are released with DP, instead of the model itself. While there is a large fixed cost to conduct private fine-tuning, differentially private prediction trades off quantity for quality, preferring to release a limited number of high-quality outputs rather than an unlimited number of low-quality ones.&lt;/p&gt;&lt;p data-block-key=&quot;ev4q7&quot;&gt;In “&lt;a href=&quot;https://www.arxiv.org/abs/2407.12108&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Private prediction for large-scale synthetic text generation&lt;/a&gt;”, we present an inference-only approach for generating DP synthetic data. The approach works by prompting an off-the-shelf LLM with many sensitive examples in parallel, and aggregating their predictions with differential privacy. We address issues related to privacy budget and efficiency, which allow us to generate thousands of high-quality synthetic data points with DP guarantees and therefore greatly expand the set of potential applications.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Method&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Our approach builds on pioneering &lt;a href=&quot;https://arxiv.org/abs/2309.11765&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2305.15594&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;work&lt;/a&gt; that introduced a method for distributing sensitive examples over independent prompts, running LLM inference (i.e., the mechanism LLMs use to generate realistic responses) on these prompts, and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1250790.1250803&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;aggregating&lt;/a&gt; next-token &lt;a href=&quot;https://arxiv.org/abs/1610.05755&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;predictions&lt;/a&gt; with differential privacy. Specifically, various prompts (each containing a single piece of sensitive data) are fed into an LLM. Then, predictions from all the prompts are aggregated and the next token is decoded with differential privacy. This ensures the selected token is not influenced strongly by any single piece of sensitive data. Finally, the selected token is appended to all prompts and the process repeats.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- For GIFs, use a default width --&gt;
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-1-Overview.width-800.gif&quot; alt=&quot;SynthData-1-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-1-Overview.width-800.gif&quot; alt=&quot;SynthData-1-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;How the algorithm works.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;jyti8&quot;&gt;Due to challenges in generating text while maintaining DP and computational efficiency, &lt;a href=&quot;https://arxiv.org/abs/2309.11765&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior work&lt;/a&gt; focused on generating a small amount of data points (&amp;lt;10) to be used for in-context learning. We show that it’s possible to generate two to three orders of magnitude more data while preserving quality and privacy by solving issues related to the &lt;i&gt;privacy budget&lt;/i&gt; and &lt;i&gt;computational efficiency&lt;/i&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;c42g1&quot;&gt;The &lt;i&gt;privacy budget&lt;/i&gt; constrains the amount of output the model can release while maintaining a meaningful DP guarantee. DP operates by introducing randomness to mask the contribution of any single data point, enabling plausible deniability. We increase output while maintaining privacy by leveraging the inherent randomness in next-token sampling to ensure privacy.&lt;/p&gt;&lt;p data-block-key=&quot;bbsg9&quot;&gt;This connects next-token sampling in language models with a DP technique called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_mechanism&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;exponential mechanism&lt;/i&gt;&lt;/a&gt;. This mechanism is used to approximately choose the best token option from a set of options, with each option accompanied by a score computed from sensitive data. It does so by sampling an option with probability proportional to the exponential of its score – this introduces randomness crucial to the DP guarantee. This operation is the same as &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Applications&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;softmax sampling&lt;/a&gt; in language models when viewing the set of all tokens as the options from which the model chooses. Based on this connection, we design a DP token sampling algorithm that is strongly aligned with the standard generation process of large language models.&lt;/p&gt;&lt;p data-block-key=&quot;1pgso&quot;&gt;For &lt;i&gt;computational efficiency&lt;/i&gt;, we propose a new privacy analysis that lets us use the same contexts for each generation step and avoid recomputation. Our analysis uses a fixed batch of examples, whereas the DP guarantee of prior work required a fresh batch of sensitive examples to be generated for &lt;i&gt;each token&lt;/i&gt;. But using a fresh batch necessitates changing the input prompt for each sampled token, which is incompatible with standard inference efficiency techniques such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#KV_caching&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;KV caching&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;3ntvv&quot;&gt;Finally, we also introduce a &lt;i&gt;public drafter&lt;/i&gt;, a model that bases its next token predictions solely on already generated synthetic text, rather than sensitive data. Via the &lt;a href=&quot;https://www.cis.upenn.edu/~aaroth/courses/slides/Lecture11.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;sparse vector technique&lt;/a&gt;, we only pay a privacy cost when the drafter’s proposals disagree with predictions made from sensitive data. Otherwise, we accept the drafter’s suggestion and do not expend any privacy budget. We find this is particularly effective for structured data, where many formatting-related tokens can be predicted by the drafter without looking at sensitive data.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Results&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Treating benchmark ML datasets as stand-ins for sensitive data, we ran our algorithm with &lt;a href=&quot;https://blog.google/technology/developers/gemma-open-models/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemma&lt;/a&gt; models to generate synthetic versions of these datasets. We evaluated how useful the resulting synthetic data is for downstream tasks, namely in-context learning with &lt;a href=&quot;https://platform.openai.com/docs/models#gpt-base&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;GPT-3&lt;/a&gt;, and for fine-tuning &lt;a href=&quot;https://huggingface.co/docs/transformers/en/model_doc/bert&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BERT&lt;/a&gt; models. The aim is to understand to what extent we can &lt;i&gt;replace&lt;/i&gt; real sensitive datasets with our DP synthetic datasets in ML applications.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;GPT-3 in-context learning&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;We prompted GPT-3 with real and synthetic few-shot examples from benchmark datasets (&lt;a href=&quot;https://huggingface.co/datasets/fancyzhx/ag_news&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AGNews&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/datasets/fancyzhx/dbpedia_14&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DBPedia&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/datasets/CogComp/trec&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;TREC&lt;/a&gt;, &lt;a href=&quot;https://sls.csail.mit.edu/publications/2012/Liu-Interspeech12.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MIT-G&lt;/a&gt;, and &lt;a href=&quot;https://sls.csail.mit.edu/publications/2012/Liu-Interspeech12.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MIT-D&lt;/a&gt;) and evaluated test set accuracy.&lt;/p&gt;&lt;p data-block-key=&quot;1uo76&quot;&gt;We determine the accuracy of classification when GPT-3 is given &lt;i&gt;k&lt;/i&gt; examples from the data source as reference before being asked for the answer. Our improvements allow us to generate more synthetic data while preserving privacy and quality. We demonstrate improved in-context learning accuracy at the same privacy level, as a consequence of our method’s ability to generate more high-quality synthetic reference examples. Notably, our synthetic data at 64 shots improves over 4 shots of real data (an approximate upper bound on the performance of prior methods that limited themselves to generating 4 synthetic examples).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-2-InContext.width-1250.png&quot; alt=&quot;SynthData-2-InContext&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-2-InContext.width-1250.png&quot; alt=&quot;SynthData-2-InContext&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;GPT-3 in-context learning results. We compare the accuracy of GPT-3 when given few-shot examples from real and synthetic data. Synthetic data is comparable to real data, and we demonstrate improved accuracies when using more of our synthetic data.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;BERT fine-tuning on Yelp synthetic data&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Next, we fine-tuned BERT models on synthetic &lt;a href=&quot;https://huggingface.co/datasets/fancyzhx/yelp_polarity&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Yelp data&lt;/a&gt; generated from DP inference and compared them to the DP fine-tuning results we discussed in a &lt;a href=&quot;https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/&quot;&gt;previous blog post&lt;/a&gt;. While it’s possible to train reasonable models from data generated from DP inference, our results showed that there remains a large gap between inference and the best fine-tuning approaches.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-3-BERT.width-1250.png&quot; alt=&quot;SynthData-3-BERT&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-3-BERT.width-1250.png&quot; alt=&quot;SynthData-3-BERT&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;BERT fine-tuning results on Yelp synthetic data. We compared the accuracy of BERT models trained on real and synthetic data. DP inference can generate enough data to finetune BERT classifiers, but accuracy falls short of the best DP fine-tuning method.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Limited data regime&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;For limited data regimes, DP inference outperforms DP fine-tuning. On a 1K subset of &lt;a href=&quot;https://paperswithcode.com/dataset/ag-news&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AGNews&lt;/a&gt;, the 16-shot GPT-3 in-context learning accuracy was 68.1%, compared to 80.1% for DP inference on the same dataset and at the same privacy level.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-4-GPT-3.width-1250.png&quot; alt=&quot;SynthData-4-GPT-3&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/SynthData-4-GPT-3.width-1250.png&quot; alt=&quot;SynthData-4-GPT-3&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nk3h0&quot;&gt;&lt;i&gt;GPT-3 in-context learning results in the limited data regime. We compare the accuracy of GPT-3 when given few-shot examples generated from a small subset of AGNews. DP inference outperforms DP fine-tuning in the limited data regime.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;What’s next?&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;Differentially private language model inference is a flexible and lightweight approach to generating DP synthetic data. Our work has shown that DP inference can be scaled up to generate large synthetic corpora.&lt;/p&gt;&lt;p data-block-key=&quot;cg2k3&quot;&gt;At scale, quality falls short of DP fine-tuning. However, analogous to tradeoffs between fine-tuning and prompt engineering, DP inference allows for fast iteration cycles — the time to first synthetic example is minutes as opposed to hours. Another important application is latency-constrained applications (e.g., agents) for which batch generation is unsuitable.&lt;/p&gt;&lt;p data-block-key=&quot;dk7o3&quot;&gt;We plan to explore further applications of DP inference and continue to make improvements to the quality and quantity of generated examples.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;jyti8&quot;&gt;&lt;i&gt;This work is the result of a collaboration between multiple people across Google Research and Google DeepMind, including (in alphabetical order by last name): Kareem Amin, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Andreas Terzis, Sergei Vassilvitskii.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/</link><guid isPermaLink="false">https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/</guid><pubDate>Mon, 17 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Machine Intelligence</category><category>Natural Language Processing</category><category>Security, Privacy and Abuse Prevention</category></item><item><title>From diagnosis to treatment: Advancing AMIE for longitudinal disease management</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;zirkj&quot;&gt;Effective clinical reasoning — the totality of all the decisions that go into patient care — is a cornerstone of healthcare. High quality clinical reasoning is a hallmark of expert clinicians and requires not only accurate diagnosis but also sophisticated reasoning about disease progression, therapeutic response, safe medication prescription, and the appropriate use of accepted guidelines or evidence in shared decision-making with patients. Even once a patient’s diagnosis has been established, an optimal management plan often requires monitoring of the patient’s trajectory and experience, personalized treatment plans with informed and shared decision-making, and proactive adjustments based on individual patient needs, preferences, and system constraints. While large language models (LLMs) &lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;have shown promise&lt;/a&gt; in capabilities underpinning diagnostic dialogue, their capabilities for clinical management reasoning over time remain under-explored.&lt;/p&gt;&lt;p data-block-key=&quot;4cto9&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2503.06074&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Towards Conversational AI for Disease Management&lt;/a&gt;”, we advance the previously-demonstrated diagnostic reasoning capabilities of the &lt;a href=&quot;https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/&quot;&gt;Articulate Medical Intelligence Explorer&lt;/a&gt; (AMIE) — our research AI system for medical reasoning and conversations — by integrating additional LLM agentic capabilities optimized specifically for clinical management reasoning and dialogue. This enhanced version of AMIE builds on the core strengths of the Gemini family of models, such as state-of-the-art long-context reasoning and lowest-in-class hallucination rates, to incorporate reasoning over the longitudinal (i.e., sequential over time) progression of disease, response to therapy, and information on safe medication prescription and clinical guidelines. It enables AMIE to go beyond diagnosis and towards the support of patients and clinicians in navigating the complexities of next steps. This latest evolution demonstrates how AMIE might engage in longitudinal interactions, ground its reasoning in an evolving body of authoritative clinical knowledge, and provide structured management plans aligned with accepted guidelines.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/AMIEMx-1-AMIEOverview.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE now supports longitudinal disease management, grounding its reasoning in clinical guidelines and adapting to patient needs across multiple visits.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;The challenge of disease management&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;Clinical care presents unique challenges that extend beyond the initial diagnostic process. Disease management requires consideration of a multitude of factors, including treatment side effects, patient adherence, lifestyle modifications, and the ever-changing landscape of medical research and clinical guidelines. The ability to perform management reasoning has remained an underexplored challenge for AI systems until now.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/AMIEMx-2-LongContext.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE leverages Gemini&#39;s long-context capabilities to access and reason over clinical guidelines, ensuring its recommendations are grounded in evidence-based medicine.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A two-agent architecture for enhanced reasoning&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;Our work addresses this challenge with a novel approach based on the interplay of two LLM-driven agents, which has similarities to &lt;a href=&quot;https://nap.nationalacademies.org/read/21794/chapter/4#59&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;how human clinicians tackle management problems&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;b4c24&quot;&gt;The &lt;i&gt;Dialogue Agent&lt;/i&gt; is user-facing and equipped to rapidly respond based on its current understanding of the patient. This agent handles the conversational aspects of the interaction, gathering information about the patient’s condition, addressing their concerns, and building rapport. By leveraging natural language processing and empathetic communication techniques, the Dialogue Agent ensures a seamless and engaging user experience.&lt;/p&gt;&lt;p data-block-key=&quot;a0pi1&quot;&gt;The &lt;i&gt;Mx Agent&lt;/i&gt; (Management Reasoning Agent) deliberately and continuously analyzes the available information, including clinical guidelines and patient-specific data, to optimize management of the patient. Leveraging Gemini’s state-of-the-art long-context capabilities, this agent synthesizes and reasons over large amounts of information — patient dialogues across several visits in addition to hundreds of pages of clinical guidelines — all at once. Using this approach, it produces structured plans for investigations, treatments, and follow-up care, taking into account the latest medical evidence, information gathered during previous visits, and individual patient preferences.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-3-2Agents.width-1250.png&quot; alt=&quot;AMIEMx-3-2Agents&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-3-2Agents.width-1250.png&quot; alt=&quot;AMIEMx-3-2Agents&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE&#39;s two-agent architecture: The Dialogue Agent interacts with the patient, while the Mx Agent creates structured management plans based on clinical guidelines. Management plans define the sequence of investigations and treatments recommended for that patient.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Grounding management in clinical guidelines&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;To ensure reliability and safety, AMIE’s management reasoning capabilities are primarily enabled by scaling test-time compute to perform deep reasoning with structural constraints while grounding recommendations in authoritative clinical knowledge. Here, too, AMIE relies on Gemini for long-context understanding to align its output with relevant and up-to-date clinical practice guidelines and drug &lt;a href=&quot;https://en.wikipedia.org/wiki/Formulary_(pharmacy)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;formularies&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;bjkci&quot;&gt;This involves selecting and processing documents from a comprehensive corpus of clinical guidelines that encompass trusted sources, such as the &lt;a href=&quot;https://www.nice.org.uk/guidance&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;UK National Institute for Health and Care Excellence Guidance&lt;/a&gt; and the &lt;a href=&quot;https://bestpractice.bmj.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BMJ Best Practice&lt;/a&gt; guidelines. The Mx Agent then uses these guidelines to inform its decision-making process, ensuring that its recommendations are evidence-based and aligned with community-established best practices.&lt;/p&gt;&lt;p data-block-key=&quot;f0c2r&quot;&gt;Intricate structured constraints help guide the model through specified reasoning strategies, while iterative drafting and merging of generated plans helps refine their quality. This allows AMIE to create personalized management plans that are both evidence-based and tailored to the individual patient&#39;s needs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-4-DeepReasoning.width-1250.png&quot; alt=&quot;AMIEMx-4-DeepReasoning&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-4-DeepReasoning.width-1250.png&quot; alt=&quot;AMIEMx-4-DeepReasoning&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE uses deep reasoning with structural constraints (A) to create structured management plans (B) grounded in a case analysis (C) and explicit management goals (D) that include in-visit investigations, ordered investigations, and treatment recommendations, all supported by citations (E). Here we present an example reasoning trace for a fictitious patient.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Evaluating AMIE&#39;s performance: The multi-visit OSCE study&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;To rigorously evaluate AMIE&#39;s ability to handle longitudinal disease management, we conducted a randomized, blinded virtual &lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structured_clinical_examination&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;objective structured clinical examination&lt;/a&gt; (OSCE) study of simulated text-chat consultations. In this study, AMIE was compared to 20 primary care physicians (PCPs) across 100 multi-visit case scenarios, allowing us to assess its performance in realistic clinical settings.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-5-OSCEStudy.width-1250.png&quot; alt=&quot;AMIEMx-5-OSCEStudy&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-5-OSCEStudy.width-1250.png&quot; alt=&quot;AMIEMx-5-OSCEStudy&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;Overview of randomized multi-visit OSCE study.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;zirkj&quot;&gt;The multi-visit design of the OSCE study allowed us to evaluate AMIE&#39;s ability to 1) remember and synthesize information from previous interactions, 2) adapt management plans based on evolving patient symptoms and test results, and 3) maintain consistent and empathetic communication with a patient throughout the course of treatment.&lt;/p&gt;&lt;p data-block-key=&quot;67mgf&quot;&gt;Specialist physicians evaluated the quality of AMIE&#39;s management plans across a range of criteria, including appropriateness, completeness, the use of clinical guidelines, and patient-centeredness.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-6-Management.width-1250.png&quot; alt=&quot;AMIEMx-6-Management&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-6-Management.width-1250.png&quot; alt=&quot;AMIEMx-6-Management&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;Specialist physicians (blinded to the source of the plans) rated AMIE&#39;s management plans as non-inferior to those of PCPs, with statistically significant improvements in treatment preciseness. Key measures here included selecting appropriate investigations and avoiding inappropriate investigations (i.e., doing tests that should be avoided given the information known). P-values are shown for statistically significant (p &amp;lt; 0.05) differences.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;zirkj&quot;&gt;Furthermore, both patient actors and specialist physicians also evaluated AMIE to determine whether its behaviors reflected clinical needs and priorities. We drew inspiration from &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/35830267/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;prior work determining a set of key features of management reasoning&lt;/a&gt; and created a pilot evaluation rubric based on these features, which we refer to as Management Reasoning Empirical Key Features (MXEKF). Key measures of MXEKF included prioritization of preferences, constraints and values, communication and shared decision making, contrasting and selection among different options, monitoring and adjustment of the management plan, and prognostication abilities.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-7-MXEKF.width-1250.png&quot; alt=&quot;AMIEMx-7-MXEKF&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-7-MXEKF.width-1250.png&quot; alt=&quot;AMIEMx-7-MXEKF&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE demonstrated consistent performance on key management reasoning metrics (MXEKF), receiving favorable ratings from both patient actors and specialist physicians.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;RxQA: Benchmarking medication reasoning&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;A critical aspect of disease management is the safe and effective use of medications. It is necessary, but not sufficient, to reliably recall medication-specific knowledge with appropriate factuality and topic-specific reasoning. To benchmark AMIE&#39;s capabilities in these axes, we contribute RxQA, a novel multiple-choice question set derived from national drug formularies, including the &lt;a href=&quot;https://www.fda.gov/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;US Food &amp;amp; Drug Administration&lt;/a&gt; and &lt;a href=&quot;https://www.nice.org.uk/bnf-uk-only&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;British National Formulary&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;5ratq&quot;&gt;RxQA comprises 600 questions designed to assess knowledge of medication indications, contraindications, dosages, side effects, and interactions. The questions were carefully validated by board-certified pharmacists to ensure their accuracy and relevance to clinical practice.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-8-RxQA.width-1250.png&quot; alt=&quot;AMIEMx-8-RxQA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-8-RxQA.width-1250.png&quot; alt=&quot;AMIEMx-8-RxQA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;Example question from the RxQA benchmark, designed to assess medication knowledge and reasoning. All data shown is synthetic (realistic but not real) patient data.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-9-RxQAResults.width-1250.png&quot; alt=&quot;AMIEMx-9-RxQAResults&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AMIEMx-9-RxQAResults.width-1250.png&quot; alt=&quot;AMIEMx-9-RxQAResults&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;1mv8j&quot;&gt;&lt;i&gt;AMIE achieved strong performance on the RxQA benchmark, demonstrating a robust understanding of medication information and guidelines. The dotted line represents accuracy achievable through random guessing.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Limitations&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;While these results showcase AMIE&#39;s potential in a new and important area for medical applications of AI, several limitations warrant consideration. The simulated OSCE scenarios, while valuable for standardized evaluation, intentionally simplify the complexities of real-world clinical practice, which includes chart review, interaction with an electronic health record, and a far broader range of patients and pathologies. In this evaluation, guidelines from a single health system were selected and no attempts were made to adapt them to local contexts, whereas that ability is one of the potential benefits of AMIE. The short intervals between simulated visits and a text-based interface, unlike the multimodal experience of real telehealth, likely underestimated real-world difficulty. The MXEKF scale, though promising as a pilot assessment rubric, requires further validation.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;AMIE&#39;s strong performance across these evaluations represents a significant step towards demonstrating the potential of conversational AI as a powerful tool to assist physicians in disease management. By combining longitudinal reasoning, clinical guideline grounding, and multi-agent system design, AMIE demonstrates the “art of the possible” for AI systems beyond differential diagnosis, towards longitudinal management.&lt;/p&gt;&lt;p data-block-key=&quot;867j9&quot;&gt;Further research is needed before real-world translation to better understand potential impacts of AMIE on clinical workflow and patient outcomes as well as the safety and reliability of the system under real-world constraints. &lt;a href=&quot;https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/&quot;&gt;We are already embarking on a prospective research study with our clinical partners&lt;/a&gt;. However, this work is an important milestone in the responsible development and the potential of AI to improve access to evidence-based care.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;zirkj&quot;&gt;&lt;i&gt;The research described here is joint work across many teams at Google Research and Google DeepMind. We are grateful to all our co-authors and would like to thank John Guilyard, Brian Gabriel and Jenn Sturgeon for contributions to the narratives and visuals. We are grateful to our partners at BMJ Best Practice, the UK National Institute for Health and Care Excellence, and the Royal Pharmaceutical Society. Finally, we thank Avinatan Hassidim, Yossi Matias, James Manyika, Ewa Dominowska, Juro Gottweis, Katherine Chou, Claire Cui, Ali Eslami, Greg S. Corrado, Michael Howell, Karen DeSalvo, Jeff Dean, Zoubin Ghahramani and Demis Hassabis for their support during the course of this project.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/</link><guid isPermaLink="false">https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/</guid><pubDate>Wed, 05 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Health &amp; Bioscience</category></item><item><title>Discovering new words with confidential federated analytics</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;4tnnv&quot;&gt;Introduced by Google Research in 2020, &lt;a href=&quot;https://research.google/blog/federated-analytics-collaborative-data-science-without-data-collection/&quot;&gt;federated analytics&lt;/a&gt; allows for the application of data science methods to the analysis of raw data stored locally on users’ devices so that only the aggregated results — and not data from any particular device — are made available to product engineers. This keeps users&#39; data private, while enabling applications from &lt;a href=&quot;https://security.googleblog.com/2021/09/introducing-androids-private-compute.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;on-device intelligence features&lt;/a&gt;, to &lt;a href=&quot;https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/&quot;&gt;keyboard modeling&lt;/a&gt;, to &lt;a href=&quot;https://arxiv.org/abs/2412.07962&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;estimating carbon emissions&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;aguo2&quot;&gt;In traditional federated analytics, devices respond to queries by sending &lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;minimized&lt;/i&gt; messages&lt;/a&gt;. Raw data stays on the device while updates that focus on a particular purpose are sent to the server for immediate aggregation (either ephemerally, where the raw data are immediately deleted after aggregation, or via cryptographic &lt;a href=&quot;https://research.google/pubs/practical-secure-aggregation-for-federated-learning-on-user-held-data/&quot;&gt;secure aggregation&lt;/a&gt;). But today, users cannot inspect how their data is being aggregated, which may undermine their confidence that their sensitive data will be managed securely.&lt;/p&gt;&lt;p data-block-key=&quot;5m4u0&quot;&gt;To address this, we created &lt;a href=&quot;https://arxiv.org/abs/2404.10764&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;c&lt;i&gt;onfidential federated analytics&lt;/i&gt;&lt;/a&gt;, a technique that leverages &lt;a href=&quot;https://en.wikipedia.org/wiki/Confidential_computing&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential computing&lt;/a&gt; to improve data privacy by publicizing the analyses that will be performed using device data &lt;i&gt;before&lt;/i&gt; the data are uploaded, and by restricting data access to &lt;i&gt;only&lt;/i&gt; those analyses. With this approach, no other analyses can be performed on the data and no human can access data from individual devices. The resulting aggregated output of the analyses can offer strong anonymization guarantees to individuals. Any subversion of these properties, accidental or malicious, would be discoverable by third parties. These properties are made possible by (and subject to the correctness of) &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted execution environments&lt;/a&gt; (TEEs) built into modern CPUs from AMD, Intel, and others.&lt;/p&gt;&lt;p data-block-key=&quot;ci8ss&quot;&gt;With confidential federated analytics, for the first time ever, all privacy-relevant server-side software is now inspectable. This means that any organization using confidential federated analytics can now be completely transparent about the privacy properties of their data processing. This approach is an application of &lt;a href=&quot;https://arxiv.org/abs/2404.10764&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential federated computations&lt;/a&gt;, a more general framework that also applies to machine learning. The source code for confidential federated analytics is available now as part of &lt;a href=&quot;https://research.google/blog/parfait-enabling-private-ai-with-research-tools/&quot;&gt;Google Parfait&lt;/a&gt;, in the &lt;a href=&quot;https://github.com/google-parfait/federated-compute&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;confidential federated compute&lt;/a&gt; and &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;trusted computations platform&lt;/a&gt; repositories. In our recent paper, “&lt;a href=&quot;https://arxiv.org/abs/2410.08892&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Federated Learning in Practice: Reflections and Projections&lt;/a&gt;”, we describe the relationship between confidential federated computation and traditional federated learning and analytics.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;How confidential federated analytics works&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;To illustrate how confidential federated analytics works, we describe its successful implementation in &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gboard&lt;/a&gt;, the popular Android keyboard app. Language evolves continuously, with new words and phrases gaining popularity as technological, cultural, and other changes shape the world. Users of phone keyboards like Gboard expect to easily type common words, even new ones that were unknown just a few months ago. This means that Gboard must have a way of discovering new common words to incorporate them into the typing model, without revealing any uncommon private words. Further, these new words should be discoverable across the 900+ languages that people type in Gboard every day, a problem &lt;a href=&quot;https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/&quot;&gt;we’ve previously studied with federated analytics&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;c27oa&quot;&gt;Our previous use of federated analytics for the problem used the &lt;a href=&quot;https://arxiv.org/abs/2404.11607&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LDP-TrieHH&lt;/a&gt; algorithm, which uses local differential privacy (LDP) that applies noise before upload to protect against a server peeking at individual device data. While LDP-TrieHH achieved a strong anonymization guarantee if the server does not peek, it achieves only a weak local DP guarantee (ε= 10 per device, per day) against servers that do. It requires weeks to run, and often fails to discover new words in languages with low volumes of users.&lt;/p&gt;&lt;p data-block-key=&quot;d72at&quot;&gt;With confidential federated analytics we can reach more devices, more languages, and discover more words. For example, we discovered 3,600 previously missing Indonesian words in just two days. And, because this approach allows the DP algorithm to be externally verified, it also offers a substantially improved DP guarantee (&lt;i&gt;ε&lt;/i&gt; = &lt;i&gt;ln&lt;/i&gt;(&lt;i&gt;3&lt;/i&gt;) per device, per week).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/ConfidentialFederatedAnalytics-1-Overview.width-1250.png&quot; alt=&quot;ConfidentialFederatedAnalytics-1-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/ConfidentialFederatedAnalytics-1-Overview.width-1250.png&quot; alt=&quot;ConfidentialFederatedAnalytics-1-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;4tnnv&quot;&gt;Like other federated analytics applications, the first step in confidential federated analytics consists of devices securely storing locally potential new words that have been typed but are not present in the on-device dictionary. Devices then encrypt their data and upload it to the server along with a set of processing steps that the server is authorized to use for decryption. For this, the devices use encryption keys managed by a central service, called a &lt;i&gt;ledger&lt;/i&gt;. The ledger releases decryption keys &lt;i&gt;only&lt;/i&gt; to those device-approved processing steps that must also run in a properly configured TEE.&lt;/p&gt;&lt;p data-block-key=&quot;a1lnr&quot;&gt;In this case, the data processing steps implement a differentially private algorithm designed to identify the most frequently occurring items within a dataset while simultaneously protecting the privacy of individual data points by adding noise. The differential privacy (DP) guarantee ensures that the algorithm’s top words list cannot be too heavily influenced by any one device, and is considered a gold standard for formal data anonymization.&lt;/p&gt;&lt;p data-block-key=&quot;2iu97&quot;&gt;For a device to know that the decryption keys are protected by a ledger, it must leverage multiple TEE properties (in our case, via Google’s &lt;a href=&quot;https://github.com/project-oak/oak&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Project Oak&lt;/a&gt; attestation stack running on &lt;a href=&quot;https://www.amd.com/en/developer/sev.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AMD SEV-SNP&lt;/a&gt;). When an application runs in a TEE, it gains &lt;i&gt;confidentiality&lt;/i&gt; (i.e., its internal state is secret, even from system administrators) and &lt;i&gt;integrity&lt;/i&gt; (system administrators cannot force the application to deviate from its correct operation). The TEE also provides &lt;i&gt;cryptographic identity&lt;/i&gt; unique to each chip, which can be used to attest to the exact state of the firmware, operating system, and software running on the CPU. The ledger creates public–private key pairs, signing the public key with a signing key tied to the TEE&#39;s attestation. This signature, along with the TEE properties listed above, allows devices to verify that they are using a public key managed by &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute/tree/main/ledger_enclave_app&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;our ledger implementation&lt;/a&gt;.&lt;/p&gt;&lt;p data-block-key=&quot;b213k&quot;&gt;The processing steps that a device approves at the time of upload are named in a structured &lt;i&gt;access policy&lt;/i&gt; associated with the uploaded data and enforced by the ledger. For our use case above, the goal is to run a differentially private discovery algorithm. Specifically, we use a &lt;a href=&quot;https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;stability-based histogram algorithm&lt;/a&gt; for the discovery of frequently typed words and their approximate counts. The algorithm works by adding noise to the counts of words in the aggregated histogram, keeping only the words with counts above a target threshold, illustrated below. The &lt;a href=&quot;https://github.com/google-parfait/tensorflow-federated/blob/b35e8d3eac8536534b6c56996001cf756d953b27/tensorflow_federated/cc/core/impl/aggregation/core/dp_group_by_factory.cc&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;algorithm implementation is published&lt;/a&gt; as open source software.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/Confidential_federatedAnalytics-2-Workflow.width-1250.png&quot; alt=&quot;Confidential federatedAnalytics-2-Workflow&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/Confidential_federatedAnalytics-2-Workflow.width-1250.png&quot; alt=&quot;Confidential federatedAnalytics-2-Workflow&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;4tnnv&quot;&gt;The &lt;i&gt;access policy&lt;/i&gt; that corresponds to the implementation of this algorithm authorizes a two-stage &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute/tree/bdbb8d167508160aa0d94991172dcf1c78ab777d/containers/fed_sql&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;data processing pipeline&lt;/a&gt;. The first stage pre-processes each client&#39;s data using per-client SQL queries to limit the number of words a client can contribute before performing a partial aggregation. The second performs final aggregation, DP noising, and thresholding. Only when the pipeline&#39;s differentially private output has been fully computed is an un-encrypted and fully anonymous result released to the data analyst.&lt;/p&gt;&lt;p data-block-key=&quot;aunf3&quot;&gt;Thus, with correctly implemented ledger and data processing steps, devices can upload data to be processed &lt;i&gt;only&lt;/i&gt; as part of a device-approved, privacy-preserving pipeline, and have confidence that it cannot be used for any other purpose. Of course, these guarantees are subject to the strengths and limitations of current generation TEEs, an area with &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3634737.3644993&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;risks&lt;/a&gt; that include side-channel attacks. Further, attacks and &lt;a href=&quot;https://eprint.iacr.org/2024/936&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;algorithmic defenses&lt;/a&gt; against them are ongoing areas of research.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Scaling confidential federated analytics&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;Applying confidential federated computations to many applications and many devices over long periods of time requires additional design considerations. For example, if the ledger TEE crashes, all datasets and intermediate results protected by that ledger will be lost (no decryption keys).&lt;/p&gt;&lt;p data-block-key=&quot;8b3e9&quot;&gt;To mitigate this single-point-of-failure, we implement our ledger as a &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform/blob/main/confidential_replicated_state_machines.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;replicated state machine&lt;/a&gt; via code hosted in our &lt;a href=&quot;https://github.com/google-parfait/trusted-computations-platform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Trusted Computations Platform&lt;/a&gt; repository, using an industry-proven consensus protocol (&lt;a href=&quot;https://raft.github.io/raft.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Raft&lt;/a&gt;) for fault-tolerance. By modeling the ledger as a replicated state machine, we also guarantee that uploaded data cannot be analyzed repeatedly by the same data processing pipeline. Such re-analyses can undermine an algorithm’s differential privacy guarantee, for example, by averaging out noise across multiple runs.&lt;/p&gt;&lt;p data-block-key=&quot;3kvdd&quot;&gt;By publishing the access policies and the TEE binaries they reference, and by publishing the ledger binaries that enforce these policies, we hope to set a new standard for external verifiability for our privacy-preserving technologies. We publish signatures to a publicly readable, immutable, and tamper-resistant transparency log (&lt;a href=&quot;https://docs.sigstore.dev/logging/overview/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Rekor&lt;/a&gt;), and before uploading any data, devices check that the ledger attestation evidence and applicable access policy have a Rekor inclusion proof. This ensures that uploaded device data can only ever be processed by binaries for which the source code and configuration are externally inspectable. We&#39;ve published a &lt;a href=&quot;https://github.com/google-parfait/confidential-federated-compute/blob/main/docs/inspecting_endorsements.md&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;step-by-step guide to inspecting attestation verification records&lt;/a&gt;, and we invite external researchers to join us in evaluating and improving these claims.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;We’re excited to have shared these initial steps toward scalable and privacy-preserving confidential federated computations by describing systems running in production on real devices today and providing more external verifiability of the end-to-end system than ever before.&lt;/p&gt;&lt;p data-block-key=&quot;7gq8d&quot;&gt;Going beyond analytics use cases, we expect to soon apply the confidential federated computations technique to Gboard&#39;s &lt;a href=&quot;https://arxiv.org/abs/2410.08892&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;model learning use cases&lt;/a&gt; as well. Furthermore, confidential federated computations promise to further strengthen the privacy guarantees offered by federated learning and analytics in many Google apps and systems, including those in Android &lt;a href=&quot;https://security.googleblog.com/2021/09/introducing-androids-private-compute.html#:~:text=We%20introduced%20Android&amp;amp;#x27;s%20Private%20Compute,re%20having%20in%20messaging%20apps&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Private Compute Core&lt;/a&gt; and &lt;a href=&quot;https://security.googleblog.com/2024/08/android-private-ai-approach.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Android’s approach to private AI&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;4tnnv&quot;&gt;&lt;i&gt;The authors would like to thank Dzmitry Huba, Hubert Eichner, Kallista Bonawitz, Mark Simborg, Peter Kairouz, Prem Eruvbetine, Sarah de Haas for their extensive feedback and editing on the blog post itself, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. In particular, we would like to thank the collaborators who directly contributed to this effort: Adria Gascon, Albert Cheu, Allie Culp, Andri Saar, Artem Lagzdin, Brendan McMahan, Brett McLarnon, Chloé Kiddon, Chunxiang Zheng, Conrad Grobler, Edo Roth, Emily Glanz, Ernesto Ocampo, Grace Ni, Haicheng Sun, Ivan Petrov, Jeremy Gillula, Jianpeng Hou, Joe Woodworth, Juliette Pluto, Katharine Daly, Katsiaryna Naliuka, Marco Gruteser, Maya Spivak, Mira Holford, Nova Fallen, Octavian Suciu, Rakshita Tandon, Shumin Zhai, Stanislav Chiknavaryan, Stefan Dierauf, Steve He, Tiziano Santoro, Tom Binder, Ulyana Kurylo, Wei Huang, Yanxiang Zhang, Yu Xiao, Yuanbo Zhang, Zachary Charles, Zheng Xu, Zhimin Yao, and Zoe Gong. This work was supported by Corinna Cortes, Four Flynn, Blaise Aguera y Arcas, and Yossi Matias.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/</link><guid isPermaLink="false">https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/</guid><pubDate>Mon, 03 Mar 2025 16:00:00 GMT</pubDate><author>Google</author><category>Mobile Systems</category><category>Security, Privacy and Abuse Prevention</category><category>Software Systems &amp; Engineering</category></item><item><title>Mind the GAP: Geometry Aware Passthrough mitigates cybersickness</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;5k9m1&quot;&gt;Virtual reality (VR) headsets are becoming increasingly popular, especially in environments that benefit from immersion, such as gaming and education. In part, they accomplish this by restricting the user&#39;s perception to the virtual world that they are projecting. Yet many headsets, such as &lt;a href=&quot;https://www.meta.com/quest&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Meta Quest&lt;/a&gt;, &lt;a href=&quot;https://www.apple.com/apple-vision-pro&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Apple Vision Pro&lt;/a&gt;, and &lt;a href=&quot;https://news.samsung.com/global/unlock-the-infinite-possibilities-of-xr-with-galaxy-ai&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Samsung Moohan&lt;/a&gt;, also give the users the option to view their real-world surroundings. These video see-through (VST) headsets accomplish this with a passthrough system that uses world-facing cameras.&lt;/p&gt;&lt;p data-block-key=&quot;7k93l&quot;&gt;As the adoption of VST devices increases, so does the need to address the discomfort and cybersickness experienced by some users of VST technology. While motion sickness in VR has been extensively studied in the last few decades, there is limited work dedicated to enhancing comfort and safety with the use of VST headsets or other augmented reality (AR) devices. Insights from VR research can be informative, but the unique experience of VST, wherein users can see and interact with the physical world, warrants dedicated investigation to guide the design of VST head-mounted displays (HMD) that are comfortable for users.&lt;/p&gt;&lt;p data-block-key=&quot;2utai&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2502.11497&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Mind the GAP: Geometry Aware Passthrough Mitigates Cybersickness&lt;/a&gt;” (to be presented at &lt;a href=&quot;https://chi2025.acm.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CHI 2025&lt;/a&gt;), we demonstrate the potential of GAP systems to mitigate cybersickness through accurate depth perception. We propose a protocol to quantitatively measure cybersickness experienced by users in VST headsets. Using this protocol, we conduct a user study to compare DP and GAP systems. To the best of our knowledge, our study is the first one to reveal significantly reduced nausea, disorientation, and overall scores of cybersickness with GAP. It also uncovers several potential avenues to further mitigate visually-induced discomfort.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Geometry Aware Passthrough&lt;/h2&gt;&lt;p data-block-key=&quot;c83k9&quot;&gt;Due to inherent hardware limitations common in VST headsets, the camera&#39;s perspective deviates from the user&#39;s natural viewpoint. Direct passthrough (DP) delivers the raw camera feed to the display, which exaggerates distances and consequently the motion of objects. Compared to natural vision (NV), DP results in visual artifacts such as disocclusion, inaccurate perception of object positions, and exaggerated motion parallax. According to the &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/2178753/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;sensory-conflict theory&lt;/a&gt;, such a mismatch between visual and inertial cues can potentially cause discomfort.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap1-ComparisonHERORevised.width-1250.png&quot; alt=&quot;MindTheGap1-ComparisonHERORevised&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap1-ComparisonHERORevised.width-1250.png&quot; alt=&quot;MindTheGap1-ComparisonHERORevised&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;n8w9i&quot;&gt;&lt;i&gt;DP delivers mismatched visual cues compared to the natural vision.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;5k9m1&quot;&gt;To mitigate this mismatch between DP and natural vision, past research has focused on utilizing geometry information to reproject camera feeds into the natural view from the eyes. We introduce the term &lt;i&gt;geometry aware passthrough&lt;/i&gt; (GAP) to describe these passthrough systems. While previous work assumes that a GAP reduces discomfort compared to DP, to the best of our knowledge, no empirical studies have directly investigated or verified this assumption.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap2-OutputRevised.width-1250.jpg&quot; alt=&quot;MindTheGap2-OutputRevised&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap2-OutputRevised.width-1250.jpg&quot; alt=&quot;MindTheGap2-OutputRevised&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;3djm5&quot;&gt;&lt;i&gt;DP versus GAP output images.Two images are shown above taken from the headset placed at the same point in the scene.We observe that DP enlarges all the objects, making the scene look closer to the user.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;3npxc&quot;&gt;Geometrical accuracy and distortion detection&lt;/h2&gt;&lt;p data-block-key=&quot;f4v8n&quot;&gt;Our analysis also involves a rigorous quantitative assessment of the geometrical accuracy and warping introduced by each system. This includes evaluating how accurately each system represents real-world geometry (position and scale of objects) within the AR environment and whether distortions get introduced in the passthrough video feed (as shown in the figure above). More details are in &lt;a href=&quot;https://arxiv.org/abs/2502.11497&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;the paper&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;User study design&lt;/h2&gt;&lt;p data-block-key=&quot;7ug87&quot;&gt;We introduce a comprehensive protocol focused on key VST use cases to holistically assess visually-induced discomfort and cybersickness in VST HMDs. We then use this to compare our GAP algorithm to DP.&lt;/p&gt;&lt;p data-block-key=&quot;jevh&quot;&gt;To achieve reproducibility, repeatability, and real-life relevance, we began with tasks identified in the literature, tested them in a pilot study, and iteratively refined the task nature and duration based on participant feedback. A total of 25 consenting participants with normal or corrected-to-normal vision completed the tasks for the study. Each participant experienced all conditions including NV, DP, and GAP, allowing for a direct comparison of each participant’s experience.&lt;/p&gt;&lt;p data-block-key=&quot;66h7e&quot;&gt;We devised our protocol focusing exclusively on passthrough-based real-world interactions and ensured no virtual elements were visible to participants. The tasks were inspired from fundamental real-world XR applications such as working with laptops for productivity, navigation in the physical world, and interaction with real-world objects. They emphasized user head motion while necessitating inspection and spatial awareness of the physical world. Specifically:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key=&quot;50ghi&quot;&gt;&lt;i&gt;Typing&lt;/i&gt;: This task was chosen to reflect emerging applications in productivity and to effectively engage both visual and motor components. Participants typed on a physical &lt;a href=&quot;https://en.wikipedia.org/wiki/Dvorak_keyboard_layout&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Dvorak keyboard&lt;/a&gt;, chosen because it required frequent gaze shifts between the keyboard and a laptop screen.&lt;/li&gt;&lt;li data-block-key=&quot;7baga&quot;&gt;&lt;i&gt;Navigation&lt;/i&gt;: The realistic and holistic use of a VST HMD involves navigating physical spaces, avoiding obstacles, and interacting with real-world objects. We designed a navigation task where participants collected and dropped off 10 numbered cones, one at a time, into a designated drop zone. This task emphasized geometry perception and required multi-directional movements.&lt;/li&gt;&lt;li data-block-key=&quot;62rro&quot;&gt;&lt;i&gt;Interaction&lt;/i&gt;: This task was designed to simulate common assembly tasks requiring both motor and cognitive skills. Participants assembled large 24-piece jigsaw puzzles by retrieving and working on only one batch of 8 puzzle pieces at a time within a rectangular frame marked on the table. The large puzzle size was selected to accommodate head motion which is often associated with motion sickness.&lt;/li&gt;&lt;/ul&gt;&lt;p data-block-key=&quot;9ma8l&quot;&gt;To measure cybersickness, we used the &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1207/s15327108ijap0303_3&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Simulator Sickness Questionnaire&lt;/a&gt; (SSQ), which categorizes symptoms along four subscales: nausea, disorientation, oculomotor, and total severity. Participants filled out the SSQ before and after each task to isolate the cybersickness experienced in each mode. In addition to the SSQ, participants rated their general discomfort on a scale of 0 to 10 after completing each task. Finally, participants provided qualitative feedback on their experiences at the end of each task.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap3-SetupFinalRevised.width-1250.jpg&quot; alt=&quot;MindTheGap3-SetupFinalRevised&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap3-SetupFinalRevised.width-1250.jpg&quot; alt=&quot;MindTheGap3-SetupFinalRevised&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4txck&quot;&gt;&lt;i&gt;User study setup: Pictures of the lab setup for the three tasks completed by the participants while wearing the VST HMD.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Findings: GAP significantly reduces cybersickness&lt;/h2&gt;&lt;p data-block-key=&quot;88r54&quot;&gt;We found that GAP significantly reduced cybersickness compared to DP. Specifically, GAP led to lower scores in the nausea, disorientation, and total severity subscales of the SSQ (𝑝 &amp;lt; 0.05); lower discomfort scores across all tasks compared to DP, including typing (𝑝 = 0.046), navigation (𝑝 = 0.041), and interaction (𝑝 = 0.022); as well as significantly lower average discomfort scores (𝑝 = 0.016). In line with previous work on VST, our results revealed a VST symptom profile for cybersickness that is distinguished from other types of motion sickness. The most reported symptoms for both DP and GAP were sweating, eyestrain, general discomfort, and headache.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap4-Results1.width-1250.png&quot; alt=&quot;MindTheGap4-Results1&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap4-Results1.width-1250.png&quot; alt=&quot;MindTheGap4-Results1&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap5-Results2.width-1250.png&quot; alt=&quot;MindTheGap5-Results2&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap5-Results2.width-1250.png&quot; alt=&quot;MindTheGap5-Results2&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ohqit&quot;&gt;&lt;i&gt;SSQ results shown as box plots of nausea, oculomotor, disorientation, and total subscores of simulator sickness comparing all conditions.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap6-Results3.width-1250.png&quot; alt=&quot;MindTheGap6-Results3&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap6-Results3.width-1250.png&quot; alt=&quot;MindTheGap6-Results3&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --small
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap7-Results4.width-1250.png&quot; alt=&quot;MindTheGap7-Results4&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MindTheGap7-Results4.width-1250.png&quot; alt=&quot;MindTheGap7-Results4&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ohqit&quot;&gt;&lt;i&gt;Subjective discomfort results shown as box plots of discomfort scores and preference for all conditions across the typing, navigation, and interaction tasks.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;5k9m1&quot;&gt;Participants also generally preferred GAP over DP subjectively. Several participants reported a mismatch between vision and motion with DP, aligning with the sensory-conflict theory. This was frequently mentioned in the context of head motion, particularly during the interaction task. DP caused impaired spatial awareness compared to GAP, and participants noted that DP caused them to move closer to objects due to inaccurate depth cues. Some users also experienced unstable gait and collisions with furniture while using DP. While most participants preferred GAP, some expressed a preference for DP for certain tasks such as typing due to the warping artifacts of GAP on the keyboard. However, some participants adapted to these artifacts over time.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Implications for future VST design&lt;/h2&gt;&lt;p data-block-key=&quot;70jig&quot;&gt;Our findings imply that VST HMD design should incorporate GAP to improve user comfort and facilitate the adoption of VST technology. However, GAP introduces additional computational demands compared to DP. Future design efforts should strive for a balance that maintains the benefits of GAP while minimizing the impact on system performance and latency.&lt;/p&gt;&lt;p data-block-key=&quot;2hiqv&quot;&gt;Participant feedback also helped identify several areas for future research into enhancing comfort in VST. Four issues emerged: frame drops, overexposed images, latency, and blurry vision. Several participants reported that slight delays when moving their heads caused nausea and discomfort. This suggests the need for more research to understand the impact of these factors on mitigating user discomfort and cybersickness.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Conclusion&lt;/h2&gt;&lt;p data-block-key=&quot;cune&quot;&gt;Mind the GAP is the first work that demonstrates that GAP significantly reduces nausea, disorientation, and total scores of cybersickness as well as subjective discomfort scores as compared to DP. We present a comprehensive protocol aimed at evaluating visually-induced discomfort and cybersickness in VST HMDs through key use cases. We hope that our comprehensive protocol sets a foundation for future studies aimed at refining these systems and enhancing user comfort in VST technologies.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;h2 data-block-key=&quot;5k9m1&quot;&gt;Acknowledgments&lt;/h2&gt;&lt;p data-block-key=&quot;15h3f&quot;&gt;&lt;i&gt;This research was conducted by Trishia El Chemaly during her time at Google as a Student Researcher, Mohit Goyal, Tinglin Duan, Vrushank Phadnis, Sakar Khattar, Bjorn Vlaskamp, Achin Kulshrestha, Eric Lee Turner, Aveek Purohit, Gregory Neiswander, and Konstantine Tsotsos. We would also like to thank Abhishek Kar for his guidance and help during the ideation of this work.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/mind-the-gap-geometry-aware-passthrough-mitigates-cybersickness/</link><guid isPermaLink="false">https://research.google/blog/mind-the-gap-geometry-aware-passthrough-mitigates-cybersickness/</guid><pubDate>Thu, 27 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Human-Computer Interaction and Visualization</category><category>Machine Perception</category></item><item><title>Accelerating scientific breakthroughs with an AI co-scientist</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;In the pursuit of scientific advances, researchers combine ingenuity and creativity with insight and expertise grounded in literature to generate novel and viable research directions and to guide the exploration that follows. In many fields, this presents a breadth and depth conundrum, since it is challenging to navigate the rapid growth in the rate of scientific publications while integrating insights from unfamiliar domains. Yet overcoming such challenges is critical, as evidenced by the many modern breakthroughs that have emerged from transdisciplinary endeavors. For example, Emmanuelle Charpentier and Jennifer Doudna won the &lt;a href=&quot;https://www.nobelprize.org/uploads/2020/10/popular-chemistryprize2020.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;2020 Nobel Prize in Chemistry&lt;/a&gt; for their work on &lt;a href=&quot;https://en.wikipedia.org/wiki/CRISPR&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CRISPR&lt;/a&gt;, which combined expertise ranging from microbiology to genetics to molecular biology.&lt;/p&gt;&lt;p data-block-key=&quot;bngi7&quot;&gt;Motivated by unmet needs in the modern scientific discovery process and building on &lt;a href=&quot;https://arxiv.org/abs/2403.05530&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;recent AI advances&lt;/a&gt;, including the ability to synthesize across complex subjects and to perform &lt;a href=&quot;https://deepmind.google/technologies/gemini/flash-thinking/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;long-term planning and reasoning&lt;/a&gt;, we developed an &lt;a href=&quot;https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;AI co-scientist system&lt;/a&gt;. The AI co-scientist is a multi-agent AI system that is intended to function as a collaborative tool for scientists. Built on &lt;a href=&quot;https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemini 2.0, AI co-scientist is&lt;/a&gt; designed to mirror the reasoning process underpinning the scientific method. Beyond standard literature review, summarization and “deep research” tools, the AI co-scientist system is intended to uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and tailored to specific research objectives.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Empowering scientists and accelerating discoveries with the AI co-scientist&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;Given a scientist’s research goal that has been specified in natural language, the AI co-scientist is designed to generate novel research hypotheses, a detailed research overview, and experimental protocols. To do so, it uses a coalition of specialized agents — &lt;i&gt;Generation&lt;/i&gt;, &lt;i&gt;Reflection&lt;/i&gt;, &lt;i&gt;Ranking&lt;/i&gt;, &lt;i&gt;Evolution&lt;/i&gt;, &lt;i&gt;Proximity&lt;/i&gt; and &lt;i&gt;Meta-review&lt;/i&gt; — that are inspired by the scientific method itself. These agents use automated feedback to iteratively generate, evaluate, and refine hypotheses, resulting in a self-improving cycle of increasingly high-quality and novel outputs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/AICoScientist-0-Hero.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;hvksh&quot;&gt;&lt;i&gt;AI co-scientist overview.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;j3yy8&quot;&gt;Purpose-built for collaboration, scientists can interact with the system in many ways, including by directly providing their own seed ideas for exploration or by providing feedback on generated outputs in natural language. The AI co-scientist also uses tools, like web-search and specialized AI models, to enhance the grounding and quality of generated hypotheses.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-1-Components.width-1250.png&quot; alt=&quot;AICoScientist-1-Components&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-1-Components.width-1250.png&quot; alt=&quot;AICoScientist-1-Components&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Illustration of the different components in the AI co-scientist multi-agent system and the interaction paradigm between the system and the scientist.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;The AI co-scientist parses the assigned goal into a research plan configuration, managed by a Supervisor agent. The Supervisor agent assigns the specialized agents to the worker queue and allocates resources. This design enables the system to flexibly scale compute and to iteratively improve its scientific reasoning towards the specified research goal.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-2-Overview.width-1250.png&quot; alt=&quot;AICoScientist-2-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-2-Overview.width-1250.png&quot; alt=&quot;AICoScientist-2-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;AI co-scientist system overview. Specialized agents (&lt;/i&gt;&lt;b&gt;&lt;i&gt;red boxes&lt;/i&gt;&lt;/b&gt;&lt;i&gt;, with unique roles and logic); scientist input and feedback (&lt;/i&gt;&lt;b&gt;&lt;i&gt;blue boxes&lt;/i&gt;&lt;/b&gt;&lt;i&gt;); system information flow (&lt;/i&gt;&lt;b&gt;&lt;i&gt;dark gray arrows&lt;/i&gt;&lt;/b&gt;&lt;i&gt;); inter-agent feedback (&lt;/i&gt;&lt;b&gt;&lt;i&gt;red arrows&lt;/i&gt;&lt;/b&gt;&lt;i&gt; within the agent section).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Scaling test-time compute for advanced scientific reasoning&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;The AI co-scientist leverages &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;test-time compute&lt;/a&gt; scaling to iteratively reason, evolve, and improve outputs. Key reasoning steps include &lt;a href=&quot;https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;self-play&lt;/a&gt;–based scientific debate for novel hypothesis generation, ranking tournaments for hypothesis comparison, and an &quot;evolution&quot; process for quality improvement. The system&#39;s agentic nature facilitates recursive self-critique, including tool use for feedback to refine hypotheses and proposals.&lt;/p&gt;&lt;p data-block-key=&quot;7ufde&quot;&gt;The system&#39;s self-improvement relies on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Elo_rating_system&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Elo&lt;/a&gt; auto-evaluation metric derived from its tournaments. Due to their core role, we assessed whether higher Elo ratings correlate with higher output quality. We analyzed the concordance between Elo auto-ratings and &lt;a href=&quot;https://arxiv.org/abs/2311.12022&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;GPQA benchmark&lt;/a&gt; accuracy on its diamond set of challenging questions, and we found that higher Elo ratings positively correlate with a higher probability of correct answers.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-3-Elo.width-1250.png&quot; alt=&quot;AICoScientist-3-Elo&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-3-Elo.width-1250.png&quot; alt=&quot;AICoScientist-3-Elo&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Average accuracy of the AI co-scientist (blue line) and reference Gemini 2.0 (red line) responses on GPQA diamond questions, grouped by Elo rating. The Elo is an auto-evaluation and is not based on an independent ground truth.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;Seven domain experts curated 15 open research goals and best guess solutions in their field of expertise. Using the automated Elo metric we observed that the AI co-scientist outperformed other state-of-the-art agentic and reasoning models for these complex problems. The analysis reproduced the benefits of scaling test-time compute using inductive biases derived from the scientific method. As the system spends more time reasoning and improving, the self-rated quality of results improve and surpass models and unassisted human experts.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-4-BestHypothesis.width-1250.png&quot; alt=&quot;AICoScientist-4-BestHypothesis&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-4-BestHypothesis.width-1250.png&quot; alt=&quot;AICoScientist-4-BestHypothesis&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-5-Top10Hypothesis.width-1250.png&quot; alt=&quot;AICoScientist-5-Top10Hypothesis&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-5-Top10Hypothesis.width-1250.png&quot; alt=&quot;AICoScientist-5-Top10Hypothesis&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Performance of the AI co-scientist improves as the system spends more time in computation. This can be seen in the automated Elo metric gradually improving over other baselines.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Top:&lt;/i&gt;&lt;/b&gt; &lt;i&gt;Elo progression of the best rated hypothesis.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Bottom:&lt;/i&gt;&lt;/b&gt;&lt;i&gt; Elo progression of the average of top-10 hypotheses.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;685z4&quot;&gt;On a smaller subset of 11 research goals, experts assessed the novelty and impact of the AI co-scientist–generated results compared to other relevant baselines; they also provided overall preference. While the sample size was small, experts assessed the AI co-scientist to have higher potential for novelty and impact, and preferred its outputs compared to other models. Further, these human expert preferences also appeared to be concordant with the previously introduced Elo auto-evaluation metric.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-6-Novelty.width-1250.png&quot; alt=&quot;AICoScientist-6-Novelty&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-6-Novelty.width-1250.png&quot; alt=&quot;AICoScientist-6-Novelty&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-7-Ranking.width-1250.png&quot; alt=&quot;AICoScientist-7-Ranking&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-7-Ranking.width-1250.png&quot; alt=&quot;AICoScientist-7-Ranking&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Human experts assessed the AI co-scientist results to have higher potential for novelty and impact (&lt;/i&gt;&lt;b&gt;&lt;i&gt;left&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) and preferred it compared to other models (&lt;/i&gt;&lt;b&gt;&lt;i&gt;right&lt;/i&gt;&lt;/b&gt;&lt;i&gt;).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Validation of novel AI co-scientist hypotheses with real-world laboratory experiments&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;To assess the practical utility of the system’s novel predictions, we evaluated end-to-end laboratory experiments probing the AI co-scientist–generated hypotheses and research proposals in three key biomedical applications: drug repurposing, proposing novel treatment targets, and elucidating the mechanisms underlying antimicrobial resistance. These settings all involved expert-in-the-loop guidance and spanned an array of complexities:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block both --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-11-Table.width-1250.png&quot; alt=&quot;AICoScientist-11-Table&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-11-Table.width-1250.png&quot; alt=&quot;AICoScientist-11-Table&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Drug repurposing for acute myeloid leukaemia&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;Drug development is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Eroom%27s_law&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;increasingly time-consuming and expensive process&lt;/a&gt; in which new therapeutics require many aspects of the discovery and development process to be restarted for each indication or disease. Drug repurposing addresses this challenge by discovering new therapeutic applications for existing drugs beyond their original intended use. But, due to the complexity of the task, it demands extensive interdisciplinary expertise.&lt;/p&gt;&lt;p data-block-key=&quot;d494o&quot;&gt;We applied the AI co-scientist to assist with the prediction of drug repurposing opportunities and, with our partners, validated predictions through computational biology, expert clinician feedback, and &lt;i&gt;in vitro&lt;/i&gt; experiments.&lt;/p&gt;&lt;p data-block-key=&quot;18gc5&quot;&gt;Notably, the AI co-scientist proposed novel repurposing candidates for &lt;a href=&quot;https://en.wikipedia.org/wiki/Acute_myeloid_leukemia&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;acute myeloid leukemia&lt;/a&gt; (AML). Subsequent experiments validated these proposals, confirming that the suggested drugs inhibit tumor viability at clinically relevant concentrations in multiple AML cell lines.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-8-DoseResponse.width-1250.png&quot; alt=&quot;AICoScientist-8-DoseResponse&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-8-DoseResponse.width-1250.png&quot; alt=&quot;AICoScientist-8-DoseResponse&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;a href=&quot;https://www.merckmanuals.com/professional/clinical-pharmacology/pharmacodynamics/dose-response-relationships&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Dose-response curves&lt;/i&gt;&lt;/a&gt;&lt;i&gt; of one of the three novel AI co-scientist–predicted AML repurposing drugs. KIRA6 inhibits KG-1 (AML cell line) viability at clinically relevant concentrations. Being able to reduce cancer cell viability at lower drug concentrations is advantageous for multiple reasons, e.g., as it reduces the potential for off-target side effects.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Advancing target discovery for liver fibrosis&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;Identifying novel treatment targets is more complex than drug repurposing, and often leads to inefficient hypothesis selection and poor prioritization for &lt;i&gt;in vitro&lt;/i&gt; and &lt;i&gt;in vivo&lt;/i&gt; experiments. AI-assisted target discovery helps to streamline the process of experimental validation, potentially helping to reduce development time costs.&lt;/p&gt;&lt;p data-block-key=&quot;8k4au&quot;&gt;We probed the AI co-scientist system&#39;s ability to propose, rank, and generate hypotheses and experimental protocols for target discovery hypotheses, focusing on &lt;a href=&quot;https://pmc.ncbi.nlm.nih.gov/articles/PMC546435/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;liver fibrosis&lt;/a&gt;. The AI co-scientist demonstrated its potential by identifying epigenetic targets grounded in preclinical evidence with significant anti-fibrotic activity in &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/28878125/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;human hepatic organoids&lt;/a&gt; (3D, multicellular tissue cultures derived from human cells and designed to mimic the structure and function of the human liver). These findings will be detailed in an upcoming report led by collaborators at Stanford University.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-9a-LiverFibrosis.width-1250.png&quot; alt=&quot;AICoScientist-9a-LiverFibrosis&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-9a-LiverFibrosis.width-1250.png&quot; alt=&quot;AICoScientist-9a-LiverFibrosis&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Comparison of treatments derived from AI co-scientist–suggested liver fibrosis targets versus a fibrosis inducer (negative control) and an inhibitor (positive control). All treatments suggested by AI co-scientist show promising activity (p-values for all suggested drugs are &amp;lt;0.01), including candidates that possibly reverse a disease phenotype. Results are detailed in an upcoming report from our Stanford University collaborators.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Explaining mechanisms of antimicrobial resistance&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;As a third validation, we focused on generating hypotheses to explain bacterial gene transfer evolution mechanisms related to antimicrobial resistance (AMR) — microbes&#39; evolved mechanisms to resist infection-treating drugs. This is another complex challenge that involves understanding the molecular mechanisms of gene transfer (&lt;a href=&quot;https://www.nature.com/scitable/definition/conjugation-prokaryotes-290/#:~:text=Conjugation%20is%20the%20process%20by,factor%2C%20or%20F%2Dfactor.&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;conjugation&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Transduction_(genetics)&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;transduction&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_transformation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;transformation&lt;/a&gt;) alongside the ecological and evolutionary pressures that drive AMR genes to spread.&lt;/p&gt;&lt;p data-block-key=&quot;9a67g&quot;&gt;For this test, expert researchers instructed the AI co-scientist to explore a topic that had already been subject to novel discovery in their group, but had not yet been revealed in the public domain, namely, to explain how &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/36596306/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;capsid-forming phage-inducible chromosomal islands&lt;/a&gt; (cf-PICIs) exist across multiple bacterial species. The AI co-scientist system independently proposed that cf-PICIs interact with diverse phage tails to expand their host range. This&lt;i&gt; in silico&lt;/i&gt; discovery, which had been experimentally validated in the original novel laboratory experiments performed prior to use of the AI co-scientist system, are described in co-timed manuscripts (&lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2025.02.11.637232v1&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://storage.googleapis.com/coscientist_paper/penades2025ai.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;2&lt;/a&gt;) with our collaborators at the Fleming Initiative and Imperial College London. This illustrates the value of the AI co-scientist system as an assistive technology, as it was able to leverage decades of research comprising all prior open access literature on this topic.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-10-RediscoveryTimeline.width-1250.png&quot; alt=&quot;AICoScientist-10-RediscoveryTimeline&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/AICoScientist-10-RediscoveryTimeline.width-1250.png&quot; alt=&quot;AICoScientist-10-RediscoveryTimeline&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;nb5b2&quot;&gt;&lt;i&gt;Timeline of AI co-scientist re-discovery of a novel gene transfer mechanism.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Blue:&lt;/i&gt;&lt;/b&gt;&lt;i&gt; Experimental research pipeline timeline for cf-PICI mobilization discovery.&lt;/i&gt; &lt;b&gt;&lt;i&gt;Red:&lt;/i&gt;&lt;/b&gt; &lt;i&gt;AI co-scientist development and recapitulation of these key findings (without prior knowledge).&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Limitations and outlook&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;In our report we address several limitations of the system and opportunities for improvement, including enhanced literature reviews, factuality checking, cross-checks with external tools, auto-evaluation techniques, and larger-scale evaluation involving more subject matter experts with varied research goals. The AI co-scientist represents a promising advance toward AI-assisted technologies for scientists to help accelerate discovery. Its ability to generate novel, testable hypotheses across diverse scientific and biomedical domains — some already validated experimentally — and its capacity for recursive self-improvement with increased compute, demonstrate its potential to accelerate scientists&#39; efforts to address grand challenges in science and medicine. We look forward to responsible exploration of the potential of the AI co-scientist as an assistive tool for scientists. This project illustrates how collaborative and human-centred AI systems might be able to augment human ingenuity and accelerate scientific discovery.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Announcing Trusted Tester access to the AI co-scientist system&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;We are excited by the early promise of the AI co-scientist system and believe it is important to evaluate its strengths and limitations in science and biomedicine more broadly. To facilitate this responsibly we will be enabling access to the system for research organizations through a Trusted Tester Program. We encourage interested research organizations around the world to consider joining this program &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSdvw_8IPrc8O7ZM8FKF46i8BnOYMeSeyLeBNiuk_yGWIlnxYA/viewform&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;685z4&quot;&gt;&lt;i&gt;The research described here is a joint effort between many Google Research, Google Deepmind and Google Cloud AI teams. We thank our co-authors at Fleming Initiative and Imperial College London, Houston Methodist Hospital, Sequome, and Stanford University — José R Penadés, Tiago R D Costa, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Jacob Blum and Gary Peltz. We appreciate Subhashini Venugopalan and Yun Liu for their detailed feedback on the manuscripts described here. We are also grateful to the many incredible scientists across institutions providing detailed technical and expert feedback — please refer to our report to see the voices and minds that aided this work. We also thank our teammates Resham Parikh, Taylor Goddu, Siyi Kou, Rachelle Sico, Amanda Ferber, Cat Kozlowski, Alison Lentz, KK Walker, Roma Ruparel, Jenn Sturgeon, Lauren Winer, Juanita Bawagan, Tori Milner, MK Blake, Kalyan Pamarthy for their support&lt;/i&gt;.&lt;i&gt; Finally, we also thank John Platt, Michael Brenner, Zoubin Ghahramani, Dale Webster, Joelle Barral, Michael Howell, Susan Thomas, Jason Freidenfelds, Karen DeSalvo, Vladimir Vuskovic, Greg Corrado, Ronit Levavi Morad, Ali Eslami, Anna Koivuniemi, Royal Hansen, Andy Berndt, Noam Shazeer, Oriol Vinyals, Burak Gokturk, Amin Vahdat, Katherine Chou, Avinatan Hassidim, Koray Kavukcuoglu, Pushmeet Kohli, Yossi Matias, James Manyika, Jeff Dean and Demis Hassabis for their support.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</link><guid isPermaLink="false">https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</guid><pubDate>Tue, 18 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Health &amp; Bioscience</category><category>Human-Computer Interaction and Visualization</category></item><item><title>Mechanism design for large language models</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;yptit&quot;&gt;Generative AI and &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;large language models&lt;/a&gt; (LLMs) facilitate the automated generation of assets across a variety of domains. For many use cases, several LLM agents may need to collaborate to create a joint output. A potential example is in the context of Internet ads, where advertisers might be represented by LLM agents capable of producing ads in reply to a user query. Or it could be that the LLMs represent stakeholders of a company, working together to write a joint report.&lt;/p&gt;&lt;p data-block-key=&quot;30q39&quot;&gt;Consider an example situation where there is a single space on a webpage to be filled with an ad creative in reply to a user searching for “Vacations in Hawaii”. Suppose there are two advertisers interested in this search query, Alpha Airlines and Beta Resort, each represented by an LLM agent. Each LLM agent is capable of producing an ad creative in reply to the search query. For example: “Fly to Hawaii with Alpha Airlines” and “Enjoy the beauty of Hawaii at Beta Resort”. However, in this case, a suitable auction design would be flexible enough to enable the creation of a joint ad creative, such as “Alpha Airlines flies you to Hawaii where you can enjoy a magic weeklong experience at Beta Resort”.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/MechanismDesign-1-Example.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;cnp6n&quot;&gt;&lt;i&gt;An illustrative example, consider separate LLM agents representing two hypothetical advertisers, Alpha Airlines and Beta Resort, which are tasked to collaborate to produce a joint ad creative.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;yptit&quot;&gt;In applications like this, each LLM agent has (potentially diverging) preferences for the joint output. For example, advertisers prefer to have their products or services mentioned, while also caring about different aspects that they wish to mention more prominently. Problems in which multiple agents collaborate to select a joint output naturally call for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Mechanism_design&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;mechanism design&lt;/a&gt; (i.e., auction design) approach.&lt;/p&gt;&lt;p data-block-key=&quot;7arve&quot;&gt;In “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3589334.3645511&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Mechanism Design for Large Language Models&lt;/a&gt;”, which won the &lt;a href=&quot;https://www2024.thewebconf.org/program/awards/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;WWW 2024 Best Paper Award&lt;/a&gt;, we argue that the problem of joint output generation through multiple LLM agents comes with several unique challenges, and we present a new class of auction mechanisms tailored to address the key challenges of this novel application domain. We describe theoretical results that inform the design of auctions&lt;footnote id=&quot;ab829e67-fc88-45a8-8024-95d624053bf5&quot;&gt;[ab829e]&lt;/footnote&gt; from that class, and we show how these insights lead to practical auction designs that yield promising outcomes when deployed with real-world LLMs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;The token auction model&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;At a high-level, our approach — which we dub the &lt;i&gt;token auction model&lt;/i&gt; — mimics the mechanics of LLMs as closely as possible. The token auction operates on a token-by-token basis and functions like one giant LLM, while also defining some payment function, monetary or otherwise. We illustrate the key ideas and concepts in the context of text creation, but the same ideas apply to the creation of other media types (such as images or videos). In this context, a token is simply an individual word, a sequence of words, punctuation mark, etc. There is also a special “end” token signifying the end of the creation process.&lt;/p&gt;&lt;p data-block-key=&quot;d3qvr&quot;&gt;Let’s first look at how an individual LLM works. On an abstract level, an LLM defines for any sequence of input tokens, a distribution over tokens. For example, the input sequence could be “Mechanism Design for”, and the output distribution could be [(“Large”, 0.8), (“Generative”, 0.2)], meaning that the next token should be “Large” with 80% probability and “Generative” with 20%. This functionality can then be used for so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;auto-regressive text generation&lt;/a&gt;. The process starts with an initial prompt, which is just a sequence of tokens. Then a token is sampled from the distribution over tokens defined by the LLM. This token is appended to the sequence being generated. Then the process is repeated with the resulting expanded sequence of tokens until the special “end” token is sampled.&lt;/p&gt;&lt;p data-block-key=&quot;dvcaq&quot;&gt;The token auction performs two key tasks: it expands the shared token sequence and determines agent payments. Both of these tasks are achieved through functions, which we refer to as the &lt;i&gt;distribution aggregation function&lt;/i&gt; and the &lt;i&gt;payment function&lt;/i&gt;. Both functions take as input the distributions of the individual LLMs and a bid by each agent. The distribution aggregation function maps this to a distribution over tokens, while the output of the payment function is a vector of payments.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-2-Architecture.width-1250.png&quot; alt=&quot;MechanismDesign-2-Architecture&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-2-Architecture.width-1250.png&quot; alt=&quot;MechanismDesign-2-Architecture&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;cnp6n&quot;&gt;&lt;i&gt;The token auction model architecture.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;yptit&quot;&gt;For example, in the situation illustrated in the figure above, the shared sequence of tokens might be “Mechanism Design for”. The distributions might be [(“Large”, 0.8), (“Generative”, 0.2)] for the LLM of Agent 1, (“Large”, 1.0) for the LLM of Agent 2, and (“Generative”, 1.0) for the LLM of Agent 3. The bids might be 1, 2, and 2, respectively. A possible aggregated distribution would be the bid-weighted average of the distributions, namely [(“Large”, 0.56), (“Generative”, 0.44)]. A possible choice for the payments would be to ask each agent to pay their bid, which would have the agents commit 1, 2, and 2, respectively.&lt;/p&gt;&lt;p data-block-key=&quot;aap6&quot;&gt;For our theoretical analysis of this model (and possible choices of distribution aggregation functions and payment functions), we assume that the agents truthfully report their distributions, but may be strategic about their bids. We believe this is a realistic assumption, as LLMs encode preferences over output text in a succinct and non-obvious way. Moreover, in order for the token auction to be able to aggregate distributions, we need to have (at least) some (minimal) information about agent’s preferences away from their “preferred” distributions. Our approach here is to assume that the agents have (known) &lt;a href=&quot;https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_orders&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;partial preference orders&lt;/a&gt; over distributions. That is, we assume that agents may be able to rank some, but not all, pairs of distributions.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Main technical results&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;We present a suite of theoretical results for the token auction model, focusing on a single step in the generation process.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Design space reduction&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;The first result is a “design space reduction”, which confines the general space of distribution aggregation functions to a much smaller sub-space. Inspired by successful practical auction designs, we formulate two desirable properties that a token-auction mechanism should satisfy, namely “payment monotonicity” and “consistent aggregation”, and show that these are equivalent to requiring monotonicity of the distribution aggregation function. Thus, we can restrict attention to monotone distribution aggregation functions, meaning that if an agent weakly increases their bid, the aggregated distribution function would only change to a distribution weakly preferred by the agent (see &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3589334.3645511&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;paper&lt;/a&gt; for details).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;“Second-price” payments&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;Next, we explore the design of payment rules. An important insight of auction theory is that second-price payments provide good incentives. In the context of a single-item auction, the idea is to give the item to the bidder with the highest bid, but make that bidder only pay the second-highest bid. As our second result, we show that (under some additional, natural assumptions) any monotone distribution aggregation function admits an analog of such second-price payments. For this we show that any such rule admits a &lt;i&gt;stable sampling&lt;/i&gt;, i.e., an implementation of the distribution aggregation function based on an explicit random seed that, for each seed, outputs only one of two tokens depending on whether the bid is below or above a certain threshold.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Optimal aggregation rules&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;As our final set of results, we explore the design of &lt;i&gt;optimal&lt;/i&gt; distribution aggregation functions. Taking inspiration from state-of-the-art LLM training, we formulate aggregated loss functions that associate with each output distribution a total loss that we seek to minimize. We consider two different formulations of the aggregated loss functions, and show that these correspond to two simple distribution aggregation functions. The first is a linear distribution aggregation function, which outputs the bid-weighted average of the distributions. The second is a log-linear distribution aggregation function, which does the same in log space.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Demonstration&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;We also conducted a series of experiments with our proposed design. For the demonstrations we used prompt tuning on an off-the-shelf LLM. That is, rather than taking a specifically trained model, we instructed the LLM with a prompt to act on behalf of a hypothetical advertiser.&lt;/p&gt;&lt;p data-block-key=&quot;91s33&quot;&gt;In the example discussed here, there are two advertisers, “Alpha Airlines” and “Beta Resort”, which want to advertise a flight with the airline and a stay at the hotel, respectively. (See the paper for the prompts that we used). Below we show the output of the token auction (parameterized by λ, the relative weight of the bid of Alpha Airlines). The first column shows the output for the linear aggregation rule, and the second column shows the output for the log-linear aggregation rule. In both cases, the behavior is as intended with ad creatives shifting from mentioning only Alpha Airlines, to mentioning both Alpha Airlines and Beta Resort, to only mentioning Beta Resort.&lt;/p&gt;&lt;p data-block-key=&quot;79ipn&quot;&gt;Additional demonstrations with different prompts, including settings with competing advertisers can be found in the paper.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-3-Example.width-1250.png&quot; alt=&quot;MechanismDesign-3-Example&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/MechanismDesign-3-Example.width-1250.png&quot; alt=&quot;MechanismDesign-3-Example&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;cnp6n&quot;&gt;&lt;i&gt;Output generated by the two distribution aggregation functions, as a function of the relative weight of the bid by Alpha Airlines.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;In this research, we designed a mechanism for aggregating LLM output. Our proposed format allows the LLM agents to influence the output through single-dimensional bids. The mechanism’s design makes minimal assumptions about the agents’ preferences. Working under this paradigm, we showed that the natural requirements on the auction mechanism’s incentive properties called for monotone aggregation. We then showed that under robust preferences, any monotone aggregation function enables second-price–style payments. As a “proof of concept” for our designed mechanism, we demonstrated promising outcomes of our aggregation methods by implementing these aggregation functions in a real-world state-of-the-art LLM using prompt tuning.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;yptit&quot;&gt;&lt;i&gt;The work described in this blog post is joint work with Vahab Mirroni, Renato Paes Leme, and Haifeng Xu. We thank Dirk Bergemann, Marina Halac, Philipp Strack, Elliot Lipnowski, Yang Cai, Vasilis Syrgkanis, Negin Gorezaei, Ido Cohen, Yoav Nagel, Yael Shemesh as well as the participants of the Yale Economics Seminar, the Stanford MS&amp;amp;E Seminar, the WWW 2024 conference, and the INFORMS Revenue and Management Pricing Section Conference for invaluable comments and suggestions to improve this manuscript. We are especially grateful to Yong Cheng from Google DeepMind for his expert guidance on the LLM-related details and literature.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/mechanism-design-for-large-language-models/</link><guid isPermaLink="false">https://research.google/blog/mechanism-design-for-large-language-models/</guid><pubDate>Wed, 12 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Algorithms &amp; Theory</category><category>Economics &amp; Electronic Commerce</category><category>Generative AI</category></item><item><title>Building AI for the pluralistic society</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Modern artificial intelligence (AI) systems rely on input from people. Human feedback helps train models to perform useful tasks, guides them toward safe and responsible behavior, and is used to assess their performance. While hailing the recent AI advancements, we should also ask: &lt;i&gt;which humans&lt;/i&gt; are we actually talking about? For AI to be most beneficial, it should &lt;i&gt;reflect&lt;/i&gt; and &lt;i&gt;respect&lt;/i&gt; the diverse tapestry of values, beliefs, and perspectives present in the pluralistic world in which we live, not just a single &quot;average&quot; or majority viewpoint. Diversity in perspectives is especially relevant when AI systems perform subjective tasks, such as deciding whether a response will be perceived as helpful, offensive, or unsafe. For instance, what one value system deems as offensive may be perfectly acceptable within another set of values.&lt;/p&gt;&lt;p data-block-key=&quot;aiqvo&quot;&gt;Since divergence in perspectives often aligns with socio-cultural and demographic lines, preferentially capturing certain groups’ perspectives over others in data may result in disparities in how well AI systems serve different social groups. For instance, we previously &lt;a href=&quot;https://aclanthology.org/2021.law-1.14/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;demonstrated&lt;/a&gt; that simply taking a majority vote from human annotations may obfuscate valid divergence in perspectives across social groups, inadvertently marginalizing minority perspectives, and consequently performing less reliably for groups marginalized in the data. How AI systems should deal with such diversity in perspectives depends on the context in which they are used. However, current models lack a systematic way to recognize and handle such contexts.&lt;/p&gt;&lt;p data-block-key=&quot;43t4g&quot;&gt;With this in mind, here we describe our ongoing efforts in pursuit of capturing diverse perspectives and building AI for the pluralistic society in which we live. We start with &lt;i&gt;understanding&lt;/i&gt; the varying perspectives in the world and, ultimately, we develop effective ways to &lt;i&gt;integrate&lt;/i&gt; these differences into the modeling pipeline. Each stage of the AI development pipeline — from conceptualization and data collection to training, evaluation, and deployment — offers unique opportunities to embed diverse perspectives, but also presents distinct challenges. A truly pluralistic AI cannot rely on isolated fixes or adjustments; it requires a holistic, layered approach that acknowledges and integrates complexity at every step. Having scalability in mind, we set out to (1) disentangle systematic differences in perspectives across social groups, (2) develop an in-depth understanding of the underlying causes for these differences, and (3) build effective ways to integrate meaningful differences into the machine learning (ML) modeling pipeline.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-1-Objectives.width-1250.png&quot; alt=&quot;PluralisticAI-1-Objectives&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-1-Objectives.width-1250.png&quot; alt=&quot;PluralisticAI-1-Objectives&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;her6d&quot;&gt;&lt;i&gt;A holistic, layered approach to (1)&lt;/i&gt; &lt;b&gt;&lt;i&gt;disentangle&lt;/i&gt;&lt;/b&gt;&lt;i&gt; systematic differences in perspectives across social groups, (2) develop an in-depth&lt;/i&gt; &lt;b&gt;&lt;i&gt;understanding&lt;/i&gt;&lt;/b&gt;&lt;i&gt; of the underlying causes for these differences, and (3) build effective ways to&lt;/i&gt; &lt;b&gt;&lt;i&gt;integrate&lt;/i&gt;&lt;/b&gt;&lt;i&gt; meaningful differences into the ML modeling pipeline. We use offensive language detection as an example task for demonstration, but the ideas presented here extend to any subjective task with varying human perspectives.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Over the last three years, we have made several advances along all of these three fronts:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Disentangling diverse perspectives&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Our efforts began with &lt;a href=&quot;https://aclanthology.org/2021.law-1.14/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;a call for action paper&lt;/a&gt; in 2021 in which we identified an issue in annotated datasets. Previous annotation methods, where only the majority vote of the annotator labels were made public, can lead to data that is inherently biased and unrepresentative of the breadth of users. Our research demonstrated that this reduction may unfairly disregard perspectives of certain annotators, and sometimes certain socio-demographic groups. We concluded that annotators are not interchangeable — that is, they draw from their socially embedded experiences and knowledge when making annotation judgments. As a result, retaining their perspectives separately in the datasets will enable dataset users to account for these differences according to their needs.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-2-LabelAgreement.width-1250.png&quot; alt=&quot;PluralisticAI-2-LabelAgreement&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-2-LabelAgreement.width-1250.png&quot; alt=&quot;PluralisticAI-2-LabelAgreement&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;her6d&quot;&gt;&lt;i&gt;Agreement between annotators and the majority label. The figure on the left illustrates distribution of annotators’ agreement with the majority vore in a sentiment detection (&lt;/i&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3173574.3173986&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Díaz, &lt;i&gt;et al.&lt;/i&gt;, 2018&lt;/a&gt;&lt;i&gt;) The figures in the center and on the right present two boxplots of annotators’ agreement with the majority label, grouped by race and political orientation. A key finding is that majority label agreement was significantly lower among annotators who identified as Black.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Following these findings, our team took on the challenge of building more inclusive datasets. We collected and published large-scale datasets aimed at capturing diverse perspectives across various socio-demographic subgroups.&lt;/p&gt;&lt;p data-block-key=&quot;34bvm&quot;&gt;Two examples are &lt;a href=&quot;https://github.com/google-research-datasets/D3code&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;D3CODE&lt;/a&gt; and &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DICES&lt;/a&gt;, both of which centered on detecting offensive and harmful language. The &lt;a href=&quot;https://github.com/google-research-datasets/D3code&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;D3CODE dataset&lt;/a&gt; is a large-scale cross-cultural dataset that captures parallel annotations for offensive language in over 4.5k sentences annotated by a pool of over 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;DICES dataset&lt;/a&gt; captures conversational safety ratings on around 1k sentences from a diverse pool of raters along gender, race, and age. The dataset is unique in providing a thorough set of labels for each individual post.&lt;/p&gt;&lt;p data-block-key=&quot;6alv&quot;&gt;Analyzing D3CODE and DICES revealed that annotator diversity is not just about individual differences, but also about the patterns of agreement and disagreement that emerge within and between groups. To quantify and identify patterns of agreement within groups, we introduced the &lt;a href=&quot;https://aclanthology.org/2024.naacl-long.190.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;GRASP metrics&lt;/a&gt;, a novel method for reliably measuring statistically significant group level associations in perspectives. When applied to an annotated dataset, the GRASP metric calculates a particular group’s congruency by comparing internal agreements with the level of agreement observed with outgroups.&lt;/p&gt;&lt;p data-block-key=&quot;6f0bg&quot;&gt;These datasets and metrics enable new lines of exploratory research to tease apart important distinctions in perspectives: e.g., &lt;a href=&quot;https://aclanthology.org/2024.nlperspectives-1.15.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;research on intersectionality in AI safety&lt;/a&gt; shows that South Asian women are 53% less likely than White men to rate a conversation safe, and &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3630106.3659021&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;a cross-cultural analysis of offensive language annotations&lt;/a&gt; shows that raters in the Global South tend to be more sensitive towards recognizing offensiveness in text.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Understanding the underlying factors&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;While detecting group-level differences is an important first step, meaningful interventions require us to develop an in-depth understanding of what factors contribute to these differences in perspectives and &lt;i&gt;why&lt;/i&gt; these perspectives diverge. This exploration often necessitates a multidisciplinary lens, drawing insights from various fields, such as psychology and sociology.&lt;/p&gt;&lt;p data-block-key=&quot;7crbs&quot;&gt;Consider the example of offensive language detection; research in moral and social psychology reveals that individuals’ judgments of offensive language can be rooted in their &lt;a href=&quot;https://books.google.com/books?id=U21BxGfm3RUC&amp;amp;printsec=frontcover&amp;amp;source=gbs_ge_summary_r&amp;amp;cad=0#v=onepage&amp;amp;q&amp;amp;f=false&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;personal values and beliefs&lt;/a&gt; as well as &lt;a href=&quot;https://www.stoppestennu.nl/sites/default/files/uploads/social_norms_and_the_expression_and_suppression_of_prejudice_the_struggle_for_internalization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;social norms of their society&lt;/a&gt; about what is right or wrong. By understanding these underlying sets of values, we can gain a more nuanced understanding of why different groups and individuals might disagree on what constitutes offensive language.&lt;/p&gt;&lt;p data-block-key=&quot;31udg&quot;&gt;In &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3630106.3659021&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;our recent research&lt;/a&gt;, we employed the social psychology framework of &lt;a href=&quot;https://moralfoundations.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;moral foundations theory&lt;/a&gt;, which posits moral beliefs across six foundations:&lt;i&gt; Care, Equality, Proportionality, Authority, Loyalty and Purity&lt;/i&gt;. We specifically relied on the moral foundations questionnaire (&lt;a href=&quot;https://psycnet.apa.org/record/2023-99083-001&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;MFQ-2&lt;/a&gt;), because it was developed and validated through extensive cross-cultural assessments of moral judgments, making it a reliable tool for integrating a pluralistic definition of values into AI research.&lt;/p&gt;&lt;p data-block-key=&quot;bvjcn&quot;&gt;Our experiment demonstrated that cross-cultural differences in perceptions of offensiveness are significantly mediated by individual moral concerns, in particular, &lt;i&gt;Care&lt;/i&gt; and &lt;i&gt;Purity&lt;/i&gt;. In other words, cultures that place a higher weight on the value of &lt;i&gt;caring&lt;/i&gt; for other individuals and &lt;i&gt;avoiding&lt;/i&gt; impure thoughts are more sensitive to offensive language. These insights provide more meaningful theoretical grounding for data and model developers in their pursuit of aligning AI systems with human values.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
              --vertical-center
            
            
                glue-grid__col--span-6-md
            
          &quot;&gt;
            &lt;div class=&quot;--mobile-spacer&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-3-LabelDistribution.width-800.png&quot; alt=&quot;PluralisticAI-3-LabelDistribution&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-3-LabelDistribution.width-800.png&quot; alt=&quot;PluralisticAI-3-LabelDistribution&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
              --vertical-center
            
            
                glue-grid__col--span-6-md
            
          &quot;&gt;
            &lt;div class=&quot;--mobile-spacer&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-4-Perspectives.width-800.png&quot; alt=&quot;PluralisticAI-4-Perspectives&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/PluralisticAI-4-Perspectives.width-800.png&quot; alt=&quot;PluralisticAI-4-Perspectives&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;8r4mt&quot;&gt;This research underscores the importance of the &lt;i&gt;why&lt;/i&gt; behind diverging human perspectives. By identifying moral values as the grounding factor in shaping different attitudes toward offensive language detection, we can move beyond acknowledging the presence of such disagreement to actually anticipating and accounting for them in the design of AI systems.&lt;/p&gt;&lt;p data-block-key=&quot;3qpd5&quot;&gt;This brings us to the next crucial step: &lt;i&gt;how&lt;/i&gt; can we integrate our understanding of pluralism into our AI models? How can we build models that are not only aware of diverse perspectives but also capable of effectively incorporating them into their decision-making process?&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Integrating pluralism in data and models&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;We have also spearheaded research on how to incorporate such diverse perspectives in ML data and model development and evaluation pipelines. For instance, we recently &lt;a href=&quot;https://aclanthology.org/2024.safety4convai-1.2.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;demonstrated&lt;/a&gt; how we can use our metrics to dynamically determine rater groups relevant for any given task. Then we can perform targeted diversification during data annotations by effectively identifying and involving annotators from specific social groups with unique perspectives to ensure representation of various viewpoints. Through simulation experiments, we observed that such an approach can efficiently increase &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;recall&lt;/a&gt; of safety issues flagged by minoritized rater groups without hurting overall &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;precision&lt;/a&gt;. We have also performed pioneering work on &lt;a href=&quot;https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00449/1986597/tacl_a_00449.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;multi-perspective modeling&lt;/a&gt; that efficiently learns a shared understanding of the problem at hand, while also catering to specific social groups with distinct perspectives in predictions. This approach also enables models to discern value-laden inputs that can have a variety of correct answers.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;Our body of research reveals the importance of accounting for pluralism in AI model development and deployment, and builds tools and resources to further study and innovate on novel ways to incorporate pluralism in AI. It responds to the fact that global society is pluralistic. But the question of whether we want AI to be pluralistic — i.e., always adapting to local values, is a more nuanced question. For instance, in a paper published in 2022, we &lt;a href=&quot;https://arxiv.org/pdf/2210.02667&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discussed&lt;/a&gt; how certain universal baseline values, such as the principles of universal human rights, may sometimes be desirable. Ultimately, we aspire towards AI that is controllable — i.e., having the ability to control when, where, and how specific values and perspectives are adopted and extensible — is able to adapt to emergent perspectives as we encounter new cultural contexts, or societal change over time, and is transparent about what values guide a particular instantiation.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;8r4mt&quot;&gt;&lt;i&gt;We would like to thank everyone on the team that contributed to the work presented in this blog post (in alphabetical order by last name): Lora Aroyo, Dylan Baker, Mark Díaz, Christopher Homan, Alicia Parrish, Charvi Rastogi, Greg Serapio-Garcia, Alex Taylor, Ding Wang, and Chris Welty.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/building-ai-for-the-pluralistic-society/</link><guid isPermaLink="false">https://research.google/blog/building-ai-for-the-pluralistic-society/</guid><pubDate>Wed, 12 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Natural Language Processing</category><category>Responsible AI</category></item><item><title>Urban mobility solutions: Calibrating digital twins at scale</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;In recent years, machine learning has enabled tremendous advances in urban planning and traffic management. However, as transportation systems become increasingly complex, due to factors like increased traveler and vehicle connectivity and the evolution of new services (e.g., ride-sharing, car-sharing, on-demand transit), finding solutions continues to be difficult. To better understand these challenges, cities are developing high-resolution urban mobility simulators, called “digital twins”, that can provide detailed descriptions of congestion patterns. These systems incorporate a variety of factors that might influence traffic flow, such as available mobility services, including on-demand rider-to-vehicle matching for ride-sharing services; network supply operations, such as traffic-responsive tolling or signal control; and sets of diverse traveler behaviors that govern driving style (e.g., risk-averse vs. aggressive), route preferences, and travel mode choices.&lt;/p&gt;&lt;p data-block-key=&quot;39ugv&quot;&gt;These simulators tackle a variety of use cases, such as the deployment of electric-vehicle charging stations, &lt;a href=&quot;https://research.google/blog/simulations-illuminate-the-path-to-post-event-traffic-flow/&quot;&gt;post-event traffic mitigation&lt;/a&gt;, &lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/trsc.2021.1043&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;congestion pricing and tolling&lt;/a&gt;, &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0191261514002240&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;sustainable traffic signal control&lt;/a&gt;, and public transportation expansions. However, it remains a challenge to estimate the inputs of these simulators, such as spatial and temporal distribution of travel demand, road attributes (e.g., number of lanes and geometry), prevailing traffic signal timings, etc., so that they can reliably replicate prevailing traffic patterns of congested, metropolitan-scale networks. The process of estimating these inputs is known as calibration.&lt;/p&gt;&lt;p data-block-key=&quot;5as4g&quot;&gt;The main goal of simulation calibration is to bridge the gap between simulated and observed traffic data. In other words, a well-calibrated simulator yields simulated congestion patterns that accurately reflect those observed in the field. Demand calibration (i.e., determining the demand for or popularity of a particular origin-to-destination trip) is the most important input to estimate, but also the most difficult. Traditionally, simulators have been calibrated using traffic sensors installed under the roadway. These sensors are present in most cities but costly to install and maintain. Also, their spatial sparsity limits the calibration quality because congestion patterns go largely unobserved. Moreover, most of the demand calibration work is based on single, typically small, road networks (e.g., an arterial).&lt;/p&gt;&lt;p data-block-key=&quot;bd9hn&quot;&gt;In “&lt;a href=&quot;https://arxiv.org/abs/2501.04783&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Traffic Simulations: Multi-City Calibration of Metropolitan Highway Networks&lt;/a&gt;”, we showcase the ability to calibrate demand for the full metropolitan highway networks of six cities — Seattle, Denver, Philadelphia, Boston, Orlando, and Salt Lake City — for all congestion levels, from free-flowing to highly congested. To calibrate, we use non-sparse traffic data, namely aggregated and anonymized path travel times, yielding more accurate and reliable models. When compared to a standard benchmark, the proposed approach is able to replicate historical travel time data 44% better on average (and as much as 80% better in some cases).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Calibration approach&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;To calibrate travel demand, urban planners and traffic operators partition metropolitan areas into zones and use simulators to estimate travel demand between pairs of zones. However, high-resolution traffic simulators can be compute-costly to use. Our solution takes advantage of recent research into &lt;a href=&quot;https://research.google/pubs/active-sequential-posterior-estimation-for-sample-efficient-simulation-based-inference/&quot;&gt;sample-efficient algorithms&lt;/a&gt; that tackle the optimization problems in a computationally fast way, avoiding the need for large samples from a costly simulator. In particular, we use &lt;a href=&quot;https://pubsonline.informs.org/doi/abs/10.1287/opre.2013.1226&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;metamodel methods&lt;/a&gt;, which leverage low-resolution, physics-informed traffic models to guide the search in the optimization process.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-1-Zones.width-1250.png&quot; alt=&quot;UrbanMobility-1-Zones&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-1-Zones.width-1250.png&quot; alt=&quot;UrbanMobility-1-Zones&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;Example of a spatial partition of a metro area into zones. Demand calibration estimates the number of trips that start in one zone and finish in another. (Source: Google Maps)&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;Historically, the most commonly used type of traffic data for modeling purposes has been spatially sparse &lt;a href=&quot;https://research.google/pubs/an-adversarial-variational-inference-approach-for-travel-demand-calibration-of-urban-traffic-simulators/&quot;&gt;vehicular count&lt;/a&gt; data. Recently, we have improved on that by incorporating &lt;a href=&quot;https://research.google/pubs/on-the-use-of-abundant-road-speed-data-for-travel-demand-calibration-of-urban-traffic-simulators/&quot;&gt;road speed data&lt;/a&gt;, which is spatially non-sparse. For our new calibration approach, we also use aggregated and anonymized path travel time data (i.e., the estimated average time it takes to travel along a specific route or path between two points within a road network), which provides a more spatially complete understanding of congestion patterns throughout the network.&lt;/p&gt;&lt;p data-block-key=&quot;cskah&quot;&gt;We calibrate traffic demand for six metropolitan areas (Seattle, Denver, Philadelphia, Boston, Orlando, and Salt Lake City) using the open-source &lt;a href=&quot;https://ieeexplore.ieee.org/document/8569938&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;SUMO simulation&lt;/a&gt; software to model their highway networks. Our calibration algorithms combine road network and historical traffic data with these SUMO simulated networks to optimize the input parameters of these simulators, yielding realistic simulated traffic statistics (e.g., travel times, speeds).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-2a-Overview.width-1250.png&quot; alt=&quot;UrbanMobility-2a-Overview&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-2a-Overview.width-1250.png&quot; alt=&quot;UrbanMobility-2a-Overview&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;Overview of our simulation calibration framework.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;The corresponding highway road networks are large scale, containing from 6,600 up to 18,600 modeled roads (or segments). The corresponding demand estimation optimization problems are high dimensional, ranging from 600 to 1,700.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-3a-Segments.width-1250.png&quot; alt=&quot;UrbanMobility-3a-Segments&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-3a-Segments.width-1250.png&quot; alt=&quot;UrbanMobility-3a-Segments&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;The number of segments (i.e., links or roads) in the network and the maximum number of origin-destination zone pairs.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;For each city, we calibrate the hourly demand from 2pm to 11pm on a typical weekday. This period includes the afternoon peak hours as well as the off-peak hours, allowing us to evaluate the algorithm’s performance under different congestion levels. For each scenario (i.e., city and hour combination), we use aggregated and anonymized path travel time data as the ground truth.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Results&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;Our baseline for comparison is the commonly used &lt;a href=&quot;https://www.jhuapl.edu/spsa/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;simultaneous perturbation stochastic approximation&lt;/a&gt; (SPSA) algorithm. The calibration quality of each approach is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Root_mean_square_deviation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;normalized root mean square error&lt;/a&gt; (nRMSE), the particular form of which is described in &lt;a href=&quot;https://arxiv.org/abs/2501.04783&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;our paper&lt;/a&gt;. The nRMSE can be interpreted as the relative error in the fit to the traffic data. For example, an nRMSE value of 0.2 means that there’s an average 20% error in the simulated travel time data compared to the historical traffic data. The plots compare the percentage improvement of the nRMSE of our metamodel calibration compared to the SPSA nRMSE.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-4-nRMSE.width-1250.png&quot; alt=&quot;UrbanMobility-4-nRMSE&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/UrbanMobility-4-nRMSE.width-1250.png&quot; alt=&quot;UrbanMobility-4-nRMSE&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;ubmu3&quot;&gt;&lt;i&gt;Path travel time nRMSE for off-peak (3pm–4pm) and peak (7pm–8pm) hours: SPSA vs. our metamodel method. Each curve considers a given hour and illustrates the performance as percentage improvement between path travel time nRMSE of the metamodel compared to SPSA.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;l7x08&quot;&gt;As the algorithm progresses, the metamodel consistently outperforms SPSA, rapidly establishing its advantage. The metamodel achieves a maximum 78% reduction in path travel time nRMSE during peak hour (Boston, 7pm) and a 76% reduction during a non-peak hour (Orlando, 3pm). Even across all cities and both hours (3pm and 7pm), the metamodel’s calibration quality is 52% better than the baseline SPSA. This sustained improvement ultimately leads to the metamodel generating higher-quality calibrated demand than SPSA, consistently across all cities and congestion levels.&lt;/p&gt;&lt;p data-block-key=&quot;btpmt&quot;&gt;Across all scenarios being calibrated, the metamodel yields a 44% average improvement. The most substantial improvement was observed for Salt Lake City during the afternoon peak traffic congestion period (i.e., 6pm–7pm) where the metamodel outperformed SPSA by 80%.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;What’s next?&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;Our work shows how non-sparse path travel times can be used to enhance the quality of urban mobility digital twins. This overcomes the main challenges of traditional calibration data: vehicular traffic counts, which rely on sparsely located sensors that are also costly to install and to maintain. Our work is shown to be scalable across metropolitan areas. Compared to SPSA, a commonly used algorithm, our method systematically and significantly improves demand calibration quality.&lt;/p&gt;&lt;p data-block-key=&quot;2cgq&quot;&gt;Nevertheless, challenges remain around the number of demand inputs leading to equivalent congestion patterns. As such, our ongoing work focuses on methods to account for this &lt;a href=&quot;https://research.google/pubs/active-sequential-posterior-estimation-for-sample-efficient-simulation-based-inference/&quot;&gt;input uncertainty&lt;/a&gt; and quantify its impact on &lt;a href=&quot;https://research.google/pubs/an-adversarial-variational-inference-approach-for-travel-demand-calibration-of-urban-traffic-simulators/&quot;&gt;output uncertainty&lt;/a&gt;. Accounting for these uncertainties helps improve &lt;a href=&quot;https://arxiv.org/abs/2402.01928&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;counterfactual robustness&lt;/a&gt;, which allows digital twins to better evaluate the impact of urban mobility policies, such as the deployment of congestion pricing or of new transit services. Other ways to improve counterfactual robustness include devising new problems and algorithmic formulations that exploit more information from traffic data, such as going beyond the use of &lt;a href=&quot;https://math.stackexchange.com/questions/628523/first-and-second-order-statistics&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;first-order sample statistics&lt;/a&gt;. This is another way to help us better understand and replicate congestion patterns, ultimately leading to less congested roads and improved quality of life.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;l7x08&quot;&gt;&lt;i&gt;This work was done in collaboration with Neha Arora, Yechen Li, and Damien Pierce. The authors also thank Andrew Tomkins, Yi-fan Chen, and Craig Boutilier for strategic guidance, Ivan Kuznetsov for product management, Iveel Tsogsuren, Martin Mladenov, and Chih-wei Hsu for general framework contributions, and Sheila de Guia for program management and coordination.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/urban-mobility-solutions-calibrating-digital-twins-at-scale/</link><guid isPermaLink="false">https://research.google/blog/urban-mobility-solutions-calibrating-digital-twins-at-scale/</guid><pubDate>Sun, 09 Feb 2025 16:00:00 GMT</pubDate><author>Google</author><category>Algorithms &amp; Theory</category><category>Climate &amp; Sustainability</category><category>General Science</category></item><item><title>Chain of Agents: Large language models collaborating on long-context tasks</title><description>&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;p data-block-key=&quot;6uu2k&quot;&gt;Over the past few years large language models (LLMs) have shown remarkable capabilities on various tasks, such as reasoning, knowledge retrieval, and generation. However, it is still challenging for LLMs to solve tasks that require long inputs, because they typically have limitations on input length, and hence, cannot utilize the full context. This issue hinders long context tasks, such as long summarization, question answering, and code completion.&lt;/p&gt;&lt;p data-block-key=&quot;6b883&quot;&gt;To mitigate this, at &lt;a href=&quot;https://neurips.cc/Conferences/2024&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;NeurIPS 2024&lt;/a&gt; we introduced &lt;a href=&quot;https://openreview.net/pdf?id=LuCLf4BJsr&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Chain-of-Agents&lt;/a&gt; (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. We perform a comprehensive evaluation of CoA on a wide range of long-context tasks, including question answering, summarization, and code completion. We demonstrate significant improvements (up to 10%) over strong baselines: &lt;a href=&quot;https://cloud.google.com/use-cases/retrieval-augmented-generation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;retrieval augmented generation&lt;/a&gt; (RAG), multi-agent LLMs, and LLMs that have had their inputs truncated once the context window is full (called “full-context”).&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A simple but effective approach to improve long-context understanding&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6uu2k&quot;&gt;Previous studies have mainly explored two major directions: &lt;i&gt;input reduction&lt;/i&gt; and &lt;i&gt;window extension&lt;/i&gt;. Input reduction reduces the length of the input context — for example, by directly truncating the input — before feeding to downstream LLMs. RAG extends this direction by breaking the input into chunks and then retrieving answers to the most relevant chunks based on embedding similarity. However, because of low retrieval accuracy, LLMs could receive an incomplete context for solving the task, hurting performance. Window extension extends the context window of LLMs via fine-tuning, training the model to consume longer inputs. For example, &lt;a href=&quot;https://gemini.google.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemini&lt;/a&gt; is able to directly process 2M tokens for each input. However, when the window becomes longer even than their extended input capacities, such LLMs still struggle to focus on the needed information to solve the task and suffer from ineffective context utilization. This long context approach is further complicated by the fact that the cost increases quadratically with length due to the design of the &lt;a href=&quot;https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/&quot;&gt;transformer architecture&lt;/a&gt; that underlies most LLMs.&lt;/p&gt;&lt;p data-block-key=&quot;1th7d&quot;&gt;Motivated by the aforementioned challenges, we designed CoA with inspiration from the way people interleave reading and processing of long contexts under our own limited working memory constraints. Whereas input reduction approaches need to start processing over shorter inputs (“read-then-process”), CoA breaks the input into chunks and then assigns workers to process each chunk sequentially before reading all of the input (“interleaved read-process”). Further, in contrast to context extension, CoA leverages the capacity of LLMs to communicate between agents rather than trying to feed a large number of tokens into the LLM. CoA is also compute cost–effective, significantly improving over full-context approaches, in particular, by reducing time complexity from &lt;i&gt;n&lt;/i&gt;2 to &lt;i&gt;nk&lt;/i&gt;, where &lt;i&gt;n&lt;/i&gt; is the number of input tokens and &lt;i&gt;k&lt;/i&gt; is the context limit of the LLM.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;A novel approach to input processing&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;6uu2k&quot;&gt;CoA contains two stages. In the first, a series of worker agents in charge of different chunks of long context collaborate and aggregate supporting data that can be used to answer the given query. To this end, the workers read and process sequentially, each receiving the message from the previous worker and transferring the useful updated information to the next. In the second stage, the manager agent receives the complete evidence from the last worker agent and generates the final response. Here is a motivating example:&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    &lt;div class=&quot;rich-text --theme- --mode-&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    




    &lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 80px;&quot; data-offset-key=&quot;4r7lu-0-0&quot;&gt;&lt;em&gt;Question:&lt;/em&gt;&lt;span data-offset-key=&quot;4r7lu-0-1&quot;&gt; “Who is the grandchild of A?”&lt;/span&gt;&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;ac7s1-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;ac7s1-0-0&quot;&gt;&lt;em&gt;Source input, separated into chunks:&lt;/em&gt;&lt;span data-offset-key=&quot;ac7s1-0-1&quot;&gt; [1],[2],[3],[4]&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;em&gt;Supporting data from each chunk:&lt;/em&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[1] – A’s spouse is D&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[2] – A’s child is B&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[3] – No additional evidence&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;bhg0a-0-0&quot;&gt;&lt;span data-offset-key=&quot;bhg0a-0-1&quot;&gt;[4] – B’s child is C&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;b6qsq-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;b6qsq-0-0&quot;&gt;&amp;nbsp;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;b6qsq-0-0&quot;&gt;&lt;strong&gt;Chain of Agents:&lt;/strong&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;2lfkb-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;2lfkb-0-0&quot;&gt;&lt;span data-offset-key=&quot;2lfkb-0-0&quot;&gt;&lt;em&gt;Question:&lt;/em&gt; &lt;/span&gt;&lt;span data-offset-key=&quot;2lfkb-0-1&quot;&gt;“Who is the grandchild of A?”&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;ffsl2-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;ffsl2-0-0&quot;&gt;&lt;em&gt;Workers assess their chunk and perform a relevant task:&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;fd9ae-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;fd9ae-0-0&quot;&gt;&lt;span data-offset-key=&quot;fd9ae-0-0&quot;&gt;[1] – topic exploration: A’s spouse is D &lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;6qvjk-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;6qvjk-0-0&quot;&gt;&lt;span data-offset-key=&quot;6qvjk-0-0&quot;&gt;[2] – answer first hop: A’s child is B &lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;eadtm-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;eadtm-0-0&quot;&gt;&lt;span data-offset-key=&quot;eadtm-0-0&quot;&gt;[3] – forward previous evidence: A’s child is B &lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&quot;padding-left: 40px;&quot; data-draftjs-conductor-fragment=&quot;{&amp;quot;blocks&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;23klf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:9,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;95aic&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Source input, separated into chunks: [1],[2],[3],[4]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:36,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;e2a64&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Supporting data from each chunk: [1: A’s spouse is D], [2:  A’s child is B], [3: No evidence], [4: B’s child is C]&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:32,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;8gveq&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Our CoA:&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;BOLD&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;dpo0h&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Question: “Who is the grandchild of A?”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:10,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;bfaj1&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Workers assess their chunk and perform a relevant task: &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:55,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;devfr&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[1] – topic exploration: A’s spouse is D &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;eht6j&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[2] – answer first hop: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c5ksf&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[3] – forward previous evidence: A’s child is B &amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;c6dgb&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}},{&amp;quot;key&amp;quot;:&amp;quot;a2ti5&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Manager: “It is C.”&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;unstyled&amp;quot;,&amp;quot;depth&amp;quot;:0,&amp;quot;inlineStyleRanges&amp;quot;:[{&amp;quot;offset&amp;quot;:0,&amp;quot;length&amp;quot;:8,&amp;quot;style&amp;quot;:&amp;quot;ITALIC&amp;quot;}],&amp;quot;entityRanges&amp;quot;:[],&amp;quot;data&amp;quot;:{}}],&amp;quot;entityMap&amp;quot;:{}}&quot;&gt;
&lt;div class=&quot;Draftail-block--unstyled &quot; style=&quot;padding-left: 40px;&quot; data-block=&quot;true&quot; data-editor=&quot;109vm&quot; data-offset-key=&quot;5n277-0-0&quot;&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; style=&quot;padding-left: 40px;&quot; data-offset-key=&quot;5n277-0-0&quot;&gt;&lt;span data-offset-key=&quot;5n277-0-0&quot;&gt;[4] – complete reasoning: A’s child is B, B’s child is C. Thus, A’s grandchild is C &lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;public-DraftStyleDefault-block public-DraftStyleDefault-ltr&quot; data-offset-key=&quot;5n277-0-0&quot;&gt;&lt;em&gt;Manager:&lt;/em&gt;&lt;span data-offset-key=&quot;ae2pm-0-1&quot;&gt; “It is C.”&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

                    
                    
    


&lt;section class=&quot;component-as-block both --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Stage 1: Worker agent: Segment comprehension and chain-communication&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;gfrcw&quot;&gt;In Stage 1, CoA contains a sequence of worker agents. Each worker receives an heuristically concatenated portion from the source text, the query, instructions for a specific task assigned to that agent, and the message passed from the previous agent. This communication chain is unidirectional, passing from each worker to the next in sequential order. The worker agents process each concatenated block and outputs a message for the next worker.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Stage 2: Manager agent: Information integration and response generation&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;In Stage 2, after multiple steps of information extraction and comprehension by worker agents, the manager agent produces the final solution. While worker agents extract relevant information in a long-context source, the manager agent synthesizes relevant information accumulated by the end of ‘’worker–agent chain&#39;&#39; to generate the final answer. Specifically, given the instruction for manager and query, the manager agent assesses the accumulated knowledge from the last worker to generate the final answer.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        &lt;div class=&quot;glue-ambient-video &quot;&gt;
            &lt;video class=&quot;glue-ambient-video__container&quot; playsinline=&quot;&quot; muted=&quot;true&quot; loop=&quot;true&quot;&gt;
                &lt;source src=&quot;https://storage.googleapis.com/gweb-research2023-media/media/CoA-1-Overview.mp4&quot; type=&quot;video/mp4&quot;&gt;
            &lt;/video&gt;
            &lt;div class=&quot;glue-ambient-video__button glue-ambient-video__button--paused&quot; aria-label=&quot;Video Play/pause&quot;&gt;
                &lt;div class=&quot;glue-ambient-video__tooltip&quot;&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-play glue-label&quot;&gt;play silent looping video&lt;/span&gt;
                  &lt;span class=&quot;glue-ambient-video__tooltip-pause glue-label&quot;&gt;pause silent looping video&lt;/span&gt;
                &lt;/div&gt;
                &lt;div class=&quot;glue-ambient-video__icon&quot;&gt;
                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-play&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#play-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                  



  

&lt;svg role=&quot;presentation&quot; aria-hidden=&quot;true&quot; class=&quot;glue-icon glue-icon--18px glue-ambient-video__icon-pause&quot;&gt;
  &lt;use href=&quot;/gr/static/assets/icons/glue-icons.svg#pause-button&quot;&gt;&lt;/use&gt;
&lt;/svg&gt;

                &lt;/div&gt;
              &lt;/div&gt;
        &lt;/div&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;High-level illustration of Chain-of-Agents. It consists of multiple worker agents that sequentially communicate to handle different segmented portions of the text, followed by a manager agent that synthesizes these contributions into a coherent final output.&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Experiments&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;To illustrate the utility of this approach, we conduct intensive experiments on nine datasets, including question answering, summarization, and code completion tasks with six LLMs, &lt;a href=&quot;https://ai.google/discover/palm2/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;PaLM 2&lt;/a&gt; (Text Bison and Text Unicorn), &lt;a href=&quot;https://gemini.google.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Gemini&lt;/a&gt; (Ultra), and &lt;a href=&quot;https://claude.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Claude 3&lt;/a&gt; (Haiku, Sonnet, and Opus) models. We compare CoA with two strong baselines chosen from input reduction and window extension approaches, respectively: (i) RAG, which uses a state-of-the-art retriever to obtain the most relevant information to feed into the LLM, and (ii) Full-Context, which feeds all input into the LLM until reaching the window limit.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Comparison with a RAG model&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;The figures show the results on question answering, summarization, and code completion tasks for three models on eight different datasets, including HotpotQA, MuSiQue, RepoBench-P(RepoB) from &lt;a href=&quot;https://aclanthology.org/2024.acl-long.172.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LongBench&lt;/a&gt;, and NarrativeQA (NQA), Qasper, QuALITY, QMSum, GovReport from &lt;a href=&quot;https://www.scrolls-benchmark.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;SCROLLS&lt;/a&gt;. CoA (8k) (where “8k” refers to the length of input for the LLM) outperforms Full-Context (8k) by a large margin on all datasets. It also outperforms the RAG (8k) model for all eight datasets.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-2-Palm2TextBison.width-1250.png&quot; alt=&quot;CoA-2-Palm2TextBison&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-2-Palm2TextBison.width-1250.png&quot; alt=&quot;CoA-2-Palm2TextBison&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;Palm 2 Text Bison&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-3-Palm2TextUnicorn.width-1250.png&quot; alt=&quot;CoA-3-Palm2TextUnicorn&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-3-Palm2TextUnicorn.width-1250.png&quot; alt=&quot;CoA-3-Palm2TextUnicorn&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;Palm 2 Text Unicorn&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-4-GeminiUltra.width-1250.png&quot; alt=&quot;CoA-4-GeminiUltra&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-4-GeminiUltra.width-1250.png&quot; alt=&quot;CoA-4-GeminiUltra&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;Gemini Ultra&lt;/p&gt;&lt;p data-block-key=&quot;3pgpa&quot;&gt;Comparison of three LLMs with RAG and Full-Context baselines. Y-axis is the performance metric on each dataset.&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Multi-agent collaboration in CoA enables complex reasoning over long context&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;Below we present a comparison of outputs from RAG and CoA for a question on the &lt;a href=&quot;https://hotpotqa.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;HotpotQA dataset&lt;/a&gt;. To find the correct answer, RAG retrieves text chunks with high semantic similarity with the query. However, conducting multi-hop reasoning is challenging as the critical first-hop answer often lacks semantic relevance to the query. In contrast, CoA operates differently: the first agent explores related topics without knowing the query’s answer, aiding subsequent inference. The second agent, also unaware of the answer, broadens the topic scope by incorporating new information. The third agent finally discovers the answer, synthesizing information from earlier agents and new data to complete the reasoning chain. This collaborative approach highlights CoA’s ability to facilitate complex reasoning across long context tasks.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --full
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-5-HotpotQA.width-1250.png&quot; alt=&quot;CoA-5-HotpotQA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-5-HotpotQA.width-1250.png&quot; alt=&quot;CoA-5-HotpotQA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;A case study of RAG (&lt;/i&gt;&lt;b&gt;&lt;i&gt;left&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) and CoA (&lt;/i&gt;&lt;b&gt;&lt;i&gt;right&lt;/i&gt;&lt;/b&gt;&lt;i&gt;) on HotpotQA. Sequential agent communication enables CoA to perform complex multi-hop reasoning over long contexts.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Comparison with long context LLMs&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;The figure below shows the comparison with long context LLMs on &lt;a href=&quot;https://arxiv.org/abs/1712.07040&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;NarrativeQA&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2105.08209&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BookSum&lt;/a&gt;. CoA (8k) significantly outperforms RAG (8k) and Full-Context (200k) baselines with three Claude 3 (Haiku, Sonnet, and Opus) models as backbones, even though the context limit of the latter is 200k.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-vertical-padding --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-6-NarrativeQA.width-1250.png&quot; alt=&quot;CoA-6-NarrativeQA&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-6-NarrativeQA.width-1250.png&quot; alt=&quot;CoA-6-NarrativeQA&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;NarrativeQA&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-7-BookSum.width-1250.png&quot; alt=&quot;CoA-7-BookSum&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-7-BookSum.width-1250.png&quot; alt=&quot;CoA-7-BookSum&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;BookSum&lt;/i&gt;&lt;/p&gt;&lt;p data-block-key=&quot;4kiep&quot;&gt;&lt;i&gt;Comparison with long context LLMs: Claude 3 Haiku, Claude 3 Sonnet and Claude 3 Opus. The number on the bar is the performance. “W” / ”w/o Trun.” indicates the source text in the sample is more/less than 200k tokens, which needs/does not need truncation for the full-context (200k) baseline. “Avg.” is the mean value across all samples.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h3 class=&quot;&quot;&gt;Greater improvement for long context models with longer inputs&lt;/h3&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;We compare the performance of CoA and Full-Context with &lt;a href=&quot;https://claude.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Claude 3&lt;/a&gt; on &lt;a href=&quot;https://arxiv.org/abs/2105.08209&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;BookSum&lt;/a&gt;. As shown in Figure, CoA can outperform the Full-Context baseline by a large margin on various source lengths. It is worth noting that, when the length of the sample increases, the performance even increases for CoA, and the improvement over Full-Context (200k) baseline becomes more significant. The improvement of CoA reaches around 100% when the length is larger than 400k. Thus, we can conclude that 1) CoA can still enhance the LLM performance even though the model has a very long context window limit; and 2) CoA delivers more performance gains when the input is longer.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  





  &lt;div class=&quot;
      dynamic_media
      glue-grid__col
      glue-grid__col--span-4-sm
      glue-grid__col--span-12-md
      glue-grid__col--span-12-lg
      --
      
        --flex
        --center
      
    &quot; data-gt-id=&quot;dynamic_media&quot; data-gt-component-name=&quot;&quot;&gt;
  



    &lt;div class=&quot;glue-grid
      
        --remove-gap
        --medium
      &quot;&gt;
      
          &lt;div class=&quot;
            dynamic_media__item
            glue-grid__col
            glue-grid__col--span-4-sm
            
                glue-grid__col--span-12-md
            
            
                glue-grid__col--span-12-lg
            
          &quot;&gt;
            &lt;div class=&quot;&quot;&gt;
              


    
        





    &lt;!-- Determine the appropriate width based on image_width --&gt;
    
        
    


&lt;!-- For mobile images, use a default width --&gt;


&lt;picture class=&quot;media__image media__image&quot;&gt;
    
    
        &lt;source media=&quot;(min-width: 768px)&quot; srcset=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-8-Claude3.width-1250.png&quot; alt=&quot;CoA-8-Claude3&quot;&gt;
    
    &lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/CoA-8-Claude3.width-1250.png&quot; alt=&quot;CoA-8-Claude3&quot; loading=&quot;lazy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
    
&lt;/picture&gt;
    


            &lt;/div&gt;
          &lt;/div&gt;
      
      
      &lt;div class=&quot;glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center&quot;&gt;
        &lt;p data-block-key=&quot;4nb5b&quot;&gt;&lt;i&gt;Performance of&lt;/i&gt; &lt;a href=&quot;https://claude.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;Claude 3&lt;/i&gt;&lt;/a&gt;&lt;i&gt; on&lt;/i&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.08209&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;i&gt;BookSum&lt;/i&gt;&lt;/a&gt;&lt;i&gt;. Improvement is more obvious for longer inputs.&lt;/i&gt;&lt;/p&gt;
      &lt;/div&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;







    &lt;/div&gt;
&lt;/section&gt;

                    
                    
    


&lt;section class=&quot;component-as-block --no-padding-top --theme-light --dbl-padding&quot;&gt;
    &lt;div class=&quot;glue-page&quot;&gt;
        
  &lt;div class=&quot;rich-text --theme-light --mode-standalone&quot; data-gt-id=&quot;rich_text&quot; data-gt-component-name=&quot;&quot;&gt;
    


    &lt;div class=&quot;component-intro &quot;&gt;
        
            
                &lt;h2 class=&quot;&quot;&gt;Conclusion&lt;/h2&gt;
            
        
        
    &lt;/div&gt;



    &lt;p data-block-key=&quot;w4vwg&quot;&gt;In this paper, we propose Chain-of-Agents (CoA), a multi-agent LLM collaboration framework for solving long context tasks. It is a training-free, task- and length-agnostic, interpretable, and cost-effective framework. Experiments show that Chain-of-Agents outperforms RAG and long context LLMs by a large margin, despite its simple design. Analysis shows that by integrating information aggregation and context reasoning, CoA performs better on longer samples.&lt;/p&gt;
&lt;/div&gt;

    &lt;/div&gt;
&lt;/section&gt;

                    
                </description><link>https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/</link><guid isPermaLink="false">https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/</guid><pubDate>Wed, 22 Jan 2025 16:00:00 GMT</pubDate><author>Google</author><category>Generative AI</category><category>Machine Intelligence</category><category>Natural Language Processing</category></item></channel></rss>