<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>ollama blog</title><link>https://ollama.com/blog</link><atom:link href="http://rss.datuan.dev/ollama/blog" rel="self" type="application/rss+xml"></atom:link><description>ollama blog - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 19 Mar 2025 23:27:29 GMT</lastBuildDate><ttl>5</ttl><item><title>Minions: where local and cloud LLMs meet</title><description>Avanika Narayan, Dan Biderman, and Sabri Eyuboglu from Christopher Ré&#39;s Stanford Hazy Research lab, along with Avner May, Scott Linderman, James Zou, have developed a way to shift a substantial portion of LLM workloads to consumer devices by having small on-device models (such as Llama 3.2 with Ollama) collaborate with larger models in the cloud (such as GPT-4o).</description><link>https://ollama.com/blog/minions</link><guid isPermaLink="false">https://ollama.com/blog/minions</guid><pubDate>Mon, 24 Feb 2025 16:00:00 GMT</pubDate></item><item><title>Structured outputs</title><description>Ollama now supports structured outputs making it possible to constrain a model&#39;s output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated to support structured outputs.</description><link>https://ollama.com/blog/structured-outputs</link><guid isPermaLink="false">https://ollama.com/blog/structured-outputs</guid><pubDate>Thu, 05 Dec 2024 16:00:00 GMT</pubDate></item><item><title>Ollama Python library 0.4 with function calling improvements</title><description>With Ollama Python library version 0.4, functions can now be provided as tools. The library now also has full typing support and new examples have been added.</description><link>https://ollama.com/blog/functions-as-tools</link><guid isPermaLink="false">https://ollama.com/blog/functions-as-tools</guid><pubDate>Sun, 24 Nov 2024 16:00:00 GMT</pubDate></item><item><title>Llama 3.2 Vision</title><description>Llama 3.2 Vision 11B and 90B models are now available in Ollama.</description><link>https://ollama.com/blog/llama3.2-vision</link><guid isPermaLink="false">https://ollama.com/blog/llama3.2-vision</guid><pubDate>Tue, 05 Nov 2024 16:00:00 GMT</pubDate></item><item><title>IBM Granite 3.0 models</title><description>Ollama partners with IBM to bring Granite 3.0 models to Ollama.</description><link>https://ollama.com/blog/ibm-granite</link><guid isPermaLink="false">https://ollama.com/blog/ibm-granite</guid><pubDate>Sun, 20 Oct 2024 16:00:00 GMT</pubDate></item><item><title>Llama 3.2 goes small and multimodal</title><description>Ollama partners with Meta to bring Llama 3.2 to Ollama.</description><link>https://ollama.com/blog/llama3.2</link><guid isPermaLink="false">https://ollama.com/blog/llama3.2</guid><pubDate>Tue, 24 Sep 2024 16:00:00 GMT</pubDate></item><item><title>Reduce hallucinations with Bespoke-Minicheck</title><description>Bespoke-Minicheck is a new grounded factuality checking model developed by Bespoke Labs that is now available in Ollama. It can fact-check responses generated by other models to detect and reduce hallucinations.</description><link>https://ollama.com/blog/reduce-hallucinations-with-bespoke-minicheck</link><guid isPermaLink="false">https://ollama.com/blog/reduce-hallucinations-with-bespoke-minicheck</guid><pubDate>Tue, 17 Sep 2024 16:00:00 GMT</pubDate></item><item><title>Tool support</title><description>Ollama now supports tool calling with popular models such as Llama 3.1. This enables a model to answer a given prompt using tool(s) it knows about, making it possible for models to perform more complex tasks or interact with the outside world.</description><link>https://ollama.com/blog/tool-support</link><guid isPermaLink="false">https://ollama.com/blog/tool-support</guid><pubDate>Wed, 24 Jul 2024 16:00:00 GMT</pubDate></item><item><title>Google Gemma 2</title><description>Gemma 2 is now available on Ollama in 3 sizes - 2B, 9B and 27B.</description><link>https://ollama.com/blog/gemma2</link><guid isPermaLink="false">https://ollama.com/blog/gemma2</guid><pubDate>Wed, 26 Jun 2024 16:00:00 GMT</pubDate></item><item><title>An entirely open-source AI code assistant inside your editor</title><description>Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.</description><link>https://ollama.com/blog/continue-code-assistant</link><guid isPermaLink="false">https://ollama.com/blog/continue-code-assistant</guid><pubDate>Thu, 30 May 2024 16:00:00 GMT</pubDate></item><item><title>Google announces Firebase Genkit with Ollama support</title><description>At Google IO 2024, Google announced Ollama support in Firebase Genkit, a new open-source framework for developers to build, deploy and monitor production-ready AI-powered apps.</description><link>https://ollama.com/blog/firebase-genkit</link><guid isPermaLink="false">https://ollama.com/blog/firebase-genkit</guid><pubDate>Sun, 19 May 2024 16:00:00 GMT</pubDate></item><item><title>Llama 3 is not very censored</title><description>Compared to Llama 2, Llama 3 feels much less censored. Meta has substantially lowered false refusal rates. Llama 3 will refuse less than 1/3 of the prompts previously refused by Llama 2.</description><link>https://ollama.com/blog/llama-3-is-not-very-censored</link><guid isPermaLink="false">https://ollama.com/blog/llama-3-is-not-very-censored</guid><pubDate>Thu, 18 Apr 2024 16:00:00 GMT</pubDate></item><item><title>Llama 3</title><description>Llama 3 is now available to run on Ollama. This model is the next generation of Meta&#39;s state-of-the-art large language model, and is the most capable openly available LLM to date.</description><link>https://ollama.com/blog/llama3</link><guid isPermaLink="false">https://ollama.com/blog/llama3</guid><pubDate>Wed, 17 Apr 2024 16:00:00 GMT</pubDate></item><item><title>Embedding models</title><description>Embedding models are available in Ollama, making it easy to generate vector embeddings for use in search and retrieval augmented generation (RAG) applications.</description><link>https://ollama.com/blog/embedding-models</link><guid isPermaLink="false">https://ollama.com/blog/embedding-models</guid><pubDate>Sun, 07 Apr 2024 16:00:00 GMT</pubDate></item><item><title>Ollama now supports AMD graphics cards</title><description>Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.</description><link>https://ollama.com/blog/amd-preview</link><guid isPermaLink="false">https://ollama.com/blog/amd-preview</guid><pubDate>Wed, 13 Mar 2024 16:00:00 GMT</pubDate></item><item><title>Windows preview</title><description>Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and serves the Ollama API including OpenAI compatibility.</description><link>https://ollama.com/blog/windows-preview</link><guid isPermaLink="false">https://ollama.com/blog/windows-preview</guid><pubDate>Wed, 14 Feb 2024 16:00:00 GMT</pubDate></item><item><title>OpenAI compatibility</title><description>Ollama now has initial compatibility with the OpenAI Chat Completions API, making it possible to use existing tooling built for OpenAI with local models via Ollama.</description><link>https://ollama.com/blog/openai-compatibility</link><guid isPermaLink="false">https://ollama.com/blog/openai-compatibility</guid><pubDate>Wed, 07 Feb 2024 16:00:00 GMT</pubDate></item><item><title>Vision models</title><description>New vision models are now available: LLaVA 1.6, in 7B, 13B and 34B parameter sizes. These models support higher resolution images, improved text recognition and logical reasoning.</description><link>https://ollama.com/blog/vision-models</link><guid isPermaLink="false">https://ollama.com/blog/vision-models</guid><pubDate>Thu, 01 Feb 2024 16:00:00 GMT</pubDate></item><item><title>Python &amp; JavaScript Libraries</title><description>The initial versions of the Ollama Python and JavaScript libraries are now available, making it easy to integrate your Python or JavaScript, or Typescript app with Ollama in a few lines of code. Both libraries include all the features of the Ollama REST API, are familiar in design, and compatible with new and previous versions of Ollama.</description><link>https://ollama.com/blog/python-javascript-libraries</link><guid isPermaLink="false">https://ollama.com/blog/python-javascript-libraries</guid><pubDate>Mon, 22 Jan 2024 16:00:00 GMT</pubDate></item><item><title>Building LLM-Powered Web Apps with Client-Side Technology</title><description>Recreate one of the most popular LangChain use-cases with open source, locally running software - a chain that performs Retrieval-Augmented Generation, or RAG for short, and allows you to “chat with your documents”</description><link>https://ollama.com/blog/building-llm-powered-web-apps</link><guid isPermaLink="false">https://ollama.com/blog/building-llm-powered-web-apps</guid><pubDate>Thu, 12 Oct 2023 16:00:00 GMT</pubDate></item><item><title>Ollama is now available as an official Docker image</title><description>Ollama can now run with Docker Desktop on the Mac, and run inside Docker containers with GPU acceleration on Linux.</description><link>https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image</link><guid isPermaLink="false">https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image</guid><pubDate>Wed, 04 Oct 2023 16:00:00 GMT</pubDate></item><item><title>Leveraging LLMs in your Obsidian Notes</title><description>This post walks through how you could incorporate a local LLM using Ollama in Obsidian, or potentially any note taking tool.</description><link>https://ollama.com/blog/llms-in-obsidian</link><guid isPermaLink="false">https://ollama.com/blog/llms-in-obsidian</guid><pubDate>Wed, 20 Sep 2023 16:00:00 GMT</pubDate></item><item><title>How to prompt Code Llama</title><description>This guide walks through the different ways to structure prompts for Code Llama and its different variations and features including instructions, code completion and fill-in-the-middle (FIM).</description><link>https://ollama.com/blog/how-to-prompt-code-llama</link><guid isPermaLink="false">https://ollama.com/blog/how-to-prompt-code-llama</guid><pubDate>Fri, 08 Sep 2023 16:00:00 GMT</pubDate></item><item><title>Run Code Llama locally</title><description>Meta&#39;s Code Llama is now available on Ollama to try.</description><link>https://ollama.com/blog/run-code-llama-locally</link><guid isPermaLink="false">https://ollama.com/blog/run-code-llama-locally</guid><pubDate>Wed, 23 Aug 2023 16:00:00 GMT</pubDate></item><item><title>Run Llama 2 uncensored locally</title><description>This post will give some example comparisons running Llama 2 uncensored model versus its censored model.</description><link>https://ollama.com/blog/run-llama2-uncensored-locally</link><guid isPermaLink="false">https://ollama.com/blog/run-llama2-uncensored-locally</guid><pubDate>Mon, 31 Jul 2023 16:00:00 GMT</pubDate></item></channel></rss>