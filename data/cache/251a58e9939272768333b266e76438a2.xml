<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Ceph Blog</title><link>https://ceph.io/en/news/blog/</link><atom:link href="http://rss.datuan.dev/ceph/blog/a11y" rel="self" type="application/rss+xml"></atom:link><description>Ceph Blog - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 19 Mar 2025 21:18:37 GMT</lastBuildDate><ttl>5</ttl><item><title>Ceph Snowflake Integration</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/snowflake/images/snowflake1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;ceph-snowflake-integration&quot;&gt;Ceph Snowflake Integration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#ceph-snowflake-integration&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/snowflake/images/snowflake1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h3 id=&quot;data-challenges-with-cloud-and-on-prem-environments&quot;&gt;Data Challenges with Cloud and On-prem Environments &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#data-challenges-with-cloud-and-on-prem-environments&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Businesses integrate datasets from multiple sources to derive valuable business insights. Conventional analytics infrastructures, often reliant on specialized hardware, can lead to data silos, lack scalability, and result in escalating costs over time.&lt;/p&gt;&lt;p&gt;The rise of modern analytics architectures in public cloud-based SaaS environments has helped overcome many limitations, allowing for efficient operations and the ability to adapt dynamically to changing workload demands without compromising performance.&lt;/p&gt;&lt;p&gt;However, despite these advancements, not all organizations can realistically shift entirely to a cloud-based environment. Several crucial reasons exist for retaining data on-premises, such as regulatory compliance, security concerns, latency, and cost considerations.&lt;/p&gt;&lt;p&gt;Consequently, many organizations are exploring the benefits of hybrid cloud architectures, making their datasets from on-premises object-based data lake environments available to Cloud SaaS data platforms including Snowflake.&lt;/p&gt;&lt;h3 id=&quot;a-hybrid-cloud-solution-with-snowflake-and-ceph&quot;&gt;A hybrid cloud solution with Snowflake and Ceph &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#a-hybrid-cloud-solution-with-snowflake-and-ceph&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Snowflake is a cloud-based data platform that enhances data-driven insights by allowing governed access to vast amounts of data for collaboration and analysis. Thanks to its native support for the S3 API, it can unify diverse data sources and integrate seamlessly with on-premises solutions including Ceph. This integration enables Snowflake to leverage Ceph&#39;s robust and scalable storage capabilities, effectively bringing cloud data warehouse functionalities into the on-premises environment while ensuring comprehensive data control and security.&lt;/p&gt;&lt;p&gt;Ceph is open-source, software-defined, runs on industry-standard hardware, and has best-in-class coverage of the lingua franca of object storage: the AWS S3 API. Ceph was designed from the ground up as an object store, contrasting with approaches that bolt S3 API servers onto a distributed file system. With Ceph, data placement is by algorithm instead of by lookup. This allows Ceph to scale well into the billions of objects, even on modestly sized clusters. Data stored in Ceph is protected with efficient erasure coding, with in-flight and at-rest checksums, encryption, and robust access control that thoughtfully integrates with enterprise identity systems. Ceph is the perfect complement to Snowflake for establishing a security-first hybrid cloud data lake environment.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.snowflake.com/en/user-guide/data-load-s3-compatible-storage#vendor-support-for-s3-compatible-storage&quot;&gt;Ceph is a supported S3 compatible storage solution for Snowflake&lt;/a&gt;. Using Ceph&#39;s S3-compatible APIs, enterprises can configure Snowflake to access data stored on Ceph through external S3 stages or external S3 tables, enabling efficient queries without requiring data migration to and from the cloud.&lt;/p&gt;&lt;h3 id=&quot;ceph-object-storage%3A-an-ideal-platform-for-data-lakes&quot;&gt;Ceph Object Storage: An ideal platform for data lakes &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#ceph-object-storage%3A-an-ideal-platform-for-data-lakes&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ceph Object Storage is the perfect platform for creating data lakes or lakehouses with key advantages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Cost-effectiveness: Ceph utilizes commodity hardware and open-source software to reduce upfront infrastructure costs and enable incremental and evolutionary upgrades and expansion over time without forklifts or downtime.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;High scalability: Ceph allows horizontal scaling to accommodate large volumes of growing data in a data lake or lakehouse.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;High flexibility: Ceph can handle various data types, including structured, semi-structured, and unstructured data, including text, images, video, and sensor data, making it versatile and appropriate for data lakes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;High availability: Ceph is designed to provide durability and reliability for information stored in a data lake or lake house. Data is always accessible despite hardware failures or disruptions in the network. Ceph offers data replication across multiple geographic locations, providing redundancy and fault tolerance to prevent data loss.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;High performance: Ceph enables parallel data access and processing through integration with data analytics frameworks to enable high throughput and low latency for data ingestion and processing within a data lake or lakehouse. Ceph Object also provides a cache data accelerator (D3N) and query pushdown with S3 Select.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Data governance: Ceph provides efficient management of metadata to enforce data governance policies, track data lineage, monitor data usage, and provide valuable information about the data stored in the data lake, including format and data source.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Security: Ceph has a broad security feature set: encryption at rest and over the wire, external identity integration, Secure Token Service, IAM roles/policies, per-object granular authorization, Object Lock, versioning, and MFA delete.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;example-of-connecting-snowflake-with-data-residing-on-s3-on-prem-ceph&quot;&gt;Example of Connecting Snowflake with data residing on S3 on-prem Ceph &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#example-of-connecting-snowflake-with-data-residing-on-s3-on-prem-ceph&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The most common way of accessing external S3 object storage from Snowflake is to create an &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/data-load-s3-compatible-storage#creating-an-external-stage-for-s3-compatible-storage&quot;&gt;External Stage&lt;/a&gt; and then use the Stage to copy the data into Snowflake or access it directly using n &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/data-load-s3-compatible-storage#extending-your-data-lake-using-external-tables&quot;&gt;External Table&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Next, we will provide two simplistic examples for reference using an on-prem Ceph cluster::&lt;/p&gt;&lt;p&gt;Our Ceph cluster has an S3 Object Gateway configured at &lt;code&gt;s3.cephlabs.blue&lt;/code&gt; and we have a bucket named &lt;code&gt;ecommtrans&lt;/code&gt; containing a CSV-formatted file named &lt;code&gt;transactions&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;$ aws s3 ls s3://ecommtrans/transactions/                                       
2024-06-04 11:33:54   13096729 transaction_data_20240604112945.csv
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The CSV file has the following format:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;client_id,transaction_id,item_id,transaction_date,country,customer_type,item_description,category,quantity,total_amount,marketing_campaign,returned
799315,f47b56a5-2392-4d7c-a3fe-fad18c8b0901,a06210e5-217f-4c3d-8ab9-06e1d8f605e2,2024-03-17 20:35:26,DK,Returning,Smartwatch,Electronics,3,1790.2,,False
858067,9351638c-9d23-4d32-9218-69bbba6b258d,858aa970-9a95-4c99-8b64-d783129dd5cb,2024-02-13 16:18:42,ES,New,Dress,Clothing,4,196.96,,False
528665,7cc494c8-a19d-4771-9686-989d7dfa4c96,0bb7529b-59e8-4d15-adb8-c224b7d7d5b9,2024-03-04 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;example-1.-copy-data-from-an-s3-external-stage-provided-by-ceph&quot;&gt;Example 1. Copy data from an S3 External Stage provided by Ceph &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#example-1.-copy-data-from-an-s3-external-stage-provided-by-ceph&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In the Snowflake UI, we open a new SQL worksheet and run the following SQL code:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE TABLE onprem_database_ingest.raw.transactions
(
  client_id VARCHAR(16777216),
  transaction_id VARCHAR(16777216),
  item_id VARCHAR(16777216),
  transaction_date TIMESTAMP_NTZ(9),
  country VARCHAR(16777216),
  customer_type VARCHAR(16777216),
  item_description VARCHAR(16777216),
  category VARCHAR(16777216),
  quantity NUMBER(38,0),
  total_amount NUMBER(38,0),
  marketing_campaign VARCHAR(16777216),
  returned BOOLEAN
);

LIST @CEPH_INGEST_STAGE/transactions/;

---&amp;gt; copy the Menu file into the Menu table

COPY INTO onprem_database_ingest.raw.transactions
FROM @CEPH_INGEST_STAGE/transactions/;

-- Sample query to verify the setup

SELECT * FROM onprem_database_ingest.raw.transactions
LIMIT 10;~
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/snowflake/images/snowflake2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h4 id=&quot;example-2.-create-an-external-table-from-an-s3-external-stage-provided-by-ceph&quot;&gt;Example 2. Create an external table from an S3 External Stage provided by Ceph &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#example-2.-create-an-external-table-from-an-s3-external-stage-provided-by-ceph&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In the Snowflake UI, we open a new SQL worksheet and run the following SQL code:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;transactions/;

-- Create the External Table with defining expressions for each column

CREATE OR REPLACE EXTERNAL TABLE onprem_database_ingest_trans.raw.trans_external
(
  client_id STRING AS (VALUE:&quot;c1&quot;::STRING),
  transaction_id STRING AS (VALUE:&quot;c2&quot;::STRING),
  item_id STRING AS (VALUE:&quot;c3&quot;::STRING),
  transaction_date TIMESTAMP AS (VALUE:&quot;c4&quot;::TIMESTAMP),
  country STRING AS (VALUE:&quot;c5&quot;::STRING),
  customer_type STRING AS (VALUE:&quot;c6&quot;::STRING),
  item_description STRING AS (VALUE:&quot;c7&quot;::STRING),
  category STRING AS (VALUE:&quot;c8&quot;::STRING),
  quantity NUMBER AS (VALUE:&quot;c9&quot;::NUMBER),
  total_amount NUMBER AS (VALUE:&quot;c10&quot;::NUMBER),
  marketing_campaign STRING AS (VALUE:&quot;c11&quot;::STRING),
  returned BOOLEAN AS (VALUE:&quot;c12&quot;::BOOLEAN)
)

LOCATION = @CEPH_INGEST_STAGE_TRANS/transactions/
FILE_FORMAT = (TYPE = &#39;CSV&#39; FIELD_OPTIONALLY_ENCLOSED_BY = &#39;&quot;&#39; SKIP_HEADER = 1 FIELD_DELIMITER = &#39;,&#39; NULL_IF = (&#39;&#39;))
REFRESH_ON_CREATE = FALSE
AUTO_REFRESH = FALSE
PATTERN = &#39;.*.csv&#39;;

-- Refresh the metadata for the external table

ALTER EXTERNAL TABLE onprem_database_ingest_trans.raw.trans_external REFRESH;

-- Sample query to verify the setup

SELECT * FROM onprem_database_ingest_trans.raw.trans_external

LIMIT 10;~
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/snowflake/images/snowflake3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h3 id=&quot;closing-comments&quot;&gt;Closing comments &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/snowflake/#closing-comments&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Hybrid cloud architectures are increasingly popular, incorporating on-premises solutions including Ceph and cloud-based SaaS platforms including Snowflake. Ceph, which is now supported by Snowflake as an S3-compatible store, makes it possible to access on-premises data lake datasets, enhancing Snowflake&#39;s data warehousing capabilities. This integration establishes a secure, scalable, cost-effective hybrid data lake environment.&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community with our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/snowflake/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/snowflake/</guid><pubDate>Tue, 18 Feb 2025 00:00:00 GMT</pubDate><author>Daniel Parkes, Anthony D&#39;Atri (IBM)</author></item><item><title>Enhancing Object Storage Logging for End Users with the S3 Bucket Logging API</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/images/log1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;enhancing-object-storage-logging-for-end-users-with-the-s3-bucket-logging-api&quot;&gt;Enhancing Object Storage Logging for End Users with the S3 Bucket Logging API &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#enhancing-object-storage-logging-for-end-users-with-the-s3-bucket-logging-api&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;introducing-ceph-s3-bucket-logging&quot;&gt;Introducing Ceph S3 Bucket Logging &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#introducing-ceph-s3-bucket-logging&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The new S3 bucket logging feature introduced as a Technology Preview in Squid 19.2.2 makes tracking, monitoring, and securing bucket operations more straightforward than ever. It aligns with the S3 self-service use case, enabling end users to configure and manage their application storage access logging through the familiar S3 API. This capability empowers users to monitor access patterns, detect unauthorized activities, and analyze usage trends without needing direct intervention from administrators.&lt;/p&gt;&lt;p&gt;By leveraging Ceph&#39;s logging features, users gain actionable insights through logs stored in dedicated buckets, offering flexibility and granularity in tracking operations.&lt;/p&gt;&lt;p&gt;It’s important to note that this feature is not designed to provide real-time performance metrics and monitoring: we have the observability stack provided by Ceph for those needs.&lt;/p&gt;&lt;p&gt;In AWS, the equivalent to Ceph&#39;s Bucket Logging S3 API is referenced as &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html&quot;&gt;S3 Server Access Logging&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In this blog, we will build an example interactive Superset dashboard for our application with the log data generated when enabling S3 bucket logging.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/images/log1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h3 id=&quot;example-use-cases-for-s3-bucket-logging&quot;&gt;Example Use Cases for S3 Bucket Logging &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#example-use-cases-for-s3-bucket-logging&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Application Compliance and Auditing&lt;br&gt;Application teams in regulated industries (finance, healthcare, insurance, etc.) must maintain detailed access logs to meet compliance requirements and ensure traceability for data operations.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Security and Intrusion Detection&lt;br&gt;Monitor bucket access patterns to identify unauthorized activities, detect anomalies, and respond to potential security breaches.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Per Application Usage Analytics&lt;br&gt;Generate detailed insights into buckets, including which objects are frequently accessed, peak traffic times, and operation patterns.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;End User Cost Optimizations&lt;br&gt;Track resource usage, such as the number of &lt;code&gt;GET&lt;/code&gt;, &lt;code&gt;PUT&lt;/code&gt;, and &lt;code&gt;DELETE&lt;/code&gt; requests, to optimize storage and operational costs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Self-Service Monitoring for End Users&lt;br&gt;In a self-service S3-as-a-service setup, end users can configure logging to have a historical view of their activity, helping them manage their data and detect issues independently of the administrators.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Change Tracking for Incremental Backups (Journal Mode Specific)&lt;br&gt;With journal mode enabled, all changes in a bucket are logged before the operations are complete, creating a reliable change log. Backup applications can use this log to inventory changes to perform efficient incremental backups. Here is an example &lt;a href=&quot;https://github.com/yuvalif/rclone/pull/1&quot;&gt;Rclone PR&lt;/a&gt; from Yuval Lifshitz that uses the bucket logging feature to allow more efficient incremental copying from S3.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;logging-modes%3A-standard-vs.-journal&quot;&gt;Logging Modes: Standard vs. Journal &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#logging-modes%3A-standard-vs.-journal&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;standard-(default)&quot;&gt;Standard (Default) &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#standard-(default)&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Log records are written to the log bucket after the operation is completed. If the logging operation fails, it does so silently without notifying the client.&lt;/p&gt;&lt;h4 id=&quot;journal&quot;&gt;Journal &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#journal&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Log records are written to the log bucket before the operation is completed. If logging fails, the operation is halted, and an error is returned to the client. An exception is for multi-delete and delete operations, where the operation may succeed even if logging fails. Note that logs may reflect successful writes even if the operation fails.&lt;/p&gt;&lt;h3 id=&quot;hands-on%3A-configuring-bucket-logging&quot;&gt;Hands-on: Configuring Bucket Logging &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#hands-on%3A-configuring-bucket-logging&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As context, I have a Ceph Object Gateway (RGW) service running on a Squid cluster.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph version
ceph version 19.2.0-53.el9cp (677d8728b1c91c14d54eedf276ac61de636606f8) squid (stable)
# ceph orch ls rgw
NAME         PORTS   RUNNING  REFRESHED  AGE  PLACEMENT                            
rgw.default  ?:8000      4/4  6m ago     8M   ceph04;ceph03;ceph02;ceph06;count:4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I have an IAM account named &lt;code&gt;analytic_ap&lt;/code&gt;, and a root user for the account named &lt;code&gt;rootana&lt;/code&gt;: the S3 user profile for &lt;code&gt;rootana&lt;/code&gt; has already been configured via the AWS CLI.&lt;/p&gt;&lt;p&gt;Through the RGW endpoint using the IAM API (no RGW admin intervention required) I will create a new user with an attached managed policy of &lt;code&gt;AmazonS3FullAccess&lt;/code&gt; so that this user can access all buckets in the &lt;code&gt;analytic_ap&lt;/code&gt; account.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile rootana iam create-user --user-name app_admin_shooters
{
 &quot;User&quot;: {
&quot;Path&quot;: &quot;/&quot;,
&quot;UserName&quot;: &quot;app_admin_shooters&quot;,
&quot;UserId&quot;: &quot;d915f592-6cbc-4c4c-adf2-900c499e8a4a&quot;,
 &quot;Arn&quot;: &quot;arn:aws:iam::RGW46950437120753278:user/app_admin_shooters&quot;,
&quot;CreateDate&quot;: &quot;2025-01-23T08:26:44.086883+00:00&quot;
 }
}

# aws --profile rootana iam create-access-key --user-name app_admin_shooters
{
 &quot;AccessKey&quot;: {
&quot;UserName&quot;: &quot;app_admin_shooters&quot;,
&quot;AccessKeyId&quot;: &quot;YI80WC6HTMHMY958G3EO&quot;,
&quot;Status&quot;: &quot;Active&quot;,
 &quot;SecretAccessKey&quot;: &quot;67Vp071aBf92fJiEe8pBtV6RYqtWBhSceneeZVLH&quot;,
&quot;CreateDate&quot;: &quot;2025-01-23T08:27:03.268781+00:00&quot;
 }
}

#  aws --profile rootana iam attach-user-policy --user-name app_admin_shooters --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I configured a new profile via the AWS CLI with the credentials of the S3 end user we just created named &lt;code&gt;app_admin_shooters&lt;/code&gt;, which with the managed policy attached to the user gives me access to the S3 resources available in the account:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3 ls
#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ok, with everything set, let’s create three source buckets. These buckets belong to three different shooter games/applications and one logging destination bucket named &lt;code&gt;shooterlogs&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3 mb s3://shooterlogs
make_bucket: shooterlogs
# aws --profile app_admin_shooters s3 mb s3://shooterapp1
make_bucket: shooterapp1
]# aws --profile app_admin_shooters s3 mb s3://shooterapp2
make_bucket: shooterapp2
# aws --profile app_admin_shooters s3 mb s3://shooterapp3
make_bucket: shooterapp3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let’s enable bucket logging for each of my shooter app buckets. I will use the target bucket &lt;code&gt;shooter logs&lt;/code&gt; and to organize the logs for each &lt;code&gt;shooterapp&lt;/code&gt; bucket, I will use a &lt;code&gt;TargetPprefix&lt;/code&gt; with the name of the bucket&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt; EOF &amp;gt; enable_logging.json
{
  &quot;LoggingEnabled&quot;: {
    &quot;TargetBucket&quot;: &quot;shooterlogs&quot;,
    &quot;TargetPrefix&quot;: &quot;shooterapp1&quot;
  }
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the JSON file is ready we can apply it to our buckets, using the &lt;code&gt;sed&lt;/code&gt; command in each iteration to change the &lt;code&gt;TargetPrefix&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3api put-bucket-logging --bucket shooterapp1 --bucket-logging-status file://enable_logging.json
# sed -i &#39;s/shooterapp1/shooterapp2/&#39; enable_logging.json
# aws --profile app_admin_shooters s3api put-bucket-logging --bucket shooterapp2 --bucket-logging-status file://enable_logging.json
# sed -i &#39;s/shooterapp2/shooterapp3/&#39; enable_logging.json
# aws --profile app_admin_shooters s3api put-bucket-logging --bucket shooterapp3 --bucket-logging-status file://enable_logging.json
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can list the logging configuration for a bucket with the following s3api command.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3api get-bucket-logging --bucket shooterapp1
{
    &quot;LoggingEnabled&quot;: {
        &quot;TargetBucket&quot;: &quot;shooterlogs&quot;,
        &quot;TargetPrefix&quot;: &quot;shooterapp1&quot;,
        &quot;TargetObjectKeyFormat&quot;: {
            &quot;SimplePrefix&quot;: {}
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We will &lt;code&gt;PUT&lt;/code&gt; some objects into our first bucket &lt;code&gt;shooterapp1&lt;/code&gt;, and also delete a set of objects so that we can create logs in our log bucket.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# for i in {1..20} ; do  aws --profile app_admin_shooters s3 cp /etc/hosts s3://shooterapp1/file${i} ; done
upload: ../etc/hosts to s3://shooterapp1/file1                    
upload: ../etc/hosts to s3://shooterapp1/file2                    
…             
# for i in {1..5} ; do  aws --profile app_admin_shooters s3 rm s3://shooterapp1/file${i} ; done
delete: s3://shooterapp1/file1
delete: s3://shooterapp1/file2
…
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When we check our configured log bucket &lt;code&gt;shooterlogs&lt;/code&gt;, it’s empty. Why !?&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3 ls s3://shooterlogs/
# 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Note from the docs&lt;/em&gt;: For performance reasons, even though the log records are written to persistent storage, the log object will appear in the log bucket only after some configurable amount of time (or if the maximum object size of 128MB is reached). This time (in seconds) could be set per source bucket via a Ceph extension to the REST API, or globally via the &lt;code&gt;rgw_bucket_logging_obj_roll_time&lt;/code&gt; configuration option. If not set, the default time is 5 minutes. Adding a log object to the log bucket is done &quot;lazily&quot;, meaning that if no more records are written to the object, it may remain outside the log bucket even after the configured time has passed.&lt;/p&gt;&lt;p&gt;If we don’t want to wait for the object roll time (defaults to 5 minutes), we can force a flush of the log buffer with the &lt;code&gt;radosgw-admin&lt;/code&gt; command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# radosgw-admin bucket logging flush --bucket shooterapp1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When we check again, the object with the logs for bucket &lt;code&gt;shooterapp1&lt;/code&gt; is there as expected:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3 ls s3://shooterlogs/
2025-01-23 08:28:16       8058 shooterapp12025-01-23-13-21-00-A54CQC9GIO7O4F9D
# aws --profile app_admin_shooters s3 cp s3://shooterlogs/shooterapp12025-01-23-13-21-00-A54CQC9GIO7O4F9D - | cat
RGW46950437120753278 shooterapp1 [23/Jan/2025:13:21:00 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323278.12172315742054314872 REST.GET.get_bucket_logging - &quot;GET /shooterapp1?logging HTTP/1.1&quot; 200 - - - - 14ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
RGW46950437120753278 shooterapp1 [23/Jan/2025:13:23:33 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323242.15617167555539888584 REST.PUT.put_obj file1 &quot;PUT /shooterapp1/file1 HTTP/1.1&quot; 200 - 333 333 - 19ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
…
RGW46950437120753278 shooterapp1 [23/Jan/2025:13:24:01 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323242.18353336346755391699 REST.DELETE.delete_obj file1 &quot;DELETE /shooterapp1/file1 HTTP/1.1&quot; 204 NoContent - 333 - 11ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
RGW46950437120753278 shooterapp1 [23/Jan/2025:13:24:02 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.311105.12134465030800156375 REST.DELETE.delete_obj file2 &quot;DELETE /shooterapp1/file2 HTTP/1.1&quot; 204 NoContent - 333 - 11ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
RGW46950437120753278 shooterapp1 [23/Jan/2025:13:24:03 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323260.3289411001891924009 REST.DELETE.delete_obj file3 &quot;DELETE /shooterapp1/file3 HTTP/1.1&quot; 204 NoContent - 333 - 9ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: To explore the output fields available with the current implementation of the bucket logging feature, check out the &lt;a href=&quot;https://docs.ceph.com/en/latest/radosgw/bucket_logging/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Let&#39;s see if bucket logging works for our other bucket, &lt;code&gt;shooterapp2&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# for i in {1..3} ; do  aws --profile app_admin_shooters s3 cp /etc/hosts s3://shooterapp2/file${i} ; done
upload: ../etc/hosts to s3://shooterapp2/file1
upload: ../etc/hosts to s3://shooterapp2/file2
upload: ../etc/hosts to s3://shooterapp2/file3
# for i in {1..3} ; do  aws --profile app_admin_shooters s3 cp s3://shooterapp2/file${i} - ; done
# radosgw-admin bucket logging flush --bucket shooterapp2
flushed pending logging object &#39;shooterapp22025-01-23-10-01-57-TJNTA3FU60TS21MK&#39; to target bucket &#39;shooterlogs&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Checking our configured log bucket, we can see that we now have two objects in the bucket with the prefix of the source bucket name.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile app_admin_shooters s3 ls s3://shooterlogs/
2025-01-23 08:28:16       8058 shooterapp12025-01-23-13-21-00-A54CQC9GIO7O4F9D
2025-01-23 10:01:57       2628 shooterapp22025-01-23-15-00-48-FIE6B8NNMANFTTFI
# aws --profile app_admin_shooters s3 cp s3://shooterlogs/shooterapp22025-01-23-15-00-48-FIE6B8NNMANFTTFI - | cat
RGW46950437120753278 shooterapp2 [23/Jan/2025:15:00:48 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323242.10550516265852869740 REST.PUT.put_obj file1 &quot;PUT /shooterapp2/file1 HTTP/1.1&quot; 200 - 333 333 - 22ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
RGW46950437120753278 shooterapp2 [23/Jan/2025:15:00:49 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a 
…
RGW46950437120753278 shooterapp2 [23/Jan/2025:15:01:36 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323278.16364063589570559207 REST.HEAD.get_obj file1 &quot;HEAD /shooterapp2/file1 HTTP/1.1&quot; 200 - - 333 - 4ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
RGW46950437120753278 shooterapp2 [23/Jan/2025:15:01:36 +0000] - d915f592-6cbc-4c4c-adf2-900c499e8a4a fcabdf4a-86f2-452f-a13f-e0902685c655.323242.2016501269767674837 REST.GET.get_obj file1 &quot;GET /shooterapp2/file1 HTTP/1.1&quot; 200 - - 333 - 3ms - - - - - - - s3.cephlabs.com.s3.cephlabs.com - -
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;unlocking-insights-with-trino-and-superset%3A-visualizing-your-application-logs&quot;&gt;Unlocking Insights with Trino and Superset: Visualizing Your Application Logs &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#unlocking-insights-with-trino-and-superset%3A-visualizing-your-application-logs&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/images/log2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h4 id=&quot;configuring-trino-to-run-sql-queries-in-our-application-logs.&quot;&gt;Configuring Trino to run SQL queries in our application logs. &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#configuring-trino-to-run-sql-queries-in-our-application-logs.&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In this guide, we&#39;ll walk you through setting up Trino to query your application logs stored in S3-compatible storage and explore some powerful SQL queries you can use to analyze your data.&lt;/p&gt;&lt;p&gt;We already have Trino up and running; I have configured the hive connector to use S3A to access our S3 endpoint for instructions on how to Setup Trino.&lt;/p&gt;&lt;p&gt;The first step is configuring an external table pointing to your log data stored in an S3-compatible bucket: in our example, &lt;code&gt;shooterlogs&lt;/code&gt;. Below is an example of how the table is created:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;trino&amp;gt; SHOW CREATE TABLE hive.default.raw_logs;
                 Create Table                 
----------------------------------------------
 CREATE TABLE hive.default.raw_logs (         
    line varchar                              
 )                                            
 WITH (                                       
    external_location = &#39;s3a://shooterlogs/&#39;, 
    format = &#39;TEXTFILE&#39;                       
 )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To make the data more usable you can parse each log line into meaningful fields including account ID, bucket name, operation type, HTTP response code, etc. To simplify querying and reuse, I will create a view that encapsulates the log parsing logic:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;trino&amp;gt; CREATE VIEW hive.default.log_summary AS
    -&amp;gt; SELECT
    -&amp;gt;     split(line, &#39; &#39;)[1] AS account_id, -- Account ID
    -&amp;gt;     split(line, &#39; &#39;)[2] AS bucket_name, -- Bucket Name
    -&amp;gt;     split(line, &#39; &#39;)[3] AS timestamp, -- Timestamp
    -&amp;gt;     split(line, &#39; &#39;)[6] AS user_id, -- User ID
    -&amp;gt;     split(line, &#39; &#39;)[8] AS operation_type, -- Operation Type
    -&amp;gt;     split(line, &#39; &#39;)[9] AS object_key, -- Object Key
    -&amp;gt;     regexp_extract(line, &#39;&quot;([^&quot;]+)&quot;&#39;, 1) AS raw_http_request, -- Raw HTTP Request
    -&amp;gt;     CAST(regexp_extract(line, &#39;&quot;[^&quot;]+&quot; ([0-9]+) &#39;, 1) AS INT) AS http_status, -- HTTP Status
    -&amp;gt;     CAST(CASE WHEN split(line, &#39; &#39;)[14] = &#39;-&#39; THEN NULL ELSE split(line, &#39; &#39;)[14] END AS BIGINT) AS object_size, -- Object Size
    -&amp;gt;     CASE WHEN split(line, &#39; &#39;)[17] = &#39;-&#39; THEN NULL ELSE split(line, &#39; &#39;)[17] END AS request_duration, -- Request Duration
    -&amp;gt;     regexp_extract(line, &#39;[0-9]+ms&#39;, 0) AS request_time, -- Request Time (e.g., 22ms)
    -&amp;gt;     regexp_extract(line, &#39; ([^ ]+) [^ ]+ [^ ]+$&#39;, 1) AS hostname -- Hostname (third-to-last field)
    -&amp;gt; FROM hive.default.raw_logs;
    -&amp;gt; 
CREATE VIEW
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the view in place, you can quickly write queries to summarize log data. For example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;trino&amp;gt; SELECT operation_type, COUNT(*) AS operation_count
    -&amp;gt; FROM hive.default.log_summary
    -&amp;gt; GROUP BY operation_type;
       operation_type        | operation_count 
-----------------------------+-----------------
 REST.DELETE.delete_obj      |               5 
 REST.HEAD.get_obj           |               3 
 REST.GET.get_bucket_logging |               1 
 REST.PUT.put_obj            |              23 
 REST.GET.list_bucket        |               1 
 REST.GET.get_obj            |               3 
(6 rows)
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;creating-meaningful-charts-with-our-application-logs&quot;&gt;Creating meaningful charts with our Application logs &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#creating-meaningful-charts-with-our-application-logs&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;With the view we created from your Trino data we can perform historical monitoring and analyze your S3 bucket activity effectively. Below is an example list of potential visualizations you can create to provide actionable insights into bucket usage and access patterns.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Request Volume by Operation Type&lt;/li&gt;&lt;li&gt;Data Transferred by Operation&lt;/li&gt;&lt;li&gt;Average Request Duration by Operation&lt;/li&gt;&lt;li&gt;Top Objects Accessed by user/account&lt;/li&gt;&lt;li&gt;Request Patterns by Time of Day&lt;/li&gt;&lt;li&gt;Failed Requests by Operation&lt;/li&gt;&lt;li&gt;Access Patterns by Bucket/User&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I will use Superset in this example, but another visualization tool could achieve the same outcome. I have a running instance of Superset, and I have configured Trino as a source database for Superset.&lt;/p&gt;&lt;p&gt;Here is the query used and the resulting graph in a superset dashboard. We present per-bucket operation type counts and average latency.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;SELECT operation_type AS operation_type, bucket_name AS bucket_name, sum(&quot;Operations Count&quot;) AS &quot;SUM(Operations Count)&quot; 
FROM (SELECT 
    bucket_name, 
    operation_type, 
    COUNT(*) AS &quot;Operations Count&quot;, 
    AVG(CAST(regexp_extract(request_time, &#39;[0-9]+&#39;, 0) AS DOUBLE)) AS &quot;Average Latency&quot;
FROM hive.default.log_summary
GROUP BY bucket_name, operation_type
ORDER BY &quot;Operations Count&quot; DESC
) AS virtual_table GROUP BY operation_type, bucket_name ORDER BY &quot;SUM(Operations Count)&quot; DESC
LIMIT 1000;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/images/log3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Another example related to HTTP requests is presenting the distribution of HTTP request codes in a pie chart.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/images/log4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Here we show the top users making requests to the &lt;code&gt;shooter&lt;/code&gt; application buckets.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/images/log5.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;These are just some basic graph examples you can create. Far more advanced graphs can be built with the features available in Superset.&lt;/p&gt;&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The introduction of the S3 bucket logging feature is a game-changer for storage access management. This feature provides transparency and control by empowering end users to configure and manage their application access logs. With the ability to log operations at the bucket level, users can monitor activity, troubleshoot issues, and enhance their security posture—all tailored to their specific requirements without admin intervention.&lt;/p&gt;&lt;p&gt;To showcase its potential, we explored how tools like Trino and Superset can analyze and visualize the log data generated by S3 Bucket Logging. These are just examples of the many possibilities the bucket logging feature enables.&lt;/p&gt;&lt;p&gt;Ceph developers are working hard to improve bucket logging for future releases, including bug fixes, enhancements, and better AWS S3 compatibility. Stay Tuned!&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community via our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/enhancing-object-storage-logging/</guid><pubDate>Thu, 13 Feb 2025 00:00:00 GMT</pubDate><author>Daniel Alexander Parkes, Anthony D&#39;Atri (IBM)</author></item><item><title>Ceph Events Survey</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/events-survey/images/theglobe2025.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;p&gt;The Ceph Foundation and Ambassadors are busy building a full calendar of Ceph events for 2025!&lt;/p&gt;&lt;p&gt;As many of you know, Ceph has always thrived on its amazing community. Events like Ceph Days and Cephalocon are key opportunities where we all can learn, connect, and share experience.&lt;/p&gt;&lt;p&gt;Following the success of Cephalocon at CERN and Ceph Days in India, we’ve announced Ceph Days London &amp;amp; Silicon Valley -- check out &lt;a href=&quot;https://ceph.io/en/community/events/&quot;&gt;https://ceph.io/en/community/events/&lt;/a&gt; to get involved. And watch that space -- Ceph Days in Seattle, New York, and Berlin will be announced soon!&lt;/p&gt;&lt;p&gt;Looking forward, we need your help to help shape our future events... and to plan our Cephalocon! If you have a moment, please share your thoughts in our Ceph Events survey: &lt;a href=&quot;https://forms.gle/Rm41d547Rb59S8xf9&quot;&gt;https://forms.gle/Rm41d547Rb59S8xf9&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Looking forward to seeing you at an event soon!&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/events-survey/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/events-survey/</guid><pubDate>Wed, 12 Feb 2025 16:00:00 GMT</pubDate><author>Dan van der Ster</author></item><item><title>TPC-DS Benchmark using Trino with Ceph Object Storage S3 Select</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tpc4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;introduction&quot;&gt;Introduction &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#introduction&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this article we analyze the results of performance benchmarks conducted on Trino with the Ceph Object S3 Select feature enabled, using TPC-DS benchmark queries at 1TB and 3TB scale. We demonstrate that, on average, queries run 2.5X faster. In some cases, we achieved nine times improvement with a network data processing reduction of 144TB compared to using Trino without the S3 Select feature enabled. Combining IBM Storage Ceph&#39;s S3 Select with Trino/Presto can enhance data lake performance, reduce costs, and simplify data access for organizations.&lt;/p&gt;&lt;p&gt;We would like to thank &lt;a href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/gsalomon@ibm.com&quot;&gt;Gal Salomon&lt;/a&gt; and &lt;a href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/twilkins@ibm.com&quot;&gt;Tim Wilkinson&lt;/a&gt; for conducting the TPC-DS benchmarking and providing us with these results.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tpc1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h2 id=&quot;what-is-trino%3F&quot;&gt;What is Trino? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#what-is-trino%3F&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Trino is a distributed SQL query engine that allows users to query data from multiple sources using a single SQL statement. It provides data warehouse-like capabilities directly on a data lake.&lt;/p&gt;&lt;h2 id=&quot;what-are-the-difference-among-trino%2C-presto-and-prestodb%3F&quot;&gt;What are the difference among Trino, Presto and PrestoDB? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#what-are-the-difference-among-trino%2C-presto-and-prestodb%3F&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;You may have encountered references to Trino, Presto, and PrestoDB, all of which originated from the same project. Presto was the initial project from Facebook, which was open-sourced in 2013. PrestoSQL became a community-based open-source project in 2018 and was rebranded to Trino in 2020.&lt;/p&gt;&lt;p&gt;Presto is an essential tool for data engineers who require a fast query engine for their higher-level Business Intelligence (BI) tools.&lt;/p&gt;&lt;h2 id=&quot;why-choose-ceph-for-s3-object-storage%3F&quot;&gt;Why Choose Ceph for S3 Object Storage? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#why-choose-ceph-for-s3-object-storage%3F&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Ceph provides a first-class, highly compatible S3 API for on-premises deployments.&lt;/li&gt;&lt;li&gt;Ceph confidently meets the needs of critical large-scale installations and the ever-growing demand for data. Its performance scales alongside capacity, resulting in substantial cost savings and the ability to manage exponential data growth.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;what-enhancements-does-ceph-bring-to-data-query-tools-like-trino%2C-presto%3F&quot;&gt;What enhancements does Ceph bring to data query tools like Trino, Presto? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#what-enhancements-does-ceph-bring-to-data-query-tools-like-trino%2C-presto%3F&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Ceph provides the S3 API S3 Select feature. S3 Select significantly improves the efficiency of SQL queries of data stored in S3-compatible object storage. By pushing the query to the Ceph cluster, S3 Select can dramatically enhance performance, processing queries faster and minimizing network and CPU resource costs. S3 Select and Trino are horizontally scalable, handling increasing data volumes and user queries without sacrificing performance. Trino&#39;s support for SQL and S3 Select&#39;s ability to query data in place enables users to access and analyze data without complex data movement or transformation tasks.&lt;/p&gt;&lt;p&gt;Ceph&#39;s Object Datacenter-Data-Delivery Network (D3N) feature uses high-speed storage such as NVMe SSDs or DRAM to cache datasets on the access side. D3N improves the performance of big-data jobs running in analysis clusters by accelerating recurring reads from the data lake or lakehouse.&lt;/p&gt;&lt;h2 id=&quot;tpc-ds-benchmarks-using-ceph-%2B-trino&quot;&gt;TPC-DS benchmarks using Ceph + Trino &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#tpc-ds-benchmarks-using-ceph-%2B-trino&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;test-procedure&quot;&gt;Test Procedure &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#test-procedure&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We executed the following &lt;a href=&quot;https://github.com/ceph/s3select/tree/master/TPCDS/sample-queries-tpcds&quot;&gt;72 TPC-DS queries&lt;/a&gt; at three different scale factors, 1TB, 2TB and 3TB, to characterize performance and resource consumption. The datasets were in uncompressed CSV format. We executed each query numerous times with and without S3 Select and ensured consistent results by monitoring the standard deviations for each run.&lt;/p&gt;&lt;p&gt;If you’re interested in exploring this topic further, please check out Gal Salomon&#39;s &lt;a href=&quot;https://github.com/ceph/s3select/blob/master/README.md#how-to-run-trino-with-cephs3select&quot;&gt;GitHub repository&lt;/a&gt;, where you will find instructions on how to set up a testing environment with Trino and Ceph. Instructions are also provided for the TPC-DS benchmarking tools used for this benchmark.&lt;/p&gt;&lt;h3 id=&quot;test-environment&quot;&gt;Test Environment &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#test-environment&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The hardware used for the benchmark was the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Trino client (driver) nodes&lt;ul&gt;&lt;li&gt;Trino Version: 405&lt;/li&gt;&lt;li&gt;3x Dell R630&lt;ul&gt;&lt;li&gt;2x E5-2683 v3 (28 total cores, 56 threads)&lt;/li&gt;&lt;li&gt;128 GB RAM&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Ceph cluster nodes&lt;ul&gt;&lt;li&gt;OS: RHEL9.2&lt;/li&gt;&lt;li&gt;Ceph version: 18.2.0-110.el9cp (Reef)&lt;/li&gt;&lt;li&gt;3x Dell R630 Monitor / Manager nodes&lt;ul&gt;&lt;li&gt;2x E5-2683 v3 (28 total cores, 56 threads)&lt;/li&gt;&lt;li&gt;128 GB RAM&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;8x Supermicro 6048R OSD / RGW nodes&lt;ul&gt;&lt;li&gt;2x Intel E5-2660 v4 (28 total cores, 56 threads)&lt;/li&gt;&lt;li&gt;256 GB RAM&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;192x BlueStore OSDs&lt;ul&gt;&lt;li&gt;24 2TB HDD per node&lt;/li&gt;&lt;li&gt;2x 800G NVMe SSDs for WAL/DB per OSD node&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;tunables&quot;&gt;Tunables &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#tunables&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;These S3 Select settings were adjusted:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;hive.max-split-size&lt;/strong&gt; The maximum size of a single file section assigned to a worker. More minor splits result in more parallelism and thus can decrease latency but incur more overhead and increase load on the system. Testing began with 4MB, 8MB, 16MB, 32MB, 64MB, and 128MB values but we eventually settled on 128MB for all tests.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;hive.max-splits-per-second&lt;/strong&gt; The maximum number of splits per second generated per table scan. It can be used to reduce the load on the storage system. There is no default limit, so Trino maximizes the parallelization of data access. All testing was performed using 10K for this setting.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;concurrency&quot;&gt;Concurrency &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#concurrency&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The Trino engine processes complex queries by dividing the original query into multiple parallel S3 Select requests. These requests split the requested table (an S3 object) into equal ranges that are then distributed across our Ceph cluster&#39;s RGW service. The load balancer efficiently channels requests among Ceph Object Gateways, ensuring optimal performance and scalability for our data processing needs.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tps2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h3 id=&quot;test-results&quot;&gt;Test Results &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#test-results&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;This next section provides an overview of TPC-DS benchmark results. These results help us understand how the Ceph Object S3 Select feature yields substantial benefits when working with CSV datasets. The benefits include improved query times and reduced data processing. We have included a diagram below that shows the total network traffic reduction achieved by using S3 Select. We can save 144TB of network traffic by utilizing this feature.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tps3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;The following graph shows the per-query speedup achieved using S3 Select for the 3TB scale dataset. The X axis value is the query number from the above repository and the Y axix value is the speed improvement for each query. During testing we observed that enabling S3 Select improved all 72 queries. The query acheiving the most speedup was 9 times faster, and the overall average improvement was around 2.5x.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tps4.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;When S3 Select is enabled we offload computational work to the Ceph Object Gateways, so as expected they saw increased CPU usage when executing the queries with S3 Select enabled. However, the CPU utilization remained at an acceptable level Memory demand increase with S3 Select enabled was barely noticeable, with an average increase 2.50%. Pushdown can process objects of any size since it does so in chunks without preloading the entire object.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tps5.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Query number 9 was able to reduce the network data processing by 18TB. The total reduction in processed data across all 72 queries was 144 TB when enabling S3 Select.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/images/tps6.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h2 id=&quot;summary-and-what%E2%80%99s-up-next&quot;&gt;Summary and What’s Up Next &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/tpc-bench-trino/#summary-and-what%E2%80%99s-up-next&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this post, we shared the results of our benchmark testing, where we ran 72 TPC-DS 72 queries at 1TB and 3TB scale. We have found that utilizing Ceph Object S3 Select pushdown performance optimizations enables queries to complete more quickly than before with significantly lower resource demands. With Trino and S3 Select, you can push the computational work of projection and prediction operations to Ceph, achieving up to 9x performance improvement in query runtime, with an average of 2.5x. This significantly reduces data transfer across the network, saving 144TB of network traffic for the 72 executed queries. Organizations can enhance data lake performance, educe costs, and simplify data access by combining Ceph S3 Select with Trino and Presto.&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community with our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/tpc-bench-trino/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/tpc-bench-trino/</guid><pubDate>Wed, 12 Feb 2025 00:00:00 GMT</pubDate><author>Daniel Parkes, Anthony D&#39;Atri (IBM)</author></item><item><title>Enhanced Visibility into Ceph Object Multisite Replication with the New HTTP Sync Status Headers</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/images/sync2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;introduction&quot;&gt;Introduction &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#introduction&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In a world where data must be quickly accessed and protected across multiple geographical regions, multi-site replication is a critical feature of object storage. Whether running global applications or maintaining a robust disaster recovery plan, replicating objects between different regions is essential for redundancy and business continuity.&lt;/p&gt;&lt;p&gt;Ceph Object storage multi-site replication is asynchronous and log-based. The nature of async replication can present challenges validating where your data currently resides or confirming that it has fully replicated to all remote zones. This is not acceptable for certain applications and use cases that require close to strong consistency on write, and all the objects to be replicated and available in all sites before they are made accessible to the application or user.&lt;/p&gt;&lt;p&gt;As a side note, complete consistency on write can be provided using a Ceph Stretch Cluster solution that provides synchronous replication, but this has its limitations for a geo-dispersed solution as latency is a key factor for stretch clusters. If you need geo-replication you thus often will implement multisite async replication for object storage as your solution.&lt;/p&gt;&lt;h2 id=&quot;the-challenge&quot;&gt;The Challenge &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#the-challenge&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Application owners often need to know if an object is already available in the destination zone before triggering subsequent actions (e.g., analytics jobs, further data processing, or regulatory archiving). Storage operations teams may want clear insight into how long replication takes, enabling them to alert and diagnose slow or faulty network links. Automation and data pipelines might require a programmatic way to track replication status before proceeding with the next step in a workflow.&lt;/p&gt;&lt;h2 id=&quot;ceph-squid-introduces-replication-headers&quot;&gt;Ceph Squid Introduces Replication Headers &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#ceph-squid-introduces-replication-headers&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To address this visibility gap, Ceph Squid introduces new HTTP response headers that expose exactly where each object is in the replication process:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;x-amz-replication-status&lt;/code&gt;&lt;br&gt;A quick way to determine if the object is pending replication, in progress, or already fully replicated. The status might show &lt;code&gt;PENDING&lt;/code&gt;, &lt;code&gt;COMPLETED&lt;/code&gt;, or &lt;code&gt;REPLICA&lt;/code&gt; depending on configuration.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;x-rgw-replicated-from&lt;/code&gt;&lt;br&gt;Shows the source zone from which the object was initially replicated.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;x-rgw-replicated-at&lt;/code&gt;&lt;br&gt;Provides a timestamp indicating when the object was successfully replicated. By comparing this to the object’s &lt;code&gt;Last-Modified&lt;/code&gt; header, you get an instant measure of replication latency that is valuable for real-time monitoring or performance tuning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These headers enable a programmatic and deterministic way to know whether data has propagated to the target zone. It’s vital to note that the primary use case for these new HTTP replication headers is to query the status of objects during ingest to help the application make decisions based on the replication status of the objects. This is not intended for infra teams to check the replication status of &lt;em&gt;all&lt;/em&gt; objects by scanning through billions of objects.&lt;/p&gt;&lt;h2 id=&quot;example-use-cases-for-http-replication-headers&quot;&gt;Example Use Cases for HTTP Replication Headers &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#example-use-cases-for-http-replication-headers&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;application-workflows&quot;&gt;Application Workflows &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#application-workflows&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Developers can integrate these headers into their application logic. After uploading an object, the application can poll the &lt;code&gt;x-amz-replication-status&lt;/code&gt; header to ensure the object is fully available in the destination zone before triggering subsequent actions.&lt;/p&gt;&lt;h3 id=&quot;operations-canary-monitoring&quot;&gt;Operations Canary Monitoring &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#operations-canary-monitoring&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;An automated job (sometimes called a &lt;em&gt;synthetic test&lt;/em&gt; or &lt;em&gt;canary test&lt;/em&gt;) can periodically upload and delete an object, checking how long replication takes. If latency breaches a certain threshold, the operations team can be alerted to investigate potential network or configuration issues.&lt;/p&gt;&lt;h3 id=&quot;data-pipelines-and-notifications&quot;&gt;Data Pipelines and Notifications &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#data-pipelines-and-notifications&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;While polling headers is often the most straightforward approach, you may leverage Ceph S3 bucket notifications for certain replication-related events. Integrating these with a message broker like Kafka can help orchestrate larger, event-driven workflows. Fore more information see the &lt;a href=&quot;https://docs.ceph.com/en/latest/radosgw/s3-notification-compatibility/&quot;&gt;Ceph docs on S3 notifications&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;basic-hands-on-example&quot;&gt;Basic Hands-on Example &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#basic-hands-on-example&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I have multi-site replication set up between two Ceph clusters. They are part of a zonegroup named &lt;code&gt;multizg&lt;/code&gt; and we have bidirectional full-zone replication configured between &lt;code&gt;zone1&lt;/code&gt; and &lt;code&gt;zone2&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# radosgw-admin sync info
{
    &quot;sources&quot;: [
        {
            &quot;id&quot;: &quot;all&quot;,
            &quot;source&quot;: {
                &quot;zone&quot;: &quot;zone2&quot;,
                &quot;bucket&quot;: &quot;*&quot;
            },
            &quot;dest&quot;: {
                &quot;zone&quot;: &quot;zone1&quot;,
                &quot;bucket&quot;: &quot;*&quot;
            },
    &quot;dests&quot;: [
        {
            &quot;id&quot;: &quot;all&quot;,
            &quot;source&quot;: {
                &quot;zone&quot;: &quot;zone1&quot;,
                &quot;bucket&quot;: &quot;*&quot;
            },
            &quot;dest&quot;: {
                &quot;zone&quot;: &quot;zone2&quot;,
                &quot;bucket&quot;: &quot;*&quot;
            },
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For detailed information about Ceph Object Storage multisite replication, see the blog series that covers this feature in-depth, from architecture to setup and fine-tuning:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part1/&quot;&gt;multisite part1&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part2/&quot;&gt;multisite part2&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part3/&quot;&gt;multisite part3&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part4/&quot;&gt;multisite part4&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part5/&quot;&gt;multisite part5&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part6/&quot;&gt;multisite part6&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part7/&quot;&gt;multisite part7&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-replication_part8/&quot;&gt;multisite part8&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A simple way to view these new replication headers is to use &lt;code&gt;s3cmd&lt;/code&gt; with the &lt;code&gt;--debug&lt;/code&gt; flag, which prints raw HTTP response headers from the Ceph Object Gateway. By filtering for &lt;code&gt;rgw-&lt;/code&gt; or &lt;code&gt;x-amz-&lt;/code&gt; lines, we can easily spot replication-related information.&lt;/p&gt;&lt;p&gt;Let&#39;s check it out. I uploaded an object into &lt;code&gt;zone1&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# s3cmd --host ceph-node-00:8088 put /etc/hosts s3://bucket1/file20
upload: &#39;/etc/hosts&#39; -&amp;gt; &#39;s3://bucket1/file20&#39;  [1 of 1]
 640 of 640   100% in    0s     7.63 KB/s  done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When I check the object&#39;s status on the source zone where I uploaded the object, it’s in the &lt;code&gt;PENDING&lt;/code&gt; state, indicating the object is still replicating. Eventually, once replication is complete, the status will transition to &lt;code&gt;COMPLETED&lt;/code&gt; in the source zone and &lt;code&gt;REPLICA&lt;/code&gt; in the destination zone.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# s3cmd --host ceph-node-00:8088 --debug info s3://bucket1/file20 2&amp;gt;&amp;amp;1 | grep -B 2 &#39;rgw-&#39;
             &#39;x-amz-replication-status&#39;: &#39;PENDING&#39;,
             &#39;x-amz-request-id&#39;: &#39;tx00000f2948c72a2d2fb8e-0067a5c961-35964-zone1&#39;,
             &#39;x-rgw-object-type&#39;: &#39;Normal&#39;},
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, let’s check on the destination zone endpoint:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# s3cmd --host ceph-node-05:8088 --debug info s3://bucket1/file20 2&amp;gt;&amp;amp;1 | grep -B 2 &#39;rgw-&#39;
             &#39;x-amz-replication-status&#39;: &#39;REPLICA&#39;,
             &#39;x-amz-request-id&#39;: &#39;tx00000a98cf7b6a584b95b-0067a5cac9-29779-zone2&#39;,
             &#39;x-rgw-object-type&#39;: &#39;Normal&#39;,
             &#39;x-rgw-replicated-at&#39;: &#39;Fri, 07 Feb 2025 08:50:07 GMT&#39;,
             &#39;x-rgw-replicated-from&#39;: &#39;b6c9ca95-6683-42a5-9dff-ba209039c61b:bucket1:b6c9ca95-6683-42a5-9dff-ba209039c61b.32035.1&#39;},
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here, the relevant headers tell us:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;x-amz-replication-status: REPLICA&lt;/code&gt;&lt;br&gt;The object has completed replication and is available in the remote zone(s).&lt;/li&gt;&lt;li&gt;&lt;code&gt;x-rgw-replicated-at: &#39;Fri, 07 Feb 2025 08:50:07 GMT&#39;&lt;/code&gt;&lt;br&gt;Shows the timestamp when replication was finished.&lt;/li&gt;&lt;li&gt;&lt;code&gt;x-rgw-replicated-from: 8f8c3759-aaaf-4e6d-b346-...:bucket1:...&lt;/code&gt;&lt;br&gt;Identifies the source zone (and bucket) from which the object was replicated.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Let&#39;s check that the status of the object in the source site has moved into the &lt;code&gt;COMPLETE&lt;/code&gt; state:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# s3cmd --host ceph-node-00:8088 --debug info s3://bucket1/file20 2&amp;gt;&amp;amp;1 | grep x-amz-replication-status
             &#39;x-amz-replication-status&#39;: &#39;COMPLETED&#39;,
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;example-of-how-to-use-these-headers-in-an-application-workflow&quot;&gt;Example of How to Use these Headers in an Application Workflow &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#example-of-how-to-use-these-headers-in-an-application-workflow&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This straightforward polling mechanism—via &lt;code&gt;HEAD&lt;/code&gt; or info requests—can be interpolated into application workflows to confirm full replication before taking further actions. Let’s check out a basic example.&lt;/p&gt;&lt;p&gt;Imagine a Content Delivery Network (CDN) scenario where you must replicate files globally to ensure low-latency access for end users across multiple geographic regions. An application in one region uploads media assets (images, videos, or static website content) that must be replicated to other RGW zones before we can make them available to the end users for consumption.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/images/sync1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gist.github.com/likid0/9b8c4db267a9b5065a711c302d1616bb&quot;&gt;Here&lt;/a&gt; is a code snippet with an example of using the Python library &lt;code&gt;boto3&lt;/code&gt; to upload media content to a site, then poll the replication status of our newly uploaded media content by querying the replication status header. Once the object has been replicated we print out relevant information including source and destination RGW zones and replication latency.&lt;/p&gt;&lt;p&gt;Application Example output:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/images/sync2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The new replication headers in Ceph Squid Object Storage mark a significant step forward in giving developers, DevOps teams, and storage administrators more granular control over and visibility into multisite replication. By querying the &lt;code&gt;x-amz-replication-status&lt;/code&gt;, &lt;code&gt;x-rgw-replicated-from&lt;/code&gt;, and &lt;code&gt;x-rgw-replicated-at&lt;/code&gt; headers, applications can confirm that objects have fully synchronized before proceeding with downstream workflows. This simple yet powerful capability can streamline CDN distribution, data analytics pipelines, and other use cases that demand multisite consistency.&lt;/p&gt;&lt;p&gt;Note that some features described here may not be available before the Squid 19.2.2 release.&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community with our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/rgw-multisite-sync-status/</guid><pubDate>Fri, 07 Feb 2025 00:00:00 GMT</pubDate><author>Daniel Parkes, Anthony D&#39;Atri (IBM)</author></item><item><title>Seamless Data Movement Across Clusters using RBD Live Migration in Squid</title><description>&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;seamless-data-movement-across-clusters-using-rbd-live-migration-in-squid&quot;&gt;Seamless Data Movement Across Clusters using RBD Live Migration in Squid &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#seamless-data-movement-across-clusters-using-rbd-live-migration-in-squid&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;introduction-to-block-storage&quot;&gt;Introduction to Block Storage &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#introduction-to-block-storage&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A block is a small, fixed-sized piece of data, like a 512-byte chunk. Imagine breaking a book into many pages: each page is a &quot;block&quot;. When you put all the pages together, you get a complete book, just like combining many smaller data blocks creates a larger storage unit.&lt;/p&gt;&lt;h4 id=&quot;where-do-we-use-blocks%3F&quot;&gt;Where Do We Use Blocks? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#where-do-we-use-blocks%3F&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Block-based storage is commonly used in computing, and in devices including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Hard drives or SSDs (your computer&#39;s storage)&lt;/li&gt;&lt;li&gt;CDs/DVDs (discs you use for music or movies)&lt;/li&gt;&lt;li&gt;Floppy disks (older storage devices)&lt;/li&gt;&lt;li&gt;Tape drives (used in big data backup systems)&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;what-are-ceph-rados-block-devices-(rbd)%3F&quot;&gt;What Are Ceph RADOS Block Devices (RBD)? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#what-are-ceph-rados-block-devices-(rbd)%3F&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Ceph RADOS Block Devices are a type of storage served by a Ceph cluster that works like a virtual physical storage drive. Instead of storing data on a single device, it spreads the data across multiple storage nodes and devices (called OSDs) in a Ceph cluster. This makes it efficient, scalable, and reliable.&lt;/p&gt;&lt;h4 id=&quot;why-are-ceph-block-devices-special%3F&quot;&gt;Why Are Ceph Block Devices Special? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#why-are-ceph-block-devices-special%3F&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Ceph Block Devices have some amazing features:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Snapshots – Like taking a photo of your data at a moment in time, so you can restore or roll back to it later.&lt;/li&gt;&lt;li&gt;Replication – Your data is copied multiple times to prevent loss.&lt;/li&gt;&lt;li&gt;Data consistency – Ensures your data is accurate and safe.&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;how-do-ceph-block-devices-work%3F&quot;&gt;How Do Ceph Block Devices Work? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#how-do-ceph-block-devices-work%3F&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Clients use the &lt;code&gt;librbd&lt;/code&gt; library to talk to the components of a Ceph cluster and manage data efficiently.&lt;/p&gt;&lt;h4 id=&quot;who-uses-ceph-block-devices%3F&quot;&gt;Who Uses Ceph Block Devices? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#who-uses-ceph-block-devices%3F&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Cloud computing platforms like OpenStack, Proxmox, AWS EBS, and IBM Cloud rely on them.&lt;/li&gt;&lt;li&gt;Virtual machines (KVM/QEMU) use them for fast and scalable storage.&lt;/li&gt;&lt;li&gt;They can even work alongside Ceph’s Object Storage for a flexible storage solution.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In short, Ceph Block Devices provide fast, scalable, and reliable storage for modern computing needs, ensuring that data is always available, accurate, and safe.&lt;/p&gt;&lt;h2 id=&quot;ceph-rbd-live-migration-of-images&quot;&gt;Ceph RBD Live Migration of Images &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#ceph-rbd-live-migration-of-images&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;As a storage administrator, you have the power to seamlessly move (&lt;em&gt;live-migrate&lt;/em&gt;) RBD images within your Ceph cluster or to a different Ceph cluster. Think of this like moving a file from one folder to another on your computer, but in this case, it happens within or between large, distributed storage clusters.&lt;/p&gt;&lt;p&gt;Note that RBD images are also known as &lt;em&gt;volumes&lt;/em&gt;, a term that is less likely to be confused with graphics files containing the likeness of Taylor Swift or cats.&lt;/p&gt;&lt;h3 id=&quot;key-features&quot;&gt;Key Features &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#key-features&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Migration between pools or RBD namespaces within a Ceph cluster&lt;/li&gt;&lt;li&gt;Migration between pools and RBD namespaces across Ceph clusters&lt;/li&gt;&lt;li&gt;Support for multiple image formats (Native, QCOW, RAW)&lt;/li&gt;&lt;li&gt;Live migration of encrypted images&lt;/li&gt;&lt;li&gt;Integration with external data sources including HTTP, S3, and NBD&lt;/li&gt;&lt;li&gt;Preservation of snapshot history and sparseness during migration&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Note: Linux KRBD kernel clients currently do not support live migration.&lt;/p&gt;&lt;h3 id=&quot;what-can-you-migrate%3F&quot;&gt;What Can You Migrate? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#what-can-you-migrate%3F&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Between different storage pools (e.g., moving from a high-performance replicated TLC SSD pool to a cost-effective erasure-coded QLC or HDD pool).&lt;/li&gt;&lt;li&gt;Within the same pool (e.g., reorganizing data for better management or to effect compression).&lt;/li&gt;&lt;li&gt;Across different formats or layouts (e.g., upgrading from an older storage format to a newer one).&lt;/li&gt;&lt;li&gt;From external data sources (e.g., migrating from a non-Ceph storage system into Ceph).&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;live-migration%3A-import-only-mode&quot;&gt;Live Migration: Import-Only Mode &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#live-migration%3A-import-only-mode&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Want to migrate data from an external source or storage provider? No problem! You can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Import data from a backup file&lt;/li&gt;&lt;li&gt;Pull data from a web URL (HTTP/HTTPS file)&lt;/li&gt;&lt;li&gt;Move data from an S3 storage bucket&lt;/li&gt;&lt;li&gt;Connect to an NBD (Network Block Device) export&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;how-does-live-migration-work%3F&quot;&gt;How Does Live Migration Work? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#how-does-live-migration-work%3F&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When you start a live migration, here’s what happens behind the scenes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Deep Copying – The system duplicates the entire image while keeping all historical snapshots.&lt;/li&gt;&lt;li&gt;Sparse Data Optimization – Only the actual used data is copied, saving storage space and speeding up the process.&lt;/li&gt;&lt;li&gt;Seamless Transition – The migration happens while the image remains usable, minimizing downtime.&lt;/li&gt;&lt;li&gt;Source Becomes Read-Only – The original image is locked so no new changes are made.&lt;/li&gt;&lt;li&gt;Automatic I/O Redirection – All applications and users automatically start using the new image without interruptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;why-is-this-important%3F&quot;&gt;Why Is This Important? &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#why-is-this-important%3F&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Keeps data flexible – Move storage based on performance, cost, or organizational needs.&lt;/li&gt;&lt;li&gt;Ensures data integrity – Snapshot history and structure remain intact.&lt;/li&gt;&lt;li&gt;Works in real-time – Migration happens without disrupting workloads.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;step-by-step-guide-to-live-migrating-ceph-rbd-images&quot;&gt;Step-by-Step Guide to Live Migrating Ceph RBD Images &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#step-by-step-guide-to-live-migrating-ceph-rbd-images&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Live migration of RBD images in Ceph allows you to move storage seamlessly between pools, RBD namespaces, and clusters in different formats with minimal downtime. Let&#39;s break it down into three simple steps, along with the necessary commands to execute them.&lt;/p&gt;&lt;h4 id=&quot;step-1%3A-prepare-for-migration&quot;&gt;Step 1: Prepare for Migration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#step-1%3A-prepare-for-migration&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Before starting the migration, a new target image is created and linked to the source image.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;If the import-only mode is not enabled, the source image will be marked read-only to prevent modifications.&lt;/li&gt;&lt;li&gt;Any attempts to read uninitialized parts of the new target image will redirect the read operation to the source image.&lt;/li&gt;&lt;li&gt;If data is written to an uninitialized part of the target image, Ceph automatically deep-copies the corresponding blocks from the source.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Syntax:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration prepare SOURCE_POOL_NAME/SOURCE_IMAGE_NAME TARGET_POOL_NAME/TARGET_IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration prepare source_pool1/source_image1 target_pool1/target_image1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Initiate the import-only live migration process by running the &lt;code&gt;rbd migration prepare&lt;/code&gt; command with the &lt;code&gt;--import-only&lt;/code&gt; flag and either &lt;code&gt;--source-spec&lt;/code&gt; or &lt;code&gt;--source-spec-path&lt;/code&gt; option, passing a JSON file that describes how to access the source image data.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Create a JSON file:&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code&gt;[ceph: root@rbd-client /]# cat testspec.json
 {
   &quot;type&quot;: &quot;raw&quot;,
    &quot;stream&quot;: {
        &quot;type&quot;: &quot;s3&quot;,
        &quot;url&quot;: &quot;https:hots_ip:80/testbucket1/image.raw&quot;,
        &quot;access_key&quot;: &quot;Access key&quot;,
        &quot;secret_key&quot;: &quot;Secret Key&quot;}
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Prepare the import-only live migration process:&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Syntax:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration prepare --import-only --source-spec-path &quot;JSON_FILE&quot; TARGET_POOL_NAME/TARGET_IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[ceph: root@rbd-client /]# rbd migration prepare --import-only --source-spec-path &quot;testspec.json&quot; target_pool/traget_image
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Check the status of the migration with the &lt;code&gt;rbd status&lt;/code&gt; command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[ceph: root@rbd-client /]# rbd status target_pool/target_image
Watchers: none
Migration:
source: {&quot;stream&quot;:{&quot;access_key&quot;:&quot;RLJOCP6345BGB38YQXI5&quot;,&quot;secret_key&quot;:&quot;oahWRB2ote2rnLy4dojYjDrsvaBADriDDgtSfk6o&quot;,&quot;type&quot;:&quot;s3&quot;,&quot;url&quot;:&quot;http://10.74.253.18:80/testbucket1/image.raw&quot;},&quot;type&quot;:&quot;raw&quot;}
destination: targetpool1/sourceimage1 (b13865345e66)
state: prepared
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;step-2%3A-execute-migration&quot;&gt;Step 2: Execute Migration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#step-2%3A-execute-migration&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;After preparation is complete, Ceph starts deep copying all existing data from the source image to the target image.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The migration runs in the background; applications can use the new target image immediately.&lt;/li&gt;&lt;li&gt;Any new write operations are stored only on the target image, ensuring a seamless transition.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Syntax:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration execute TARGET_POOL_NAME/TARGET_IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration execute target_pool1/target_image1
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;step-3%3A-finalizing-the-migration&quot;&gt;Step 3: Finalizing the Migration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#step-3%3A-finalizing-the-migration&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;After the data has been fully transferred, commit or abort the migration.&lt;/p&gt;&lt;h4 id=&quot;option-1%3A-commit-the-migration&quot;&gt;Option 1: Commit the Migration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#option-1%3A-commit-the-migration&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Committing the migration removes all links between the source and target images.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;If import-only mode was not used, the source image is automatically deleted.&lt;/li&gt;&lt;li&gt;The target image becomes fully independent.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Syntax:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration commit TARGET_POOL_NAME/TARGET_IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration commit target_pool1/target_image1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;option-2%3A-abort-the-migration&quot;&gt;Option 2: Abort the Migration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#option-2%3A-abort-the-migration&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Migrations can be cancelled. Cancelling a migration will cause the following to happen:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Remove any cross-links between the images.&lt;/li&gt;&lt;li&gt;Delete the target image, restoring the source image to its previous state.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Syntax:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration abort TARGET_POOL_NAME/TARGET_IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;rbd migration abort targetpool1/targetimage1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following example shows how to migrate data from one Ceph cluster to another Ceph cluster, here named &lt;em&gt;cluster1&lt;/em&gt; and &lt;em&gt;cluster2:&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[ceph: root@rbd-client /]# cat /tmp/native_spec
{
  &quot;cluster_name&quot;: &quot;c1&quot;,
  &quot;type&quot;: &quot;native&quot;,
  &quot;pool_name&quot;: &quot;pool1&quot;,
  &quot;image_name&quot;: &quot;image1&quot;,
  &quot;snap_name&quot;: &quot;snap1&quot;
}
[ceph: root@rbd-client /]# rbd migration prepare --import-only --source-spec-path /tmp/native_spec c2pool1/c2image1 --cluster c2
[ceph: root@rbd-client /]# rbd migration execute c2pool1/c2image1 --cluster c2
Image migration: 100% complete...done.
[ceph: root@rbd-client /]# rbd migration commit c2pool1/c2image1 --cluster c2
Commit image migration: 100% complete...done.
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;supported-image-formats&quot;&gt;Supported Image Formats &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#supported-image-formats&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Live migration supports three primary formats:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;em&gt;Native format&lt;/em&gt; uses Ceph&#39;s internal format for efficient migration.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The native format does not include the stream since it utilizes native Ceph operations. For example, to import from the image &lt;code&gt;rbd/ns1/image1@snap1&lt;/code&gt;, the source specification can be constructed as below:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;{
&quot;type&quot;: &quot;native&quot;,
&quot;pool_name&quot;: &quot;rbd&quot;,
&quot;pool_namespace&quot;: &quot;ns1&quot;,
&quot;image_name&quot;: &quot;image1&quot;,
&quot;snap_name&quot;: &quot;snap1&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;&lt;em&gt;QCOW format&lt;/em&gt; is compatible with QEMU Copy-On-Write volume images.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The QCOW format describes a QEMU copy-on-write (QCOW) block device. QCOW v1 and v2 formats are currently supported, with the exception of certain features including compression, encryption, backing files, and external data files. Use the QCOW format with any supported stream source:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;{
    &quot;type&quot;: &quot;qcow&quot;,
    &quot;stream&quot;: {
      &quot;type&quot;: &quot;file&quot;,
  &quot;file_path&quot;: &quot;/mnt/image.qcow&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;&lt;em&gt;Raw format&lt;/em&gt; is used for thick-provisioned block device exports.&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;&lt;code&gt;{
    &quot;type&quot;: &quot;raw&quot;,
    &quot;stream&quot;: {
      &quot;type&quot;: &quot;file&quot;,
      &quot;file_path&quot;: &quot;/mnt/image-head.raw&quot;
    },
    &quot;snapshots&quot;: [
        {
            &quot;type&quot;: &quot;raw&quot;,
            &quot;name&quot;: &quot;snap1&quot;,
            &quot;stream&quot;: {
              &quot;type&quot;: &quot;file&quot;,
       &quot;file_path&quot;: &quot;/mnt/image-snap1.raw&quot;
            }
        },
    (optional oldest to newest ordering of snapshots)
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;supported-streams&quot;&gt;Supported Streams &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#supported-streams&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Multiple stream types are available for importing from various data sources:&lt;/p&gt;&lt;h4 id=&quot;file-stream-(local-files)&quot;&gt;File Stream (Local files) &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#file-stream-(local-files)&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Use a &lt;code&gt;file&lt;/code&gt; stream to import from a locally accessible POSIX file source.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;{
    &amp;lt;format-specific parameters&amp;gt;
    &quot;stream&quot;: {
        &quot;type&quot;: &quot;file&quot;,
        &quot;file_path&quot;: &quot;FILE_PATH&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;http-stream-(remote-web-server)&quot;&gt;HTTP Stream (remote web server) &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#http-stream-(remote-web-server)&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Use an &lt;code&gt;HTTP&lt;/code&gt; stream to import from an HTTP or HTTPS web server.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;{
    &amp;lt;format-specific parameters&amp;gt;
    &quot;stream&quot;: {
        &quot;type&quot;: &quot;http&quot;,
        &quot;url&quot;: &quot;URL_PATH&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;s3-stream-(aws-or-compatible-object-storage)&quot;&gt;S3 Stream (AWS or compatible object storage) &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#s3-stream-(aws-or-compatible-object-storage)&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Use an &lt;code&gt;s3&lt;/code&gt; stream to import from an S3 bucket.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;{
    &amp;lt;format-specific parameters&amp;gt;
    &quot;stream&quot;: {
        &quot;type&quot;: &quot;s3&quot;,
        &quot;url&quot;: &quot;URL_PATH&quot;,
        &quot;access_key&quot;: &quot;ACCESS_KEY&quot;,
        &quot;secret_key&quot;: &quot;SECRET_KEY&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;nbd-stream-(network-block-device-exports)&quot;&gt;NBD Stream (Network Block Device exports) &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#nbd-stream-(network-block-device-exports)&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Use an &lt;code&gt;NBD&lt;/code&gt; stream to import from a remote NBD export.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;{
    &amp;lt;format-specific parameters&amp;gt;
    &quot;stream&quot;: {
        &quot;type&quot;: &quot;nbd&quot;,
        &quot;uri&quot;: &quot;&amp;lt;nbd-uri&amp;gt;&quot;,
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;nbd-uri&lt;/code&gt; parameter must follow the &lt;a href=&quot;https://github.com/NetworkBlockDevice/nbd/blob/master/doc/uri.md&quot;&gt;NBD URI specification&lt;/a&gt;. The default NBD port is &lt;code&gt;tcp/10809&lt;/code&gt;.&lt;/p&gt;&lt;h3 id=&quot;use-cases-of-rbd-live-migration&quot;&gt;Use Cases of RBD Live Migration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#use-cases-of-rbd-live-migration&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Disaster Recovery and Data Migration&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: An organization runs mission-critical applications on a primary Ceph cluster in one data center. Due to an impending maintenance window, potential hardware failure, or a disaster event, they need to migrate RBD images to a secondary Ceph cluster in a different location.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Live migration ensures that applications using RBD volumes can continue functioning with minimal downtime and no data loss during the transition to the secondary cluster.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Bursting and Workload Distribution&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: An organization operates a Ceph cluster that accommodates routine workloads but occasionally requires extra capacity during peak usage. By migrating RBD images to an external Ceph cluster (possibly deployed in a cloud) they can temporarily scale operations then scale then back.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Dynamic workload balancing helps admins leverage external resources only when needed, reducing operational costs and improving scalability.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Data Center Migration&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: An organization is migrating infrastructure from one physical data center to another due to an upgrade, consolidation, or relocation. All RBD images from the source Ceph cluster need to be moved to a destination Ceph cluster in the new location.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Live migration minimizes disruptions to services during data center migrations, maintaining application availability.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Compliance and Data Sovereignty&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: A organization must comply with local data residency regulations that require sensitive data to be stored within specific geographic boundaries. Data held in RBD images thus must be migrated from a general-purpose Ceph cluster to one dedicated to and within the regulated region.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: The live migration feature enables seamless relocation of RBD data without halting ongoing operations, ensuring compliance with regulations.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-Cluster Load and Capacity Balancing&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: An organization runs multiple Ceph clusters to handle high traffic workloads. To prevent overloading any single cluster, they redistribute RBD images among clusters as workload patterns shift.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Live migration allows for efficient rebalancing of workloads across Ceph clusters, optimizing resource utilization and performance.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Dev/Test to Production Migration&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: Developers run test environments on a dedicated Ceph cluster. After testing is complete, production-ready RBD images can be migrated to the production Ceph cluster without data duplication or downtime.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Simplifies the process of promoting test data to production while maintaining data integrity.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hardware Lifecycle Management&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: A Ceph cluster is running on older hardware that is nearing the end of its lifecycle. The admin plans to migrate RBD images to a new Ceph cluster with upgraded hardware for better performance and reliability.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Live migration facilitates a smooth transition from legacy to modern infrastructure without impacting application uptime.&lt;br&gt;&lt;em&gt;Note&lt;/em&gt;: In many situations one can incrementally replace 100% of Ceph cluster hardware in situ without downtime or migration, but in others it may be desirable to stand up a new, independent cluster and migrate data between the two.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Global Data Replication&lt;br&gt;&lt;em&gt;Scenario&lt;/em&gt;: An enterprise has Ceph clusters distributed across locations to improve latency for regional end users. RBD images can be migrated from one region to another based on data center additions or closures, changes in user traffic patterns, or business priorities.&lt;br&gt;&lt;em&gt;Benefit&lt;/em&gt;: Enhances user experience by moving data closer to the point of consumption while maintaining data consistency.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rbd-live-migration/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ceph live migration of RBD images provides a seamless and efficient way to move storage data and workloads without disrupting operations. By leveraging native Ceph operations and external stream sources, administrators can ensure smooth and flexible data migration processes.&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community by through our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/rbd-live-migration/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/rbd-live-migration/</guid><pubDate>Fri, 07 Feb 2025 00:00:00 GMT</pubDate><author>Sunil Angadi, Anthony D&#39;Atri, Ila Dryomov (IBM)</author></item><item><title>v19.2.1 Squid released</title><description>&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;p&gt;This is the first backport release in the Squid series. We recommend all users update to this release.&lt;/p&gt;&lt;h2 id=&quot;notable-changes&quot;&gt;Notable Changes &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/v19-2-1-squid-released/#notable-changes&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;CephFS: The command &lt;code&gt;fs subvolume create&lt;/code&gt; now allows tagging subvolumes by supplying the option &lt;code&gt;--earmark&lt;/code&gt; with a unique identifier needed for NFS or SMB services. The earmark string for a subvolume is empty by default. To remove an already present earmark, an empty string can be assigned to it. Additionally, the commands &lt;code&gt;ceph fs subvolume earmark set&lt;/code&gt;, &lt;code&gt;ceph fs subvolume earmark get&lt;/code&gt;, and &lt;code&gt;ceph fs subvolume earmark rm&lt;/code&gt; have been added to set, get and remove earmark from a given subvolume.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CephFS: Expanded removexattr support for CephFS virtual extended attributes. Previously one had to use setxattr to restore the default in order to &quot;remove&quot;. You may now properly use removexattr to remove. You can also now remove layout on the root inode, which then will restore the layout to the default.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RADOS: A performance bottleneck in the balancer mgr module has been fixed.&lt;/p&gt;&lt;p&gt;Related Tracker: &lt;a href=&quot;https://tracker.ceph.com/issues/68657&quot;&gt;https://tracker.ceph.com/issues/68657&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RADOS: Based on tests performed at scale on an HDD-based Ceph cluster, it was found that scheduling with mClock was not optimal with multiple OSD shards. For example, in the test cluster with multiple OSD node failures, the client throughput was found to be inconsistent across test runs coupled with multiple reported slow requests. However, the same test with a single OSD shard and with multiple worker threads yielded significantly better results in terms of consistency of client and recovery throughput across multiple test runs. Therefore, as an interim measure until the issue with multiple OSD shards (or multiple mClock queues per OSD) is investigated and fixed, the following change to the default HDD OSD shard configuration is made:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;osd_op_num_shards_hdd = 1&lt;/code&gt; (was 5)&lt;/li&gt;&lt;li&gt;&lt;code&gt;osd_op_num_threads_per_shard_hdd = 5&lt;/code&gt; (was 1)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For more details, see &lt;a href=&quot;https://tracker.ceph.com/issues/66289&quot;&gt;https://tracker.ceph.com/issues/66289&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;mgr/REST: The REST manager module will trim requests based on the &#39;max_requests&#39; option. Without this feature, and in the absence of manual deletion of old requests, the accumulation of requests in the array can lead to Out Of Memory (OOM) issues, resulting in the Manager crashing.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;changelog&quot;&gt;Changelog &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/v19-2-1-squid-released/#changelog&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;doc/rgw/notification: add missing admin commands (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60609&quot;&gt;pr#60609&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: [RGW] Fix the handling of HEAD requests that do not comply with RFC standards (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59123&quot;&gt;pr#59123&lt;/a&gt;, liubingrun)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: a series of optimizations for kerneldevice discard (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59065&quot;&gt;pr#59065&lt;/a&gt;, Adam Kupczyk, Joshua Baergen, Gabriel BenHanokh, Matt Vandermeulen)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: Add Containerfile and build&lt;span&gt;&lt;/span&gt;.sh to build it (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60229&quot;&gt;pr#60229&lt;/a&gt;, Dan Mick)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: AsyncMessenger: Don&#39;t decrease l_msgr_active_connections if it is negative (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60447&quot;&gt;pr#60447&lt;/a&gt;, Mohit Agrawal)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: blk/aio: fix long batch (64+K entries) submission (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58676&quot;&gt;pr#58676&lt;/a&gt;, Yingxin Cheng, Igor Fedotov, Adam Kupczyk, Robin Geuze)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: blk/KernelDevice: using join() to wait thread end is more safe (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60616&quot;&gt;pr#60616&lt;/a&gt;, Yite Gu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: bluestore/bluestore_types: avoid heap-buffer-overflow in another way to keep code uniformity (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58816&quot;&gt;pr#58816&lt;/a&gt;, Rongqi Sun)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-bluestore-tool: Fixes for multilple bdev label (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59967&quot;&gt;pr#59967&lt;/a&gt;, Adam Kupczyk, Igor Fedotov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: add call to &lt;code&gt;ceph-bluestore-tool zap-device&lt;/code&gt; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59968&quot;&gt;pr#59968&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: add new class UdevData (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60091&quot;&gt;pr#60091&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: add TPM2 token enrollment support for encrypted OSDs (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59196&quot;&gt;pr#59196&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: do not convert LVs&#39;s symlink to real path (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58954&quot;&gt;pr#58954&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: do source devices zapping if they&#39;re detached (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58964&quot;&gt;pr#58964&lt;/a&gt;, Guillaume Abrioux, Igor Fedotov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: drop unnecessary call to &lt;code&gt;get\_single\_lv()&lt;/code&gt; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60353&quot;&gt;pr#60353&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: fix dmcrypt activation regression (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60734&quot;&gt;pr#60734&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: fix generic activation with raw osds (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59598&quot;&gt;pr#59598&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: fix OSD lvm/tpm2 activation (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59953&quot;&gt;pr#59953&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: pass self&lt;span&gt;&lt;/span&gt;.osd_id to create_id() call (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59622&quot;&gt;pr#59622&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph-volume: switch over to new disk sorting behavior (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59623&quot;&gt;pr#59623&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: ceph&lt;span&gt;&lt;/span&gt;.spec&lt;span&gt;&lt;/span&gt;.in: we need jsonnet for all distroes for make check (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60075&quot;&gt;pr#60075&lt;/a&gt;, Kyr Shatskyy)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephadm/services/ingress: fixed keepalived config bug (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58381&quot;&gt;pr#58381&lt;/a&gt;, Bernard Landon)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: cephadm: bootstrap should not have &quot;This is a development version of cephadm&quot; message (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60880&quot;&gt;pr#60880&lt;/a&gt;, Shweta Bhosale)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephadm: emit warning if daemon&#39;s image is not to be used (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59929&quot;&gt;pr#59929&lt;/a&gt;, Matthew Vernon)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephadm: fix apparmor profiles with spaces in the names (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58542&quot;&gt;pr#58542&lt;/a&gt;, John Mulligan)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephadm: pull container images from quay&lt;span&gt;&lt;/span&gt;.io (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60354&quot;&gt;pr#60354&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephadm: Support Docker Live Restore (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59933&quot;&gt;pr#59933&lt;/a&gt;, Michal Nasiadka)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephadm: update default image and latest stable release (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59827&quot;&gt;pr#59827&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephfs,mon: fix bugs related to updating MDS caps (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59672&quot;&gt;pr#59672&lt;/a&gt;, Rishabh Dave)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephfs-shell: excute cmd &#39;rmdir_helper&#39; reported error (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58810&quot;&gt;pr#58810&lt;/a&gt;, teng jie)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephfs: Fixed a bug in the readdir_cache_cb function that may have us… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58804&quot;&gt;pr#58804&lt;/a&gt;, Tod Chen)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephfs_mirror: provide metrics for last successful snapshot sync (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59070&quot;&gt;pr#59070&lt;/a&gt;, Jos Collin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephfs_mirror: update peer status for invalid metadata in remote snapshot (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59406&quot;&gt;pr#59406&lt;/a&gt;, Jos Collin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cephfs_mirror: use snapdiff api for incremental syncing (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58984&quot;&gt;pr#58984&lt;/a&gt;, Jos Collin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: client: calls to _ll_fh_exists() should hold client_lock (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59487&quot;&gt;pr#59487&lt;/a&gt;, Venky Shankar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: client: check mds down status before getting mds_gid_t from mdsmap (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58587&quot;&gt;pr#58587&lt;/a&gt;, Yite Gu, Dhairya Parmar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cls/user: reset stats only returns marker when truncated (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60164&quot;&gt;pr#60164&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: cmake: use ExternalProjects to build isa-l and isa-l_crypto libraries (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60107&quot;&gt;pr#60107&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: common,osd: Use last valid OSD IOPS value if measured IOPS is unrealistic (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60660&quot;&gt;pr#60660&lt;/a&gt;, Sridhar Seshasayee)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: common/dout: fix FTBFS on GCC 14 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59055&quot;&gt;pr#59055&lt;/a&gt;, Radoslaw Zarzynski)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: common/options: Change HDD OSD shard configuration defaults for mClock (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59973&quot;&gt;pr#59973&lt;/a&gt;, Sridhar Seshasayee)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: corpus: update submodule with mark cls_rgw_reshard_entry forward_inco… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58923&quot;&gt;pr#58923&lt;/a&gt;, NitzanMordhai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore/cached_extent: add the &quot;refresh&quot; ability to lba mappings (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58957&quot;&gt;pr#58957&lt;/a&gt;, Xuehan Xu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore/lba_manager: do batch mapping allocs when remapping multiple mappings (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58820&quot;&gt;pr#58820&lt;/a&gt;, Xuehan Xu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore/onode: add hobject_t into Onode (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58830&quot;&gt;pr#58830&lt;/a&gt;, Xuehan Xu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore/transaction_manager: consider inconsistency between backrefs and lbas acceptable when cleaning segments (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58837&quot;&gt;pr#58837&lt;/a&gt;, Xuehan Xu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore: add checksum offload to RBM (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59298&quot;&gt;pr#59298&lt;/a&gt;, Myoungwon Oh)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore: add writer level stats to RBM (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58828&quot;&gt;pr#58828&lt;/a&gt;, Myoungwon Oh)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/os/seastore: track transactions/conflicts/outstanding periodically (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58835&quot;&gt;pr#58835&lt;/a&gt;, Yingxin Cheng)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson/osd/pg_recovery: push the iteration forward after finding unfound objects when starting primary recoveries (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58958&quot;&gt;pr#58958&lt;/a&gt;, Xuehan Xu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson: access coll_map under alien tp with a lock (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58841&quot;&gt;pr#58841&lt;/a&gt;, Samuel Just)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson: audit and correct epoch captured by IOInterruptCondition (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58839&quot;&gt;pr#58839&lt;/a&gt;, Samuel Just)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: crimson: simplify obc loading by locking excl for load and demoting to needed lock (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58905&quot;&gt;pr#58905&lt;/a&gt;, Matan Breizman, Samuel Just)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: debian pkg: record python3-packaging dependency for ceph-volume (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59202&quot;&gt;pr#59202&lt;/a&gt;, Kefu Chai, Thomas Lamprecht)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc,mailmap: update my email / association to ibm (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60338&quot;&gt;pr#60338&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/ceph-volume: add spillover fix procedure (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59540&quot;&gt;pr#59540&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: add malformed-JSON removal instructions (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59663&quot;&gt;pr#59663&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: Clarify &quot;Deploying a new Cluster&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60809&quot;&gt;pr#60809&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: clean &quot;Adv&lt;span&gt;&lt;/span&gt;. OSD Service Specs&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60679&quot;&gt;pr#60679&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: correct &quot;ceph orch apply&quot; command (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60432&quot;&gt;pr#60432&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: how to get exact size_spec from device (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59430&quot;&gt;pr#59430&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: link to &quot;host pattern&quot; matching sect (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60644&quot;&gt;pr#60644&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephadm: Update operations&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60637&quot;&gt;pr#60637&lt;/a&gt;, rhkelson)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: add cache pressure information (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59148&quot;&gt;pr#59148&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: add doc for disabling mgr/volumes plugin (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60496&quot;&gt;pr#60496&lt;/a&gt;, Rishabh Dave)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: edit &quot;Disabling Volumes Plugin&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60467&quot;&gt;pr#60467&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: edit &quot;Layout Fields&quot; text (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59021&quot;&gt;pr#59021&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: edit 3rd 3rd of mount-using-kernel-driver (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61080&quot;&gt;pr#61080&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: improve &quot;layout fields&quot; text (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59250&quot;&gt;pr#59250&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: improve cache-configuration&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59214&quot;&gt;pr#59214&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: rearrange subvolume group information (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60435&quot;&gt;pr#60435&lt;/a&gt;, Indira Sawant)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: s/mountpoint/mount point/ (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59294&quot;&gt;pr#59294&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: s/mountpoint/mount point/ (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59289&quot;&gt;pr#59289&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/cephfs: use &#39;p&#39; flag to set layouts or quotas (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60482&quot;&gt;pr#60482&lt;/a&gt;, TruongSinh Tran-Nguyen)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev/peering: Change acting set num (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59062&quot;&gt;pr#59062&lt;/a&gt;, qn2060)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev/release-checklist: check telemetry validation (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59813&quot;&gt;pr#59813&lt;/a&gt;, Yaarit Hatuka)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev/release-checklists&lt;span&gt;&lt;/span&gt;.rst: enable rtd for squid (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59812&quot;&gt;pr#59812&lt;/a&gt;, Neha Ojha)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev/release-process&lt;span&gt;&lt;/span&gt;.rst: New container build/release process (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60971&quot;&gt;pr#60971&lt;/a&gt;, Dan Mick)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev: add &quot;activate latest release&quot; RTD step (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59654&quot;&gt;pr#59654&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev: instruct devs to backport (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61063&quot;&gt;pr#61063&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/dev: remove &quot;Stable Releases and Backports&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60272&quot;&gt;pr#60272&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/glossary&lt;span&gt;&lt;/span&gt;.rst: add &quot;Dashboard Plugin&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60896&quot;&gt;pr#60896&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/glossary: add &quot;ceph-ansible&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59007&quot;&gt;pr#59007&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/glossary: add &quot;flapping OSD&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60864&quot;&gt;pr#60864&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/glossary: add &quot;object storage&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59424&quot;&gt;pr#59424&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/glossary: add &quot;PLP&quot; to glossary (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60503&quot;&gt;pr#60503&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/governance: add exec council responsibilites (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60139&quot;&gt;pr#60139&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/governance: add Zac Dover&#39;s updated email (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60134&quot;&gt;pr#60134&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/install: Keep the name field of the created user consistent with … (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59756&quot;&gt;pr#59756&lt;/a&gt;, hejindong)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/man: edit ceph-bluestore-tool&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59682&quot;&gt;pr#59682&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/mds: improve wording (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59585&quot;&gt;pr#59585&lt;/a&gt;, Piotr Parczewski)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/mgr/dashboard: fix TLS typo (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59031&quot;&gt;pr#59031&lt;/a&gt;, Mindy Preston)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados/operations: Improve health-checks&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59582&quot;&gt;pr#59582&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados/troubleshooting: Improve log-and-debug&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60824&quot;&gt;pr#60824&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: add &quot;pgs not deep scrubbed in time&quot; info (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59733&quot;&gt;pr#59733&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: add blaum_roth coding guidance (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60537&quot;&gt;pr#60537&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: add confval directives to health-checks (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59871&quot;&gt;pr#59871&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: add link to messenger v2 info in mon-lookup-dns&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59794&quot;&gt;pr#59794&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: add osd_deep_scrub_interval setting operation (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59802&quot;&gt;pr#59802&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: correct &quot;full ratio&quot; note (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60737&quot;&gt;pr#60737&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: document unfound object cache-tiering scenario (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59380&quot;&gt;pr#59380&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: edit &quot;Placement Groups Never Get Clean&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60046&quot;&gt;pr#60046&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: fix sentences in health-checks (2 of x) (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60931&quot;&gt;pr#60931&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: fix sentences in health-checks (3 of x) (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60949&quot;&gt;pr#60949&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: make sentences agree in health-checks&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60920&quot;&gt;pr#60920&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rados: standardize markup of &quot;clean&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60500&quot;&gt;pr#60500&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw/multisite: fix Configuring Secondary Zones -&amp;gt; Updating the Period (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60332&quot;&gt;pr#60332&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw/qat-accel: Update and Add QATlib information (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58874&quot;&gt;pr#58874&lt;/a&gt;, Feng, Hualong)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw: Improve archive-sync-module&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60852&quot;&gt;pr#60852&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw: Improve archive-sync-module&lt;span&gt;&lt;/span&gt;.rst more (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60867&quot;&gt;pr#60867&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw: Improve config-ref&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59578&quot;&gt;pr#59578&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw: improve qat-accel&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59179&quot;&gt;pr#59179&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw: s/Poliicy/Policy/ (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60707&quot;&gt;pr#60707&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/radosgw: update rgw_dns_name doc (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60885&quot;&gt;pr#60885&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rbd: add namespace information for mirror commands (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60269&quot;&gt;pr#60269&lt;/a&gt;, N Balachandran)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/README&lt;span&gt;&lt;/span&gt;.md - add ordered list (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59798&quot;&gt;pr#59798&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/README&lt;span&gt;&lt;/span&gt;.md: create selectable commands (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59834&quot;&gt;pr#59834&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/README&lt;span&gt;&lt;/span&gt;.md: edit &quot;Build Prerequisites&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59637&quot;&gt;pr#59637&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/README&lt;span&gt;&lt;/span&gt;.md: improve formatting (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59785&quot;&gt;pr#59785&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/README&lt;span&gt;&lt;/span&gt;.md: improve formatting (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59700&quot;&gt;pr#59700&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rgw/account: Handling notification topics when migrating an existing user into an account (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59491&quot;&gt;pr#59491&lt;/a&gt;, Oguzhan Ozmen)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rgw/d3n: pass cache dir volume to extra_container_args (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59767&quot;&gt;pr#59767&lt;/a&gt;, Mark Kogan)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rgw/notification: clarified the notification_v2 behavior upon upg… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60662&quot;&gt;pr#60662&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/rgw/notification: persistent notification queue full behavior (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59233&quot;&gt;pr#59233&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/start: add supported Squid distros (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60557&quot;&gt;pr#60557&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/start: add vstart install guide (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60461&quot;&gt;pr#60461&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/start: fix &quot;are are&quot; typo (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60708&quot;&gt;pr#60708&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/start: separate package chart from container chart (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60698&quot;&gt;pr#60698&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc/start: update os-recommendations&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60766&quot;&gt;pr#60766&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: Correct link to Prometheus docs (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59559&quot;&gt;pr#59559&lt;/a&gt;, Matthew Vernon)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: Document the Windows CI job (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60033&quot;&gt;pr#60033&lt;/a&gt;, Lucian Petrut)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: Document which options are disabled by mClock (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60671&quot;&gt;pr#60671&lt;/a&gt;, Niklas Hambüchen)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: documenting the feature that scrub clear the entries from damage… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59078&quot;&gt;pr#59078&lt;/a&gt;, Neeraj Pratap Singh)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: explain the consequence of enabling mirroring through monitor co… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60525&quot;&gt;pr#60525&lt;/a&gt;, Jos Collin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: fix email (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60233&quot;&gt;pr#60233&lt;/a&gt;, Ernesto Puerta)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: fix typo (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59991&quot;&gt;pr#59991&lt;/a&gt;, N Balachandran)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: Harmonize &#39;mountpoint&#39; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59291&quot;&gt;pr#59291&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: s/Whereas,/Although/ (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60593&quot;&gt;pr#60593&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: SubmittingPatches-backports - remove backports team (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60297&quot;&gt;pr#60297&lt;/a&gt;, Zac Dover)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: Update &quot;Getting Started&quot; to link to start not install (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59907&quot;&gt;pr#59907&lt;/a&gt;, Matthew Vernon)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: update Key Idea in cephfs-mirroring&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60343&quot;&gt;pr#60343&lt;/a&gt;, Jos Collin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: update nfs doc for Kerberos setup of ganesha in Ceph (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59939&quot;&gt;pr#59939&lt;/a&gt;, Avan Thakkar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc: update tests-integration-testing-teuthology-workflow&lt;span&gt;&lt;/span&gt;.rst (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59548&quot;&gt;pr#59548&lt;/a&gt;, Vallari Agrawal)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: doc:update e-mail addresses governance (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60084&quot;&gt;pr#60084&lt;/a&gt;, Tobias Fischer)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: docs/rados/operations/stretch-mode: warn device class is not supported (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59099&quot;&gt;pr#59099&lt;/a&gt;, Kamoltat Sirivadhna)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: global: Call getnam_r with a 64KiB buffer on the heap (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60127&quot;&gt;pr#60127&lt;/a&gt;, Adam Emerson)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: librados: use CEPH_OSD_FLAG_FULL_FORCE for IoCtxImpl::remove (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59284&quot;&gt;pr#59284&lt;/a&gt;, Chen Yuanrun)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: librbd/crypto/LoadRequest: clone format for migration source image (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60171&quot;&gt;pr#60171&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: librbd/crypto: fix issue when live-migrating from encrypted export (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59145&quot;&gt;pr#59145&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: librbd/migration/HttpClient: avoid reusing ssl_stream after shut down (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61095&quot;&gt;pr#61095&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: librbd/migration: prune snapshot extents in RawFormat::list_snaps() (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59661&quot;&gt;pr#59661&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: librbd: avoid data corruption on flatten when object map is inconsistent (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61168&quot;&gt;pr#61168&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: log: save/fetch thread name infra (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60279&quot;&gt;pr#60279&lt;/a&gt;, Milind Changire)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: Make mon addrs consistent with mon info (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60751&quot;&gt;pr#60751&lt;/a&gt;, shenjiatong)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mds/QuiesceDbManager: get requested state of members before iterating… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58912&quot;&gt;pr#58912&lt;/a&gt;, junxiang Mu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mds: CInode::item_caps used in two different lists (&lt;a href=&quot;https://github.com/ceph/ceph/pull/56887&quot;&gt;pr#56887&lt;/a&gt;, Dhairya Parmar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mds: encode quiesce payload on demand (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59517&quot;&gt;pr#59517&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mds: find a new head for the batch ops when the head is dead (&lt;a href=&quot;https://github.com/ceph/ceph/pull/57494&quot;&gt;pr#57494&lt;/a&gt;, Xiubo Li)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mds: fix session/client evict command (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58727&quot;&gt;pr#58727&lt;/a&gt;, Neeraj Pratap Singh)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mds: only authpin on wrlock when not a locallock (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59097&quot;&gt;pr#59097&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/balancer: optimize &#39;balancer status detail&#39; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60718&quot;&gt;pr#60718&lt;/a&gt;, Laura Flores)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm/services/ingress Fix HAProxy to listen on IPv4 and IPv6 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58515&quot;&gt;pr#58515&lt;/a&gt;, Bernard Landon)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: add &quot;original_weight&quot; parameter to OSD class (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59410&quot;&gt;pr#59410&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: add --no-exception-when-missing flag to cert-store cert/key get (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59935&quot;&gt;pr#59935&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: add command to expose systemd units of all daemons (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59931&quot;&gt;pr#59931&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: bump monitoring stacks version (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58711&quot;&gt;pr#58711&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: make ssh keepalive settings configurable (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59710&quot;&gt;pr#59710&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: redeploy when some dependency daemon is add/removed (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58383&quot;&gt;pr#58383&lt;/a&gt;, Redouane Kachach)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: Update multi-site configs before deploying daemons on rgw service create (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60321&quot;&gt;pr#60321&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/cephadm: use host address while updating rgw zone endpoints (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59948&quot;&gt;pr#59948&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/client: validate connection before sending (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58887&quot;&gt;pr#58887&lt;/a&gt;, NitzanMordhai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: add cephfs rename REST API (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60620&quot;&gt;pr#60620&lt;/a&gt;, Yite Gu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Add group field in nvmeof service form (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59446&quot;&gt;pr#59446&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: add gw_groups support to nvmeof api (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59751&quot;&gt;pr#59751&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: add gw_groups to all nvmeof endpoints (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60310&quot;&gt;pr#60310&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: add restful api for creating crush rule with type of &#39;erasure&#39; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59139&quot;&gt;pr#59139&lt;/a&gt;, sunlan)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Changes for Sign out text to Login out (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58988&quot;&gt;pr#58988&lt;/a&gt;, Prachi Goel)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: Cloning subvolume not listing _nogroup if no subvolume (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59951&quot;&gt;pr#59951&lt;/a&gt;, Dnyaneshwari talwekar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: custom image for kcli bootstrap script (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59879&quot;&gt;pr#59879&lt;/a&gt;, Pedro Gonzalez Gomez)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Dashboard not showing Object/Overview correctly (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59038&quot;&gt;pr#59038&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Fix adding listener and null issue for groups (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60078&quot;&gt;pr#60078&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix bucket get for s3 account owned bucket (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60466&quot;&gt;pr#60466&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix ceph-users api doc (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59140&quot;&gt;pr#59140&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix doc links in rgw-multisite (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60154&quot;&gt;pr#60154&lt;/a&gt;, Pedro Gonzalez Gomez)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix gateways section error:”404 - Not Found RGW Daemon not found: None” (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60231&quot;&gt;pr#60231&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix group name bugs in the nvmeof API (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60348&quot;&gt;pr#60348&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix handling NaN values in dashboard charts (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59961&quot;&gt;pr#59961&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix lifecycle issues (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60378&quot;&gt;pr#60378&lt;/a&gt;, Pedro Gonzalez Gomez)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Fix listener deletion (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60292&quot;&gt;pr#60292&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: fix setting compression type while editing rgw zone (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59970&quot;&gt;pr#59970&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: Forbid snapshot name &quot;&lt;span&gt;&lt;/span&gt;.&quot; and any containing &quot;/&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59995&quot;&gt;pr#59995&lt;/a&gt;, Dnyaneshwari Talwekar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: handle infinite values for pools (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61096&quot;&gt;pr#61096&lt;/a&gt;, Afreen)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: ignore exceptions raised when no cert/key found (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60311&quot;&gt;pr#60311&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Increase maximum namespace count to 1024 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59717&quot;&gt;pr#59717&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: introduce server side pagination for osds (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60294&quot;&gt;pr#60294&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: mgr/dashboard: Select no device by default in EC profile (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59811&quot;&gt;pr#59811&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: multisite sync policy improvements (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59965&quot;&gt;pr#59965&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: NFS Export form fixes (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59900&quot;&gt;pr#59900&lt;/a&gt;, Dnyaneshwari Talwekar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Nvme mTLS support and service name changes (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59819&quot;&gt;pr#59819&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: provide option to enable pool based mirroring mode while creating a pool (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58638&quot;&gt;pr#58638&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: remove cherrypy_backports&lt;span&gt;&lt;/span&gt;.py (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60632&quot;&gt;pr#60632&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: remove orch required decorator from host UI router (list) (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59851&quot;&gt;pr#59851&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Rephrase dedicated pool helper in rbd create form (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59721&quot;&gt;pr#59721&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: RGW multisite sync remove zones fix (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59825&quot;&gt;pr#59825&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: rm nvmeof conf based on its daemon name (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60604&quot;&gt;pr#60604&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: service form hosts selection only show up to 10 entries (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59760&quot;&gt;pr#59760&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: show non default realm sync status in rgw overview page (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60232&quot;&gt;pr#60232&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Show which daemons failed in CEPHADM_FAILED_DAEMON healthcheck (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59597&quot;&gt;pr#59597&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: sync policy&#39;s in Object &amp;gt;&amp;gt; Multi-site &amp;gt;&amp;gt; Sync-policy, does not show the zonegroup to which policy belongs to (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60346&quot;&gt;pr#60346&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: The subvolumes are missing from the dropdown menu on the &quot;Create NFS export&quot; page (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60356&quot;&gt;pr#60356&lt;/a&gt;, Dnyaneshwari Talwekar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: unable to edit pipe config for bucket level policy of bucket (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60293&quot;&gt;pr#60293&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Update nvmeof microcopies (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59718&quot;&gt;pr#59718&lt;/a&gt;, Afreen Misbah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: update period after migrating to multi-site (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59964&quot;&gt;pr#59964&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: update translations for squid (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60367&quot;&gt;pr#60367&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: use grafana server instead of grafana-server in grafana 10&lt;span&gt;&lt;/span&gt;.4&lt;span&gt;&lt;/span&gt;.0 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59722&quot;&gt;pr#59722&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: Wrong(half) uid is observed in dashboard when user created via cli contains $ in its name (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59693&quot;&gt;pr#59693&lt;/a&gt;, Dnyaneshwari Talwekar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/dashboard: Zone details showing incorrect data for data pool values and compression info for Storage Classes (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59596&quot;&gt;pr#59596&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Squid: mgr/dashboard: zonegroup level policy created at master zone did not sync to non-master zone (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59892&quot;&gt;pr#59892&lt;/a&gt;, Naman Munet)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/nfs: generate user_id &amp;amp; access_key for apply_export(CephFS) (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59896&quot;&gt;pr#59896&lt;/a&gt;, Avan Thakkar, avanthakkar, John Mulligan)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/orchestrator: fix encrypted flag handling in orch daemon add osd (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59473&quot;&gt;pr#59473&lt;/a&gt;, Yonatan Zaken)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/rest: Trim requests array and limit size (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59372&quot;&gt;pr#59372&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/rgw: Adding a retry config while calling zone_create() (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59138&quot;&gt;pr#59138&lt;/a&gt;, Kritik Sachdeva)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/rgwam: use realm/zonegroup/zone method arguments for period update (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59945&quot;&gt;pr#59945&lt;/a&gt;, Aashish Sharma)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mgr/volumes: add earmarking for subvol (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59894&quot;&gt;pr#59894&lt;/a&gt;, Avan Thakkar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: Modify container/ software to support release containers and the promotion of prerelease containers (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60962&quot;&gt;pr#60962&lt;/a&gt;, Dan Mick)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mon/ElectionLogic: tie-breaker mon ignore proposal from marked down mon (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58669&quot;&gt;pr#58669&lt;/a&gt;, Kamoltat)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mon/MonClient: handle ms_handle_fast_authentication return (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59306&quot;&gt;pr#59306&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mon/OSDMonitor: Add force-remove-snap mon command (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59402&quot;&gt;pr#59402&lt;/a&gt;, Matan Breizman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mon/OSDMonitor: fix get_min_last_epoch_clean() (&lt;a href=&quot;https://github.com/ceph/ceph/pull/55865&quot;&gt;pr#55865&lt;/a&gt;, Matan Breizman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: mon: Remove any pg_upmap_primary mapping during remove a pool (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58914&quot;&gt;pr#58914&lt;/a&gt;, Mohit Agrawal)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: msg: insert PriorityDispatchers in sorted position (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58991&quot;&gt;pr#58991&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: node-proxy: fix a regression when processing the RedFish API (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59997&quot;&gt;pr#59997&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: node-proxy: make the daemon discover endpoints (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58482&quot;&gt;pr#58482&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: objclass: deprecate cls_cxx_gather (&lt;a href=&quot;https://github.com/ceph/ceph/pull/57819&quot;&gt;pr#57819&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: orch: disk replacement enhancement (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60486&quot;&gt;pr#60486&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: orch: refactor boolean handling in drive group spec (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59863&quot;&gt;pr#59863&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: enable async manual compactions (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58740&quot;&gt;pr#58740&lt;/a&gt;, Igor Fedotov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Fix BlueFS allocating bdev label reserved location (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59969&quot;&gt;pr#59969&lt;/a&gt;, Adam Kupczyk)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Fix ceph-bluestore-tool allocmap command (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60335&quot;&gt;pr#60335&lt;/a&gt;, Adam Kupczyk)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Fix repair of multilabel when collides with BlueFS (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60336&quot;&gt;pr#60336&lt;/a&gt;, Adam Kupczyk)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Improve documentation introduced by #57722 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60893&quot;&gt;pr#60893&lt;/a&gt;, Anthony D&#39;Atri)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Multiple bdev labels on main block device (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59106&quot;&gt;pr#59106&lt;/a&gt;, Adam Kupczyk)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Mute warnings (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59217&quot;&gt;pr#59217&lt;/a&gt;, Adam Kupczyk)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: os/bluestore: Warning added for slow operations and stalled read (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59464&quot;&gt;pr#59464&lt;/a&gt;, Md Mahamudur Rahaman Sajib)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: osd/scheduler: add mclock queue length perfcounter (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59035&quot;&gt;pr#59035&lt;/a&gt;, zhangjianwei2)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: osd/scrub: decrease default deep scrub chunk size (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59791&quot;&gt;pr#59791&lt;/a&gt;, Ronen Friedman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: osd/scrub: exempt only operator scrubs from max_scrubs limit (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59020&quot;&gt;pr#59020&lt;/a&gt;, Ronen Friedman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: osd/scrub: reduce osd_requested_scrub_priority default value (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59885&quot;&gt;pr#59885&lt;/a&gt;, Ronen Friedman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: osd: fix require_min_compat_client handling for msr rules (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59492&quot;&gt;pr#59492&lt;/a&gt;, Samuel Just, Radoslaw Zarzynski)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: PeeringState&lt;span&gt;&lt;/span&gt;.cc: Only populate want_acting when num_osds &amp;lt; bucket_max (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59083&quot;&gt;pr#59083&lt;/a&gt;, Kamoltat)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/cephadm: extend iscsi teuth test (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59934&quot;&gt;pr#59934&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/cephfs: fix TestRenameCommand and unmount the clinet before failin… (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59398&quot;&gt;pr#59398&lt;/a&gt;, Xiubo Li)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/cephfs: ignore variant of MDS_UP_LESS_THAN_MAX (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58788&quot;&gt;pr#58788&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/distros: reinstall nvme-cli on centos 9 nodes (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59471&quot;&gt;pr#59471&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/rgw/multisite: specify realm/zonegroup/zone args for &#39;account create&#39; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59603&quot;&gt;pr#59603&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/rgw: bump keystone/barbican from 2023&lt;span&gt;&lt;/span&gt;.1 to 2024&lt;span&gt;&lt;/span&gt;.1 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61023&quot;&gt;pr#61023&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/rgw: fix s3 java tests by forcing gradle to run on Java 8 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61053&quot;&gt;pr#61053&lt;/a&gt;, J. Eric Ivancich)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/rgw: force Hadoop to run under Java 1&lt;span&gt;&lt;/span&gt;.8 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61120&quot;&gt;pr#61120&lt;/a&gt;, J. Eric Ivancich)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/rgw: pull Apache artifacts from mirror instead of archive&lt;span&gt;&lt;/span&gt;.apache&lt;span&gt;&lt;/span&gt;.org (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61101&quot;&gt;pr#61101&lt;/a&gt;, J. Eric Ivancich)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/standalone/scrub: fix the searched-for text for snaps decode errors (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58967&quot;&gt;pr#58967&lt;/a&gt;, Ronen Friedman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/standalone/scrub: increase status updates frequency (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59974&quot;&gt;pr#59974&lt;/a&gt;, Ronen Friedman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/standalone/scrub: remove TEST_recovery_scrub_2 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60287&quot;&gt;pr#60287&lt;/a&gt;, Ronen Friedman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites/crimson-rados/perf: add ssh keys (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61109&quot;&gt;pr#61109&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites/rados/thrash-old-clients: Add noscrub, nodeep-scrub to ignorelist (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58629&quot;&gt;pr#58629&lt;/a&gt;, Kamoltat)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites/rados/thrash-old-clients: test with N-2 releases on centos 9 (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58607&quot;&gt;pr#58607&lt;/a&gt;, Laura Flores)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites/rados/verify/validater: increase heartbeat grace timeout (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58785&quot;&gt;pr#58785&lt;/a&gt;, Sridhar Seshasayee)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites/rados: Cancel injectfull to allow cleanup (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59156&quot;&gt;pr#59156&lt;/a&gt;, Brad Hubbard)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites/rbd/iscsi: enable all supported container hosts (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60089&quot;&gt;pr#60089&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/suites: drop --show-reachable=yes from fs:valgrind tests (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59068&quot;&gt;pr#59068&lt;/a&gt;, Jos Collin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/task: update alertmanager endpoints version (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59930&quot;&gt;pr#59930&lt;/a&gt;, Nizamudeen A)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/tasks/mgr/test_progress&lt;span&gt;&lt;/span&gt;.py: deal with pre-exisiting pool (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58263&quot;&gt;pr#58263&lt;/a&gt;, Kamoltat)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/tasks/nvme_loop: update task to work with new nvme list format (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61026&quot;&gt;pr#61026&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa/upgrade: fix checks to make sure upgrade is still in progress (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59472&quot;&gt;pr#59472&lt;/a&gt;, Adam King)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa: adjust expected io_opt in krbd_discard_granularity&lt;span&gt;&lt;/span&gt;.t (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59232&quot;&gt;pr#59232&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa: ignore container checkpoint/restore related selinux denials for c… (&lt;a href=&quot;http://tracker.ceph.com/issues/67117&quot;&gt;issue#67117&lt;/a&gt;, &lt;a href=&quot;http://tracker.ceph.com/issues/66640&quot;&gt;issue#66640&lt;/a&gt;, &lt;a href=&quot;https://github.com/ceph/ceph/pull/58808&quot;&gt;pr#58808&lt;/a&gt;, Venky Shankar)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa: load all dirfrags before testing altname recovery (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59521&quot;&gt;pr#59521&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa: remove all bluestore signatures on devices (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60021&quot;&gt;pr#60021&lt;/a&gt;, Guillaume Abrioux)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: qa: suppress __trans_list_add valgrind warning (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58790&quot;&gt;pr#58790&lt;/a&gt;, Patrick Donnelly)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: RADOS: Generalize stretch mode pg temp handling to be usable without stretch mode (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59084&quot;&gt;pr#59084&lt;/a&gt;, Kamoltat)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rbd-mirror: use correct ioctx for namespace (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59771&quot;&gt;pr#59771&lt;/a&gt;, N Balachandran)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rbd: &quot;rbd bench&quot; always writes the same byte (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59502&quot;&gt;pr#59502&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rbd: amend &quot;rbd {group,} rename&quot; and &quot;rbd mirror pool&quot; command descriptions (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59602&quot;&gt;pr#59602&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rbd: handle --{group,image}-namespace in &quot;rbd group image {add,rm}&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61172&quot;&gt;pr#61172&lt;/a&gt;, Ilya Dryomov)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/beast: optimize for accept when meeting error in listenning (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60244&quot;&gt;pr#60244&lt;/a&gt;, Mingyuan Liang, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/http: finish_request() after logging errors (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59439&quot;&gt;pr#59439&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/kafka: refactor topic creation to avoid rd_kafka_topic_name() (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59754&quot;&gt;pr#59754&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/lc: Fix lifecycle not working while bucket versioning is suspended (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61138&quot;&gt;pr#61138&lt;/a&gt;, Trang Tran)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/multipart: use cls_version to avoid racing between part upload and multipart complete (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59678&quot;&gt;pr#59678&lt;/a&gt;, Jane Zhu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/multisite: metadata polling event based on unmodified mdlog_marker (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60792&quot;&gt;pr#60792&lt;/a&gt;, Shilpa Jagannath)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/notifications: fixing radosgw-admin notification json (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59302&quot;&gt;pr#59302&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/notifications: free completion pointer using unique_ptr (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59671&quot;&gt;pr#59671&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/notify: visit() returns copy of owner string (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59226&quot;&gt;pr#59226&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw/rados: don&#39;t rely on IoCtx::get_last_version() for async ops (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60065&quot;&gt;pr#60065&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: add s3select usage to log usage (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59120&quot;&gt;pr#59120&lt;/a&gt;, Seena Fallah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: decrement qlen/qactive perf counters on error (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59670&quot;&gt;pr#59670&lt;/a&gt;, Mark Kogan)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: decrypt multipart get part when encrypted (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60130&quot;&gt;pr#60130&lt;/a&gt;, sungjoon-koh)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: ignore zoneless default realm when not configured (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59445&quot;&gt;pr#59445&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: load copy source bucket attrs in putobj (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59413&quot;&gt;pr#59413&lt;/a&gt;, Seena Fallah)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: optimize bucket listing to skip past regions of namespaced entries (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61070&quot;&gt;pr#61070&lt;/a&gt;, J. Eric Ivancich)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: revert account-related changes to get_iam_policy_from_attr() (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59221&quot;&gt;pr#59221&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: RGWAccessKey::decode_json() preserves default value of &#39;active&#39; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60823&quot;&gt;pr#60823&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: switch back to boost::asio for spawn() and yield_context (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60133&quot;&gt;pr#60133&lt;/a&gt;, Casey Bodley)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgwlc: fix typo in getlc (ObjectSizeGreaterThan) (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59223&quot;&gt;pr#59223&lt;/a&gt;, Matt Benjamin)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: RGW|BN: fix lifecycle test issue (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59010&quot;&gt;pr#59010&lt;/a&gt;, Ali Masarwa)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: RGW|Bucket notification: fix for v2 topics rgw-admin list operation (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60774&quot;&gt;pr#60774&lt;/a&gt;, Oshrey Avraham, Ali Masarwa)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: seastar: update submodule (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58955&quot;&gt;pr#58955&lt;/a&gt;, Matan Breizman)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: src/ceph_release, doc: mark squid stable (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59537&quot;&gt;pr#59537&lt;/a&gt;, Neha Ojha)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: src/crimson/osd/scrub: fix the null pointer error (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58885&quot;&gt;pr#58885&lt;/a&gt;, junxiang Mu)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: src/mon/ConnectionTracker&lt;span&gt;&lt;/span&gt;.cc: Fix dump function (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60003&quot;&gt;pr#60003&lt;/a&gt;, Kamoltat)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: suites/upgrade/quincy-x: update the ignore list (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59624&quot;&gt;pr#59624&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: suites: adding ignore list for stray daemon (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58267&quot;&gt;pr#58267&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: suites: test should ignore osd_down warnings (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59147&quot;&gt;pr#59147&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: test/neorados: remove depreciated RemoteReads cls test (&lt;a href=&quot;https://github.com/ceph/ceph/pull/58144&quot;&gt;pr#58144&lt;/a&gt;, Laura Flores)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: test/rgw/notification: fixing backport issues in the tests (&lt;a href=&quot;https://github.com/ceph/ceph/pull/60545&quot;&gt;pr#60545&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: test/rgw/notification: use real ip address instead of localhost (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59303&quot;&gt;pr#59303&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: test/rgw/notifications: don&#39;t check for full queue if topics expired (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59917&quot;&gt;pr#59917&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: test/rgw/notifications: fix test regression (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61119&quot;&gt;pr#61119&lt;/a&gt;, Yuval Lifshitz)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: Test: osd-recovery-space&lt;span&gt;&lt;/span&gt;.sh extends the wait time for &quot;recovery toofull&quot; (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59041&quot;&gt;pr#59041&lt;/a&gt;, Nitzan Mordechai)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;upgrade/cephfs/mds_upgrade_sequence: ignore osds down (&lt;a href=&quot;https://github.com/ceph/ceph/pull/59865&quot;&gt;pr#59865&lt;/a&gt;, Kamoltat Sirivadhna)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: rgw: Don&#39;t crash on exceptions from pool listing (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61306&quot;&gt;pr#61306&lt;/a&gt;, Adam Emerson)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: container/Containerfile: replace CEPH_VERSION label for backward compact (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61583&quot;&gt;pr#61583&lt;/a&gt;, Dan Mick)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: container/build&lt;span&gt;&lt;/span&gt;.sh: fix up org vs&lt;span&gt;&lt;/span&gt;. repo naming (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61584&quot;&gt;pr#61584&lt;/a&gt;, Dan Mick)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;squid: container/build&lt;span&gt;&lt;/span&gt;.sh: don&#39;t require repo creds on NO_PUSH (&lt;a href=&quot;https://github.com/ceph/ceph/pull/61585&quot;&gt;pr#61585&lt;/a&gt;, Dan Mick)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/v19-2-1-squid-released/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/v19-2-1-squid-released/</guid><pubDate>Wed, 05 Feb 2025 16:00:00 GMT</pubDate><author>Yuri Weinstein</author></item><item><title>Performance at Scale with NVMe over TCP</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/images/nvme3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;performance-at-scale-with-nvme-over-tcp&quot;&gt;Performance at Scale with NVMe over TCP &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#performance-at-scale-with-nvme-over-tcp&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;exploring-the-performance-of-nvme-over-tcp%3A-revolutionizing-data-storage&quot;&gt;Exploring the Performance of NVMe over TCP: Revolutionizing Data Storage &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#exploring-the-performance-of-nvme-over-tcp%3A-revolutionizing-data-storage&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In the data-driven world, the demand for faster, more efficient storage solutions is escalating. As businesses, cloud providers, and data centers look to handle ever-growing volumes of data, the performance of storage becomes a critical factor. One of the most promising innovations in this space is NVMe over TCP (NVMe/TCP aka NVMeoF), which allows the deployment of high-performance Non-Volatile Memory Express (NVMe) storage devices over traditional TCP/IP networks. This blog delves into Ceph and the performance of our newest block protocol: NVMe over TCP, its benefits, challenges, and the outlook for this technology. We will explore performance profiles and nodes populated with NVMe SSDs to detail a design optimized for high performance.&lt;/p&gt;&lt;h3 id=&quot;understanding-nvme-and-tcp%3A-a-quick-overview&quot;&gt;Understanding NVMe and TCP: A Quick Overview &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#understanding-nvme-and-tcp%3A-a-quick-overview&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Before diving into performance specifics, let’s clarify the key technologies involved:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;NVMe (Non-Volatile Memory Express)&lt;/strong&gt; is a protocol designed to provide fast data access to storage media by leveraging the high-speed PCIe (Peripheral Component Interconnect Express) bus. NVMe reduces latency, improves throughput, and enhances overall storage performance compared to legacy storage like SATA and SAS, while maintaining a price point that is at most slightly increased on a $/TB basis. Comparatively speaking, when concerned with performance, scale and throughput, NVMe drives are the clear cost-performer in this arena.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;TCP/IP (Transmission Control Protocol/Internet Protocol)&lt;/strong&gt; is one of the pillars of modern networking. It is a reliable, connection-oriented protocol that ensures data is transmitted correctly across networks. TCP is known for its robustness and widespread use, making it an attractive option for connecting NVMe devices over long distances and in cloud environments.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Ceph brings NVMe over TCP to market offering NVMe speed and low latency access to networked storage solutions, without the need for specialized hardware like Fibre Channel, InfiniBand or RDMA.&lt;/p&gt;&lt;h3 id=&quot;nvme-over-tcp-performance%3A-what-to-expect&quot;&gt;NVMe over TCP Performance: What to Expect &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#nvme-over-tcp-performance%3A-what-to-expect&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The performance of NVMe over TCP largely depends on the underlying network infrastructure, storage architecture and design, and the workload being handled. However, there are a few key factors to keep in mind:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Latency and Throughput&lt;/strong&gt;: NVMe is designed to minimize latency, and this benefit carries over into NVMe over TCP. While TCP itself introduces some latency due to its connection management features (compared to lower-latency protocols like RDMA), NVMe over TCP still offers significantly better latency than traditional network storage protocols. Notable regarding Ceph: at scale there is a clear tradeoff between latency and throughput, as you will see in this article. As the limits of performance are pushed for a defined architecture, Ceph doesn’t falter or fail, we simply see an increase in latency when serving higher demand in throughput. This is highly important to keep in mind when designing for both IOPS and Latency for any given workload.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Network Congestion and Packet Loss&lt;/strong&gt;: TCP is known for its reliability in the face of network congestion and packet loss, thanks to its built-in error correction and retransmission mechanisms. However, these features can sometimes introduce performance bottlenecks, especially in environments with high traffic or unreliable network connections. For example, if the network becomes congested, TCP&#39;s flow control mechanisms may throttle performance to ensure data integrity. To mitigate this, businesses often deploy Quality of Service (QoS) and Class of Service (CoS) mechanisms to fine-tune network parameters and ensure smooth data transmission.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;CPU Overhead&lt;/strong&gt;: While NVMe over TCP eliminates the need for specialized hardware, it can introduce some CPU overhead due to the protocol processing required by the TCP/IP stack. NVMe/TCP requires more from the CPU, where storage workload processing happens. However, we do see performance benefits in scaling CPU cores consumed with increased workload demand, thus driving down latency and enhancing throughput.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Optimization and Tuning&lt;/strong&gt;: To get the best performance from NVMe over TCP, network administrators often need to fine-tune several parameters, including TCP window size, buffer sizes, and congestion control settings. With optimizations such as TCP offloading and TCP/UDP-based congestion control, the performance of NVMe over TCP can be enhanced to better meet the needs of demanding workloads. For Ceph we can dive deeper into software parameters that, when sized for your hardware platform, can maximize performance without additional cost or hardware complexity.&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;definitions&quot;&gt;Definitions &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#definitions&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let’s define some important terms in the Ceph world to ensure that we see which parameters can move the needle for performance and scale.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;OSD&lt;/strong&gt; (Object Storage Daemon) is the object storage daemon for the Ceph software defined storage system. It manages data on physical storage drives with redundancy and provides access to that data over the network. For the purposes of this article, we can state that an OSD is the software service that manages disk IO for a given physical device.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reactor / Reactor Core&lt;/strong&gt;: this is an event handling model in software development that comprises an event loop running a single thread which handles IO requests for NVMe/TCP. By default, we begin with 4 reactor core threads, but this model is tunable via software parameters.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;BDevs_per_cluster&lt;/strong&gt;: BDev is short for Block Device, this driver is how the NVMe Gateways talk to Ceph RBD images. This is important because by default the NVMe/TCP Gateway leverages 32 BDevs in a single cluster context per librbd client (&lt;code&gt;bdevs_per_cluster=32&lt;/code&gt;), or storage client connecting to the underlying volume. This tunable parameter can be adjusted to provide scaling all the way to a 1:1 context for NVMe volume to librbd client, creating an uncontested path to performance for a given volume at the expense of more compute resources.&lt;/p&gt;&lt;h3 id=&quot;performance&quot;&gt;Performance &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#performance&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Starting off strong, below we see how adding drives (OSDs) and nodes to a Ceph cluster can increase IO performance across the board. A 4-node Ceph cluster with 24 drives per node can provide over 450,000 IOPS with a 70:30 read/write profile, using a 16k block size with 32 FIO clients. That’s over 100K IOPS average per node! This trend scales linearly as nodes and drives are added, showing a top-end of nearly 1,000,000 IOPS with a 12 node, 288 OSD cluster. It is noteworthy that the higher end numbers are shown with 12 reactors and 1 librbd client per namespace (&lt;code&gt;bdevs_per_cluster=1&lt;/code&gt;), which demonstrates how the addition of librbd clients enables more throughput to the OSDs serving the underlying RBD images and their mapped NVMe namespaces.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/images/nvme1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;The next test below shows how tuning an environment to the underlying hardware can show massive improvements in software defined storage. We begin with a simple 4-node cluster, and show scale points of 16, 32, 64 and 96 OSDs. In this test the Ceph Object Storage Daemons have been mapped 1:1 directly to physical NVMe drives.&lt;/p&gt;&lt;p&gt;It may seem like adding drives and nodes alone only gains a modicum of performance, but with software defined storage there is always a trade-off between server utilization and storage performance – in this case for the better. When the same cluster has the default reactor cores increased from 4 to 10 (thus consuming more CPU cycles), and ``bdevs_per_cluster` is configured to increase software throughput via the addition of librbd clients, the performance nearly doubles. All this by simply tuning your environment to the underlying hardware and enabling Ceph to take advantage of this processing power.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/images/nvme2.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;The chart below shows the IOPS delivered at the three “t-shirt” sizes of tuned 4-node, 8-node and 12-node configurations, and a 4-node cluster with the defaults enabled for comparison. Again we see that, for &amp;lt;2ms latency workloads, Ceph scales linearly and in a dependable, expectable fashion. Note: as I/O becomes congested, at a certain point the workloads are still serviceable but with higher latency response times. Ceph continues to commit the required reads and writes, only plateauing once the existing platform design boundaries become saturated.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/images/nvme3.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h3 id=&quot;the-future-of-nvme-over-tcp&quot;&gt;The Future of NVMe over TCP &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#the-future-of-nvme-over-tcp&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As storage needs continue to evolve, NVMe over TCP is positioned to become a key player in the high-performance storage landscape. With continued advancements in Ethernet speeds, TCP optimizations, and network infrastructure, NVMe over TCP will continue to offer compelling advantages for a wide range of applications, from enterprise data centers to edge computing environments.&lt;/p&gt;&lt;p&gt;Ceph is positioned to be the top performer in software defined storage for NVMe over TCP, by enabling not only high-performance, scale-out NVMe storage platforms, but also by enabling more performance on platform by user-controlled software enhancements and configuration.&lt;/p&gt;&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ceph’s NVMe over TCP Target offers a powerful, scalable, and cost-effective solution for high-performance storage networks.&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community by through our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/nvme-gateway-perf-mb/</guid><pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate><author>Mike Burkhart, Anthony D&#39;Atri (IBM)</author></item><item><title>Performance investigation on the Ceph Crimson OSD CPU core allocation. Part One</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/fio-workflow.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;crimson%3A-the-new-osd-high-performance-architecture&quot;&gt;Crimson: the new OSD high performance architecture &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#crimson%3A-the-new-osd-high-performance-architecture&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://ceph.io/en/news/crimson/&quot;&gt;Crimson&lt;/a&gt; is the project name for the new OSD high performance architecture. Crimson is built on top of &lt;a href=&quot;http://www.seastar-project.org/&quot;&gt;Seastar framework&lt;/a&gt; an advanced, open-source C++ framework for high-performance server applications on modern hardware. Seastar implements I/O reactors in a share-nothing architecture, using asynchronous computation primitives, like futures, promises, and coroutines. The I/O reactor threads are normally pinned to a specific CPU core in the system. However, to support interaction with legacy software that is, non-reactor blocking tasks, the mechanism of Alien threads are available in Seastar. These allow the interface between non-reactor and I/O reactor architecture. In Crimson, the alien threads are used to support Bluestore.&lt;/p&gt;&lt;p&gt;There are very good introductions to the project, in particular check the following videos by &lt;a href=&quot;https://www.youtube.com/watch?v=LaP4YX1lQ3I&quot;&gt;Sam Just&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=8N_1WAEPw0o&quot;&gt;Matan Breizman &lt;/a&gt;in the Ceph community Youtube channel.&lt;/p&gt;&lt;p&gt;An important question from the performance point of view is the allocation of Seastar reactor threads to the available CPU cores. This is particularly key in the modern NUMA (Non-Uniform Memory Access) architectures, where there is a latency penalty for accessing memory from a different CPU socket as opposed to local memory belonging to the same CPU socket where the thread is running. We also want to ensure mutual exclusion between Reactors and other non-reactor threads within the same CPU core. The main reason is that the Seastar reactor threads are non-blocking, whereas non-reactor threads are allowed to block.&lt;/p&gt;&lt;p&gt;As part of &lt;a href=&quot;https://github.com/ceph/ceph/pull/60822&quot;&gt;this PR&lt;/a&gt; we introduced a new option in the &lt;code&gt;vstart.sh&lt;/code&gt; script to set the CPU allocation strategy for the OSD threads:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;OSD-based&lt;/strong&gt;: this consists on allocating CPU cores from the same NUMA socket to the same OSD. For simplicity, if the OSD id is even, all its reactor threads are allocated to NUMA socket 0, and consequently if the OSD id is odd, all its reactor threads are allocated to NUMA socket 1. The following figure illustrates this strategy (the &#39;R&#39; stands for &#39;Reactor&#39; thread, the numeric id corresponds to the OSD id):&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_bal_osd.png&quot; alt=&quot;alt_text&quot; title=&quot;OSD-balanced strategy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;NUMA socket based&lt;/strong&gt;: this consists of allocating evenly CPU cores from each NUMA socket to the reactors, so all the OSDs end up with reactors allocated on both NUMA sockets.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_bal_socket.png&quot; alt=&quot;alt_text&quot; title=&quot;NUMA socket balanced strategy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;By default, if the option is not given, vstart will allocate the reactor threads in consecutive order, as follows:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_default.png&quot; alt=&quot;alt_text&quot; title=&quot;Default CPU allocation strategy&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Worth mentioning that the &lt;code&gt;vstart.sh&lt;/code&gt; script is used in Developer mode only, very useful for experimenting, as in this case.&lt;/p&gt;&lt;p&gt;The structure of this blog entry is as follows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;first we briefly describe the hardware and performance tests we executed, illustrated with some snippets. Readers familiarised with Ceph might want to skip this section.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the second section, we show the results of the performance tests, comparing the three CPU allocation strategies. We used the three backend classes supported by Crimson, which are:&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;Cyanstore&lt;/em&gt;: this is an in-memory pure-reactor OSD class, which does not exercise the physical drives in the system. The reason for using this OSD class is to saturate the memory access rate in the machine, to identify the highest I/O rate possible in Crimson without interference (that is latencies) from physical drives.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;Seastore&lt;/em&gt;: this is also a pure-reactor OSD class which exercises the physical NVMe drives in the machine. We expect that the overall performance of this class would be a fraction of that achieved by Cyanstore.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;Bluestore&lt;/em&gt;: this is the default OSD class for Crimson, as well for the Classic OSD in Ceph. This class involves the participation of Alien threads, which is the technique in Seastore to deal with blocking thread pools.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The comparison results are interesting as they highlight both limitations and opportunities for performance optimisations.&lt;/p&gt;&lt;h2 id=&quot;performance-testing-plans&quot;&gt;Performance testing plans &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#performance-testing-plans&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In a nutshell, we want to measure the performance using some typical client workloads (random 4K write, random 4K read; sequential 64K write, sequential 64K read) for a number of cluster configurations involving a fixed number of OSD and ranging over a number of I/O reactors (which implicitly ranges over the corresponding number of CPU cores). We want to compare across the exisiting object stores: Cyanstore (in memory), Seastore and Bluestore. The former two are &quot;pure reactor&quot;, whilst the latter involves (blocking) Alien thread pools.&lt;/p&gt;&lt;p&gt;In terms of the client, we exercise an RBD volume of 10 GiB size, using FIO for the typical workloads as mentioned before. We synthetise the client results from an FIO .json output (I/O throughput and latency) and integrate it with measurements from the OSD process. These typically involve CPU and Memory utilisation (from the Linux command top). This workflow is illustrated in the following diagram.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/fio-workflow.png&quot; alt=&quot;alt_text&quot; title=&quot;Performance testing based on a FIO workflow.&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;In our actual experimentations, we ranged over a number of OSD (1, 3, 5, 8), as well as over the number of reactors (1,2,4,6). Since the number of results would be considerably large and would make reading this blog rather tedious, we decided only show the representative 8 OSDs and 5 I/O reactors.&lt;/p&gt;&lt;p&gt;We used a single node cluster, with the following hardware and system configuration:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CPU: 2 x Intel(R) Xeon(R) Platinum 8276M CPU @ 2.20GH (56 cores each)&lt;/li&gt;&lt;li&gt;Memory: 394 GiB&lt;/li&gt;&lt;li&gt;Drives: 8 x 93.1 TB NVMe&lt;/li&gt;&lt;li&gt;OS: Centos 9.0 on kernel 5.14.0-511.el9.x86_64&lt;/li&gt;&lt;li&gt;Ceph: version b2a220 (bb2a2208867d7bce58b9697570c83d995a1c5976) squid (dev)&lt;/li&gt;&lt;li&gt;podman version 5.2.2.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We build Ceph with the following options:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# ./do_cmake.sh -DWITH_SEASTAR=ON -DCMAKE_BUILD_TYPE=RelWithDebInfo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At a high level, we initiate the performance tests as follows:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;/root/bin/run_balanced_crimson.sh -t cyan
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will run the test plan for the Cyanstore object backend, producing data for response curves over the three CPU allocation strategies. The argument &#39;&lt;code&gt;-t&lt;/code&gt;&#39; is used to specify the object storage backend: &lt;code&gt;cyan&lt;/code&gt;, &lt;code&gt;sea&lt;/code&gt; and &lt;code&gt;blue&lt;/code&gt; for Cyanstore, Seastore and Bluestore, respectively.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;crimson_be_table[&quot;cyan&quot;]=&quot;--cyanstore&quot;
crimson_be_table[&quot;sea&quot;]=&quot;--seastore --seastore-devs ${STORE_DEVS}&quot;
crimson_be_table[&quot;blue&quot;]=&quot;--bluestore --bluestore-devs ${STORE_DEVS}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the execution completes, the results are archived in .zip files according to the workloads and saved in the output directory which can be specified with option &lt;code&gt;-d&lt;/code&gt; (&lt;code&gt;/tmp&lt;/code&gt; by default).&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see a snapshot of the generated results.&lt;/summary&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread.zip      cyan_8osd_5reactor_8fio_bal_socket_rc_1procs_seqread.zip   cyan_8osd_6reactor_8fio_bal_osd_rc_1procs_randread.zip      cyan_8osd_6reactor_8fio_bal_socket_rc_1procs_seqread.zip
cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randwrite.zip     cyan_8osd_5reactor_8fio_bal_socket_rc_1procs_seqwrite.zip  cyan_8osd_6reactor_8fio_bal_osd_rc_1procs_randwrite.zip     cyan_8osd_6reactor_8fio_bal_socket_rc_1procs_seqwrite.zip
cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_seqread.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;p&gt;Each archive contains the result output files and measurements from that workload execution.&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see inside a results .zip file.&lt;/summary&gt;The files more important are:&lt;ul&gt;&lt;li&gt;&lt;code&gt;cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread.json&lt;/code&gt;: the (combined) FIO output file, which contains the I/O throughput and latency measurements. It contains integrated the CPU and MEM utilisation from the OSD process as well.&lt;ul&gt;&lt;li&gt;&lt;code&gt;cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread_cpu_avg.json&lt;/code&gt;: the OSD and FIO CPU and MEM utilisation averages. These have been collected from the OSD process and the FIO client process via top (30 samples over 5 minutes).&lt;/li&gt;&lt;li&gt;&lt;code&gt;cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread_diskstat.json&lt;/code&gt;: the diskstat output. A sample is taken before and after the test, the .json contains the differences.&lt;/li&gt;&lt;li&gt;&lt;code&gt;cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread_top.json&lt;/code&gt;: the output from top, parsed via jc to produce a .json. &lt;strong&gt;Note&lt;/strong&gt;: jc does not yet support individual CPU core utilisation, so we have to rely on the overall CPU utilisation (per thread).&lt;/li&gt;&lt;li&gt;&lt;code&gt;new_cluster_dump.json&lt;/code&gt;: the output from &lt;code&gt;ceph tell ${osd_i} dump_metrics&lt;/code&gt; command, which contains the individual OSD performance metrics.&lt;/li&gt;&lt;li&gt;&lt;code&gt;FIO_cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread_top_cpu.plot&lt;/code&gt;: the plot mechanically generated from the top output, showing the CPU utilisation over time for the FIO client.&lt;/li&gt;&lt;li&gt;&lt;code&gt;OSD_cyan_8osd_5reactor_8fio_bal_osd_rc_1procs_randread_top_mem.plot&lt;/code&gt;: the plot mechanically generated from the top output, showing the MEM utilisation over time for the OSD process.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/details&gt;&lt;p&gt;To produce the post-processing and side-by-side comparison, the following script is run:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# /root/bin/pp_balanced_cpu_cmp.sh -d /tmp/_seastore_8osd_5_6_reactor_8fio_rc_cmp \
   -t sea -o seastore_8osd_5vs6_reactor_8fio_cpu_cmp.md
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The arguments are: the input directory that contains the runs we want to compare, the type of object store backend, and the output .md to produce.&lt;/p&gt;&lt;p&gt;We will show the comparisons produced in the next section.&lt;/p&gt;&lt;p&gt;We end this section by looking behind the curtains of the above scripts, showing details on preconditioning the drives, creation of the cluster, execution of FIO and the metrics collected.&lt;/p&gt;&lt;h3 id=&quot;preconditioning-the-drives&quot;&gt;Preconditioning the drives &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#preconditioning-the-drives&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In order to ensure that the drives are in a consistent state, we run a write workload using FIO with the &lt;code&gt;steadystate&lt;/code&gt; option. This option ensures that the drives are in a steady state before the actual performance tests are run. We precondition up to 70 percent of the total capacity of the drive.&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see the command.&lt;/summary&gt;&lt;p&gt;We take a measurement of the diskstats before and after the test, and we use the &lt;code&gt;diskstat_diff.py&lt;/code&gt; script to calculate the difference. The script is available in the &lt;code&gt;ceph&lt;/code&gt; repository under &lt;code&gt;src/tools/contrib&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# jc --pretty /proc/diskstats &amp;gt; /tmp/blue_8osd_6reactor_192at_8fio_socket_cond.json
# fio rbd_fio_examples/randwrite64k.fio &amp;amp;&amp;amp; jc --pretty /proc/diskstats \
 | python3 diskstat_diff.py -d /tmp/ -a blue_8osd_6reactor_192at_8fio_socket_cond.json

Jobs: 8 (f=8): [w(8)][30.5%][w=24.3GiB/s][w=398k IOPS][eta 13m:41s]
nvme0n1p2: (groupid=0, jobs=8): err= 0: pid=375444: Fri Jan 31 11:43:35 2025
  write: IOPS=397k, BW=24.2GiB/s (26.0GB/s)(8742GiB/360796msec); 0 zone resets
    slat (nsec): min=1543, max=823010, avg=5969.62, stdev=2226.84
    clat (usec): min=57, max=50322, avg=5152.50, stdev=2982.28
     lat (usec): min=70, max=50328, avg=5158.47, stdev=2982.27
    clat percentiles (usec):
     |  1.00th=[  281],  5.00th=[  594], 10.00th=[ 1037], 20.00th=[ 2008],
     | 30.00th=[ 3032], 40.00th=[ 4080], 50.00th=[ 5145], 60.00th=[ 6194],
     | 70.00th=[ 7242], 80.00th=[ 8291], 90.00th=[ 9241], 95.00th=[ 9634],
     | 99.00th=[10028], 99.50th=[10421], 99.90th=[14091], 99.95th=[16188],
     | 99.99th=[19268]
   bw (  MiB/s): min=15227, max=24971, per=100.00%, avg=24845.68, stdev=88.47, samples=5768
   iops        : min=243638, max=399547, avg=397527.12, stdev=1415.43, samples=5768
  lat (usec)   : 100=0.01%, 250=0.61%, 500=3.18%, 750=2.90%, 1000=2.88%
  lat (msec)   : 2=10.28%, 4=19.25%, 10=59.90%, 20=1.00%, 50=0.01%
  lat (msec)   : 100=0.01%
  cpu          : usr=19.80%, sys=15.60%, ctx=104026691, majf=0, minf=2647
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &amp;gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.1%
     issued rwts: total=0,143224767,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256
  steadystate  : attained=yes, bw=24.3GiB/s (25.5GB/s), iops=398k, iops mean dev=1.215%

Run status group 0 (all jobs):
  WRITE: bw=24.2GiB/s (26.0GB/s), 24.2GiB/s-24.2GiB/s (26.0GB/s-26.0GB/s), io=8742GiB (9386GB), run=360796-360796msec
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;h3 id=&quot;creating-the-cluster&quot;&gt;Creating the cluster &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#creating-the-cluster&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We use the &lt;code&gt;vstart.sh&lt;/code&gt; script to create the cluster, with the appropriate options for Crimson.&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see a snippet of the cluster creation.&lt;/summary&gt;We have a table with the CPU allocation strategies:&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# CPU allocation strategies
declare -A bal_ops_table
bal_ops_table[&quot;default&quot;]=&quot;&quot;
bal_ops_table[&quot;bal_osd&quot;]=&quot; --crimson-balance-cpu osd&quot;
bal_ops_table[&quot;bal_socket&quot;]=&quot;--crimson-balance-cpu socket&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We essentially traverse over the order of the CPU strategies, for each of the Crimson backends. In the snippet, we iterate over the number of OSDs and reactors, and set the CPU allocation strategy with the new option.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: Notice that we set the list of CPU cores available for vstart with the &lt;code&gt;VSTART_CPU_CORES&lt;/code&gt; variable. We use this to ensure we &quot;reserve&quot; some CPUs to be used by the FIO client (since we are using a single node cluster).&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Run balanced vs default CPU core/reactor distribution in Crimson using either Cyan, Seastore or  Bluestore
fun_run_bal_vs_default_tests() {
  local OSD_TYPE=$1
  local NUM_ALIEN_THREADS=7 # default 
  local title=&quot;&quot;

  for KEY in default bal_osd bal_socket; do
    for NUM_OSD in 8; do
      for NUM_REACTORS in 5 6; do
        title=&quot;(${OSD_TYPE}) $NUM_OSD OSD crimson, $NUM_REACTORS reactor, fixed FIO 8 cores, response latency &quot;

        cmd=&quot;MDS=0 MON=1 OSD=${NUM_OSD} MGR=1 taskset -ac &#39;${VSTART_CPU_CORES}&#39; /ceph/src/vstart.sh \
 --new -x --localhost --without-dashboard\
 --redirect-output ${crimson_be_table[${OSD_TYPE}]} --crimson --crimson-smp ${NUM_REACTORS}\
 --no-restart ${bal_ops_table[${KEY}]}&quot;
        # Alien setup for Bluestore, see below.

        test_name=&quot;${OSD_TYPE}_${NUM_OSD}osd_${NUM_REACTORS}reactor_8fio_${KEY}_rc&quot;
        echo &quot;${cmd}&quot;  | tee &amp;gt;&amp;gt; ${RUN_DIR}/${test_name}_cpu_distro.log
        echo $test_name
        eval &quot;$cmd&quot; &amp;gt;&amp;gt; ${RUN_DIR}/${test_name}_cpu_distro.log
        echo &quot;Sleeping for 20 secs...&quot;

        sleep 20 
        fun_show_grid $test_name
        fun_run_fio $test_name
        /ceph/src/stop.sh --crimson
        sleep 60
      done
    done
  done
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For Bluestore, we have a special case, where we set the number of alien threads to be 4 times the number of backend CPU cores (crimson-smp).&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;        if [ &quot;$OSD_TYPE&quot; == &quot;blue&quot; ]; then
            NUM_ALIEN_THREADS=$(( 4 *NUM_OSD * NUM_REACTORS ))
            title=&quot;${title} alien_num_threads=${NUM_ALIEN_THREADS}&quot;
            cmd=&quot;${cmd}  --crimson-alien-num-threads $NUM_ALIEN_THREADS&quot;
            test_name=&quot;${OSD_TYPE}_${NUM_OSD}osd_${NUM_REACTORS}reactor_${NUM_ALIEN_THREADS}at_8fio_${KEY}_rc&quot;
        fi
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;p&gt;Once the cluster is online, we create the pools and the RBD volume.&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see the pool creation.&lt;/summary&gt;&lt;p&gt;We first take some measurements of the cluster, then we create a single RBD pool and volume(s) as appropriate. We also show the status of the cluster, the pools and the PGs.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Take some measurements
if pgrep crimson; then
bin/ceph daemon -c /ceph/build/ceph.conf osd.0 dump_metrics &amp;gt; /tmp/new_cluster_dump.json
else
bin/ceph daemon -c /ceph/build/ceph.conf osd.0 perf dump &amp;gt; /tmp/new_cluster_dump.json
fi

# Create the pools
bin/ceph osd pool create rbd
bin/ceph osd pool application enable rbd rbd
[ -z &quot;$NUM_RBD_IMAGES&quot; ] &amp;amp;&amp;amp; NUM_RBD_IMAGES=1
for (( i=0; i&amp;lt;$NUM_RBD_IMAGES; i++ )); do
  bin/rbd create --size ${RBD_SIZE} rbd/fio_test_${i}
  rbd du fio_test_${i}
done
bin/ceph status
bin/ceph osd dump | grep &#39;replicated size&#39;
# Show a pool’s utilization statistics:
rados df
# Turn off auto scaler for existing and new pools - stops PGs being split/merged
bin/ceph osd pool set noautoscale
# Turn off balancer to avoid moving PGs
bin/ceph balancer off
# Turn off deep scrub
bin/ceph osd set nodeep-scrub
# Turn off scrub
bin/ceph osd set noscrub
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here is an example of the default pools shown after the cluster has been created. Notice the default replica set as Crimson does not support Erasure Coding yet.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;    pool &#39;rbd&#39; created
enabled application &#39;rbd&#39; on pool &#39;rbd&#39;
NAME        PROVISIONED  USED
fio_test_0       10 GiB   0 B
  cluster:
    id:     da51b911-7229-4eae-afb5-a9833b978a68
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum a (age 97s)
    mgr: x(active, since 94s)
    osd: 8 osds: 8 up (since 52s), 8 in (since 60s)

  data:
    pools:   2 pools, 33 pgs
    objects: 2 objects, 449 KiB
    usage:   214 MiB used, 57 TiB / 57 TiB avail
    pgs:     27.273% pgs unknown
             21.212% pgs not active
             17 active+clean
             9  unknown
             7  creating+peering

pool 1 &#39;.mgr&#39; replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode off last_change 15 flags hashpspool,nopgchange,crimson stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr read_balance_score 7.89
pool 2 &#39;rbd&#39; replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode off last_change 33 flags hashpspool,nopgchange,selfmanaged_snaps,crimson stripe_width 0 application rbd read_balance_score 1.50
POOL_NAME     USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS      RD  WR_OPS       WR  USED COMPR  UNDER COMPR
.mgr       449 KiB        2       0       6                   0        0         0      41  35 KiB      55  584 KiB         0 B          0 B
rbd            0 B        0       0       0                   0        0         0       0     0 B       0      0 B         0 B          0 B

total_objects    2
total_used       214 MiB
total_avail      57 TiB
total_space      57 TiB
noautoscale is set, all pools now have autoscale off
nodeep-scrub is set
noscrub is set
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;h3 id=&quot;running-fio&quot;&gt;Running FIO &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#running-fio&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We have written some basic infrastructure via stand-alone tools to drive &lt;a href=&quot;https://github.com/axboe/fio&quot;&gt;FIO&lt;/a&gt;, the Linux flexible I/O exerciser. All of these tools are publically available at my github project repo &lt;a href=&quot;https://github.com/perezjosibm/ceph-aprg/tree/main&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In essence, this basic infrastructure consists of:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A set of predefined FIO configuration files for the different workloads (random 4K write, random 4K read; sequential 64K write, sequential 64K read). These can be automatically generated on demand, especially for multiple clients, multiple RBD volumes, etc.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A set of performance test profiles, namely &lt;em&gt;response latency curves&lt;/em&gt; which produce throughput and latency measurements for a range of I/O depths, with resource utilisation integrated. We can also produce quick &lt;em&gt;latency target&lt;/em&gt; tests, which are useful to identify the maximum I/O throughput for a given latency target.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A set of monitoring routines, to measure resource utilisation (CPU, MEM) from the FIO client and the OSD process. We use the &lt;code&gt;top&lt;/code&gt; command, and we parse the output with &lt;code&gt;jc&lt;/code&gt; to produce a .json file. We integrate this with the FIO output in a single .json file, and generate gnuplot scripts dynamically. We also take a snapshot of the diskstats before and after the test, and we calculate the difference. We also aggregate the FIO traces, in terms of gnuplot charts.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;details&gt;&lt;summary&gt;Click to see the FIO execution.&lt;/summary&gt;&lt;ul&gt;&lt;li&gt;We use the &lt;code&gt;iodepth&lt;/code&gt; option to control the number of I/O requests that are issued to the device. Since we are interested in &lt;em&gt;response latency curves&lt;/em&gt; (a.k.a hockey stick performance curves) we traverse from one to sixty four. We use a single job per RBD volume (but this could also be variable if required).&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Option -w (WORKLOAD) is used as index for these:
declare -A m_s_iodepth=( [hockey]=&quot;1 2 4 8 16 24 32 40 52 64&quot;  ...)
declare -A m_s_numjobs=( [hockey]=&quot;1&quot; ... )
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;As a preliminary step, we prime the volume(s) with a write workload. This is done before the actual performance tests are run. We exercise a client per volume, so the execution is concurrent.&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Prime the volume(s) with a write workloads
   RBD_NAME=fio_test_$i RBD_SIZE=&quot;64k&quot; fio ${FIO_JOBS}rbd_prime.fio 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null &amp;amp; 
   echo &quot;== priming $RBD_NAME ==&quot;;
 ...
   wait;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;In the main loop, we iterate over the number of I/O depths, and we run the FIO command for each of the predefined workloads. We also take a snapshot of the diskstats before and after the test, and we calculate the difference. We collect the PIDs of the FIO process as well as the OSD processes, which would be used to monitor their resource utilisation. We have an heuristic in place to exit early the loops if the standard deviation of the latency disperses too much from the median.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: the attentive reader would notice the use of the &lt;code&gt;taskset&lt;/code&gt; command to bind the FIO client to a set of CPU cores. This is to ensure that the FIO client does not interfere with the reactors of OSD process. The order of execution of the workloads is important to ensure reproducibility.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;
  for job in $RANGE_NUMJOBS; do
    for io in $RANGE_IODEPTH; do

      #&amp;nbsp;Take diskstats measurements before FIO instances
      jc --pretty /proc/diskstats &amp;gt; ${DISK_STAT}
...
        for (( i=0; i&amp;lt;${NUM_PROCS}; i++ )); do

        export TEST_NAME=${TEST_PREFIX}_${job}job_${io}io_${BLOCK_SIZE_KB}_${map[${WORKLOAD}]}_p${i};
        echo &quot;== $(date) == ($io,$job): ${TEST_NAME} ==&quot;;
        echo fio_${TEST_NAME}.json &amp;gt;&amp;gt; ${OSD_TEST_LIST}
        fio_name=${FIO_JOBS}${FIO_JOB_SPEC}${map[${WORKLOAD}]}.fio

          #&amp;nbsp;Execute FIO
          LOG_NAME=${log_name} RBD_NAME=fio_test_${i} IO_DEPTH=${io} NUM_JOBS=${job} \
            taskset -ac ${FIO_CORES} fio ${fio_name} --output=fio_${TEST_NAME}.json \
            --output-format=json 2&amp;gt; fio_${TEST_NAME}.err &amp;amp;
          fio_id[&quot;fio_${i}&quot;]=$!
          global_fio_id+=($!)
      done # loop NUM_PROCS
      sleep 30; # ramp up time
...
    fun_measure &quot;${all_pids}&quot; ${top_out_name} ${TOP_OUT_LIST} &amp;amp;
...
      wait;
      #&amp;nbsp;Measure the diskstats after the completion of FIO
      jc --pretty /proc/diskstats | python3 /root/bin/diskstat_diff.py -a ${DISK_STAT}

      #&amp;nbsp;Exit the loops if the latency disperses too much from the median
      if [ &quot;$RESPONSE_CURVE&quot; = true ] &amp;amp;&amp;amp; [ &quot;$RC_SKIP_HEURISTIC&quot; = false ]; then
        mop=${mode[${WORKLOAD}]}
        covar=$(jq &quot;.jobs | .[] | .${mop}.clat_ns.stddev/.${mop}.clat_ns.mean &amp;lt; 0.5 and \
          .${mop}.clat_ns.mean/1000000 &amp;lt; ${MAX_LATENCY}&quot; fio_${TEST_NAME}.json)
                  if [ &quot;$covar&quot; != &quot;true&quot; ]; then
                    echo &quot;== Latency std dev too high, exiting loops ==&quot;
                    break 2
                  fi
      fi
    done # loop IODEPTH
  done # loop NUM_JOBS 

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The basic monitoring routine is shown below, which is executed concurrently as FIO progresses.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;fun_measure() {
  local PID=$1 #comma separated list of pids
  local TEST_NAME=$2
  local TEST_TOP_OUT_LIST=$3

  top -b -H -1 -p &quot;${PID}&quot; -n ${NUM_SAMPLES} &amp;gt;&amp;gt; ${TEST_NAME}_top.out
  echo &quot;${TEST_NAME}_top.out&quot; &amp;gt;&amp;gt; ${TEST_TOP_OUT_LIST}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;&lt;p&gt;We have written a custom profile for top, so we get information about the parent process id, last CPU the thread was executed on, etc. (which are not normally shown by default). We also plan to extend jc to support individual CPU core utilisation.&lt;/p&gt;&lt;p&gt;We extended and implemented new tools in the CBT (Ceph Benchmarking Tool) as standalones since they can be used in either local laptop as well as in the &lt;em&gt;client endpoints&lt;/em&gt;. Further proof of concepts are in progress.&lt;/p&gt;&lt;h2 id=&quot;performance-results-and-comparison&quot;&gt;Performance results and comparison &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#performance-results-and-comparison&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this section, we show the performance results for the three CPU allocation strategies across the three object storage backends. We show the results for the 8 OSDs and 5 reactors configuration.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: since these results are based on a single node cluster, due to the nature and limitations of the hardware used, these results should be considered as experimental, and hence not used as ultimate reference.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It is interesting to point out that there is not a single CPU allocation strategy has a significant advantage over the others, but there are workloads that seem to gain benefits for different CPU allocation strategies. The results are consistent across the different object storage backends, for most of the workloads.&lt;/p&gt;&lt;p&gt;The response latency curves are extended with yerror bars describing the standard deviation of the latency. This is useful to observe how the latency disperses from the median (which is the average latency). For all the results shown we disabled the heuristic mentioned above so we see all the data points as requested (from iodepth 1 to 64).&lt;/p&gt;&lt;p&gt;For each workload, we show the comparison of the three CPU allocation strategies across the three object storage backends. At the end, we compare the results for a single CPU allocation strategy across the three object storage backends.&lt;/p&gt;&lt;h3 id=&quot;random-4k-read&quot;&gt;Random 4K read &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#random-4k-read&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;cyanstore&quot;&gt;Cyanstore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#cyanstore&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randread_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randread_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;p&gt;We first show the CPU and MEM utilisation for the OSD process and then the FIO client.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;It is interesting to observe that the default CPU allocation strategy has a higher memory utilisation than the balanced CPU allocation strategies, but only for Cyanstore, so it should not be an issue of concern.&lt;/li&gt;&lt;/ul&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randread_osd_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randread_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randread_osd_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randread_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randread_fio_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randread_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randread_fio_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randread_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;seastore&quot;&gt;Seastore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#seastore&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randread_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randread_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;ul&gt;&lt;li&gt;It is a good sign to see a constant memory utilisation for the OSD process across all the iodepths. The NUMA socket strategy has only a .6% higher than the other strategies, which is not significant to cause concern.&lt;/li&gt;&lt;/ul&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randread_osd_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randread_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randread_osd_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randread_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randread_fio_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randread_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randread_fio_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randread_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;bluestore&quot;&gt;Bluestore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#bluestore&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Interestingly, the NUMA socket balanced CPU allocation strategy seemed to have a slight advantage over the other CPU allocation strategies for Bluestore.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randread_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randread_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randread_osd_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randread_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randread_osd_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randread_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randread_fio_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randread_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randread_fio_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randread_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h3 id=&quot;random-4k-write&quot;&gt;Random 4K write &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#random-4k-write&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This is a pleasant workload in the sense that the resource utilisation graphs are smooth with gentle gradient. The NUMA socket strategy has a slight advantage over the other strategies for Cyanstore and Seastore, but the default strategy has a slight advantage for Bluestore.&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;cyanstore-1&quot;&gt;Cyanstore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#cyanstore-1&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randwrite_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randwrite_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randwrite_osd_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randwrite_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randwrite_osd_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randwrite_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randwrite_fio_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randwrite_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_randwrite_fio_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_randwrite_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;seastore-1&quot;&gt;Seastore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#seastore-1&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randwrite_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randwrite_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randwrite_osd_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randwrite_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randwrite_osd_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randwrite_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randwrite_fio_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randwrite_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_randwrite_fio_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_randwrite_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;bluestore-1&quot;&gt;Bluestore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#bluestore-1&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randwrite_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randwrite_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randwrite_osd_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randwrite_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randwrite_osd_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randwrite_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randwrite_fio_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randwrite_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_randwrite_fio_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_randwrite_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h3 id=&quot;sequential-64k-read&quot;&gt;Sequential 64K read &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#sequential-64k-read&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;It seems that a nature of this workload causes bursts of activity in the CPU utilisation. This is consistent across the three CPU allocation strategies, and across the three object storage backends. It deserves further investigation.&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;cyanstore-2&quot;&gt;Cyanstore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#cyanstore-2&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqread_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqread_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqread_osd_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqread_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqread_osd_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqread_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqread_fio_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqread_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqread_fio_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqread_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;seastore-2&quot;&gt;Seastore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#seastore-2&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqread_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqread_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqread_osd_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqread_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqread_osd_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqread_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqread_fio_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqread_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqread_fio_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqread_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;bluestore-2&quot;&gt;Bluestore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#bluestore-2&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqread_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqread_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqread_osd_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqread_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqread_osd_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqread_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqread_fio_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqread_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqread_fio_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqread_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h3 id=&quot;sequential-64k-write&quot;&gt;Sequential 64K write &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#sequential-64k-write&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;It is hard to explain the initial spike in the CPU utilisation for the three CPU allocation strategies, across the three object store backends. We might need to run a localised version of this test to confirm.&lt;/li&gt;&lt;/ul&gt;&lt;h4 id=&quot;cyanstore-3&quot;&gt;Cyanstore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#cyanstore-3&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqwrite_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqwrite_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqwrite_osd_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqwrite_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqwrite_osd_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqwrite_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqwrite_fio_cpu.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqwrite_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/cyan_8osd_5reactor_8fio_seqwrite_fio_mem.png&quot; alt=&quot;cyan_8osd_5reactor_8fio_seqwrite_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;seastore-3&quot;&gt;Seastore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#seastore-3&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqwrite_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqwrite_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqwrite_osd_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqwrite_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqwrite_osd_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqwrite_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqwrite_fio_cpu.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqwrite_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/sea_8osd_5reactor_8fio_seqwrite_fio_mem.png&quot; alt=&quot;sea_8osd_5reactor_8fio_seqwrite_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h4 id=&quot;bluestore-3&quot;&gt;Bluestore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#bluestore-3&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqwrite_bal_vs_unbal_iops_vs_lat.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqwrite_bal_vs_unbal_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;details&gt;&lt;summary&gt;Click to see CPU and MEM utilisation.&lt;/summary&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;OSD MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqwrite_osd_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqwrite_osd_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqwrite_osd_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqwrite_osd_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO CPU&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;FIO MEM&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqwrite_fio_cpu.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqwrite_fio_cpu&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/blue_8osd_5reactor_160at_8fio_seqwrite_fio_mem.png&quot; alt=&quot;blue_8osd_5reactor_160at_8fio_seqwrite_fio_mem&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/details&gt;&lt;h3 id=&quot;comparison-of-default-cpu-allocation-strategy-across-the-three-object-storage-backends&quot;&gt;Comparison of default CPU allocation strategy across the three object storage backends &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#comparison-of-default-cpu-allocation-strategy-across-the-three-object-storage-backends&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We briefly show the comparison of the default CPU allocation strategy across the three storage backends. We choose the default CPU allocation strategy as it is the currently used in the field/community.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Cyanstore is the higher performer for random read 4k and sequential 64K write, which makes sense since there is no latency involved in memory access.&lt;/li&gt;&lt;li&gt;Bluestore is the higher performer for random write 4k, which is a bit of a surprise. Worth deserving further investigation to figure out why Bluestore is more efficient in writing small blocks.&lt;/li&gt;&lt;li&gt;It is also surprising that for sequential 64K read, Bluestore seems to be an order of magnitude ahead compared to the pure Reactor object backends. This deserves a closer look to verify for any mistake.&lt;/li&gt;&lt;/ul&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;Random read 4K&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;Random Write 4k&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/8osd_5reactor_8fio_default_rc_1procs_randread_iops_vs_lat.png&quot; alt=&quot;8osd_5reactor_8fio_default_rc_1procs_randread_iops_vs_lat.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/8osd_5reactor_8fio_default_rc_1procs_randwrite_iops_vs_lat.png&quot; alt=&quot;8osd_5reactor_8fio_default_rc_1procs_randwrite_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;Seq read 64K&lt;/em&gt;&lt;/th&gt;&lt;th style=&quot;text-align:center&quot;&gt;&lt;em&gt;Seq Write 64k&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/8osd_5reactor_8fio_default_rc_1procs_seqread_iops_vs_lat.png&quot; alt=&quot;8osd_5reactor_8fio_default_rc_1procs_seqread_iops_vs_lat.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/images/8osd_5reactor_8fio_default_rc_1procs_seqwrite_iops_vs_lat.png&quot; alt=&quot;8osd_5reactor_8fio_default_rc_1procs_seqwrite_iops_vs_lat&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/#conclusions&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this blog entry, we have shown the performance results for the three CPU allocation strategies across the three object storage backends. It is interesting that none of the CPU allocation strategies significantly outperforms the existing default strategy, which is a bit of a surprise. Please notice that in order to keep a disciplined methodology, we used the same Ceph build (with the commit hash cited above) across all the tests, to ensure only a single parameter is modified on each step, as appropriate. Hence, these results do not represent the latest Seastore development progress yet.&lt;/p&gt;&lt;p&gt;This is my very first blog entry, and I hope you have found it useful. I am very thankful to the Ceph community for their support and guidance in this my first year of being part of such a vibrant community, especially to Matan Breizman, Yingxin Cheng, Aishwarya Mathuria, Josh Durgin, Neha Ojha and Bill Scales. I am looking forward to the next blog entry, where we will take a deep dive into the internals of performance metrics in Crimson OSD. We will try use some flamegraphs on the major code landmarks, and leverage the existing tools to identify latencies per component.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/crimson-balance-cpu-part1/</guid><pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate><author>Jose Juan Palacios Perez (IBM)</author></item><item><title>SMB Meets Squid: Introducing the New Ceph SMB Manager Module for SMB Service Management in Ceph</title><description>&lt;div class=&quot;to-lg:w-full-breakout&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;mb-8 lg:mb-10 xl:mb-12 w-full&quot; loading=&quot;lazy&quot; src=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/images/smb1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/div&gt;&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;smb-meets-squid%3A-introducing-the-new-ceph-smb-manager-module-for-smb-service-management-in-ceph&quot;&gt;SMB Meets Squid: Introducing the New Ceph SMB Manager Module for SMB Service Management in Ceph &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#smb-meets-squid%3A-introducing-the-new-ceph-smb-manager-module-for-smb-service-management-in-ceph&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;introduction&quot;&gt;Introduction &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#introduction&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;SMB (Server Message Block) is a widely-used network protocol that facilitates the sharing of files, printers, and other resources across a network. To seamlessly integrate SMB services within a Ceph environment, Ceph 8.0 introduces the powerful SMB Manager module, which enables users to deploy, manage, and control Samba services for SMB access to CephFS. This module offers a user-friendly interface for managing clusters of Samba services and SMB shares, with the flexibility to choose between two management methods: imperative and declarative. By enabling the SMB Manager module using the command &lt;code&gt;ceph mgr module enable smb&lt;/code&gt;, administrators can efficiently streamline their SMB service operations, whether through the command-line or via orchestration with YAML or JSON resource descriptions. With the new SMB Manager module, Ceph admins can effortlessly extend file services, providing robust SMB access to CephFS while enjoying enhanced control and scalability.&lt;/p&gt;&lt;h3 id=&quot;managing-smb-cluster-and-shares&quot;&gt;Managing SMB Cluster and Shares &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#managing-smb-cluster-and-shares&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Admins can interact with the Ceph Manager SMB module using following methods:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Imperative Method: Ceph commands to interact with the Ceph Manager SMB module.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Declarative Method: Resources specification in YAML or JSON format.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;simple-ceph-squid-with-smb-configuration-workflow&quot;&gt;Simple Ceph Squid with SMB Configuration Workflow &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#simple-ceph-squid-with-smb-configuration-workflow&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/images/smb1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h4 id=&quot;imperative-method&quot;&gt;Imperative Method &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#imperative-method&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Create CephFS Volume/Subvolume&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph fs volume create cephfs
# ceph fs subvolumegroup create cephfs smb
# ceph fs subvolume create cephfs sv1 --group-name=smb --mode=0777
# ceph fs subvolume create cephfs sv2 --group-name=smb --mode=0777
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Enable SMB Management Module&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph mgr module enable smb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Creating SMB Cluster/Share&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb cluster create smb1 user --define-user-pass=user1%passwd
# ceph smb share create smb1 share1 cephfs / --subvolume=smb/sv1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Map a network drive from MS Windows clients&lt;/em&gt;&lt;/p&gt;&lt;h4 id=&quot;declarative-method&quot;&gt;Declarative Method &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#declarative-method&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Create CephFS volume/subvolume&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph fs volume create cephfs
# ceph fs subvolumegroup create cephfs smb
# ceph fs subvolume create cephfs sv1 --group-name=smb --mode=0777
# ceph fs subvolume create cephfs sv2 --group-name=smb --mode=0777
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Enable SMB Management Module&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph mgr module enable smb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Creating SMB Cluster/Share&lt;/em&gt;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb apply -i - &amp;lt;&amp;lt;&#39;EOF&#39;
# --- Begin Embedded YAML
- resource_type: ceph.smb.cluster
  cluster_id: smb1
  auth_mode: user
  user_group_settings:
    - {source_type: resource, ref: ug1}
  placement:
    count: 1
- resource_type: ceph.smb.usersgroups
  users_groups_id: ug1
  values:
    users:
      - {name: user1, password: passwd}
      - {name: user2, password: passwd}
    groups: []
- resource_type: ceph.smb.share
  cluster_id: smb1
  share_id: share1
  cephfs:
    volume: cephfs
    subvolumegroup: smb
    subvolume: sv1
    path: /
- resource_type: ceph.smb.share
  cluster_id: smb1
  share_id: share2
  cephfs:
    volume: cephfs
    subvolumegroup: smb
    subvolume: sv2
    path: /
# --- End Embedded YAML
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Map a network drive from MS Windows clients&lt;/em&gt;&lt;/p&gt;&lt;h3 id=&quot;basic-smb-manager-module-cli-commands&quot;&gt;Basic SMB Manager Module CLI Commands &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#basic-smb-manager-module-cli-commands&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;creating-an-smb-cluster&quot;&gt;Creating an SMB Cluster &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#creating-an-smb-cluster&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# ceph smb cluster create &amp;lt;cluster_id&amp;gt; {user} [--domain-realm=&amp;lt;domain_realm&amp;gt;] \
            [--domain-join-user-pass=&amp;lt;domain_join_user_pass&amp;gt;] \
    [--define-user-pass=&amp;lt;define_user_pass&amp;gt;] [--custom-dns=&amp;lt;custom_dns&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;h5 id=&quot;auth_mode%3Auser&quot;&gt;Auth_Mode:User &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#auth_mode%3Auser&quot;&gt;¶&lt;/a&gt;&lt;/h5&gt;&lt;pre&gt;&lt;code&gt;# ceph smb cluster create smb1 user --define_user_pass user1%passwd --placement label:smb --clustering default
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&quot;auth_mode%3A-ad&quot;&gt;Auth_Mode: AD &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#auth_mode%3A-ad&quot;&gt;¶&lt;/a&gt;&lt;/h5&gt;&lt;pre&gt;&lt;code&gt;# ceph smb cluster create smb1 active-directory --domain_realm samba.qe --domain_join_user_pass Administrator%Redhat@123 --custom_dns 10.70.44.153 --placement label:smb --clustering default
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;creating-an-smb-cluster-with-the-declarative-method&quot;&gt;Creating an SMB Cluster with the Declarative method &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#creating-an-smb-cluster-with-the-declarative-method&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# ceph smb apply -i [--format &amp;lt;value&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb apply -i resources.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;creating-smb-share&quot;&gt;Creating SMB Share &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#creating-smb-share&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# ceph smb share create &amp;lt;cluster_id&amp;gt; &amp;lt;share_id&amp;gt; &amp;lt;cephfs_volume&amp;gt; &amp;lt;path&amp;gt; [&amp;lt;share_name&amp;gt;] [&amp;lt;subvolume&amp;gt;] [--readonly] [--format]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb share create smb1 share1 cephfs / --subvolume=smb/sv1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Listng SMB Shares&lt;/p&gt;&lt;pre&gt;&lt;code&gt;#  ceph smb share ls &amp;lt;cluster_id&amp;gt; [--format &amp;lt;value&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb share ls smb1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;viewing-smb-cluster-details&quot;&gt;Viewing SMB Cluster Details &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#viewing-smb-cluster-details&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;#  ceph smb show [&amp;lt;resource_names&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb show ceph.smb.cluster.smb1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;deleting-smb-share&quot;&gt;Deleting SMB Share &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#deleting-smb-share&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# ceph smb share rm &amp;lt;cluster_id&amp;gt; &amp;lt;share_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb share rm smb1 share1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;deleting-smb-cluster&quot;&gt;Deleting SMB Cluster &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#deleting-smb-cluster&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# ceph smb cluster rm &amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph smb share rm smb1
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/smb-manager-module/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The Ceph SMB Manager module in Ceph Squid brings an innovative and efficient way to manage SMB services for CephFS file systems. Whether through imperative or declarative methods, users can easily create, manage, and control SMB clusters and shares. This integration simplifies the setup of Samba services, enhances scalability, and offers greater flexibility for administrators.With the ability to manage SMB access to CephFS seamlessly, users can now have a streamlined process for providing secure and scalable file services.&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community by facilitating our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/smb-manager-module/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/smb-manager-module/</guid><pubDate>Tue, 28 Jan 2025 00:00:00 GMT</pubDate><author>Mohit Bisht, Anthony D&#39;Atri (IBM)</author></item><item><title>Simplyfing Ceph Object Deployments: Production-ready Ceph Object Gateway (RGW) with New Cephadm Features</title><description>&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h3 id=&quot;introduction&quot;&gt;Introduction &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#introduction&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Deploying a &lt;em&gt;production-ready object storage solution&lt;/em&gt; can be challenging, particularly when managing complex requirements including SSL/TLS encryption, optimal data placement, and multisite replication. During deployment, it’s easy to overlook configuration options that become crucial once the system is live in production.&lt;/p&gt;&lt;p&gt;Traditionally, configuring Ceph for high availability, security, and efficient data handling required users to manually adjust multiple parameters based on their needs, such as Multisite Replication, Encryption, and High Availability. This initial complexity made it tedious to achieve a production-ready Object Storage configuration.&lt;/p&gt;&lt;p&gt;To tackle these challenges, we have introduced several new features to Ceph&#39;s orchestrator that simplify the deployment of Ceph RGW and its associated services. Enhancing the Ceph Object Gateway and Ingress service specification files enables an out-of-the-box, production-ready RGW setup with just a few configuration steps. These enhancements include automated SSL/TLS configurations, virtual host bucket access support, erasure coding for cost-effective data storage, and more.&lt;/p&gt;&lt;p&gt;These improvements aim to provide administrators with a seamless deployment experience that ensures secure, scalable, and production-ready configurations for the Ceph Object Gateway and Ingress Service (load balancer).&lt;/p&gt;&lt;p&gt;In this blog post, we&#39;ll explore each of these new features, discuss the problems they solve, and demonstrate how they can be easily configured using &lt;code&gt;cephadm&lt;/code&gt; spec files to achieve a fully operational Ceph Object Gateway setup in minutes.&lt;/p&gt;&lt;h3 id=&quot;feature-highlights&quot;&gt;Feature Highlights &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#feature-highlights&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;virtual-host-bucket-access-and-self-signed-certificates&quot;&gt;Virtual Host Bucket Access and Self-Signed Certificates &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#virtual-host-bucket-access-and-self-signed-certificates&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;One of the major challenges in deploying RGW is ensuring seamless access to buckets using virtual host-style URLs. For applications and users that rely on virtual host bucket access, proper SSL/TLS certificates that include the necessary Subject Alternative Names (SANs) are crucial. To simplify this, we&#39;ve added the option to automatically generate self-signed certificates for the Object Gateway if the user does not provide custom certificates. These self-signed certificates include SAN entries that allow TLS/SSL to work seamlessly with virtual host bucket access.&lt;/p&gt;&lt;h4 id=&quot;complete-tls%2Fssl-client-to-rgw-encryption&quot;&gt;Complete TLS/SSL Client-to-RGW Encryption &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#complete-tls%2Fssl-client-to-rgw-encryption&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Security is a top priority for any production-grade deployment, and the Ceph community has increasingly requested full TLS/SSL encryption from the client to the Object Gateway service. Previously, our ingress implementation only supported terminating SSL at the HAProxy level, which meant that communication between HAProxy and RGW could not be encrypted.&lt;/p&gt;&lt;p&gt;To address this, we&#39;ve added configurable options that allow users to choose whether to re-encrypt traffic between HAProxy and RGW or to use passthrough mode, where the TLS connection remains intact from the client to RGW. This flexibility allows users to achieve complete end-to-end encryption, ensuring sensitive data is always protected in transit.&lt;/p&gt;&lt;h4 id=&quot;multisite-replication-configuration&quot;&gt;Multisite Replication Configuration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#multisite-replication-configuration&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In the past, Ceph multisite deployments involved running many commands to configure your Realm, zonegroup, and zone, and also establishing the relationship between the zones that will be involved in the Multisite replication. Thanks to the RGW manager module, the multisite bootstrap and configuration can now be done in two steps. There is an example in the Object Storage Replication blog post.&lt;/p&gt;&lt;p&gt;In Squid release, we have also added the possibility of configuring dedicating Object Gateways just for client traffic purposes through the &lt;code&gt;cephadm&lt;/code&gt; spec file with the RGW spec file option:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;disable_multisite_sync_traffic: True
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The advantages of dedicating Ceph Object Gateways to specific tasks are covered in the blog post: &lt;em&gt;Ceph Object Storage Multisite Replication Series. Part Three&lt;/em&gt;&lt;/p&gt;&lt;h4 id=&quot;configure-your-erasure-coded-data-pool-in-the-spec-file-during-bootstrap.&quot;&gt;Configure your Erasure coded data pool in the Spec file during bootstrap. &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#configure-your-erasure-coded-data-pool-in-the-spec-file-during-bootstrap.&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Object Storage often uses Erasure Coding for the data pool to reduce the TCO of the object storage solution. We have included options for configuring erasure-coded (EC) pools in the spec file. This allows users to define the EC profile, device class, and failure domain for RGW data pools, which provides control over data placement and storage efficiency.&lt;/p&gt;&lt;h3 id=&quot;ceph-object-deployment-walkthrough&quot;&gt;Ceph Object Deployment Walkthrough &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#ceph-object-deployment-walkthrough&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;If you are new to Ceph and &lt;code&gt;cephadm&lt;/code&gt;, the &lt;em&gt;Automating Ceph Cluster Deployments with Ceph: A Step-by-Step Guide Using Cephadm and Ansible (Part 1)&lt;/em&gt; blog post will give you a good overview of &lt;code&gt;cephadm&lt;/code&gt; and how we can define the desired state of Ceph services in a declarative YAML spec file to deploy and configure Ceph.&lt;/p&gt;&lt;p&gt;Below, we&#39;ll walk through the CLI commands required to deploy a production-ready RGW setup using the new features added to the &lt;code&gt;cephadm&lt;/code&gt; orchestrator.&lt;/p&gt;&lt;h4 id=&quot;enabling-the-rgw-manager-module&quot;&gt;Enabling the RGW Manager Module &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#enabling-the-rgw-manager-module&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The first step is to enable the RGW manager module. This module is required to manage RGW services through &lt;code&gt;cephadm&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph mgr module enable rgw
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we create a spec file for the Object Gateway service. This spec file includes realm, zone, and zonegroup settings, SSL/TLS, EC profile for the data pool, etc.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt; EOF &amp;gt; /root/rgw-client.spec
service_type: rgw
service_id: client
service_name: rgw.client
placement:
  label: rgw
  count_per_host: 1
networks:
  - 192.168.122.0/24
spec:
  rgw_frontend_port: 4443
  rgw_realm: multisite
  rgw_zone: zone1
  rgw_zonegroup: multizg
  generate_cert: true
  ssl: true
  zonegroup_hostnames:
    - s3.cephlab.com
  data_pool_attributes:
    type: ec
    k: 2
    m: 2
extra_container_args:
  - &quot;--stop-timeout=120&quot;
config:
  rgw_exit_timeout_secs: &quot;120&quot;
  rgw_graceful_stop: true
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this spec file we specify that the RGW service should use erasure coding with a 2+2 profile &lt;code&gt;(k: 2, m: 2)&lt;/code&gt; for the data pool, which reduces storage costs compared to a replicated setup. We also generate a self-signed certificate &lt;code&gt;(generate_cert: true)&lt;/code&gt; for the RGW service to ensure secure SSL/TLS communication. With &lt;code&gt;zonegroup_hostnames&lt;/code&gt;, we enable virtual host bucket access using the specified domain &lt;code&gt;bucket.s3.cephlab.com&lt;/code&gt;. Thanks to the config parameter &lt;code&gt;rgw_gracefull_stop&lt;/code&gt;, we configure graceful stopping of object gateway services. During a graceful stop, the service will wait until all client connections are closed (drained) subject to the specified 120 second timeout.&lt;/p&gt;&lt;h4 id=&quot;bootstrapping-the-rgw-realm&quot;&gt;Bootstrapping the RGW Realm &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#bootstrapping-the-rgw-realm&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Once the spec file is created, we bootstrap RGW services. This step creates and deploys RGW services with the configuration specified in our spec file.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph rgw realm bootstrap -i rgw-client.spec
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;verifying-the-rgw-services&quot;&gt;Verifying the RGW Services &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#verifying-the-rgw-services&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The cephadm bootstrap command will asynchronously apply the configuration defined in our spec file. Soon the RGW services will be up and running, and we can verify their status using the &lt;code&gt;ceph orch ps command&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph orch ps --daemon_type rgw
NAME                            HOST                      PORTS                 STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION          IMAGE ID      CONTAINER ID
rgw.client.ceph-node-05.yquamf  ceph-node-05.cephlab.com  192.168.122.175:4443  running (32m)    94s ago  32m    91.2M        -  19.2.0-53.el9cp  fda78a7e8502  a0c39856ddd8
rgw.client.ceph-node-06.zfsutg  ceph-node-06.cephlab.com  192.168.122.214:4443  running (32m)    94s ago  32m    92.9M        -  19.2.0-53.el9cp  fda78a7e8502  82c21d350cb7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This output shows that the RGW services run on the specified nodes and are accessible via the configured &lt;code&gt;4443/tcp&lt;/code&gt; port.&lt;/p&gt;&lt;h4 id=&quot;verifying-the-data-pool&quot;&gt;Verifying the Data Pool &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#verifying-the-data-pool&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;To verify that the RGW data pools are correctly configured with erasure coding, we can use the following command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph osd pool ls detail | grep data
pool 24 &#39;zone1.rgw.buckets.data&#39; erasure profile zone1_zone_data_pool_ec_profile size 4 min_size 3 crush_rule 1 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 258 lfor 0/0/256 flags hashpspool stripe_width 8192 application rgw
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;viewing-the-erasure-code-profile&quot;&gt;Viewing the Erasure Code Profile &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#viewing-the-erasure-code-profile&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;To get more details about the erasure code profile used for the data pool, we can run the below:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph osd erasure-code-profile get zone1_zone_data_pool_ec_profile
crush-device-class=
crush-failure-domain=host
crush-num-failure-domains=0
crush-osds-per-failure-domain=0
crush-root=default
jerasure-per-chunk-alignment=false
k=2
m=2
plugin=jerasure
technique=reed_sol_van
w=8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This confirms that the erasure code profile is configured with &lt;code&gt;k=2&lt;/code&gt; and &lt;code&gt;m=2&lt;/code&gt; and uses the Reed-Solomon technique.&lt;/p&gt;&lt;h4 id=&quot;configuring-the-ingress-service&quot;&gt;Configuring the Ingress Service &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#configuring-the-ingress-service&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Finally, we must configure the ingress service to load balance traffic to multiple RGW daemons. We create a spec file for the ingress service:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt; EOF &amp;gt; rgw-ingress.yaml
service_type: ingress
service_id: rgw
placement:
  hosts:
    - ceph-node-06.cephlab.com
    - ceph-node-07.cephlab.com
spec:
  backend_service: rgw.client
  virtual_ip: 192.168.122.152/24
  frontend_port: 443
  monitor_port:  1967
  use_tcp_mode_over_rgw: True
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This spec file sets up the ingress service with the virtual (floating) IP (VIP) address &lt;code&gt;192.168.122.152&lt;/code&gt; and specifies that it should use TCP mode for communication with the Object Gateway, ensuring that SSL/TLS is maintained throughout. With the &lt;code&gt;backend_service&lt;/code&gt; we specify the RGW service we want to use as the backend for HAproxy, as it is possible for a Ceph cluster to run multiple, unrelated RGW services.&lt;/p&gt;&lt;h4 id=&quot;testing-load-balancer-and-ssl%2Ftls-configuration&quot;&gt;Testing Load Balancer and SSL/TLS Configuration &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#testing-load-balancer-and-ssl%2Ftls-configuration&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Our ingress service stack uses &lt;code&gt;keepalived&lt;/code&gt; for HA of the VIP, and HAproxy takes care of the load balancing:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph orch ps --service_name ingress.rgw
NAME                                HOST                      PORTS       STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION         IMAGE ID      CONTAINER ID
haproxy.rgw.ceph-node-06.vooxuh     ceph-node-06.cephlab.com  *:443,1967  running (58s)    46s ago  58s    5477k        -  2.4.22-f8e3218  0d25561e922f  4cd458e1f6b0
haproxy.rgw.ceph-node-07.krdmsb     ceph-node-07.cephlab.com  *:443,1967  running (56s)    46s ago  56s    5473k        -  2.4.22-f8e3218  0d25561e922f  4d18247e7615
keepalived.rgw.ceph-node-06.cwraia  ceph-node-06.cephlab.com              running (55s)    46s ago  55s    1602k        -  2.2.8           6926947c161f  50fd6cf57187
keepalived.rgw.ceph-node-07.svljiw  ceph-node-07.cephlab.com              running (53s)    46s ago  53s    1598k        -  2.2.8           6926947c161f  aaab5d79ffdd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When we check the haproxy configuration on &lt;code&gt;ceph-node-06&lt;/code&gt; where the service is running, we confirm that we are using TCP passthrough for the backend configuration of our Object Gateway services.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ssh ceph-node-06.cephlab.com cat /var/lib/ceph/93d766b0-ae6f-11ef-a800-525400ac92a7/haproxy.rgw.ceph-node-06.vooxuh/haproxy/haproxy.cfg | grep -A 10 &quot;frontend frontend&quot;
...
backend backend
    mode        tcp
    balance     roundrobin
    option ssl-hello-chk
    server rgw.client.ceph-node-05.yquamf 192.168.122.175:4443 check weight 100 inter 2s
    server rgw.client.ceph-node-06.zfsutg 192.168.122.214:4443 check weight 100 inter 2s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To verify that the SSL/TLS configuration is working correctly, we can use &lt;code&gt;curl&lt;/code&gt; to test the endpoint. We can see that the CA is not trusted by our client system where we are running the curl command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# curl https://192.168.122.152
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.se/docs/sslcerts.html
curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To fix this, we need to add the cephadm root CA certificate to the trusted store of our client system:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph orch cert-store get cert cephadm_root_ca_cert &amp;gt; /etc/pki/ca-trust/source/anchors/cephadm-root-ca.crt
# update-ca-trust
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After updating the trusted store, we can test again:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# curl  https://s3.cephlab.com
&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&amp;gt;&amp;lt;Owner&amp;gt;&amp;lt;ID&amp;gt;anonymous&amp;lt;/ID&amp;gt;&amp;lt;/Owner&amp;gt;&amp;lt;Buckets&amp;gt;&amp;lt;/Buckets&amp;gt;&amp;lt;/ListAllMyBucketsResult&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This confirms that the SSL/TLS self-signed certificate configuration works correctly and that the RGW service is accessible using HTTPS. As you can see, we have configured our DNS subdomain &lt;code&gt;s3.cephlab.com&lt;/code&gt; and the wildcard &lt;code&gt;*.s3.cephlab.com&lt;/code&gt; to point to our VIP address &lt;code&gt;192.168.122.152&lt;/code&gt;. Also, it&#39;s important to mention that you can have more than one VIP address configured so not all the traffic goes through a single haproxy LB node; when using a list of VIP IPs, you need to use the option: &lt;code&gt;virtual_ips_list&lt;/code&gt;&lt;/p&gt;&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;These new features in the &lt;code&gt;cephadm&lt;/code&gt; orchestrator represents significant steps forward in making Ceph RGW deployments more accessible, secure, and production-ready. By automating complex configurations—such as SSL/TLS encryption, virtual host bucket access, multisite replication, and erasure coding administrators can now deploy an RGW setup ready for production with minimal manual intervention.&lt;/p&gt;&lt;p&gt;For further details on the Squid release, check Laura Flores&#39; &lt;a href=&quot;https://ceph.io/en/news/blog/2024/v19-2-0-squid-released/&quot;&gt;blog post&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Note that some features described here may not be available before the Squid 19.2.2 release.&lt;/p&gt;&lt;h3 id=&quot;footnote&quot;&gt;Footnote &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/#footnote&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community by facilitating our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/simplifying-object-new-cephadm/</guid><pubDate>Sat, 28 Dec 2024 00:00:00 GMT</pubDate><author>Daniel Parkes, Anthony D&#39;Atri (IBM)</author></item><item><title>Ceph Object Storage Tiering Enhancements. Part Two</title><description>&lt;div class=&quot;intro-para richtext&quot;&gt;&lt;h2 id=&quot;introducing-policy-based-data-retrieval-for-ceph&quot;&gt;Introducing Policy-Based Data Retrieval for Ceph &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#introducing-policy-based-data-retrieval-for-ceph&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;h2 id=&quot;introduction-and-feature-overview&quot;&gt;Introduction and Feature Overview &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#introduction-and-feature-overview&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the first part of this series, we explored the fundamentals of Ceph Object Storage and its policy-based archive to cloud/tape feature, which enables seamless data migration to remote S3-compatible storage classes. This feature is instrumental in offloading data to cost-efficient storage tiers, such as cloud or tape-based systems. However, in the past, the process has been unidirectional. Once objects are transitioned, retrieving them requires direct access to the cloud provider’s S3 endpoint. This limitation has introduced operational challenges, particularly when accessing archived or cold-tier data.&lt;/p&gt;&lt;p&gt;We are introducing policy-based data retrieval in the Ceph Object Storage ecosystem to address these gaps. This enhancement empowers administrators and operations teams to retrieve objects transitioned to cloud or tape tiers directly back into the Ceph cluster, aligning with operational efficiency and data accessibility needs.&lt;/p&gt;&lt;h3 id=&quot;why-this-matters&quot;&gt;Why This Matters &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#why-this-matters&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Policy-based data retrieval transforms the usability of cloud-transitioned objects in Ceph. Whether the data resides in cost-efficient tape archives or high-latency/low-cost cloud tiers, this feature ensures that users can seamlessly access and manage their objects without relying on the external provider&#39;s S3 endpoints. This capability simplifies workflows and enhances compliance with operational policies and data lifecycle requirements.&lt;/p&gt;&lt;h2 id=&quot;policy-based-data-retrieval&quot;&gt;Policy-Based Data Retrieval &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#policy-based-data-retrieval&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This new functionality offers a dual approach to retrieving transitioned objects to remote cloud/tape S3-compatible endpoints:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;S3 RestoreObject API Implementation: Similar to the AWS S3 &lt;code&gt;RestoreObject&lt;/code&gt; API, this feature allows users to retrieve objects manually using the S3 &lt;code&gt;RestoreObject&lt;/code&gt; API. The object restore operation can be permanent or temporary based on the retention period specified in the &lt;code&gt;RestoreObject&lt;/code&gt; API Call.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Read-Through Mode: By introducing a configurable &lt;code&gt;--allow-read-through&lt;/code&gt; capability Ceph can serve read requests for transitioned objects in the cloud tier storage class configuration. Upon receiving a &lt;code&gt;GET&lt;/code&gt; request, the system asynchronously retrieves the object from the cloud tier, stores it locally, and serves the data to the user. This eliminates the &lt;code&gt;InvalidObjectState&lt;/code&gt; error previously encountered for cloud-transitioned objects.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/images/t2-1.png&quot; alt=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;h3 id=&quot;s3-restoreobject-temporary-restore&quot;&gt;S3 RestoreObject Temporary Restore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#s3-restoreobject-temporary-restore&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The restored data is treated as temporary and will exist in the Ceph cluster only for the duration specified during the restore request. Once the specified period expires, the restored data will be deleted, and the object will revert to a stub, preserving metadata and cloud transition configurations.&lt;/p&gt;&lt;h4 id=&quot;lifecycle-transition-rules-are-skipped&quot;&gt;Lifecycle Transition Rules Are Skipped &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#lifecycle-transition-rules-are-skipped&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;During the temporary restore period, the object is exempted from lifecycle rules that might otherwise move it to a different tier or delete it. This ensures uninterrupted access until the expiry date.&lt;/p&gt;&lt;h4 id=&quot;default-storage-class-for-restored-data&quot;&gt;Default Storage Class for Restored Data &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#default-storage-class-for-restored-data&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Restored objects are, by default, written to the &lt;code&gt;STANDARD&lt;/code&gt; storage class within the Ceph cluster. However, for temporary objects, the &lt;code&gt;x-amz-storage-class&lt;/code&gt; header will still return the original cloud-tier storage class This is in line with AWS Glacier semantics, where restored objects’ storage class remains the same.&lt;/p&gt;&lt;h3 id=&quot;s3-restoreobject-api-walkthrough&quot;&gt;S3 RestoreObject API walkthrough &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#s3-restoreobject-api-walkthrough&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We are uploading an object called &lt;code&gt;2gb&lt;/code&gt; to the on-prem Ceph cluster using a bucket called &lt;code&gt;databucket&lt;/code&gt;. In part one of this blog post series, we configured &lt;code&gt;databucket&lt;/code&gt; with a lifecycle policy that will tier/archive data into IBM COS after 30 days. Wee set up the AWS CLI client with a profile called &lt;code&gt;tiering&lt;/code&gt; to interact with Ceph Object Gateway S3 API endpoint.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;aws --profile tiering --endpoint https://s3.cephlabs.com s3 cp 2gb s3://databucket upload: ./2gb to s3://databucket/2gb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can check the size of the uploaded object in the &lt;code&gt;STANDARD&lt;/code&gt; storage class within our on-prem Ceph cluster:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;aws --profile tiering --endpoint https://s3.cephlabs.com s3api head-object --bucket databucket --key 2gb
{
    &quot;AcceptRanges&quot;: &quot;bytes&quot;,
    &quot;LastModified&quot;: &quot;2024-11-26T21:31:05+00:00&quot;,
    &quot;ContentLength&quot;: 2000000000,
    &quot;ETag&quot;: &quot;\&quot;b459c232bfa8e920971972d508d82443-60\&quot;&quot;,
    &quot;ContentType&quot;: &quot;binary/octet-stream&quot;,
    &quot;Metadata&quot;: {},
    &quot;PartsCount&quot;: 60
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After 30 days, the lifecycle transition kicks in, and the object is transitioned to the cloud tier. First, as an admin, we check with the &lt;code&gt;radosgw-admin&lt;/code&gt; command that lifecycle (LC) processing has completed, and then as a user, we use the S3 &lt;code&gt;HeadObject&lt;/code&gt; API call to query the status of the object:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# radosgw-admin lc list| jq .[1]
{
  &quot;bucket&quot;: &quot;:databucket:fcabdf4a-86f2-452f-a13f-e0902685c655.310403.1&quot;,
  &quot;shard&quot;: &quot;lc.23&quot;,
  &quot;started&quot;: &quot;Tue, 26 Nov 2024 21:32:15 GMT&quot;,
  &quot;status&quot;: &quot;COMPLETE&quot;
}

# aws --profile tiering --endpoint https://s3.cephlabs.com s3api  head-object --bucket databucket --key 2gb
{
    &quot;AcceptRanges&quot;: &quot;bytes&quot;,
    &quot;LastModified&quot;: &quot;2024-11-26T21:32:48+00:00&quot;,
    &quot;ContentLength&quot;: 0,
    &quot;ETag&quot;: &quot;\&quot;b459c232bfa8e920971972d508d82443-60\&quot;&quot;,
    &quot;ContentType&quot;: &quot;binary/octet-stream&quot;,
    &quot;Metadata&quot;: {},
    &quot;StorageClass&quot;: &quot;ibm-cos&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As an admin, we can use the radosgw-admin bucket stats command to check the space used. We can see that &lt;code&gt;rgw.main&lt;/code&gt; is empty, and our &lt;code&gt;rgw.cloudtiered&lt;/code&gt; placement is the only one with data stored.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# radosgw-admin bucket stats --bucket databucket | jq .usage
{
  &quot;rgw.main&quot;: {
    &quot;size&quot;: 0,
    &quot;size_actual&quot;: 0,
    &quot;size_utilized&quot;: 0,
    &quot;size_kb&quot;: 0,
    &quot;size_kb_actual&quot;: 0,
    &quot;size_kb_utilized&quot;: 0,
    &quot;num_objects&quot;: 0
  },
  &quot;rgw.multimeta&quot;: {
    &quot;size&quot;: 0,
    &quot;size_actual&quot;: 0,
    &quot;size_utilized&quot;: 0,
    &quot;size_kb&quot;: 0,
    &quot;size_kb_actual&quot;: 0,
    &quot;size_kb_utilized&quot;: 0,
    &quot;num_objects&quot;: 0
  },
  &quot;rgw.cloudtiered&quot;: {
    &quot;size&quot;: 1604857600,
    &quot;size_actual&quot;: 1604861952,
    &quot;size_utilized&quot;: 1604857600,
    &quot;size_kb&quot;: 1567244,
    &quot;size_kb_actual&quot;: 1567248,
    &quot;size_kb_utilized&quot;: 1567244,
    &quot;num_objects&quot;: 3
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that the object has transitioned to our IBM COS Cloud tier let&#39;s restore it to our Ceph Cluster using the S3 &lt;code&gt;RestoreObject&lt;/code&gt; API call. In this example, we&#39;ll request a temporary restore and set the expiration to three days:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api restore-object --bucket databucket --key 2gb --restore-request Days=3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we attemp to get an object that is still being restored, we get an error message like this:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api  get-object --bucket databucket --key 2gb /tmp/2gb
An error occurred (RequestTimeout) when calling the GetObject operation (reached max retries: 2): restore is still in progress
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using the S3 API, we can issue the &lt;code&gt;HeadObject&lt;/code&gt; call and check the status of the &lt;code&gt;Restore&lt;/code&gt; attribute. In this example, we can see how our restore from the IBM COS cloud endpoint to Ceph has finished, as &lt;code&gt;ongoing request&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt;. We have an expiry date for the object, as we used the &lt;code&gt;RestoreObject&lt;/code&gt; call with &lt;code&gt;--restore-request days=3O&lt;/code&gt;. Other things to check from this output: the occupied size of the object on our local Ceph cluster is 2GB, as expected once it is restored. Also, the storage class is &lt;code&gt;ibm-cos&lt;/code&gt;. As noted before for temporarily transitioned objects, even when using the &lt;code&gt;STANDARD&lt;/code&gt; Ceph RGW storage class, we still keep the &lt;code&gt;ibm-cos&lt;/code&gt; storage class. Now that the object has been restored, we can issue an S3 &lt;code&gt;GET&lt;/code&gt; API call from the client to access the object.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api  head-object --bucket databucket --key 2gb
{
    &quot;AcceptRanges&quot;: &quot;bytes&quot;,
    &quot;Restore&quot;: &quot;ongoing-request=\&quot;false\&quot;, expiry-date=\&quot;Thu, 28 Nov 2024 08:46:36 GMT\&quot;&quot;,
    &quot;LastModified&quot;: &quot;2024-11-27T08:36:39+00:00&quot;,
    &quot;ContentLength&quot;: 2000000000,
    &quot;ETag&quot;: &quot;\&quot;\&quot;0c4b59490637f76144bb9179d1f1db16-382\&quot;\&quot;&quot;,
    &quot;ContentType&quot;: &quot;binary/octet-stream&quot;,
    &quot;Metadata&quot;: {},
    &quot;StorageClass&quot;: &quot;ibm-cos&quot;
}

# aws --profile tiering --endpoint https://s3.cephlabs.com s3api  get-object --bucket databucket --key 2gb /tmp/2gb
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;s3-restoreobject-permanent-restore&quot;&gt;S3 RestoreObject Permanent Restore &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#s3-restoreobject-permanent-restore&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Restored data in a permanent restore will remain in the Ceph cluster indefinitely, making it accessible as a regular object. Unlike temporary restores, no expiration period is defined, and the object will not revert to a stub after retrieval. his is suitable for scenarios where long-term access to the object is required without additional re-restoration steps.&lt;/p&gt;&lt;h4 id=&quot;lifecycle-transition-rules-are-reinstated&quot;&gt;Lifecycle Transition Rules Are Reinstated &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#lifecycle-transition-rules-are-reinstated&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Once permanently restored, the object is treated as a regular object within the Ceph cluster. All lifecycle rules (such as transition to cloud storage or expiration policies) are reapplied, and the restored object is fully integrated into the bucket&#39;s data lifecycle workflows.&lt;/p&gt;&lt;h4 id=&quot;default-storage-class-for-restored-data-1&quot;&gt;Default Storage Class for Restored Data &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#default-storage-class-for-restored-data-1&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;By default, permanently restored objects are written to the &lt;code&gt;STANDARD&lt;/code&gt; storage class within the Ceph cluster. Unlike temporary restores, the object’s &lt;code&gt;x-amz-storage-class&lt;/code&gt; header will reflect the &lt;code&gt;STANDARD&lt;/code&gt; storage class, reflecting permanent residency in the cluster.&lt;/p&gt;&lt;h3 id=&quot;s3-restoreobject-api-permanent-restore-walkthrough&quot;&gt;S3 RestoreObject API permanent restore walkthrough &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#s3-restoreobject-api-permanent-restore-walkthrough&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Restore the object permanently by not supplying a number of days to the &lt;code&gt;--restore-request&lt;/code&gt; argument:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api restore-object --bucket databucket --key hosts2 --restore-request {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify the restored object: it&#39;s part of the &lt;code&gt;STANDARD&lt;/code&gt; storage class, so the object is a first-class citizen of the Ceph cluster, ready for integration into broader operational workflows.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api head-object --bucket databucket --key hosts2
{
    &quot;AcceptRanges&quot;: &quot;bytes&quot;,
    &quot;LastModified&quot;: &quot;2024-11-27T08:28:55+00:00&quot;,
    &quot;ContentLength&quot;: 304,
    &quot;ETag&quot;: &quot;\&quot;01a72b8a9d073d6bcae565bd523a76c5\&quot;&quot;,
    &quot;ContentType&quot;: &quot;binary/octet-stream&quot;,
    &quot;Metadata&quot;: {},
    &quot;StorageClass&quot;: &quot;STANDARD&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;object-read-through(%2Fget)-mode&quot;&gt;Object Read-Through(/GET) Mode &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#object-read-through(%2Fget)-mode&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Objects accessed through the Read-Through Restore mechanism are restored temporarily into the Ceph cluster. When a &lt;code&gt;GET&lt;/code&gt; request is made for a cloud-transitioned object, the system retrieves the object from the cloud tier asynchronously. It makes it available for a specified duration defined by the &lt;code&gt;read_through_restore_days&lt;/code&gt; value. After the expiry period, the restored data is deleted, and the object reverts to its stub state, retaining metadata and transition configurations.&lt;/p&gt;&lt;h4 id=&quot;object-read-through-(%2Fget)-mode-walkthrough&quot;&gt;Object Read-Through (/GET) Mode walkthrough &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#object-read-through-(%2Fget)-mode-walkthrough&quot;&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Before enabling read-through mode if we try to access a stub object in our local Ceph cluster that has been transitioned to a remote S3 endpoint via policy-based archival, we will get the following error message:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api get-object --bucket databucket --key 2gb6 /tmp/2gb6
An error occurred (InvalidObjectState) when calling the GetObject operation: Read through is not enabled for this config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So let&#39;s first enable read-through mode. As a Ceph admin we need to modify our current &lt;code&gt;ibm-cos&lt;/code&gt; cloud-tier storage class and add two new tier-config parameters: &lt;code&gt;--tier-config=allow_read_through=true,read_through_restore_days=3&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# radosgw-admin zonegroup placement modify --rgw-zonegroup default \
  --placement-id default-placement --storage-class ibm-cos \
  --tier-config=allow_read_through=true,read_through_restore_days=3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you have not performed any previous multisite configuration, a default zone and zonegroup are created for you, and changes to the zone/zonegroup will not take effect until the Ceph Object Gateways (RGW daemons) are restarted. If you have created a realm for multisite, the zone/zonegroup changes will take effect once the changes are committed with &lt;code&gt;radosgw-admin period update --commit&lt;/code&gt;. In our case, it&#39;s enough to restart RGW daemons to apply changes:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# ceph orch restart rgw.default
Scheduled to restart rgw.default.ceph02.fvqogr on host &#39;ceph02&#39;
Scheduled to restart rgw.default.ceph03.ypphif on host &#39;ceph03&#39;
Scheduled to restart rgw.default.ceph04.qinihj on host &#39;ceph04&#39;
Scheduled to restart rgw.default.ceph06.rktjon on host &#39;ceph06&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once read-trough mode is enabled, and the RGW services are restated when a &lt;code&gt;GET&lt;/code&gt; request is made for an object in the cloud tier, the object will automatically be restored to the Ceph cluster and served to the user.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;# aws --profile tiering --endpoint https://s3.cephlabs.com s3api  get-object --bucket databucket --key 2gb6 /tmp/2gb6
{
    &quot;AcceptRanges&quot;: &quot;bytes&quot;,
    &quot;Restore&quot;: &quot;ongoing-request=\&quot;false\&quot;, expiry-date=\&quot;Thu, 28 Nov 2024 08:46:36 GMT\&quot;&quot;,
    &quot;LastModified&quot;: &quot;2024-11-27T08:36:39+00:00&quot;,
    &quot;ContentLength&quot;: 2000000000,
    &quot;ETag&quot;: &quot;\&quot;\&quot;0c4b59490637f76144bb9179d1f1db16-382\&quot;\&quot;&quot;,
    &quot;ContentType&quot;: &quot;binary/octet-stream&quot;,
    &quot;Metadata&quot;: {},
    &quot;StorageClass&quot;: &quot;ibm-cos&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;future-work&quot;&gt;Future Work &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#future-work&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ceph developers are improving the policy-based data retrieval feature with upcoming enhancements that include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Tape/DiamondBack Support&lt;/em&gt;: Using the &lt;code&gt;RestoreObject&lt;/code&gt; API to fetch objects instead of &lt;code&gt;GET&lt;/code&gt; from S3 endpoints that use the Glacier API&lt;/li&gt;&lt;li&gt;&lt;em&gt;Admin Commands for Enhanced Monitoring&lt;/em&gt;: Including restore status, listing restored/in-progress objects, and re-triggering restore operations for failures&lt;/li&gt;&lt;li&gt;&lt;em&gt;Compression and Encryption Support&lt;/em&gt;: Restoring compressed or encrypted objects effectively&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#conclusion&quot;&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Policy-based data retrieval for Ceph Storage is a crucial addition that enhances current object storage tiering capabilites. Feel free to share your thoughts or questions about this new feature on the &lt;a href=&quot;https://ceph.io/en/community/connect/&quot;&gt;ceph-users mailing list&lt;/a&gt;. We’d love to hear how you plan to use it or if you’d like to see any aspects enhanced.&lt;/p&gt;&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes &lt;a class=&quot;link-anchor&quot; href=&quot;https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/#footnotes&quot;&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For more information about RGW placement targets and storage classes, visit &lt;a href=&quot;https://docs.ceph.com/en/latest/radosgw/placement&quot;&gt;this page&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For a related take on directing data to multiple RGW storage classes, view &lt;a href=&quot;https://www.youtube.com/watch?v=m0Ok5X2I5Ps&quot;&gt;this presentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The authors would like to thank IBM for supporting the community by facilitating our time to create these posts.&lt;/p&gt;&lt;/div&gt;</description><link>https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/</link><guid isPermaLink="false">https://ceph.io/en/news/blog/2025/rgw-tiering-enhancements-part2/</guid><pubDate>Fri, 27 Dec 2024 00:00:00 GMT</pubDate><author>Daniel Parkes, Anthony D&#39;Atri (IBM)</author></item></channel></rss>