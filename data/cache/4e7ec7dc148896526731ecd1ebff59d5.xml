<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://rsshub.email-once.com/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 19 Mar 2025 21:23:39 GMT</lastBuildDate><ttl>5</ttl><item><title>昇腾 MindSpeed：分布式训练加速库的创新实践｜QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京盛大召开。此次大会以 “智能融合，引领未来” 为主题，汇聚各领域技术先锋与创新者，共同探讨行业发展新趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;华为高级工程师郑加利已确认出席，并发表题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6409&quot;&gt;昇腾 MindSpeed：分布式训练加速库的创新实践与突破&lt;/a&gt;&quot;》的主题分享。大模型训练过程复杂，面临着模型参数量和计算量剧增、单卡计算能力不足、大规模集群计算复杂性增加等诸多挑战。在这样的背景下，昇腾 MindSpeed 分布式训练加速库通过多维度优化，有效提升了大模型训练效率。本次演讲中，郑加利将详细介绍其创新实践与突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑加利主导了 MindSpeed 框架基础架构设计构建，推动大模型训练效率显著提升，还主导微软大模型训练框架 DeepSpeed 原生支持华为昇腾软件栈，拓展了昇腾生态兼容性。此外，他深度参与华为昇腾重点模型开发和客户项目攻关，多次荣获昇腾领域总裁嘉奖令。本次会议中，他的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲大纲：大模型训练的趋势与挑战模型规模与计算需求：大模型参数量与计算量呈指数级增长。分布式训练的复杂性：大规模集群计算带来诸多挑战。效率瓶颈：现有解决方案存在局限性。业界加速套件主流加速库的特点与不足：分析主流加速库的优势与短板。昇腾 MindSpeed 的差异化优势：阐述昇腾 MindSpeed 相比其他加速库的独特优势。MindSpeed 架构设计整体架构概览：介绍 MindSpeed 的整体架构。核心模块与功能：讲解 MindSpeed 的核心模块及其功能。MindSpeed 优化策略通信优化：采用高效通信协议与算法；分享通信性能提升案例。内存优化：运用显存管理与优化技术；展示内存优化的实际效果。计算优化：进行算法加速与硬件适配；点明计算效率提升的关键点。并行优化：灵活组合并行策略并实践。MindSpeed 的实战效果性能提升案例：展示具体模型训练的加速效果。效率提升：体现训练时间缩短与资源利用率提升。行业应用：介绍 MindSpeed 在不同场景中的落地实践。未来展望昇腾 MindSpeed 的发展方向：展望 MindSpeed 未来的发展路径。对大模型训练的持续支持与创新：阐述对大模型训练持续创新和支持的计划。您认为，这样的技术在实践过程中有哪些痛点？在此次演讲中提到的大部分技术点都属于使用场景广泛，成本较小。并行优化中会有一些优化点开发工作量相对较多。演讲亮点：显存优化中通过 BF16 与 FP32 数据格式同指数位的特点，共享显存地址，以节省显存开销，当前业界无此方案。听众收益：了解昇腾分布式训练加速库的一些前沿的技术和成果。开拓一些新思路，用新想法解决大模型中的显存和通信耗时问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/tWIjflx2biTSGQCNCtzz</link><guid isPermaLink="false">https://www.infoq.cn/article/tWIjflx2biTSGQCNCtzz</guid><pubDate>Wed, 19 Mar 2025 15:37:17 GMT</pubDate><author>QCon全球软件开发大会</author><category>华为</category><category>AI&amp;大模型</category></item><item><title>当大模型接管编程：NASA 疯狂的“反人类”编程要求，为何仍被奉为行业圣典？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在软件工程领域，有些 “老派” 的方法和理念，是经过时间检验的真理，值得我们重新审视和学习。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大多数大型软件开发项目都会使用编码规范，旨在规定编写软件的基本规则：代码应如何构建，以及应该使用和避免哪些语言特性，尤其是在代码的正确性会对设备产生决定性影响的领域，如潜水艇、飞机、将宇航员送上同步轨道的航天器，以及距离居民区仅几公里之外的核电站等设施运行的控制代码等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在众多编码规范中，NASA 的编码规则以其严苛性和有效性反复被提起。近期，油管博主 ThePrime Time 发布的解读 NASA 安全编码规则的视频，甚至短时间内引发了超百万观看。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别是在 AI 编程和“氛围编程”流行的当下，重新审视严谨、可验证的编程规范，是对软件工程本质的回归。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有声音说，“老派的 NASA 编码方式是最好的方式。”也有人评价，“在 C 语言中使用这些标准的编码人员是真正的战士。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/00/003439c1289c033d5a8e2083e3d8c125.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/20/20d668f695f36b9b9a1a85fe14fdb941.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;NASA 程序员在编写航天设备运行代码时都遵守一套严格的规则，这套编码规则由 NASA 喷气推进实验室（JPL）首席科学家 Gerard J. Holzmann 所提出，名为《The Power of Ten – Rules for Developing Safety Critical Code1》（十倍力量：安全关键代码开发规则）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其在开头指出，“大多数现有的规范包含远远超过 100 条规则，而且有些规则的合理性存疑。有些规则，特别是那些试图规定程序中空白使用方式的规则（提到了 Python），可能是出于个人偏好而制定的。其他一些规则则是为了防止同一组织早期编码工作中出现的非常特定且不太可能发生的错误类型。毫不奇怪，现有编码规范对开发人员实际编写代码的行为影响甚微。许多规范最致命的方面是它们很少允许进行全面的基于工具的合规性检查。基于工具的检查很重要，因为对于大型应用程序编写的数十万行代码，手动审查通常是不可行的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time 对此表达了强烈地赞同，称“确实有很多个人偏好被写入了代码规范中。我认同目前提到的所有内容，代码就应该可靠。自动化和工具的使用应该杜绝个人偏好。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;NASA 的编码规则主要针对 C 语言，力求优化更全面检查用 C 语言编写的关键应用程序可靠性的能力。原因是，“在包括 JPL 在内的许多组织中，关键代码都是用 C 语言编写的。由于其悠久的历史，这种语言有广泛的工具支持，包括强大的源代码分析器、逻辑模型提取器、度量工具、调试器、测试支持工具，以及成熟稳定的编译器选择。因此，C 语言也是大多数已开发的编码规范的目标。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time 表示，“我知道现在有很多软件开发人员，一听到用 C 语言编写安全关键代码，可能就会想‘怎么又是这个’ 。你们可没有像 ‘旅行者号’ （NASA 研制的太空探测器）那样的项目，你们还不是顶尖开发者。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，Holzmann 认为，“为了有效，规则集必须很小，并且必须足够清晰，以便于理解和记忆。规则必须足够具体，以便可以机械地进行检查。当然，这么小的规则集不可能涵盖所有情况，但它可以为我们提供一个立足点，对软件的可靠性和可验证性产生可衡量的影响。”因此，他将 NASA 的编码规则限制在十条。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这十条规则正在 NASA 喷气推进实验室用于关键任务软件的编写实验，取得了令人鼓舞的成果。据 Holzmann 介绍，一开始，NASA 的开发人员对遵守如此严格的限制存在合理的抵触情绪，但克服之后，他们常常发现，遵守这些规则确实有助于提高代码的清晰度、可分析性和安全性。这些规则减轻了开发人员和测试人员通过其他方式确定代码关键属性（例如终止性、有界性、内存和栈的安全使用等）的负担。“这些规则就像汽车上的安全带，起初可能有点让人不舒服，但过一段时间后，使用它们会成为习惯，不使用反而难以想象。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time 最后对 NASA 编码规则给出的整体评价是，“我喜欢这份文档，即便我并非完全认同其中所有的规则。我只是很惊讶，政府机构编写的内容竟如此条理清晰。这是一份极其连贯的文档，似乎出自一位追求务实的人之手。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不少与 NASA 工程师共事过的开发者们，都对这则 NASA 十大编码规则的解读视频深有感触：“他们的编码指南并不‘疯狂’，反而实际上相当理智。我们没有以这种方式编程才是疯狂的”，并分享了许多个人的相关经历。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e7/e7a0e9100c478a92b1dccee188f539c1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“在学习 C 语言的时候，我的教授曾为卫星编写 C 程序 / 代码。他把自己的方法教给了我们，这种方法要求我们在电脑上编程之前，先把所有内容都写在纸上。这种方式迫使我们准确理解自己正在编写的内容、内存分配等知识，还能编写出更高效的代码，并掌握相关知识。我很庆幸自己是通过这种方式学习的，因为在面试时，我能轻松地在白板上编写代码。”一名工程师说。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一位与前 NASA 工程师共同开发过游戏的程序员透露，“他的代码是我见过的最整洁、最易读的。当时我还是一名初级程序员，仅仅通过和他一起编写代码，我就学到了很多东西。我们使用的是 C++ 语言，但他的编程风格更像是带有类的 C 语言。他的代码本身就很易于理解（具有自解释性），不过他仍然对自己的代码进行了注释（既有代码中的注释，也有实际的文档说明）。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一位自述“和 NASA 一位级别很高的程序员关系非常密切”的开发者表示，“我听过很多故事，这些故事都能说明制定所有这些标准的合理性。客观来讲，从 Java 1.5 升级到 1.7 的成本，比从零开始重建任务控制中心（MCC）还要高。而重建任务控制中心是用 C 语言完成的，其中另一位首席工程师曾是 C++ 专家，他认定最初的 C 语言更可靠。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时有前 NASA 工程师出来现身说法道，“曾参与构建云基础设施，他们的指导原则可不是闹着玩的，代码审查简直是人间炼狱。‘严苛’这个词用来形容再贴切不过了。不过，相比我之前在电信、金融科技领域的工作经历，以及后来在其他科技公司的工作，我在 NASA 工作期间对可靠性方面的了解要多得多。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“NASA 的编码要求太疯狂了”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于这十条规则，Holzmann 已经声明，“为了支持强大的审查，这些规则有些严格，甚至可以说严苛。但这种权衡是有道理的。在关键时候，尤其是开发安全关键代码时，多费些功夫，遵守更严格的限制是值得的。这样我们就能更有力地证明关键软件能按预期运行。”并且，每条规则之后都附了其被纳入的简短理由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time 对这些编码规则及理由一一进行了评价和分析，以下是经不改变原意的翻译和编辑后整理出来的解读内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则一：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将所有代码限制在非常简单的控制流结构中，不要使用 goto 语句、setjmp 或 longjmp 结构以及直接或间接递归。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：更简单的控制流意味着更强的验证能力，并且通常能提高代码的清晰度。禁止递归可能是这里最让人意外的一点。不过，如果没有递归，我们就能保证有一个无环的函数调用图，这能被代码分析器利用，还能直接帮助证明所有本应有限的执行实际上都是有限的。（注意，这条规则并不要求所有函数都有一个单一的返回点——尽管这通常也会简化控制流。不过，有很多情况下，提前返回错误是更简单的解决方案。）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 我不知道间接递归是什么意思。间接递归是指两个函数相互调用吗？在实际情况中，这种情况确实会发生，而且发生过很多次。比如有一个函数需要调用另一个函数去做某些事情并进行一些检查，然后再通过某种不受你控制的方式返回结果。特别是在那些没有异步 / 等待（async/await）机制的语言里，我猜你只能阻塞线程了，对吧？规则是说如果没有异步机制，就只能阻塞线程，然后在一个while循环里处理，是这样吗？我得想想我自己使用间接递归的场景。实际上，我在处理套接字重连时就用到了间接递归。具体来说，当我打开一个套接字（就像这样，打开这个套接字），重连机制会调用一个私有函数来重置状态，然后调用reconnect函数，reconnect函数会调用connect函数，当连接断开时，connect函数又会再次调用自己。从技术上讲，这就是一种间接递归。我在想，也许我可以把它改成用while循环加上等待机制，这样会不会更简单呢？现在真的让我开始思考这个问题了，这可能只是一种替代方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哎呀，我已经违反规则一了。不过间接递归确实是一种非常强大的无限问题解决机制，但我可以尝试不用它，对吧？我完全不介意尝试新的做法。理由很简单，“更简单的控制流意味着更强的验证能力，并且通常能提高代码的清晰度。”我认同这一点，确实看到代码里有类似这样的逻辑时会觉得有点绕。比如在一个 close 函数中调用 reconnect，然后在 reconnect 中检查是否已经启动，如果没有完成，就返回。这些语句的顺序会导致问题，所以我可以理解为什么会出现这种情况。我甚至可以说服自己接受这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说实话，这是一个非常务实的观点，解释了为什么不应该使用递归。而且说句公道话，我大学三四年级的时候（不对，其实是大二），老师让我们用非递归的方式实现 AVL 树。如果你不熟悉 AVL 树，这是一种使用旋转的自平衡二叉搜索树，有四种不同的旋转：右旋、左旋、先右后左旋和先左后右旋。做起来其实很有趣，假设我们有一个二叉树，像这样：a（根节点），b 是左子节点，c 是右子节点。我们想重新组织成 b 作为根节点，a 作为左子节点，c 作为右子节点。如果你有一棵二叉树，看起来像 “a b c”，你就把它重组为 “a c b”，然后进行旋转。很简单直接，对吧？老师说我们要实现这个程序，但不能用递归。这对我来说是一次很棒的学习经历。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则二：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所有循环必须有固定的上限。检查工具必须能够轻易地静态证明循环的预设迭代上限不会被突破。如果无法静态证明循环的上限，就视为违反规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：没有递归且存在循环上限可以防止代码失控。当然，这条规则不适用于那些本就不打算终止的迭代（例如在进程调度器中）。在这些特殊情况下，适用相反的规则：必须能静态证明迭代不会终止。支持这条规则的一种方法是，给所有迭代次数可变的循环添加明确的上限（比如遍历链表的代码）。当超过上限时，会触发断言失败，包含失败迭代的函数将返回错误。（关于断言的使用，请参见规则 5）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 这是不是意味着不能使用像数组的forEach这样的方法呢？因为从技术上讲，其上限是根据数组动态变化的，没有固定值。还是说不能使用while (true)这种循环呢？这是一条有趣的规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则三：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;初始化后不要使用动态内存分配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：这条规则在安全关键软件中很常见，并且出现在大多数编码规范里。原因很简单：像 malloc 这样的内存分配器和垃圾回收器，其行为往往不可预测，可能会对性能产生重大影响。一类显著的编码错误也源于对内存分配和释放例程的不当处理，比如忘记释放内存、释放后继续使用内存、试图分配超过实际可用的内存，以及越界访问已分配的内存等等。强制所有应用程序在固定的、预先分配好的内存区域内运行，可以避免很多这类问题，也更容易验证内存的使用情况。需要注意的是，在不使用堆内存分配的情况下，动态申请内存的唯一方法是使用栈内存。在没有递归的情况下（规则 1），可以静态推导出栈内存使用的上限，从而可以证明应用程序将始终在其预先分配的内存范围内运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 这么一来，JavaScript 和 Go 语言可就有点麻烦了。不过说实在的，其实在 JavaScript 和 Go 里也能做到这一点。显然，在大多数情况下是可以的。但我敢肯定，只要调用类似G Funk（这里可能是随意提及的某个函数）这样的函数，它背地里肯定会分配一些你不知道的内存。当然，不是所有解释型语言都这样，这么说不太准确，不是所有解释型语言都有这个问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我猜这条规则意味着不能使用闭包，对吧？因为闭包会涉及到内存分配。准确地说，你得使用内存池（Arena）进行内存分配。也就是说，不能随意使用列表或字符串吗？也不是，其实可以使用列表和字符串，只是意味着所有内存分配都必须在程序开始时完成。我猜这里说的是堆内存，而不是栈内存。另外，我是这么理解的，比如说你从服务器获取一系列响应数据，你得事先分配一块足够大的内存区域，用来存储所有可能的响应数据，然后像使用环形缓冲区一样循环利用这块内存，这样就不会有额外的内存分配操作了，所有可能用到的数据都已经预先分配好了。所以一开始你就应该拥有所需的所有内存，这意味着可以使用字符串，只是得预先定义好字符串占用内存的大小。天呐，这得好好琢磨琢磨，确实很费脑筋，不过环形缓冲区的概念真的很有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“在不使用堆内存分配的情况下，动态申请内存的唯一方式是使用栈内存。根据规则一，在没有递归的情况下，可以静态推导出栈内存使用的上限，这样就能证明应用程序始终在其预先分配的内存范围内运行。”这听起来有点疯狂，但其实挺酷的，仔细想想还挺有道理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我听说在游戏开发里有这样一种做法，如果我说错了也请大家指正。在游戏开发中，每个子团队会有各自的资源预算，包括内存和 CPU 时间。在你负责的程序部分，你只能使用分配给你的那部分资源。一旦超出预算，就会有类似这样的提示：“嘿，物理模拟团队，你们用的时间太多了，能不能想想办法？” 我觉得这听起来挺不错的。我知道有这么回事，我举的这个例子是希望普通的游戏开发者也能理解。就好比今年两家小的游戏工作室因为资源超支没拿到奖金，大致就是这么个情况。宽泛来讲，实际情况比在 Twitch 聊天里说的要复杂一些，但差不多就是这样。我只是以一个普通游戏开发者的角度来解释这个规则，我开发过一些小游戏，但我也知道自己还算不上专业的游戏开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则四：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;任何函数的长度都不应超过以标准参考格式打印在一张纸上的长度，即每行写一条语句、每行写一个声明。通常情况下，这意味着每个函数的代码行数不应超过 60 行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：每个函数都应该是代码中的一个逻辑单元，可以作为一个单元来理解和验证。跨越计算机显示器多个屏幕或打印时多页的逻辑单元要难得多。过长的函数往往是代码结构不佳的表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 好的，这挺合理的。60 行代码的空间足够你把事情弄清楚了。鲍勃大叔（Uncle Bob ，著名编程大师 Robert C. Martin）规定每个函数一般只能有三到五行代码，相比之下 60 行代码算很多了。不过这里说的是打印在一张纸上，对吧？就是说代码打印在单张纸上，大概就是这个意思。注意，这其实也不算特别严格的硬性规定，但从能打印在纸上这个角度来说，它又算是个硬性规定。我觉得 60 行代码能表达很多内容，肯定有办法打破这个规则，但我感觉自己通常能轻松写出最多 60 行代码的函数，我觉得这一点都不难。没错，Ghost 标准库有数千行代码，但我不会把 Ghost 标准库当作史上最整洁、最出色的代码之一。老实说，我个人觉得 Ghost 标准库读起来真的很糟糕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这条规则的理由是，每个函数都应该是代码中的一个逻辑单元，能够作为一个整体被理解和验证。如果一个逻辑单元跨越计算机显示器的多个屏幕，或者打印出来有好多页，理解起来就困难得多。过长的函数往往意味着代码结构不佳。我基本上同意这个观点，我觉得实际上很少能见到超过 60 行代码的函数。而且一般来说，当你遇到这样的函数时，要么是因为功能本身非常复杂，由于行为的关联性必须写在一起；要么这个函数写得很糟糕。除非你写 React 代码，我觉得我说的这些还是适用的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则五：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;代码的断言密度平均每个函数至少应有两个断言。断言用于检查在实际执行中不应发生的异常情况。断言必须始终无副作用，并且应定义为布尔测试。当断言失败时，必须采取明确的恢复措施，例如，向执行失败断言的函数的调用者返回错误条件。任何静态检查工具能够证明永远不会失败或永远不会成立的断言都违反此规则。（也就是说，不能通过添加无用的 “assert (true)” 语句来满足该规则。）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：工业编码工作的统计数据表明，单元测试通常每编写 10 到 100 行代码就能发现至少一个缺陷。断言密度越高，拦截缺陷的几率就越大。断言的使用通常也被推荐作为强防御性编码策略的一部分。断言可用于验证函数的前置和后置条件、参数值、函数返回值以及循环不变式。由于断言无副作用，因此在测试后，可以在对性能关键的代码中有选择地禁用它们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 我很喜欢这条规则。我觉得它有很多优点，而且我觉得自己需要更多地实践这条规则。我还得继续坚持，因为就像我之前说的，我的代码库里已经有不少断言了。我确实经常使用断言，但目前我代码里的断言一旦触发，程序就会直接崩溃。比如说，当程序内部生成的消息与我预期的不一致时，我就会认为自己犯了严重错误，觉得整个程序都得“炸掉” ，结果整个程序就会受到影响。有一点很棒，如果你能想出某种模糊测试策略，就能测试你的程序。你可以往程序里输入一堆随机数据，而程序里应该有防御性的语句，以确保不会触发更多的断言。这样一来，你甚至可以减少很多针对模糊测试的特定单元测试，这是不是很神奇？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面是一个典型的断言使用示例：如果条件 C 成立，且断言 p 大于等于为真，就返回错误。假设断言定义如下：定义一个名为c_assert的断言，用于调试，当断言失败时，输出文件和行号等信息，这里设置为false 。在这个定义中，file和line由宏预处理器预先定义，用于输出失败断言所在的文件名和行号。语法#e会将断言条件e转换为字符串，作为错误消息的一部分打印出来。在嵌入式程序代码中，通常没有地方打印错误消息，在这种情况下，对测试调试的调用会变成空操作，断言就变成了纯粹的布尔测试。这有助于从异常行为中恢复错误。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我开始接触这 “十条规则” 的契机是，有个来自 Tiger Beetle（一个项目）的人加入了我们。他们在 Tiger Beetle 项目中对断言的使用非常严格。他可以往 Tiger Beetle 里输入任何数据，而程序始终能正常运行。他们每天在每次构建时，都会进行相当于 200 年查询量的测试，程序不断接受大量测试，而且运行得非常稳定。这真是个超酷的项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则六：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据对象必须在尽可能小的作用域级别声明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：这条规则支持数据隐藏的基本原则。显然，如果一个对象不在作用域内，其值就不能被引用或破坏。同样，如果必须诊断一个对象的错误值，可能分配该值的语句越少，诊断问题就越容易。该规则不鼓励将变量重复用于多个不兼容的目的，这可能会使故障诊断复杂化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 有人说这只是因为 C 语言没有恰当的错误处理和语法特性，我强烈反对这种说法。实际上，断言非常有用。比如说，Tiger Beetle 项目是用 Zig 语言编写的，Zig 有恰当的错误处理工具，它有结果对象，而且其自身的错误处理结果对象能提供比标准错误更好的堆栈跟踪信息，这真的很酷。我记得在采访时，他说当时 Tiger Beetle 项目里有 8000 个断言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;没错，断言不是错误处理机制，实际上断言不是用于处理错误的，它是一种不变式，可以说是硬性终止条件。我们来看看，这和在 Zig 语言里直接返回错误有什么不同呢？在 Zig 里，你可以返回一个错误或者一个值，但这就是一种错误处理方式。Zig 里不会硬性终止程序，它必须有恢复机制。我觉得这样也挺好，无论是硬性终止，还是软性终止并搭配某种错误恢复机制，都需要构建相应的错误恢复机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我其实并没有完全理解规则六，除了感觉好像是说在使用变量的地方定义它，这样作用域就是最小的，是这个意思吗？听起来好像是这样，这里是在说封装的概念吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则七：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非 void 函数的返回值必须由每个调用函数检查，并且每个函数内部必须检查参数的有效性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：这可能是最常被违反的规则，因此作为一般规则有些可疑。从严格意义上讲，这条规则意味着即使是 printf 语句和文件关闭语句的返回值也必须被检查。不过也有观点认为，如果对错误的响应与对成功的响应没有区别，那么显式检查返回值就没什么意义。这通常是调用 printf 和 close 的情况。在这种情况下，可以接受将函数返回值显式转换为 (void)—— 这表明程序员是有意忽略返回值，而非不小心遗漏。在更可疑的情况下，应该有注释解释为什么返回值无关紧要。不过，在大多数情况下，函数的返回值不应被忽略，尤其是在必须将错误返回值沿函数调用链向上传播的情况下。标准库因违反此规则而臭名昭著，并可能导致严重后果。例如，如果不小心执行 strlen (0)，或者使用标准 C 字符串库执行 strcat (s1, s2, -1)—— 结果就很糟糕。遵循这条通用规则，我们可以确保例外情况必须有合理的解释，并且机械检查器会标记违规行为。通常，遵守这条规则比解释为什么不符合规则更容易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 这实际上是我非常喜欢 Zig 的一个原因。我想 Rust 语言也有类似的情况，只不过当你忽略返回的结果或异步操作的返回值时，Rust 只是给出警告。我喜欢这条规则，我觉得这是一条很棒的规则。这是一条普遍适用的好规则。要知道，编程的很大一部分就是学习这些技巧，避免自己给自己挖坑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则八：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预处理器的使用必须仅限于包含头文件和简单的宏定义。不允许使用令牌粘贴、可变参数列表（省略号）和递归宏调用。所有宏必须展开为完整的语法单元。条件编译指令的使用通常也值得怀疑，但并非总是可以避免。这意味着，即使在大型软件开发项目中，除了避免同一头文件多次包含的标准样板代码外，也很少有理由使用超过一两个条件编译指令。每次此类使用都应通过基于工具的检查器标记，并在代码中说明理由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：C 预处理器是一个强大的混淆工具，可能会破坏代码的清晰度，并使许多基于文本的检查器感到困惑。即使手头有正式的语言定义，不受限制的预处理器代码中的构造的效果也可能极其难以破译。在 C 预处理器的新实现中，开发人员通常不得不求助于使用早期实现作为解释 C 标准中复杂定义语言的裁判。对条件编译持谨慎态度的理由同样重要。请注意，仅使用十个条件编译指令，就可能有多达 2 的 10 次方种可能的代码版本，每种版本都必须进行测试 —— 导致所需的测试工作量大幅增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 我认为对预处理宏保持谨慎肯定没错。我理解代码时遇到的困难，没有比处理预处理宏更多的了。预处理宏真的是最难懂的部分之一。这条规则实际上会让我们的开发工作变得更加复杂，我一点都不喜欢。有意思的是，总体来说我不喜欢预处理器。我能理解预处理器肯定会引发一堆问题，人们通常把它们叫做宏。一般来说，宏可能很有用，但它们通常也非常难以理解，理解起来特别费劲。谢天谢地 C 语言没有宏（这里表述有误，C 语言有宏，作者可能想表达宏的复杂性让人头疼 ）。宏是一种强大的工具，但就像所有强大的工具一样，它们非常危险。预处理器是一个强大的混淆工具，会破坏代码的清晰度，让许多基于文本的检查器感到困惑。即使手头有正式的语言定义，不受限制的预处理代码中的结构效果也极难解读。在 C 预处理器的新实现中，开发人员常常不得不借助早期的实现来解读 C 标准中的复杂定义语言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对条件编译保持谨慎的理由同样重要。要知道，仅仅 10 个条件编译指令就可能产生多达 2 的 10 次方种代码版本，每个版本都必须进行测试，这会大幅增加所需的测试工作量。我是说，这一点非常关键。一般来说，条件编译就是一场噩梦，尽管有时又不得不使用它。我觉得这条规则务实的地方在于，它认识到虽然无法避免使用条件编译，但条件编译确实非常困难且麻烦。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;没错，我猜 Rust 语言的 cargo 特性主要是因为 Rust 编译速度慢才存在的。我认识的大多数人对 Rust 二进制文件不太感兴趣，更多的是担心编译时间太慢。我敢肯定，最终 Rust 二进制文件非常重要，但就我所知，很多时候问题就出在编译速度上。正是因为编译慢，才引出了一系列问题，比如我总是会遇到这样的情况，使用 clap 时忘记添加 feature derive，然后又得去添加；使用 request 时又忘记添加 request feature 之类的，情况越来越糟。你们看过 AutoSAR 的 C 代码吗？我敢肯定那代码很糟糕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则九：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;指针的使用应受到限制。具体来说，允许的解引用级别不超过一级。指针解引用操作不得隐藏在宏定义或 typedef 声明中。不允许使用函数指针。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：即使是经验丰富的程序员也容易误用指针。它们可能使程序中的数据流难以跟踪或分析，尤其是对于基于工具的静态分析器。同样，函数指针可能会严重限制静态分析器可以执行的检查类型，只有在有充分理由使用它们的情况下才应使用，并且理想情况下应提供替代方法来帮助基于工具的检查器确定控制流和函数调用层次结构。例如，如果使用函数指针，工具可能无法证明没有递归，因此必须提供替代保证来弥补分析能力的损失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 比如说，怎么处理异步相关的操作和中断呢？我觉得他们可能不处理异步操作，但我又确定他们肯定会处理。处理异步操作肯定得使用某种互斥锁，对吧？比如使用信号量互斥锁，然后还得涉及对内存的引用。异步操作具有不确定性，所以不太好处理。其实也不是完全不确定，只是你得在一定程度上进行处理。想象一下，你有一个探测器，上面有个小摄像头正在拍照。在某个时刻你拍了照，照片进行处理后存储到内存中，处理完成后会有提示。然后某些代码需要被唤醒，这不也在一定程度上涉及到函数指针吗？我猜得用互斥锁，对吧？所以我猜你会有一段代码，比如说处理拍照的代码。你得在这里进行一些操作，比如获取信号量，在 C 语言里信号量的值为 1，具体是用 lock unlock（锁定解锁 ）还是 lock acquire（获取锁 ）我记不清了。代码就停在这里等待，当照片数据传入后，代码开始处理，处理完之后再回到等待状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我理解这个，不过这条规则感觉更难落实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规则十：&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所有代码从开发的第一天起，就必须在编译器最严格的设置下启用所有编译器警告进行编译。所有代码必须在此设置下编译且不发出任何警告。所有代码必须每天至少使用一个，但最好是多个最先进的静态源代码分析器进行检查，并且应以零警告通过分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理由：如今市场上有几种非常有效的静态源代码分析器，还有相当多的免费工具。任何软件开发工作都没有理由不使用这种现成的技术。即使对于非关键代码的开发，也应将其视为常规做法。零警告规则甚至适用于编译器或静态分析器给出错误警告的情况：如果编译器或静态分析器感到困惑，应重写导致困惑的代码，使其更简单有效。很多开发者一开始认为某个警告肯定是无效的，结果后来才意识到，由于一些不那么明显的原因，该警告实际上是合理的。早期的静态分析器，比如 lint，大多会给出无效的提示信息，这让静态分析器的名声有些不好，但现在情况已经不同了。当今最好的静态分析器速度快，并且会生成有针对性且准确的提示消息。在任何一个严肃的软件项目中，它们的使用都不应有商量余地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ThePrime Time： 说实话，我觉得这挺合理的。尤其是对于新手而言，如果你刚开始接触软件开发，就应该能够做到这一点。公平地说，我不算专业的软件开发人员，所以我能理解这一点。我觉得这真的很棒，规则 10 简直太实用了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，要把这条规则应用到很多项目中可能会很困难。比如说在 JavaScript 开发中，大家都知道 JavaScript 的 lint 工具体验很差，大多数 ESLint 规则纯粹是些主观的好坏评判标准。比如在处理 Promise 时，使用re和reject作为参数实际上是不好的做法，对吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=JWKadu0ks20&quot;&gt;https://www.youtube.com/watch?v=JWKadu0ks20&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;声明：本文为 InfoQ 整理，不代表平台观点，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/MmfUCdZGpWaW8xoP5juV</link><guid isPermaLink="false">https://www.infoq.cn/article/MmfUCdZGpWaW8xoP5juV</guid><pubDate>Wed, 19 Mar 2025 10:36:47 GMT</pubDate><author>华卫</author><category>编程语言</category></item><item><title>下一代自主智算系统：超大规模集群的工程实践与挑战 | QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基流科技创始人、CEO 胡效赫已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6397&quot;&gt;《下一代自主智算系统：超大规模集群的工程实践与挑战》&lt;/a&gt;&quot;的主题分享，重点探讨基于可扩展、高可用、国产化原则的下一代自主计算系统方案选型，如何通过自研通信库、拥塞控制和负载均衡优化策略、高效能算力调度、自动化集群运维、国产 AI 通信系统、算存协同广域调度等技术构建超大规模自主智算集群，解决算力基础设施卡脖子问题，为前沿大模型发展提供算力支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;胡效赫本科至博士均就读于清华大学，在清华就读博士期间，他已成功实现了全国首个 TB 级网络产品的落地，并在超级计算领域部署了千卡规模的模型通信方案。在高校及公司期间，完成数十万亿参数推荐大模型通信优化，首个软件定义自动化的国家级课题、14 篇网络系统方向 CCF-A 顶级论文，博士和博后期间负责及参与项目的累计经费近 2000 万元，导师和合作导师所参与公司的累积市值 500 亿美金。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 基础设施发展趋势Scale out、Scale up 层面实现超大规模集群建设大模型基础设施全景图大规模智算集群核心痛点及工程要求2. 超大规模自主计算集群方案Galaxy 自主智算集群构建要点可扩展设计：架构设计、通信库、网络优化、并行框架等关键技术高可用设计：自研高效能算力调度、自动化算力运维平台国产化设计：基于国产AI通信系统实现开放通用设计、垄断方案解耦3. 十万卡智算集群解决方案算存协同广域组网调度长距离大模型训练实践您认为，这样的技术在实践过程中有哪些痛点?在 GPU 解耦层面，基于国产芯片的迁移适配、多元异构混合训练需要联合 GPU 厂商支持，且国产 GPU 在 Scale up 层面进展较慢，构建基于国产 GPU 的下一代自主智算系统难度较大在国产 AI 通信层面，目前基于国产交换芯片的交换机最大交换容量是 25.6T（64 个 400 G 端口），三层组网最大支持 65536 张卡，需要创新设计十万卡集群架构演讲亮点国产全栈端到端 AI 通信系统方案：在交换机、集合通信库、拥塞控制调优、网络运维平台、长距和异构通信等方面，通过软硬件协同设计，形成全国产智算网络全栈产品和解决方案丰富的大规模集群项目实施经验：方案中的关键技术累积应用在超过多个大规模智算集群，集群规模累积超过 4 万张国际先进 GPU 卡&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/jbRjBFC6HVo5tEfpKifD</link><guid isPermaLink="false">https://www.infoq.cn/article/jbRjBFC6HVo5tEfpKifD</guid><pubDate>Wed, 19 Mar 2025 06:54:49 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category></item><item><title>Uber 的云旅程：在 x86 世界中拥抱 ARM</title><description>&lt;p&gt;2023 年 2 月，Uber 开始从本地数据中心战略性地迁移到 Oracle 云基础设施（OCI）和 Google 云平台。此次迁移的一个关键环节是将基于 ARM 的计算机集成到以 x86 为主的集群中，以降低成本、提高性价比，并在供应链不稳定的情况下确保硬件灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x86 和 ARM 架构代表了处理器设计中的两种完全不同的理念，它们的区别奠定了数十年来的计算产业格局。x86 处理器通常为计算密集型任务提供更高的峰值性能，但消耗更多电量，这使得它们在电源插座随时可用的台式机和服务器领域占据主导地位；与此同时，ARM 处理器在能效方面表现出色，提供更好的每瓦性能比，使其成为移动设备、嵌入式系统以及日益注重功耗的数据中心的首选架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多架构集成面临的挑战不仅在于部署新的硬件。对于 Uber 的基础设施团队来说，这意味着重新看待多年来完全基于 x86 的基础系统。这一历程也显示出架构假设可以深度渗透到技术栈的每一层中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次转变的基础是 Oracle Cloud Infrastructure 对 Ampere Computing 的 ARM 处理器的战略性引入。这些芯片提供了卓越的能效——这是 ARM 在移动领域的最显著优势，现已扩展到数据中心环境。对于云提供商来说，这意味着大幅节省电力和提高计算密度，从而降低能源成本和物理占用空间要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于 Uber 来说，这些优势与其可持续发展目标完美契合。随着公司努力实现零排放，采用节能计算基础设施是在减少环境影响的同时改善成本结构的重要一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个转换过程从主机级的准备工作开始——创建包含操作系统、内核和基本基础设施组件的 ARM 兼容镜像。主机启动后，团队开始着手构建各种管道，找出了复杂的 Web 依赖关系。Uber 的容器系统依赖于 Makisu，这是一种针对 x86 优化的工具，无法针对 ARM 进行交叉编译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6d/6d29123cc6941b308a7b4e2444857f89.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;为容器镜像构建管道&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;团队没有重写 5,000 多个服务构建流程，而是采用了一种巧妙的引导方法。他们利用 Google Bazel 构建了 Makisu 本身的 ARM 版本，然后就可以原生构建其他服务了。这个看似简单的任务暴露了一种循环依赖关系：Makisu 在 Buildkite 上运行，而 Buildkite 在 Uber 的 Odin 平台上运行，Odin 平台又依赖主机代理——所有这些都是用 Makisu 构建的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;打破这种循环依赖关系需要使用 Bazel 的多架构特性有条不紊地重建每一层。团队从主机代理开始，然后重建 Odin 的组件，接着是 Buildkite，最后是 Makisu。这个基础启用了分布式构建管道，可以生成统一的多架构容器镜像。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然这种方法使构建成本翻倍（每周有超过 400,000 个容器构建），但从经济角度来看，采用 ARM 仍然是有利可图的。分布式构建系统还提供了一个关键优势：它支持逐步、受控的迁移，而不是全有或全无的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;部署系统需要类似的增强。Uber 实施了针对架构的放置约束和自动回退机制，如果出现兼容性问题，这些机制将恢复到 x86。这些保护措施让团队可以逐步迁移服务，同时保持生产可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成功部署他们的第一批基于 ARM 的服务标志着一个技术里程碑，证明了多架构基础设施可以在 Uber 的规模下正常工作。然而，从最初的成功到迁移数千个服务的过程还需要额外的策略和工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着云提供商扩展其处理器架构选项，Uber 和 Bitmovin 等组织展示了将各种计算架构整合到大型基础设施系统中的挑战和潜在好处。Bitmovin 将其编码服务完全迁移到 ARM 处理器的历程，以及 Uber 的经验，为企业如何在大规模范围内应对架构异构性提供了宝贵的见解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Uber&#39;s Cloud Journey: Embracing ARM in an x86 World(https://www.infoq.com/news/2025/02/uber-arm-cloud/)&lt;/p&gt;</description><link>https://www.infoq.cn/article/sRh1LtfFD2k2cWqahMM2</link><guid isPermaLink="false">https://www.infoq.cn/article/sRh1LtfFD2k2cWqahMM2</guid><pubDate>Wed, 19 Mar 2025 06:00:00 GMT</pubDate><author>作者：Claudio Masolo</author><category>编程语言</category></item><item><title>面向 AI Agents 的高性能数据基座：架构和工程实践 | QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;晨章数据创始人、首席架构师 陈亮已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6377&quot;&gt;《面向 AI Agents 的高性能数据基座：架构和工程实践》&lt;/a&gt;&quot;的主题分享，介绍晨章数据团队关于 AI 时代数据基座架构的思考，探讨如何通过该架构解决 AI 原生应用的数据挑战，以及如何在云计算、新硬件环境下实现高性能数据基座的工程实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d7f5e70e5b9f4fb5ac5fd6ba289293aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈亮在数据库领域深耕多年，曾任前微软亚洲研究员首席研究员、微软 SQL Server XML 索引发明人和架构师、微软 Cosmos DB 图数据库架构师。曾在 SIGMOD、VLDB、 ICDE 等国际顶级会议上发表多篇学术论文。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. AI agent 驱动的 AI 原生应用2. AI 原生应用面临的数据挑战多种数据模态数据规模灵活多变管理复杂3. 多模态数据基座数据库解耦基于数据基层的多模数据基座灵活扩缩容多模态一致性访问4. 面向未来的工程实现趋势：云计算，弹性扩缩容趋势：新硬件打造服务 AI agents 高性能数据基座服务5. 总结和展望演讲亮点通过创新的工程实践，将现有开源组件和新的数据基座整合，减少用户使用门槛面向新一代编程框架的实现 (协程，异步 IO 等)听众收益给 AI 原生应用开发者提供数据管理的新思路给 AI Infra 开发者在面向云计算、新一代硬件开发的参考和启发&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/34/340af64de1760a8341210ee42e8a8332.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SFxuYQie6uxp6uKHqblr</link><guid isPermaLink="false">https://www.infoq.cn/article/SFxuYQie6uxp6uKHqblr</guid><pubDate>Wed, 19 Mar 2025 03:53:42 GMT</pubDate><author>QCon全球软件开发大会</author><category>架构</category><category>AI&amp;大模型</category></item><item><title>完成“云转型”后，金蝶宣布下一步目标是 AI 转型</title><description>&lt;p&gt;3 月 18 日，金蝶集团举办 2024 年度业绩交流会，并在会上强调，公司已成功实现云转型，下一目标是 AI 转型，目标到2030年成为全球领先的企业管理 AI 公司。InfoQ 从业绩会现场获悉，在云转型基本完成后，金蝶正以“AI 优先”为核心战略，全方位推进 AI 在企业管理中的落地应用。同时，金蝶集团董事会主席兼CEO 徐少春表示，未来不排除通过并购方式，进一步扩大技术产品和市场优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;优化研发投入：从云计算到AI优先&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据财报数据，金蝶集团2024年实现收入约62.56亿元（人民币，下同），同比增长10.2%；净亏损1.42亿元，同比缩窄32.3%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近年来，金蝶业务已逐步由传统软件向SaaS转型，2024年其云服务收入占比已经达到82%。面向未来，管理层在会上表示，未来增长的核心动力将来自 AI 技术的深度应用，研发投入的方向也随之调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对研发投入方向的变化，金蝶集团执行董事、首席财务官林波在会上表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“金蝶整个研发平台，原来我们有传统产品，有云产品，所以我们聚合在金蝶云·苍穹平台上，在提高整体的研发投入。”他进一步补充，“所以在未来几年，我们是结构调整，在 AI 方面加大投资。但是在一些退出性的产品上，我们就会比较减少投入了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一调整意味着，金蝶将在AI驱动的企业管理解决方案上倾斜更多研发资源，而对传统软件的投入将有所收缩。与此同时，林波还提到，金蝶会在 AI 人才方面加大投入，“今年我们重点的部署人才就是在 AI，在算力，在模型能力方面，在架构能力方面。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 推动行业演进，有利于订阅收入增长&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在企业管理软件市场，AI的应用正在经历从“辅助决策”向“深度智能化”的进阶。金蝶在此次业绩会上介绍了 AI 在其云产品中的实际应用，特别是 Agent 的落地，已在多个企业客户中初步实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;金蝶集团总裁章勇举例道，金蝶早前推出了 BOSS助理，能够直接连接企业内部数据，帮助管理者快速获取公司财务、人力资源、业务运营等关键信息，从而大幅提升管理效率。过去，CEO要获取这些信息，可能需要财务部门整理报表，来回沟通，但现在只要向BOSS助理提问，就能立即获得答案。章勇认为，“它可以增厚我们每一个客户的产出。” 这一工具的商业化落地，也意味着金蝶SaaS业务的单客户价值有望进一步提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，金蝶正在优化SaaS产品在大企业市场的交付模式，以解决定制化开发带来的高成本和低效率问题。对于大型企业而言，个性化需求往往导致定制化成本高、交付周期长，成为SaaS厂商拓展大客户市场的一大挑战。对此，金蝶提出从标准化产品边界和生态合作两方面入手，借助AI优化企业级SaaS产品交付模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“过去很多企业其实没有重视到，都希望二开不断地去响应客户的需求，我们现在觉得要管理好这些需求。” 章勇表示，金蝶正在优化标品的边界，明确哪些功能应作为标准化产品的一部分，哪些需求可以通过合作伙伴实现，从而减少无序的个性化开发，提升交付效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，金蝶正利用AI赋能生态合作伙伴，使其能够承担更多定制化交付任务，从而进一步优化企业级SaaS的交付模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI来了之后，金蝶云·苍穹平台已经升级为一个企业级的AI管理平台，也是一个Agent的平台。”章勇指出，在这一过程中，金蝶正大力赋能合作伙伴，帮助他们理解并开发AI Agent，共同推动企业级SaaS产品的交付。他补充道，未来，金蝶希望让更多合作伙伴加入企业级SaaS产品的交付过程，伙伴负责个性化落地，而金蝶专注于标准化产品的优化，从而提升整体交付效率，降低企业陷入“定制化泥潭“的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;金蝶管理层表示，整体来看，AI 将推动SaaS市场的进一步扩张，并提高单客户的订阅价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“假设我们金蝶云·星空每个客户的产出，比如说订阅每人按5万来假设，可能每个客户保守估计就要增厚1万-2万。” 林波指出，AI将直接推动SaaS业务的价值增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;章勇也表示，“AI 对SaaS的变革和推动的场景在加大了。今年我们大量的改造就是要把金蝶的业务加入AI的能力。”并预计，AI+SaaS 模式将成为未来企业管理软件增长的主要驱动力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;出海寻求新增长点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，金蝶正在利用 AI 技术打开全球市场，寻找新的增长曲线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，金蝶去年已经在东南亚进行布局，开设了新加坡的数据节点，同时在泰国、马来西亚、越南和印尼等地，也开设了分公司和办事处。对于中东市场，金蝶也在展开布局，在上个月宣布设立卡塔尔公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“卡塔尔政府希望推动数字化，我们展示了 AI 财务管理系统，他们非常兴奋。他们过去报销一个流程基本上要一个月的时间，而我们的 AI 智能报销系统可以让整个审批流程缩短到几分钟。”章勇补充道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，金蝶也在考虑通过并购加速扩张。对于可能的并购机会，徐少春表示：“未来几年，我们是增效不增人，把我们的人均单产提高上去。所以我想我们未来几年公司的价值会有很大的提升。当然，还有一个就是我们也不排除有合适的并购对象，我们会进行并购。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为业绩会的最后总结，徐少春明确表示，“金蝶云转型已经成功，下一个目标就是AI转型。” 他说道，中国企业的管理软件一定会在全球崛起。金蝶将与AI同行，助力中国企业在全球市场站稳脚跟。&lt;/p&gt;</description><link>https://www.infoq.cn/article/plytGNfWpNS1uvKM3jyp</link><guid isPermaLink="false">https://www.infoq.cn/article/plytGNfWpNS1uvKM3jyp</guid><pubDate>Wed, 19 Mar 2025 03:41:47 GMT</pubDate><author>罗燕珊</author><category>AI&amp;大模型</category><category>数字化转型</category></item><item><title>用“千行代码”作弊软件骗过大厂！00后拿4个顶级Offer后潇洒拒掉：技术面试早该淘汰了？</title><description>&lt;p&gt;想进入像谷歌、亚马逊这样的大型科技公司工作，面试过程往往非常具有挑战性，尤其是“技术面”。应聘者需要在镜头前实时解决复杂的编码问题，这给许多人带来了巨大的压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，最近一位大二的哥伦比亚大学计算机专业在校生，轻松通过了亚马逊、Meta、TikTok 和 Capital One 的面试，并收到了这些公司的录用通知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更令人震惊的是，他还录制了自己在亚马逊接受技术面试的完整过程，并将未经剪辑的视频发布在了 YouTube 上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5e/5e042c96a782ff475a025c6d65830dcb.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可想而知，这样的行为很快引起了争议。视频上传两天并被广泛传播之后，哥伦比亚大学收到了一封匿名信，指控这位名为Chungin&amp;nbsp;“Roy”Lee的21岁学生在面试中作弊。直到校方将删减版的信件转发给他，并通知他将于 3 月 11 日举行纪律听证会时，Lee才得知自己被投诉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;邮件内容写道：“Chungin（他更喜欢被称为 Roy）最近参加了我们亚马逊 SDE 实习生岗位的面试，并展现出了很强的竞争力。然而，不久后，我收到了一条由 Roy 本人发布的 YouTube 视频，其中显示他在面试过程中使用了一种隐形作弊工具，以获取不正当且未经批准的优势。这一情况本已令人震惊，而进一步调查发现，Roy 还在向其他学生和工程师销售该工具，并将其广泛传播。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“亚马逊与哥伦比亚工程学院有着长期的合作传统……看到这种情况发生，我们深感担忧。我们相信哥伦比亚大学会对这名学生采取适当的行动，我们希望继续这种长期的合作关系。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;收到校方转发的邮件后，Lee随即在X平台上发布了这份邮件的截图，并附上了一句带有嘲讽意味的评论：“亚马逊的高管气疯了，笑死 LOLLL。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/597fa97a7bd2ead2c8412d70bcafd864.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee表示从未打算在亚马逊工作，并公然宣称他的目的就是展示AI如何玩弄编程面试，迫使公司重新考虑招聘。尽管如此，这条视频还是被YouTube下架，理由是“有违版权声明”，但Lee继续将之上传到了开源存档工具PreserveTube上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bb/bbf1f676ddad59d616d9740ab5a6bcf1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，他又分享了一张据称来自Meta的电子邮件截图，内容是撤销实习录用通知。“嗨，Roy。跟进我们之前的电话沟通，确认您在Meta的录用通知已立即撤销。感谢您对此的兴趣。”邮件内容如此写道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/368d7a4719600525e8d7380ce4086779.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;学校的处分对Lee而言意义不大，因为他已决定将作弊变成一门生意。他利用亚马逊面试的视频和几家大公司的录用通知作为广告，继续推广他的面试作弊软件“Interview Coder”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，大型科技公司已不值得应聘，因为大语言模型（LLM）的进步让这一切变得毫无意义。“也许我这么说有点傻，”他说，“但大多数依赖人类智能的工作在两年内都会被淘汰。所以，我只有两年的时间去做点真正有价值的事。这意味着我必须放手一搏，而不是在大厂耗上两年——更何况，我也不想这么做了……等我毕业时，LLM可能已经发展到让我无法再通过智力工作为社会创造价值的地步。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;仅靠一款简单的软件就能愚弄大厂？！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这款软件订阅费用为每月 60 美元，而且据说该公司有望在五月中旬实现 100 万美元的年度经常性收入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了推广这款软件，他最近聘请了知名网红“Costco Guys”制作宣传视频。在视频中，“Costco Guys”表示：“如果你还在为通过 Leetcode 面试而挣扎，想在科技大厂找到一份工作，那千万不要错过 Interview Coder。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“其实，这款产品的原理非常简单，”他说。“你只需要拍张照片，然后问 ChatGPT：‘能帮我解决这道题吗？’基本上，这就是整个流程。甚至有人可能只需不到 1,000 行代码，就能搭建出一个可用的原型。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一款专门针对远程面试的软件，只有面试者才能看到AI实时给出的完美答案，就算打开了屏幕共享，对方也完全看不到这个AI工具的存在。操作流程非常简单，面试者只需截图题目，工具就会自动生成答案。而且给出的每行代码，都有详细思路解释，可以用快捷键控制，不用碰鼠标，就可以复制粘贴答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/71/711fd3b18785f6583ccbc9d17a238325.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Interview Coder的官方网站表示，其线上面试工具不受Zoom和Google Meet等产品中屏幕检测功能的影响。Lee 还特别强调，该软件采用了大量防摄像头检测的设计。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee 一直在宣传 Interview Coder 的“不会被录屏软件捕捉”这一特性。正因为如此，他录下了自己通过亚马逊面试的过程，并将视频发布到了 YouTube 上。视频一经发布，包括亚马逊在内的几家大厂纷纷撤回了录取通知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee的视频之下则收到数百条支持评论。初创公司GreatFrontEnd联合创始人、前Meta工程师Yangshun Tay在LinkedIn上发帖讨论了Lee和他的视频，表示“身为面试官，我很讨厌这家伙；但从应聘者的角度讲，我又很崇拜他。作弊确实不对，但说真的，我实在受够了这些愚蠢的算法面试题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee 表示，产品推出后几乎立刻得到了市场验证，月经常性收入（MRR）迅速达到了 1 万美元左右。由于其营销方式极具争议性，产品持续走红，并频繁在网络上引发关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee 透露，他和联合创始人 Neel Shanmugam 仅用 4 天时间就构建了 Interview Coder 的核心技术，采用了 Electron、React、托管在 Vercel 上的无服务器边缘函数，并结合 OpenAI API，打造了一个简洁的产品。之后，他们花了大约一个月来优化用户体验，并为产品设置了付费墙和身份验证保护，而 Electron 中的深层链接问题则成为了最大的挑战之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee 还提到，除了技术上的困难，他们还遭遇了 DDoS 攻击，导致在 Vercel 托管和 API 使用上的费用达到了约 2000 美元。关于产品决策，Lee 表示，他主要通过求职季的实时测试来调整功能。在开发的绝大多数时间里，他是唯一的用户，利用真实的面试场景来验证新功能的效果。他补充道，之所以有时间做这些工作，是因为他已经习惯逃课，把时间都花在各种编程项目上。此外，产品最初的运营成本非常低，每月仅需约 20 美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“去他的Leetcode”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“InterviewCoder”公司的主页上，最醒目的就是用大号灰色字体写着的：“去他的Leetcode！”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee强调，Leetcode正是他开发Interview Coder的灵感来源。他说，在AI的帮助下，他用了不到一周时间就完成了服务创建。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/249787211e0ad0e1d4d2839e3ff1a259.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我最初也想进入科技大厂工作，为此我花了 600 个小时刷 Leetcode 题库。”他的 Leetcode 个人资料充分证明了他的投入。他说：“这让我开始讨厌编程。技术面试就是这样进行的，而且过去二十年来一直如此，这实在太荒谬了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a4612e9d5f1771adc70092cc3ddabc20.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lee的社交媒体帖子中，也充斥着其他程序员表达类似挫败感的言论。不少用户评论称“说得好”，还有人特别欣赏他“戏耍科技大厂”的行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同类软件Leetcode Wizard也是受到人们对Leetcode普遍厌恶的启发而诞生。Leetcode Wizard营销主管Isabel De Vries在一份声明中表示，Leetcode这类面试内容并不能准确衡量求职者的开发技能，也无法反映真实场景下的日常工作内容。De Vries强调，“我们的产品正是源自诸多用户经历过的严重挫败感。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Leetcode Wizard在其官方网站上自称是“排名第一的AI编码面试作弊应用”，以及“在任何编码面试中均可实现「理想结果」并轻松帮助用户加入FAANG大厂的完美工具。”Leetcode Wizard的“Pro”版本每月订阅费为49欧元（约合53美元）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司在采访中表示，目前已经有超过1.6万人使用该应用，而且“数百名用户”表示他们在这款产品的帮助下收到了录用通知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;De Vries指出，“什么时候Leetcode面试能够彻底被扔进历史的垃圾堆，就证明我们的产品成功了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据ResumeBuilder.com网站去年10月委托进行的一项调查，目前有半数企业在招聘过程中使用AI，到2025年底将有68%的企业使用AI技术。Lee认为，既然企业一直标榜自己的AI优先策略，就更应该鼓励求职者们使用这项技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在被问及是否担心软件工程师彻底推动科技行业的信任时，Lee停顿了一下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“呃……”，“我觉得，任何对于市场变化反应迟钝的企业都会受到损害，但这是企业的错。如果有更好的工具，但企业却拿不出更好的应对方案，那绝对就是他们活该。我绝对不会因为没有迎合企业的迟缓无能而感到哪怕一丝一毫的内疚。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谷歌决定恢复线下面试&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2022年末，随着OpenAI&amp;nbsp;ChatGPT的横空出世，生成式AI迎来了一波大爆发。从那时起，科技企业解雇了数万名程序员，同时大肆宣扬将使用AI高效编写代码。例如，谷歌公司CEO&amp;nbsp;Sundar&amp;nbsp;Pichai在去年10月就明确告知投资者，谷歌超过四分之一的新代码是由AI编写而成。AI的快速发展、软件开发者的大规模裁员以及持续的远程/混合办公环境，给招聘人员带来了新的难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在面试的时候，现有的AI作弊工具，能帮你实时写代码、回答问题，让你面试看起来像大神。但面试官也不是傻子，他们会盯着你的小动作，比如眼神飘忽、回答像背书、或者突然冒出一句“呃……”来拖延时间，这些都是作弊的信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;身为软件开发者兼纽约Studio.init公司联合创始人，Henry Kirk表示，此类作弊行为过去比较容易被发现，但现在情况大为不同。“但现在揪出作弊的难度更高了。”毕竟AI技术正变得越来越智能，用户往往不需要移动目光也能看到答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;“眼神飘忽曾经是最大的证据，但现在他们并不需要转动眼球。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kirk去年6月则为空缺工程职位组织过线上编码挑战赛，当时报名者多达700人。Kirk记录了第一轮面试流程，想看看候选人们会不会使用包括大语言模型在内的各种方式尝试作弊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;“从结果来看，超过五成的参与者都有作弊行为。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;专家表示，AI作弊工具在这一年间迎来了巨大改进，以至于如今几乎无法将其发现。除了Lee的Interview Coder之外，软件工程师还可以使用Leetcode Wizard或者ChatGPT等辅助程序。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kirk表示，虽然此举有可能会收窄人才来源，但他的初创公司仍在考虑重新转向线下面试。“最大的问题是，现在我越来越不敢相信线上面试的结果。除了现场面试，我实在不知道还能怎么办。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于 AI 作弊现象过于普遍，Pichai 在今年 2 月的谷歌员工大会上不得不建议招聘经理重新考虑恢复线下面试。根据相关录音，谷歌公司已经意识到 AI 作弊带来的巨大影响。Pichai 在这次内部会议上专门讨论了这一问题，谷歌高管们阅读了员工提交并由 AI 总结的问题和评论。管理层基于此提出了一个问题：“我们能不能恢复线下面试？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“大家就 AI 作弊的问题发了很多邮件和帖子。如果我们尽量控制预算，能不能让求职者回到可控的指定办公室或环境下参加面试？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，Pichai 将话题转向了通过线上面试加入谷歌的招聘副总裁 Brian Ong，“Brian，能不能考虑使用混合面试？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ong表示，求职者和谷歌员工普遍表示更喜欢线上求职面试，因为安排视频通话往往比在特定时段在会议室里当面交流更简单。他还补充称，线上面试的流程比传统面试平均快大约两周。他还提到，现在考验的是面试官能力，即探究求职者的回答以判断他们到底知不知道自己在说什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ong坦言，“我们肯定还有很多工作要做，以适应AI在面试过程中愈发普遍这个基本事实。”他提到，招聘部门正在与谷歌软件工程师指导委员会合作，希望找到进一步改善面试流程的可行方法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pichai回应称，“考虑到我们已经在采用混合方式工作，我觉得最值得认真考虑的方案就是将部分面试转向线下。我认为这也将帮助求职者了解并感受谷歌的企业文化，这对双方应该都有好处。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ong也表示，这是“我们自己包括所有竞争对手企业都在关注的重要问题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，其他企业已经开始改变招聘方式，旨在应对AI作弊行为的迅速升温。根据去年9月的一份报告，德勤将其英国研究生项目重新转为线下面试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI聊天机器人Claude背后的开发商Anthropic在今年2月的招聘须知中发布了新的指导意见，要求应聘者在面试过程中不得使用AI助手。新条款称，“虽然我们鼓励大家在工作期间使用AI系统以加快工作速度、提升工作效率，但请不要在面试过程中使用AI助手。我们希望在不受AI干扰的情况下了解您对于Anthropic的关注，同时借此评估您在非AI辅助条件下的沟通技巧。如果您已阅读并同意，请注明「好的」。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊公司也在采取措施打击AI作弊行为。亚马逊发言人 Margaret Callahan表示，亚马逊欢迎应聘者分享使用生成式人工智能工具的经验，但表示这些应聘者需要承诺在面试过程中不使用未经授权的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/yangshun_leetcode-faang-amazon-activity-7301441557373534208-yJNV/&quot;&gt;https://www.linkedin.com/posts/yangshun_leetcode-faang-amazon-activity-7301441557373534208-yJNV/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gizmodo.com/a-student-used-ai-to-beat-amazons-brutal-technical-interview-he-got-an-offer-and-someone-tattled-to-his-university-2000571562&quot;&gt;https://gizmodo.com/a-student-used-ai-to-beat-amazons-brutal-technical-interview-he-got-an-offer-and-someone-tattled-to-his-university-2000571562&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnbc.com/2025/03/09/google-ai-interview-coder-cheat.html&quot;&gt;https://www.cnbc.com/2025/03/09/google-ai-interview-coder-cheat.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.indiehackers.com/post/from-zero-to-1m-arr-in-36-days-by-publically-messing-with-big-tech-SRC9H50RFF9dV4n83ohl&quot;&gt;https://www.indiehackers.com/post/from-zero-to-1m-arr-in-36-days-by-publically-messing-with-big-tech-SRC9H50RFF9dV4n83ohl&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/73qhqwkouU5lYZK8gFC3</link><guid isPermaLink="false">https://www.infoq.cn/article/73qhqwkouU5lYZK8gFC3</guid><pubDate>Wed, 19 Mar 2025 02:31:32 GMT</pubDate><author>核子可乐,Tina</author><category>生成式 AI</category></item><item><title>月之暗面稳定高效的 LLM 基础设施构建之道 | QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;月之暗面系统工程师黄维啸已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6356&quot;&gt;《月之暗面稳定高效的 LLM 基础设施构建之道》&lt;/a&gt;&quot;的主题分享，重点介绍月之暗面在训推混部集群中的实践经验，探讨如何快速定位并隔离故障，实现任务的高效恢复，从而提升系统整体稳定性。另外还会分享如何在资源有限的情况下最大化利用率，避免浪费，并进一步将该思路应用于强化学习任务的训练中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/76/ef/76b6ee4d0b4a3b98b9663e36f3a84cef.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄维啸毕业于清华大学，拥有 7 年 AI Infra 系统经验。目前在月之暗面负责 Infra 平台、系统优化相关工作。曾在旷视科技公司主导公司 AI 平台 Brain++ 从 0 到 1 的研发工作。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 大规模训练推理集群的挑战大规模集群中机器故障频率高，任务失败率高推理场景中的用户请求具备潮汐效应强化学习场景存在资源浪费情况2. 提高训推混部集群稳定性和资源利用率任务检查点的高效存储和回复故障节点快速发现和隔离，提高训练可观察性，快速找到慢节点潮汐优先级调度，最大化提高资源利用率3. 通过混合架构提高强化学习效率混合 Sidecar 部署架构训推任务高效切换您认为，这样的技术在实践过程中有哪些痛点？大规模集群中机器故障问题难以彻底解决，系统上需要做一些权衡训推混部集群存在资源利用率不均衡的问题演讲亮点通过实际大规模集群的训推混部经验以及框架侧的优化，真实提高了整个系统的可靠性和可扩展性听众收益了解大模型训练和推理中遇到的稳定性问题及相关实践方案，并进一步提高资源利用率了解在强化学习中如何高效利用显存并提高系统可扩展性&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/7cYaaXeSExszah3RvDrZ</link><guid isPermaLink="false">https://www.infoq.cn/article/7cYaaXeSExszah3RvDrZ</guid><pubDate>Wed, 19 Mar 2025 02:26:53 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category></item><item><title>英伟达软硬件“双拳出击”：Blackwell Ultra、Rubin 芯片炸场，开源Dynamo让DeepSeek R1 token生成暴涨40倍！</title><description>&lt;p&gt;北京时间3月19日凌晨 1：00大洋彼岸的美国加利福尼亚州圣何塞会议中心灯火通明，人声鼎沸。全球科技界瞩目的年度盛会——英伟达GTC 2025大会在这里盛大开幕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为全球人工智能和计算机图形学领域的顶级峰会，GTC大会一直被视为行业风向标，每年都吸引着来自世界各地的科技巨头、专家学者和开发者齐聚一堂，共同探讨AI技术的未来发展方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;活动一开始，英伟达首席执行官黄仁勋率先出场强调了计算和软件的重要性表示。老黄表示：世界正在进行一场过渡——计算方式迎来变革、软件的未来需要资本投资。本次演讲，也主要围绕这两方面进行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，黄仁勋讨论了 AI 如何发展。现在我们处于生成式人工智能阶段，但根据黄仁勋的图表，我们正走向代理式人工智能时代，随后是物理人工智能。这就是机器人发挥作用的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd5214cf2fd35bd6a9dd60db46a9f346.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋提到，人工智能之所以变得更有用，是因为它“更聪明，可以推理”，而且它的使用范围越来越广。一方面，我们能够进行的数据和人工训练是有限的，另一方面，训练和推理这些模型所需的计算量也大幅增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表示：“人工智能的Scaling Law更具弹性，事实上是超加速的，”黄仁勋声称去年该行业对计算需求的理解存在错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，黄仁勋对数据中心业务相当有信心，“我之前说过，我预计数据中心建设将达到1万亿美元。我相当确定我们很快就会达到这个目标。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/5537e4e842084ca117f1825e26edf178.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋还表示，我们也正在从使用主要由人类编写的软件转向由AI模型驱动的软件。“在未来，计算机会为软件生成代码片段……而不是仅仅作为文件的检索器。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为CUDA加速&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“英伟达是一家软件公司”的含金量还在上升。这次，黄仁勋在演讲前半程将精力放在了软件上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“NVIDIA 一直使用通用计算机，以超慢的速度运行软件为他人设计加速计算机”黄仁勋说道，“直到最近，我们才有针对CUDA优化的软件库。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ecbd43ae11c87009f5ca8f85761280a7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CUDA 是 NVIDIA 于 2006 年推出的并行计算核心，为众多应用提供计算加速能力。NVIDIA 已构建了 900 多个特定领域的 NVIDIA CUDA-X 库和 AI 模型，现在，CUDA-X 将加速计算带入了一系列新的工程学科，包括天文学、粒子物理学、量子物理学、汽车、航空航天和半导体设计。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中，cuDSS库用于解决涉及稀疏矩阵的大型工程模拟问题，适用于设计优化、电磁模拟工作流程等。cuDSS 使用 Grace GPU 内存和高带宽 NVLink-C2C 互连来分解和解决通常无法放入设备内存的大型矩阵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用 Warp（一个基于Python的框架，用于加速数据生成和空间计算应用），Autodesk使用八个GH200节点可以进行最多48亿个单元的模拟，这比使用八个NVIDIA H100节点进行的模拟大了超过5倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外， CUDA-X 中还有用于 NumPy 的 cuPYNUMERIC、用于决策优化的 cuOPT（NVIDIA 将对其进行开源）、用于量子计算研究cuQuantum，以及用于天气分析的 Earth-2 和用于医学成像的 MONAI等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋表示：“这只是实现加速计算的众多库中的一小部分。”所有这些库都依赖 NVIDIA 的 CUDA 核心来完成工作，“如果没有CUDA以及我们拥有如此庞大的使用基础，这些库不会对使用它们的开发人员有任何作用。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋指出，人工智能始于云端，因为云数据中心拥有支持人工智能的基础设施。他认为，未来每家公司都会有两个工厂：一个用于生产产品，另一个用于 AI 数学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;发布“AI工厂的操作系统”Dynamo&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dynamo的发布，把本场会议再次推向了一个小高潮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/96/96dc99d53af22ca41f65a46e7ef1c36c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dynamo是一款开源推理软件，用于以最低的成本和最高的效率加速和扩展 AI 工厂中的 AI 推理模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在大量 GPU 中高效编排和协调 AI 推理请求对于确保 AI 工厂以最低成本运行以最大化token收入至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着人工智能推理成为主流，每个人工智能模型都会生成数以万计的token，用于在每次提示时“思考”。提高推理性能并不断降低推理成本可加速增长并增加服务提供商的收入机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达Dynamo 是Triton推理服务器的后继产品，是一款新型 AI 推理服务软件，旨在为部署推理 AI 模型的 AI 工厂最大限度地创造token收入。它协调和加速数千个 GPU 之间的推理通信，并使用分解服务将大型语言模型的处理和生成阶段分离在不同 GPU 上。这允许每个阶段根据其特定需求进行独立优化，并确保最大程度地利用 GPU 资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋表示：“世界各地的行业都在训练 AI 模型以不同的方式思考和学习，随着时间的推移，它们会变得更加复杂。为了实现自定义推理 AI 的未来，Dynamo 有助于大规模服务这些模型，从而推动整个 AI 工厂的成本节约和效率提高。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用相同数量的 GPU，Dynamo 使在当今 NVIDIA Hopper 平台上为 Llama 模型提供服务的 AI 工厂的性能和收入翻了一番。在 GB200 NVL72 机架的大型集群上运行 DeepSeek-R1 模型时，Dynamo 的智能推理优化还将每个 GPU 生成的token数量提高了 40 倍以上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了实现这些推理性能改进，NVIDIA Dynamo 整合了可提高吞吐量和降低成本的功能。它可以根据不断变化的请求量和类型动态添加、移除和重新分配 GPU，以及在大型集群中精确定位特定 GPU，以最大限度地减少响应计算和路由查询。它还可以将推理数据卸载到更便宜的内存和存储设备，并在需要时快速检索它们，从而最大限度地降低推理成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;NVIDIA Dynamo 完全开源，支持 PyTorch、SGLang、NVIDIA TensorRT-LLM 和 vLLM，使企业、初创公司和研究人员能够开发和优化跨分解推理服务 AI 模型的方法。它将使用户能够加速 AI 推理的采用，包括 AWS、Cohere、CoreWeave、Dell、Fireworks、Google Cloud、Lambda、Meta、Microsoft Azure、Nebius、NetApp、OCI、Perplexity、Together AI 和 VAST。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dynamo 将推理系统在内存中保存的知识映射到可能数千个 GPU 上的先前请求（称为 KV 缓存）。然后，它将新的推理请求路由到具有最佳知识匹配的 GPU，从而避免昂贵的重新计算并释放 GPU 来响应新的传入请求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;推出 Blackwell Ultra 和 Vera Rubin 芯片&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年，Blackwell AI芯片因复杂设计导致量产延迟，近期才开始大批量出货。尽管如此，Blackwell仍被寄予厚望，预计将成为英伟达明年AI业务的主要支柱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋表示，现在Grace Blackwell 解决方案已全面投入生产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b8c19bb91f8b655b79161f3362e5883.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而接下来，万众瞩目的Blackwell Ultra终于登场了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老黄在演讲中提到：“NVIDIA Blackwell Ultra 增强了训练和测试时间扩展推理（在推理过程中应用更多计算以提高准确性的艺术），使世界各地的组织能够加速 AI 推理、代理 AI 和物理 AI 等应用。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Blackwell Ultra 以一年前推出的突破性 Blackwell 架构为基础，包括 NVIDIA GB300 NVL72 机架级解决方案和 NVIDIA HGX B300 NVL16 系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，Blackwell Ultra将推出两个版本，其中一个配备两个与Nvidia Arm CPU配对的芯片，称为GB300；另一个版本仅配备GPU，称为B300。它还将推出带有八个GPU的单个服务器刀片版本，以及一个包含72个Blackwell芯片的机架版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Blackwell Ultra NVL72 平台将于 2025 年下半年上市。它的带宽是原来的两倍，内存速度是原来的 1.5 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋表示：“人工智能已经取得了巨大的飞跃——推理和代理人工智能需要更高数量的计算性能。我们为这一刻设计了 Blackwell Ultra——它是一个单一的多功能平台，可以轻松高效地进行预训练、后训练和推理人工智能推理。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/55ee13aac2ffd61b7795aae3d9267040.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋表示，“在推理模型中，Blackwell 的性能是 Hopper 的 40 倍”。在展示中，传统模型的代表是 Meta Llama 3.3，而推理模型是 DeepSeek 的 R1。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与 Hopper 一代相比，NVIDIA HGX B300 NVL16 在大型语言模型上的推理速度提高了 11 倍，计算能力提高了 7 倍，内存增加了 4 倍，从而为 AI 推理等最复杂的工作负载提供了突破性的性能。“只有在英伟达，你才会被数学折磨。”黄仁勋调侃道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“无论如何，买得越多，省得越多。”看得出来，黄仁勋确实很想提高Blackwell销量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达表示，四大云计算公司部署的 Blackwell 芯片数量是 Hopper 芯片的三倍。云提供商可以使用 Blackwell Ultra 为时间敏感型应用程序提供高端AI 服务，从而使其从新芯片中获得的收入达到 2023 年推出的 Hopper 一代的 50 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但值得注意的是，老黄在现场并未透露 Blackwell Ultra 比原版 Blackwell 有多好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但据外媒消息，在与记者的一次事先简报会上，英伟达曾透露，单个 Ultra 芯片将提供与 Blackwell 相同的 20 petaflops AI 性能，但现在拥有 288GB 的 HBM3e 内存，而不是 192GB。同时，Blackwell Ultra DGX GB300“Superpod”集群将提供与 Blackwell 版本相同的 288 个 CPU、576 个 GPU 和 11.5 exaflops FP4 计算能力，但拥有 300TB 的内存，而不是 240TB。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7b/7bfb9070f88dffc271076cc456bdb32f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接着，黄仁勋发布了下一代GPU 系列Vera Rubin——以美国天文学家 Vera Rubin名字命名。据悉，Vera Rubin有两个主要组件：一个称为 Vera 的 CPU 和一个称为 Rubin 的新 GPU 设计，具有 NVLink 144。该公司表示，Vera 是 Nvidia 的首款定制 CPU 设计，它基于名为 Olympus 的核心设计。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与 Vera 搭配使用时，Rubin 可以在进行推理时实现每秒 50 千万亿次浮点运算，比 Blackwell 每秒 20 千万亿次浮点运算的速度高出一倍多。Rubin 还可以支持高达 288 GB 的快速内存，这是 AI 开发人员关注的核心规格之一。Rubin Ultra 将于 2027 年下半年推出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达还在对其所谓的GPU进行调整。英伟达表示，Rubin实际上是两个GPU。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前市场上的Blackwell GPU实际上是两个独立的芯片组装在一起作为一个芯片工作。从Rubin开始，当将两个或多个芯片结合成一个单一芯片时，它会将这些芯片称为独立的GPU。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/71/71c20b0009edea05d49b579314045150.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2027年下半年，英伟达计划发布一款名为“Rubin Next”的芯片，将四个芯片结合成一个单一芯片，使Rubin的速度翻倍，并将其称为四个GPU。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋展示了&amp;nbsp;Vera Rubin NVLink576 的外观和参数，并宣称&amp;nbsp;Rubin 的性能可达&amp;nbsp;Hopper 的 900 倍，而 Blackwell 是 Hopper 的 68 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d3/d3025deb0226ed52442c689a0030d61e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋说：“在过去的两到三年里，人工智能取得了重大突破和根本性进展，我们称之为‘agentic AI’，它可以推理如何回答或如何解决问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，黄仁勋探讨了英伟达进军以太网领域的原因：基本上，英伟达能够制造高性能以太网解决方案，帮助在超级计算机之间传输数据。黄仁勋发布了 NVIDIA Photonics，这似乎是该公司迄今为止最强大的 Spectrum-X 以太网产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在演示中，黄仁勋似乎被搞得一团糟，网友开玩笑道，“搞乱这些电缆的人很可能会被解雇。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋还表示，英伟达继Rubin之后的下一代芯片将以物理学家Richard Feynman的名字命名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/63/63b66fcdd6c5b957aecea9253a2e4d20.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，黄仁勋手里拿着一台 20 千万亿次浮点计算机——DGX Station登上演讲台并说道“这是人工智能时代的计算机。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;开源全球首个人形机器人基础模型GROOT N1&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2个小时后，本场发布会的另一个小高潮，是英伟达宣布开源全球首个人形机器人基础模型GROOT N1。据老黄称，这是世界上第一个开源的、完全可定制的通用人形推理和技能基础模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老黄宣称：“机器人的时代已经到来。我们清楚地知道，世界劳动力严重短缺——短缺 5000 万人。” 所以我们对机器人的需求与日俱增。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c9beadce6837374f8f40a929d96f6d9d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;GR00T N1 现已上市，是英伟达将预训练并发布给全球机器人开发人员的一系列完全可定制模型中的第一个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋表示：“通用机器人时代已经到来。借助 NVIDIA Isaac GR00T N1 以及新的数据生成和机器人学习框架，世界各地的机器人开发人员将开拓 AI 时代的下一个前沿。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Groot N1 是英伟达 Project Groot 的演进版本，该公司在去年的 GTC 大会上推出了该项目。Project Groot 面向工业用例，但 Groot N1 将重点扩大到各种不同外形的人形机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;GR00T N1 基础模型采用双系统架构，灵感来自人类认知原理。“系统 1”是一种快速思考的行动模型，反映了人类的反应或直觉。“系统 2”是一种慢速思考的模型，用于深思熟虑、有条不紊的决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在视觉语言模型的支持下，系统 2 可以推理其环境和收到的指令，从而规划行动。然后，系统 1 将这些计划转化为精确、连续的机器人动作。系统 1 接受人类演示数据和NVIDIA Omniverse 平台生成的大量合成数据的训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;GR00T N1 可以轻松实现常见任务（例如抓取、用一只或两只手臂移动物体以及将物品从一只手臂转移到另一只手臂），或者执行需要长时间上下文和一般技能组合的多步骤任务。这些功能可应用于物料搬运、包装和检查等用例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发人员和研究人员可以使用真实或合成数据对GR00T N1 进行后期训练，以适应特定的人形机器人或任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4bbdf365e70ec6de08c5b9b418ba12fc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了该模型，英伟达还发布了用于生成合成训练数据的模拟框架和蓝图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，老黄还表示，英伟达正与 DeepMind 和 迪士尼研究中心合作开发新平台 Newton，这是一个开源物理引擎，可让机器人学习如何更精确地处理复杂任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4ba9735b5c54aa17fc0f61ce6090cd71.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Newton 基于NVIDIA Warp框架构建，将针对机器人学习进行优化，并与 Google DeepMind 的 MuJoCo 和NVIDIA Isaac Lab等模拟框架兼容。此外，三家公司还计划让 Newton 使用迪士尼的物理引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“AI将进入每个行业”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a6/a6dc2edd08d4a496fe3e19459ac14c3c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今，AI进入千行百业早已是事实。但是，当全球范围内不同行业在平台、需求以及其他方面存在如此多差异的时候，我们要如何将人工智能推广到全球呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋指出，背景信息和先验知识可能是实现下一步突破的关键，尤其是在边缘计算领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他转向自动驾驶汽车（AV）——这通常是人工智能领域最大的领域之一。他指出，几乎每家自动驾驶汽车公司都在使用英伟达的技术，从特斯拉到 Waymo，从软件到硬件，都试图推动该行业向前发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过今天又有一位新合作伙伴——黄仁勋宣布英伟达将与通用汽车在 AI 领域展开合作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋宣称：“自动驾驶汽车的时代已经到来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，英伟达宣布推出NVIDIA Halos，这是一种综合安全系统，将 NVIDIA 的汽车硬件和软件安全解决方案系列与其在 AV 安全领域的尖端 AI 研究结合在一起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Halos 涵盖芯片、软件、工具和服务，帮助确保从云端到汽车的 AV 安全开发，重点是基于 AI 的端到端 AV 堆栈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;NVIDIA 行业安全副总裁 Riccardo Mariani 表示：“通过推出 Halos，我们让合作伙伴和开发者能够选择他们所需的先进技术元素，打造他们独特的产品，推动共同的使命，打造安全可靠的自动驾驶汽车。Halos 是对现有安全实践的补充，并有可能加速标准化和法规遵从。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Halos 是一个涵盖三个不同但互补的层面的整体安全系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术层面，它涵盖平台、算法和生态系统安全。在开发层面，它包括设计时、部署时和验证时防护措施。在计算层面，它涵盖从 AI 训练到部署的整个过程，使用三台强大的计算机——用于 AI 训练的NVIDIA DGX 、在NVIDIA OVX上运行的 NVIDIA&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Omniverse 和 NVIDIA Cosmos用于模拟，以及用于部署的NVIDIA DRIVE AGX。作为 Halos 的入口点，NVIDIA AI 系统检查实验室可让汽车制造商和开发商验证其产品与英伟达技术的安全集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，大会接近结束时，英伟达股价下跌3%，通用汽车下跌1.5%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/37/371815858646e43c7e44134d970e75fc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/52OcU1EkRek7MxDzfdtQ</link><guid isPermaLink="false">https://www.infoq.cn/article/52OcU1EkRek7MxDzfdtQ</guid><pubDate>Wed, 19 Mar 2025 00:00:00 GMT</pubDate><author>李冬梅,褚杏娟</author><category>英伟达</category><category>芯片&amp;算力</category></item><item><title>AI 时代的人机协作与智能 Agent 实践｜QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京盛大召开。此次大会以 “智能融合，引领未来” 为主题，汇聚各领域技术先锋与创新者，共同探讨行业发展新趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PingCAP 副总裁刘松已确认出席，并发表题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6414&quot;&gt;技术管理新范式：AI 时代的人机协作与智能 Agent 实践&lt;/a&gt;&quot;》的主题演讲。传统技术管理面临着诸多挑战，如信息孤岛、知识管理效率低下以及团队协作的复杂性。在这样的背景下，AI 已不再仅仅是工具，而是成为组织中不可或缺的协作伙伴。本次演讲中，刘松将深入探讨 AI 时代下人机协作的新模式，以及智能 Agent 如何重塑技术管理的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刘松拥有 20 多年 IT 领域从业经验，曾担任甲骨文大中国区技术战略部总经理、阿里云副总裁等职务。长期深耕于企业软件解决方案、数据库产品市场、云计算生态发展等多个领域，在前沿技术与行业转型结合方面积累了丰富的咨询与智库经验，主导了数字化转型、产业互联网领域的多项图书与研究报告撰写工作。本次会议中，他的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;1. 传统技术管理的挑战信息孤岛现象数据分散，难以整合信息流通不畅，影响决策效率知识管理效率低下知识更新不及时，难以共享知识库维护成本高，使用率低团队协作的复杂性跨部门协作困难，沟通成本高项目管理中信息不对称，导致效率低下2. 重塑技术管理范式人机协作的优势从传统管理到智能管理的转型路径智能 Agent 在技术管理中的应用场景3. 智能 Agent 的落地应用实践知识管理智能化自动化协作流程智能决策支持4. 经验教训实施过程中的技术挑战如何平衡 AI 的效率与人类团队的协作5. 未来展望构建更具适应性和前瞻性的技术管理架构在组织中培养 AI 时代的管理思维对企业技术管理的长期建议您认为，这样的技术在实践过程中有哪些痛点？内部阻力与文化冲突：推动技术管理变革可能面临内部阻力，尤其是当新技术与现有企业文化冲突时缺乏高层支持：如果没有高层管理者的支持，AI 和智能 Agent 的引入可能难以顺利推进听众收益理解 AI 时代技术管理的新范式，提前布局，增强企业对技术变革的适应能力了解先行者在实施过程中遇到的挑战和应对策略，减少自身实践中的风险和成本&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0Lu3R7wO5ouF54DyBjhD</link><guid isPermaLink="false">https://www.infoq.cn/article/0Lu3R7wO5ouF54DyBjhD</guid><pubDate>Tue, 18 Mar 2025 10:38:50 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category><category>管理/文化</category></item><item><title>历时5年，这款开源全栈框架火爆问世，GitHub 狂揽 15K 星！网友：不支持Vue，差评</title><description>&lt;p&gt;Wasp（Web应用程序规范）是一个类似Rails的框架，用于React、Node.js和Prisma。该开源框架，旨在通过React、Node.js和Prisma简化全栈Web应用程序的开发，它通过为JavaScript开发人员提供类似Rails的体验而受到关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该框架使用简单的配置文件（.wasp）来描述高级应用程序细节，结合在熟悉的框架如React和Prisma中编写独特逻辑的能力，使其成为开发人员快速原型设计和部署Web应用程序的优选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wasp最引人注目的一个方面是其编译器，使用Haskell构建，从.wasp文件生成前端、后端和部署的完整源代码。这种“智能”方法使开发人员能够专注于编写业务逻辑而不是样板代码，具有内置的全栈身份验证、简单部署和全栈类型安全等功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而关于Wasp的故事要从7年前说起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1c/1cfe15ca8012c0bca88ec53bc3ef3b31.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;两次资金申请被拒，创始团队曾几近崩溃&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“他们选择了一段艰难但意义重大的旅程，且前人均以失败告终。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2020年，Wasp的两位创建者第一次在这个社区发布项目帖子时，他们还没想好自己到底要做什么，刚刚，这个项目在GitHub上斩获了1.5万star，并且得到了众多个人创业者、初创公司和财富五百强企业的广泛应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目已经在Discord中汇聚了约四千名开发者，而如今的Wasp尚处于Beta测试阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么要做这样一个项目？这要从7年前两位创始人遇到的难题说起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wasp这两位创始人是：Matija Sosic和Martin Sosic，他们不仅是双胞胎兄弟，还是技术上的强力搭档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matija 拥有计算机科学硕士学位，也是一名软件工程师，拥有丰富的软件开发经验，他擅长干净代码、函数式编程和 Web 开发，热衷于组建团队打造实用产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matija 的职业经历丰富：2020年以前，曾在 Lifebit Biotech 担任工程团队负责人，带领团队解决技术难题，推动生物技术与软件开发融合项目的进展。2017 年，加入 Techstars Tech Associate（伦敦）团队，参与跨国项目，积累国际合作经验。他也曾担任TalkBook 的首席执行官兼联合创始人，凭借敏锐市场洞察力，带领团队推出创新产品，促进公司业务增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72541ddac95e1680767dc112c190e4d1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Matija Sosic&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Martin则在算法和系统设计方面有着深厚的背景。他对开发产品、开源、函数式编程、Web 开发以及干净代码满怀热忱。曾作为算法竞赛选手，在逻辑与编程能力上崭露头角。他曾在谷歌、Palantir 等企业实习，并在Lifebit.ai 的工程团队负责人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/12/12cc0c5d13ce45e43d6f5c316810f1b0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Martin Sosic&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2018年，在为一家位于伦敦的初创公司开发用于在云端运行生物信息学分析的网络平台时，Matija和Martin萌生了创建 Wasp 的初步想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时，他们要使用最新最热门的堆栈从头开始创建全栈应用程序。于是他们选择了React/Node.js；对于以前的项目，他们通常会在后端使用 PHP/Java/Node.js，在前端使用了 jQuery/Backbone/Angular。因为兄弟二人花了很多时间重新学习如何使用最新的堆栈，只是为了重新构建相同的功能（身份验证、CRUD、表单、异步作业等）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以当时两人就有这样的疑问：“为什么不使用与堆栈无关的高级语言（例如 SQL 对数据库所做的那样）抽象这些常见功能，以便永远不再重新实现它们？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在构建Wasp之前，他们充分地调研了项目的可行性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matija表示：“在之前的创业公司中，我们发现与用户交谈非常有帮助，所以我们决定在 Wasp 中再次这样做”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是兄弟二人在大约一个月的时间里，进行了 25 次问题调研，探讨“用户在 Web 应用开发中面临的最大挑战是什么？”汇总结果后，他们确定了以下四个问题最为重要，并决定在 v1 版本中重点关注它们：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;快速启动一个新的 Web 应用程序并确保遵循最佳实践是很困难的。在管理前端、后端和数据库的状态时存在大量重复/样板。每个新应用程序都会重新实现许多常见功能。开发人员因日益复杂的工具而不知所措，不想负责管理它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们还根据主题对得到的答案进行了聚类，以便能够更深入地研究并确定最受关注的领域：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/be/be0d35046b4292112b805d048a545f8e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;受访者遇到的有关启动和设置新网络应用程序的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在与其他开发人员确认并澄清了问题后，兄弟二人觉得终于应该开始编写代码了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2019年年初，兄弟二人在 GitHub 上创建了一个新的 repo，并开始设置工具并尝试这个概念。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在接下来的几个月里，他们将 Wasp 视为一个附带项目/实验，没有进行任何营销。但是，他们非常清楚外部反馈的重要性。因此，在构建了一个非常基本的代码生成功能后，他们还创建了一个项目页面，可以与其他人分享，以解释他们正在做的事情并征求反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa661ae7c8b46c6c9eb32f6cc041a6d6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是 Wasp 的第一个页面，虽然它没有很好地解释 Wasp 的作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那时，他们为 Wasp 想出了第一个“真正”的名字 -&amp;nbsp;STIC：规范到实现编译器，因为 Wasp 的远大愿景是成为一种与堆栈无关的规范语言，让人们可以从中生成实际的代码，例如 React 和 Node.js 甚至其他一些堆栈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们分发 STIC 项目页面的首选方式是通过相关的 subreddits - r/webdev、r/coding、r/javascript、r/Haskell、r/ProgrammingLanguages 等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是他们发布的第一篇有关 Wasp 的&lt;a href=&quot;https://www.reddit.com/r/javascript/comments/f38h1t/askjs_we_are_developing_a_declarative_dsl_for/&quot;&gt;Reddit 帖子&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f1/f1283912dd4e6a752618adac40baa636.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他们很快了解到的一件重要的事情是，Reddit 不喜欢任何形式的宣传和推销。有时，即使兄弟俩认为当时他们只是在征求反馈，而非推销。继 Reddit 之后，他们又在 Hacker News 上发布了产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martin 和Matija还跟进了他们之前采访过的人，询问他们在 Web 开发方面遇到的问题。兄弟二人向这些受访者展示了 STIC 项目页面并征求意见。从他们收集到的所有反馈中，两人发现了以下问题：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发人员不熟悉“DSL”这个术语。我们几乎每天都会使用 DSL（例如 SQL、HCL（Terraform）、HTML），但它不是一个流行的术语。开发人员害怕学习一门新的编程语言。虽然Wasp的目标从来不是取代 Java 或 Typescript，而是让 Wasp 与之协同工作，但兄弟二人未能很好地传达这一点。他们的信息让开发人员觉得，如果他们想使用 Wasp，他们必须放弃所有以前的知识，从头开始。没有人可以尝试 Wasp + 除了项目页面之外没有任何文档。尽管 Wasp 的代码是公开的，但目前除了项目页面外，几乎没有其他文档。此外，他们尚未建立构建和分发系统，这意味着只有经验丰富的 Haskell 开发人员才能从源代码构建它。这种情况使得开发人员难以理解其高层愿景，因为他们缺乏足够的资源和支持。Web 框架和语言通常需要实际体验才能判断其优劣，但由于缺乏可尝试的版本，开发人员很难评估 Wasp 的实际效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在处理完这些反馈后，兄弟俩意识到下一步要让 Wasp 达到这样的状态：开发人员可以轻松试用它，而无需任何额外的知识或面对从源代码编译的麻烦。这意味着要稍微完善一下，添加一些关键功能，并编写我们的第一个文档，以便用户知道如何使用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们当时的宗旨是专注于一件事，要么开发，要么社区。由于 Wasp 团队只有兄弟两人，所以很难同时做多件事。文档发布后，Wasp 已准备好轻松下载，他们将此版本称为“Alpha”，并再次切换到“社区”模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从0到15k star：加入YC，一步步落地&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像很多初创企业一样，Wasp 团队一心搞研发之际，也遇上了创业路上的第一道坎——资金短缺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2020年5月，在第一次资金申请被拒绝后，Wasp团队第二次向知名创业孵化器Y Combinator（YC）提交了申请。然而，他们收到了YC合伙人Harj Taggar的回复，表示YC决定不资助Wasp这一批次的申请。Harj在邮件中肯定了团队成员的编程能力，并指出他们致力于简化Web应用开发的目标是有价值的。然而，YC认为Wasp所提出的领域——为Web应用开发创建领域特定语言（DSL）——是一个许多程序员都曾尝试解决的难题，因此决定不予以资助。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3c/3c300c0a7a3c879b22aed8348a2e4c87.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一决定对Wasp团队来说无疑是一个打击。当时，他们已经在Wasp项目上投入了一年半的时间，其中最后9个月更是全职开发。为了追求这个梦想，团队的两兄弟辞去了之前的工作，全身心投入到项目中。到了这个阶段，他们在心理、生理和经济上都已感到精疲力竭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管如此，Wasp团队并没有放弃。与此同时，他们也更冷静地同时重新审视最初的决定——为什么要创建这样一款开源项目？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matija和Martin 在回忆这段旅程时表示：“刚开始，Wasp只是我们的一个念头——或者更确切地讲，是一个现实问题：‘为什么还没人做过？如果动手试试，我们会遇到哪些挑战？’在经历了十年之间持续开发Web应用程序，并使用各类主要技术栈（包括服务器端的PHP、Java和Node.js，再到客户端这边的Backbone、Angular和React）之后，我们对于‘框架疲劳’可谓深有体会。换言之，每种新技术栈都在重新发明轮子。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是他们开始思考这个问题，并把需要解决的问题记录下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们再次坚定了Wasp项目立足的根源——要创建一套框架，通过提供更高级别的抽象来消除大量样板，同时保持足够的灵活度，避免其与特定技术栈和架构牢牢绑定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;既然目标已经十分明确，接下来就是一步一步坚定地走下去。与大多数成功故事一样，一个优秀项目的诞生往往并非线性推进。它的开端往往经经历漫长的“沉寂”，但偶尔也会出现希望。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wasp项目也不例外。在兄弟俩一往无前的努力之下，之后的事情开始步入正轨并迎来快速发展期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cf/cf3b77f1bbc28f3ac02ec968b229d367.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在全职工作了大约九个月之后，两兄弟开始获得初步获得一定关注，也收到了来自Reddit、Hacker News和Product Hunt的积极反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2020 年 12 月 6 日，兄弟二人在 PH（Product Hunt&amp;nbsp;） 上推出了 Wasp，它最终成为当日明星产品！这让Wasp的star数量和整体吸引力都有所提升。PH 的另一个好处是，Wasp 也出现在他们的每日新闻通讯中，据称该通讯拥有超过一百万订阅者。所有这些都给了他们很大的推动力和知名度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与此同时，他们也意识到这样一套全栈Web框架的落地可用对应着多少工作量，毕竟如此雄心勃勃的预期和要求绝非一朝一夕可以实现。寻求一定的外部支持仍是必要的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/33/3361f7f76b02d1a94f8a0b08e0e73f83.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2021年，Y Combinator（YC）兄弟俩在第三次申请YC时成功入围。YC一直在关注兄弟俩过去一年的进展，并在感受到社区的热情之后，最终决定支持他们的“疯狂”想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，项目迎来快速发展期，并在后面的几年内出现了两个重要拐点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一个重要拐点发生在2023年7月。当时他们发布了MAGE，这是一款使用Wasp的GPT SaaS上手程序（大家可以理解成一次性的Loveable/Bolt）。它是首批能够生成可运行全栈Web应用程序的大模型产品之一，并成功为Wasp博得了广泛关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/20625f0414f8d43bf47283a0ed44e4cd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个拐点则出现在2023年12月，当时推出的OpenSaaS是我们基于Wasp构建的开源SaaS上手程序，目前在GitHub也获得了近10k&amp;nbsp;star。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过这两个拐点，Wasp项目团队意识到，大多数开发者都迫切希望加快实现进程，而不愿把时间和心力浪费在挑选和择善而从SaaS中的各类不同功能上——包括身份验证、付款、管理仪表板、发送电子邮件、博客等等……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是该项目的意义所在——打造一套基于React、Node.js、Prisma和Wasp，而且100%免费开源的高质量SaaS入门版本。OpenSaaS顺利成为Wasp的“杀手级应用”，它吸引到开发者的关注，并在尝试之后感受到了这套框架的实用价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenSaaS还与Cursor实现了完美搭配——在Wasp结构与高级原语支持下，许多开发者意识到OpenSaaS加Cursor这套理想组合，能帮助他们的SaaS项目在几天之内从创意转化为生产应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;语言、DSL还是框架——Wasp到底是什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f4/f49e0849b5569c6715a222e077066f7d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从上面的例子可以看出，兄弟二人过去曾将Wasp称为一种语言，即DSL领域特定语言。结合这样的定位，他们最初只打造设计一个抽象层，确保其未来能够与任何语言、库和架构配合使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，他们需要引入自己的编译器。该编译器会首先分析开发者通过Wasp定义的应用程序规范（例如路由、异步作业、数据库操作等），将其与我们在React和Node.js中编写的“本机”代码相结合，最后生成一款React/Node.js应用程序。也就是说尽管功能有限、机制简单，但Wasp确实成为了由他们一手发明的语言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在一段时间的推进之后，兄弟二人发现这种定位思考是错误的。Wasp的功能更多应该是Web框架，类似于Laravel、Rails或者Next.js。它在底层使用编译器只是一种实现细节，是为了达成功能所必需的前提。例如，借助这种方法，用户可以轻松对整个应用程序的拓扑结构进行可视化，包括数据库、服务器乃至客户端组件：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/2062f08cae15c53681be69dfcec58c59.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然现在看来这没什么不大了，但却为其他一些有趣工具的功能实现打开了大门。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友怎么看？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着 Wasp 1.0 版本的发布，这一开源框架吸引了广泛的关注。由于 Wasp 常被与 Laravel 等成熟的开发框架相提并论，许多网友对其商业模式产生了浓厚的兴趣和疑问。Laravel 通过提供高级工具、托管服务和生态系统支持成功实现了商业化，而 Wasp 作为一个新兴的开源项目，目前仍专注于框架的完善和开发者社区的构建。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此许多网友提出质疑，现阶段Wasp的团队成员的收入来源在哪里？有网友直接提问：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“你如何从这样的项目中赚钱？YC 的资金足以让你们两个继续下去吗？在此之前，你们自己很富有吗？”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，Matija表示：“第一年，我们用个人储蓄来资助它，但之后我们在 YC 之后立即筹集了一轮150万美元的融资，这使我们能够扩大团队规模（目前我们有 8 人）并全职专注于它。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于wasp目前仅支持React，有网友对他们不支持 Vue表示遗憾。该网友评论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“太糟糕了。我认为Vue 比 React 的缺陷少得多！”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有网友出来辩论称：“从商业角度来看，创始团队的决定是正确的，虽然我更喜欢 Vue，但我也会为这个项目选择 React。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matija也在评论区回复了该网友：“Wasp 的愿景是支持不同的 UI 框架/库，但目前我们专注于 React。至少到 1.0 版为止是这样的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来规划&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么接下来该做什么？经过近7年的开发和从广大受众处获得的反馈，Martin兄弟二人对于Wasp 1.0的功能定位和最终形态有了非常清晰的认知。他们最终的目标是在JS上实现Laravel之于PHP、Rails之于Ruby的效果——构建一款思路明确的全栈、功能完备的框架，确保它能随意部署并随着业务成长而扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;兄弟二人表示：“我们清楚，自Laravel/Rails/Django诞生以来，技术界对于框架的要求和期望已经发生了巨大变化，但其背后对应的生产力与开发体验将始终成为我们不懈追求的目标。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了技术能力外，一款产品是否能在市场中长久地立足也要考虑其盈利模式。关于未来如何盈利的问题，Matija也在Reddit给出了回答：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“目前，我们正专注于将开源框架升级到 1.0 版本，并证明我们能够创建下一个“JavaScript 领域的 Laravel”。这是我们工作的核心基础，如果没有这一基础，我认为考虑盈利是没有意义的。在盈利方面，像 Laravel、Vercel 和 Hashicorp 这样的公司是非常值得研究的案例。我们也在企业环境中获得了初步的采用，这让我们感到意外，并激发了我们之前未曾考虑过的新思路。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管 Wasp 团队尚未明确公布具体的盈利计划，但他们表示，只有在框架达到稳定且广泛采用的阶段后，才会考虑商业化路径。这种以开发者体验和开源生态为核心的策略，与 Laravel、Vercel 等成功案例有相似之处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种“先开源，后盈利”的模式在技术领域并不罕见，关键在于如何在保持开发者信任的同时，找到可持续的商业化路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://old.reddit.com/r/webdev/comments/1jbyxxr/my_brother_and_i_built_laravel_for_js_and_it_just/&quot;&gt;https://old.reddit.com/r/webdev/comments/1jbyxxr/my_brother_and_i_built_laravel_for_js_and_it_just/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.ycombinator.com/companies/wasp&quot;&gt;https://www.ycombinator.com/companies/wasp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bbxu7y7ruvoe1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14341da2bc4d0396335e92285284d5a01ef9a059&quot;&gt;https://preview.redd.it/bbxu7y7ruvoe1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14341da2bc4d0396335e92285284d5a01ef9a059&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://wasp.sh/&quot;&gt;https://wasp.sh/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://wasp.sh/blog/2022/09/29/journey-to-1000-gh-stars&quot;&gt;https://wasp.sh/blog/2022/09/29/journey-to-1000-gh-stars&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/qzQSPU2aO1DaJqrzPcI3</link><guid isPermaLink="false">https://www.infoq.cn/article/qzQSPU2aO1DaJqrzPcI3</guid><pubDate>Tue, 18 Mar 2025 09:14:50 GMT</pubDate><author>李冬梅</author><category>框架</category></item><item><title>云原生时代的架构革新，Apache Doris 存算分离如何实现弹性与性能双重提升</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;导读： 随着云基础设施的成熟，Apache Doris 3.0 正式支持了存算分离全新模式。基于这一架构，能够实现更低成本、极致弹性以及负载隔离。本文将介绍存算分离架构及其优势，并通过导入性能、查询性能、资源成本的测试，直观展现存算分离架构下的性能表现，为读者提供具体场景下的使用参考。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据驱动决策的时代，数据规模增长加快、分析需求不断变化。数据从多种渠道（如应用日志、网络交互、移动设备等）源源不断地流入，包含结构化、半结构化、非格式化等多种数据格式，这对数据的存储和分析提出了很高的挑战。与此同时，企业对实时分析、探索性查询的需求激增，要求系统在保证毫秒级响应能力的同时，兼具极致的成本效益与弹性扩展能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Apache Doris 诞生于存算一体的分布式时代，是典型的 Shared Nothing 架构：BE 节点上存储与计算紧密耦合、多 BE 节点采用 MPP 分布式计算架构，这种架构带来了高可用、极简部署、横向可扩展以及强大的实时分析性能等一系列核心特色。在实时分析、小规模数据处理等场景中，凭借可预期的稳定低延迟表现，具有不可替代的优势。但在面对大规模数据处理时面临一些挑战，主要体现在：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成本高且弹性不足： 合理平衡存储和计算资源较为困难。存储方面，集群必须足够大以容纳所有数据；因此计算资源的分配既要满足查询请求，又要避免浪费。然而，调整集群规模通常耗时较长，企业往往选择过度配置集群以简化运维操作，这就导致资源浪费和成本增加。受限的负载隔离： Apache Doris 2.0 开始支持 Workload Group 实现软限隔离，也支持 Resource Group 实现一定程度的硬限隔离，但是这两种方式都无法达到彻底的物理隔离。运维难度较大： OLAP 系统内置分布式存储系统，不仅需要运维计算节点，还需有效管理存储系统。而存储系统的管理要求非常高，任何不当操作都可能导致数据丢失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;即使如此，在没有稳定且大规模存储支持的情况下，存算一体架构依然是最好的选择。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着云基础设施的成熟，越来越多的企业都希望 Apache Doris 能更深度地适应公有云、私有云和 K8s 容器平台，以提供更灵活的弹性能力。公有云提供无需预置空间的成熟对象存储和计算资源按需购买，私有云则通过 K8s、MinIO 等技术构建资源平台。云基础设施的成熟，也推动了 Apache Doris 存算分离架构的实现，使更低成本、极致弹性、负载隔离成为可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Apache Doris 支持存算分离全新模式&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞轮科技基于 Apache Doris 内核研发全托管企业级云数仓产品（即 &lt;a href=&quot;https://www.selectdb.com/cloud&quot;&gt;SelectDB Cloud&lt;/a&gt;&quot;）过程中，设计并实现了全新的云原生存算分离架构，并将这一架构的实现贡献至 Apache Doris 社区。Apache Doris 在 3.0 版本中正式支持存算分离这一模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;SelectDB Cloud 是飞轮科技基于 Apache Doris 内核构建，能够为  Apache Doris 提供原生的存算分离架构云托管服务，并在线下稳定运行近 3 年，获得客户广泛认可。 它支持全球主流云平台，推出开箱即用的 SaaS 部署模式和更符合客户合规需求的 BYOC 部署模式，为企业提供多云一致的使用体验。此外，飞轮科技与阿里云共同打造阿里云一级产品——阿里云瑶池数据库 SelectDB 版，能够为用户提供阿里云上的全托管服务。目前，SelectDB Cloud 及 阿里云瑶池数据库 SelectDB 版已为点点互动、华瓴科技、名创优品、趣丸、波司登、MiniMax、新东方等数百家知名企业提供服务，帮助这些企业实现查询性能、成本效益、整体性价比的综合提升。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 存算分离架构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在存算分离模式下，Apache Doris 整体架构由共享存储层、计算层和元数据层三部分构成：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/309734c587eb85c96f3b6a4d4a369a4b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;共享存储层： 数据持久化到共享存储层，计算节点可以共享数据，这为计算节点的灵活性提供了基础。成熟稳定的共享存储为系统带来极低的存储成本和极高的数据可靠性，同时，公有云对象存储或者企业内部专业团队运维的共享存储可以大幅降低 Doris 的运维负担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;计算层： 计算层由任意数目的计算组构成，负责执行查询计划，每个查询仅在一个计算组内执行。计算节点无状态，本地配置本地磁盘作为高速缓存以加速查询，共享同一份数据和元数据服务，一个或者多个计算节点组成一个计算组。计算组之间是物理隔离的，可以独立扩缩容，计算节点的本地高速缓存都是隔离的，这样尽可能地保证较好的隔离性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;元数据层： 提供系统的元数据管理服务，包括数据库、表、Schema、rowset 元数据、事务信息等，支持横向扩展。未来 Apache Doris 存算分离模式下的 FE 节点也将实现无状态化，内存占用不再与集群规模相关，仅需极少的内存即可正常运行，消除 FE 内存瓶颈问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 方案设计及思考&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于存算分离架构来说，写入性能和小文件数量是影响架构稳定性和效率的关键因素。而业内常见的存算分离方案，将数据和元数据存储在共享存储中，其事务能力依赖于 FE 单点，通常面临以下挑战：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;写入性能差：FE Master 驱动的两阶段提交导致高延迟和低吞吐。小文件问题：频繁写入元数据生成大量小文件，影响系统稳定性并增加成本。扩展性差：FE 内存存储元数据，随着 Tablet 数量的增加，内存压力增大，导致写入瓶颈。数据删除风险：通过差集计算删除数据，依赖超时机制，难以避免写入和删除的冲突，存在误删除的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，Apache Doris 通过共享元数据服务有效解决了这些问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实时写入：元数据服务提供全局一致的视图，支持低延迟和高吞吐的写入。经测试，Doris 存算分离模式在 50 并发下是其他方案的 100 倍；在 500 并发下是其他方案的 11 倍。小文件控制：数据写入共享存储，元数据写入服务，可有效控制小文件数量。经测试发现，Doris 存算分离模式下的写入文件数仅是业内其它存算分离方案的 1/2。扩展性提升：未来计划将 FE 的元数据迁移至元数据服务，消除集群规模的限制。数据删除保障：Doris 采用正向删除，基于全局一致视图，确保写入和删除互斥，避免误删除数据的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 存算分离整体优势&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Apache Doris 存算分离架构，主要提供了提更低成本、极致弹性以及负载隔离这三大优势：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更低成本：与存算一体架构相比，存算分离架构综合成本降低超 90%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按需付费：相较于存算一体，无需再预置计算和存储资源，存储可按实际使用付费，计算资源则可以灵活弹性扩展。单副本存储：数据仅需在低成本的对象存储中存储一份副本，而不再在高成本的块存储中存储三个副本。热数据则缓存于块存储中，这不仅降低了存储量和硬件资源需求，还显著降低了存储成本（例如，AWS 上的 EBS 价格是 S3 的 2 到 4 倍），同时保证了查询性能。资源消耗降低：Compaction 操作所消耗的资源和副本数量成正比，在存算分离模式下只需要处理单一数据副本，资源消耗大幅减少。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极致弹性：得益于无状态的计算节点设计，能够更加灵活地应对不断变化的业务需求，提供高效、弹性的计算资源管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;弹性扩缩容：支持灵活调整计算资源，更好应对业务高峰或波动。当系统负载增加时，计算节点可以迅速扩容；而在需求减少时，计算资源又可以灵活缩减，从而避免不必要的资源浪费。计算节点按需分配：支持将不同配置的计算节点灵活分配到各计算组中，根据任务需求精确分配资源。例如，高性能计算节点被用于复杂查询或高并发场景，而标准配置节点分配于简单查询或低频请求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;负载隔离：提供高效的资源管理和负载隔离，为不同业务需求提供精细化的计算资源调度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业务间负载隔离：针对不同业务需求，可为每业务配置独立计算组，并实现物理隔离。确保各业务计算任务在专用资源上运行，减少相互干扰，保障系统的稳定性和高效性。离线负载隔离：对于大规模离线数据处理任务，可将其分配到特定的计算组，使用低成本的资源进行批量数据处理，而不影响实时业务的计算性能。读写隔离：可分别为读、写操作创建计算组和用户。写计算组专门处理数据写入（插入、更新等），而读计算组专门处理查询请求，确保在线业务的查询延迟稳定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;性能评测及对比&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我们将从导入、查询、资源成本等多个维度对 Doris 存算分离模式进行对比测试，以更直观地展示其性能表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 导入性能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高并发导入&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在相同计算资源下，针对 Doris 存算分离、Doris 存算一体、业内常见存算分离方案，进行两组实时导入性能对比测试：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;50 并发：写入 250 个 20000 行的数据文件500 并发：写入 10000 个 500 行的数据文件&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;测试结果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 50 并发下，Doris 存算分离与存算一体的写入性能基本相当，是业内其他存算分离方案的 100 倍。在 500 并发下，虽然 Doris 存算分离相比存算一体写入性能稍有损耗，但比业内其他存算分离方案仍有超 11 倍的性能优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9e9398790646181a01122bd3910abfe.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;批量导入&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在批量数据导入场景，分别基于 Doris 存算分离、Doris 存算一体进行导入 TPC-H 1TB 和 TPC-DS 1TB 测试数据集，观察其性能表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导入方式： 采用 S3 Load 导入方式，在默认配置下，对多张表的数据进行串行导入，对导入总时间进行对比。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硬件配置如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器数量： 4 台阿里云主机（1 FE，3 BE）CPU：48 核内存：192G网络带宽：21Gbps磁盘：阿里云 ESSD&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/06/06e8d93248ab682b458097eb52acb5e9.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经测试，在存算一体模式同样采用单副本的情况下，存算分离模式批量数据写入性能较存算一体模式分别提升了 20.05% 和 27.98%。（实际部署中存算一体模式一般会采用三副本，那么存算分离模式的写入性能优势会更加明显。)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 查询性能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在存算分离模式下，Apache Doris 通过多层缓存机制加速查询，这些缓存层级有效地提升数据访问速度，减少对共享存储的访问，提升查询效率。具体层次如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Doris Page Cache：使用内存缓存解压后的数据Linux Page Cache：使用内存缓存压缩后的数据本地磁盘 Cache：缓存压缩数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硬件配置如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器数量： 4 台阿里云主机（1 FE，3 BE）CPU：48 核内存：192G网络带宽：21Gbps磁盘：阿里云 ESSD&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们对存算一体和存算分离模式进行了不同缓存下的性能测试。以 TPC-DS 1TB 测试集为例，测试结果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cd/cda789e5b3c31984cbef83967121325a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;完全命中缓存时，即跑两遍取第二遍的完成时间，第一遍把所有数据均加载进缓存中，存算分离与存算一体模式查询性能完全持平；部分命中缓存时，即测试开始前清空所有缓存，取第一遍的完成时间，测试过程中数据被逐渐加载进缓存中。与存算一体相比，查询性能基本相当，总体性能损耗约 10% ，这一测试场景也与用户实际应用最为接近。完全未命中缓存时，即在每个 SQL 执行前，清理所有缓存。与存算一体相比，性能损耗约 35%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 资源成本&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;在线业务成本&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们以某企业在线业务为例，对比基于 Doris 存算一体与存算分离模式下的成本差异：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存算一体： 保存于 Doris 中的单副本数据是 100T，三副本后的总数据量是 300T。考虑到避免频繁扩缩容影响业务，磁盘使用率平常保持在 50% 左右。整体的成本如下表所示，每月资源成本支出为 36962.7 美金。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/8156445848f2f86b39c933bca913db2e.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存算分离： 同样的数据规模，使用存算分离模式后，仅需要存储单副本存储在对象存储，热数据在本地磁盘上 cache 一份。如下表所示，每月资源成本支出下降到 22212.65 美金，相较存算一体模式节省了 40% 成本支出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/2579cb0d51b55ebba300ab9f606cf3e9.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;历史数据成本&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 200TB 的历史数据为例，存算一体和存算分离模式下的资源占用如下表。存算一体的资源成本为 48851.1 美金/月，而存算分离的成本仅为 4502.4 美金/月，成本可以降低 90% 以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa50a5181b3cf4a5c5e6e5cf05fea4dd.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用户声音&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“网易 163 邮箱使用 Apache Doris 替换了原先 Clickhouse、Elasticsearch、Starrocks 等多技术栈。凭借其存算分离架构、湖仓一体、倒排索引能力，支撑了亿级数据实时写入与秒级查询响应，并实现存储成本降低 10 倍、日志检索性能提升 10 倍的成效。”     ——— 网易 163 邮箱&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“波司登集团基于阿里云数据库 SelectDB 版作为数仓升级方案，借助阿里云 SelectDB 云原生存算分离架构，实现资源隔离、弹性扩缩容，并取得查询性能提升 2-5 倍、总体成本降低 30% 以上、效率提升  30% 的可观收益。”  ——— 波司登集团&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“阿里云数据库 SelectDB 版作为 MiniMax 日志存储服务的核心支撑，为在线和离线业务提供了高效、稳定的查询与聚合分析能力。其存算分离的弹性架构，以及对实时物化视图、租户资源隔离及冷热分离等企业级特性的支持，不仅有效解决了日志场景下 PB 级别数据查询的性能瓶颈，还通过智能化的资源调度与存储优化，实现了成本与效率的最佳平衡，为业务的高效运转提供了坚实保障。”   ——— MiniMax infra 团队&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“阿里云数据库 SelectDB 版为新东方多个核心场景提供了强有力的技术支撑。在湖仓一体实践中，它作为 Paimon 的高性能查询分析引擎，为湖上数据提供了强大的加速分析能力，实现 3-5 倍的性能提升。在半结构化数据处理场景中，依托存算分离架构、Variant 类型高压缩率、倒排索引以及 OSS 低成本存储方案，快速实现海量日志的检索分析，与 Elasticsearc 相比，查询分析性能提升 3-倍，存储成本降低 5 倍，极大优化了资源利用率。”    ———新东方&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“高途教育使用 SelectDB 应用在 BI 报表类高并发低延迟的场景，解决了需要根据数据选择引擎的难题。其存算分离架构和快速扩容的特性，保障了高峰场景数据的秒级产出；多租户的资源隔离方案为成本控制和资源分摊等提供了坚实的基础。期待未来在数据湖和动态扩容方面的进一步建设，加深双方合作。”   ———高途教育&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结束语&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以上就是对 Apache Doris 存算分离核心特性及使用方案的完整介绍。存算分离架构的引入，带来更低成本、极致弹性、负载隔离等显著优势。在这一架构的加持下，Doris 凭借其强大的实时数据分析、湖仓融合分析及半结构化数据处理能力，已然成为搭建现代化数据平台的理想选择，能够有效简化当前纷繁复杂的数据分析技术栈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，Apache Doris 将持续增强存算分离模式的功能，包括支持存算分离的快照、时间旅行以及 CCR，提升系统的稳定性和易用性，比如 FE 无状态。为使用者提供更加弹性、高效、稳定、流畅的使用体验。&lt;/p&gt;</description><link>https://www.infoq.cn/article/zaUGz5EB7W3375ZDBPgm</link><guid isPermaLink="false">https://www.infoq.cn/article/zaUGz5EB7W3375ZDBPgm</guid><pubDate>Tue, 18 Mar 2025 09:14:10 GMT</pubDate><author>SelectDB</author><category>数据库</category></item><item><title>大模型应用开发的破局之路：字节跳动 Eino 框架实践 ｜QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;字节跳动研发工程师沈桐已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6349&quot;&gt;《大模型应用开发的破局之路：字节跳动 Eino 框架实践》&lt;/a&gt;&quot;的主题分享。在大模型应用开发过程中，如何将研究成果与软件工程紧密结合，并解决新领域中的新问题是开发者面临的重大挑战。本次分享将介绍基于 Golang 的解决方案，以字节跳动的实际产品功能为例，重点介绍 Eino 框架在 “组件” 抽象和业务编排方面的创新实践，解决字节实际业务场景中的问题。希望能让听众收获 Eino 框架在大规模生产环境中的应用价值与实践经验参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;沈桐毕业于北大，在字节跳动工作三年半，近一年在 AI 相关部门聚焦 AI 应用开发平台相关工作，是&amp;nbsp;&lt;a href=&quot;https://github.com/cloudwego/eino&quot;&gt;Eino&lt;/a&gt;&quot;&amp;nbsp;开发框架的核心开发者之一。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 大模型应用开发过程的挑战研究成果与软件工程的脱节语言和生态差异认知和术语差异新领域的新问题可枚举的组件，多变但有迹可循的编排模式基于数据帧的流式处理2. 基于 Golang 的解决方案基于 Golang 的大模型应用开发框架：EinoGolang 的优势：编译期强类型检查定义组件抽象，提供组件实现解决组件编排使用的问题两个重要的解决方案：通过类型对齐，尽早发现代码错误流式数据传输和处理3. 分享一个字节跳动的实际产品功能从需求到生产的过程需求引发框架功能的扩展编排实现业务流程具体痛点解决过程4. 经验总结应用开发框架作为基础设施的真正作用和正确使用姿势从实践出发，按需跳出框架的范围您认为，这样的技术在实践过程中有哪些痛点？基于 Golang 的 AI 应用生态，相比 Python 的 AI 应用生态，在覆盖面和丰富度上有不足Eino 作为一个基础设施性质的开发底座，需要时刻权衡“积极提供最新的功能扩展”和“最大程度保证 API 稳定，谨慎迭代”两个相反的考量演讲亮点基于实践经验，总结大模型应用开发过程中的真实痛点，并给出可落地的解决方案分享真实案例，剖析实际问题解决过程听众收益针对大模型应用开发过程中的真实痛点，获取可落地的解决方案和思路通过具体案例和时机演示，获得感性认识，提供思考的养料&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/foiExHh6HgFskylH7AGI</link><guid isPermaLink="false">https://www.infoq.cn/article/foiExHh6HgFskylH7AGI</guid><pubDate>Tue, 18 Mar 2025 09:07:29 GMT</pubDate><author>QCon全球软件开发大会</author><category>字节跳动</category><category>AI&amp;大模型</category><category>软件工程</category></item><item><title>昇腾，又赢一局</title><description>&lt;p&gt;在 DeepSeek 轰炸 2025 春节之前，昇腾几乎就已经是中国市场最成功的“玩家”之一，而这种成功甚至不是完全通过出货量来定义的——虽然有数据称 2023 年华为海思（含昇腾系列）以 24.8% 的份额位居国内 AI 芯片市场首位，领先第二名至少 10%，但从 2024 年之后，网络上就不再有可参考的具体数据了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 InfoQ 近期沟通的多位业内人士的意见来看，业界对昇腾的信心，并没有随着数据图表的消失而变弱，反而越来越强。究其根源，在于昇腾的技术能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;GPU 的制造有五大部分关键技术：制程工艺、指令集设计、计算单元设计、内存和缓存设计，以及对应的软件栈设计。除了代工由其他公司完成，从指令集到软件栈，华为完成了全栈自研。其中 CANN 软件栈，对标英伟达的 CUDA，综合下来，使得单卡算力逼近英伟达 A100 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更大的启发在于，DeepSeek 绕过 CUDA 后，实现了惊人的性能提升，比如用 PTX 语言实现的硬件效率，比 Meta 等其他公司高出 10 倍，能够在五天内完成其他模型需十天才能实现的训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;昇腾全栈自研，想必也有这样的潜能。这是许多人看好昇腾的内在逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，国内的 AI 算力市场，留给其他玩家的空间并不大，DeepSeek 的爆发，是非常难得的搅局机会。但很可惜，这一次，昇腾似乎又跑在了国产芯片的最前方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;昇腾不喜欢玩虚的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以前人们只知道 DeepSeek，最近大家才知道，有个东西叫做“满血版”的 DeepSeek。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“满血版”，是大家认知中的 DeepSeek：参数规模 671B，支持 200k tokens 超长上下文理解，性能媲美 OpenAI o1。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而对应的是所谓的蒸馏版，如 DeepSeek-R1-Distill-Llama-70B。一些量化技术压缩后的版本，参数量仅为 DeepSeek-R1 的 1%-5%，使用体验也大幅退步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;功能上的阉割相对更加严重，DeepSeek 提供良好体验的前提是，同时勾选深度思考和联网搜索，但在一些厂商工具集成的入口上，又变成了“二选一”：想体验推理能力，就不能联网搜索，效果大打折扣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这背后的关键在于，部署 DeepSeek-R1 还是相当耗费硬件资源的，比如使用 BF16 权重进行 DeepSeek-R1 的推理，硬件成本至少在 50-170 万之间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/1825f1884901d2097b223ebd947d0b97.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源有限，但想蹭热点，心态可以理解，不过上线一个“空壳”版本给用户，多少有点敷衍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于昇腾而言，这显然不是什么问题——作为 DeepSeek 推理服务的算力供应商，有业内人士透露，昇腾目前是国内唯一一个从预训练、微调、强化学习全流程支持 DeepSeek 的 AI 训练平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有接近华为的人士透露，昇腾的技术团队围绕 DeepSeek 做了许多技术上的优化，比如：通信效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 OpenAI 为代表的模型厂商走的是“大专家”路线，专家数量以 16 个为主，单个专家的参数量较大；而 DeepSeek 走的是“小专家”路线，单个专家的参数量较小，但专家数量超过 256 个。更多的专家数量，意味着更高的通信开销，如果通信效率不能得到优化，推理速度就会受到限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;昇腾通过伪 EP 混合并行算法来优化通信效率。所谓伪 EP 混合并行算法，顾名思义，是对专家并行（Expert Parallel，EP）算法的模拟和简化，可以理解为适用于大规模分布式 AI 模型训练场景的并行计算策略。具体有三种优化方式：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;专家并行（EP）：将模型中的不同专家（Expert）分配到不同的计算设备上，每个设备负责处理特定的专家。这种方式可以减少单个设备的计算负担，提高训练效率。数据并行（DP）：将数据集分割成多个子集，并在不同的设备上同时进行训练。这种方式可以充分利用多个设备的计算能力，加速模型训练。张量并行（TP）：将模型中的张量（多维数组或矩阵）按照特定维度分割，并分配到不同计算设备上进行并行计算。这种方式可以减少单个设备的内存需求，提高模型的可扩展性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;昇腾借此将 DeepSeek-R1 推理时的通信性能提升了 30%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，昇腾对 DualPipe、跨节点 All2All ，尤其是 DeepSeek 团队提出的强化学习算法 Gpro 也进行了适配，借此使基于昇腾运行 DeepSeek 的性能和效率可以更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一个“虚拟联盟”正在结成&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，通信优化是适配 DeepSeek 相当关键的技术命题，而这一直都是华为擅长的内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如 2024 年 9 月发布的昇腾超节点技术，通过机柜级大带宽通信设计，将 NPU 节点从单机 8 卡拓展到机柜几十卡以上，中间采用了大量的自研通信协议，如：HCCS（High-Performance Computing and Communication System）、NB2.0、NHR 等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;昇腾官方口径数据称，昇腾超节点技术可以将带宽利用率从不足 40% 提升到了 60% 以上，可实现 2250 节点（等效于 18000 张卡）超大规模无收敛集群组网。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这使得昇腾成为国内少有的具备超大规模集群组建能力的厂商，并直接促成了华为和三大运营商、科大讯飞等企业的合作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，华为的这种技术优势，以及对 DeepSeek 的深度优化能力，正在成为其撬动行业资源，形成统一阵地的跳板。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;春节前，昇腾和硅基流动达成了深度合作。InfoQ 独家消息显示，硅基流动是 DeepSeek-R1 发布后，主动找到昇腾，提议合作的第一家 AI Infra 公司。双方围绕 DeepSeek 的合作进展非常快，以至于集结了昇腾、硅基流动、幻方三方的座谈会，以及基于昇腾的性能调优版本的上线，全部发生于大年初一前。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到了 2 月 12 日，又有消息称，小鹏汽车董事长何小鹏、宇树科技创始人王兴兴、投资人徐新、硅基流动 CEO 袁进辉、面壁智能 CEO 李大海、霸王茶姬张俊杰等前往华为拜访学习，并与任正非进行交流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至少在公开报道中，这样的闭门会已经很久没发生过了，简直不像是华为的风格。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c7283b203c54ead18b0bfbbc233991ac.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;或许，DeepSeek-R1 的诞生，已经触发了中国 AI 产业发展的一个隐藏的“开关”，围绕华为昇腾结成的“虚拟产业联盟”，正在进一步加深交流和合作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大量的 DeepSeek 一体机，于近期密集发布，发布厂商包括华鲲振宇、宝德、神州鲲泰、长江计算等，全部基于昇腾产品构建。DeepSeek &amp;nbsp;一体机，几乎成了产业合作的一个象征和徽记。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另有数据显示，目前各行业已有80+企业基于昇腾快速适配/上线了 DeepSeek 系列模型，并在对外提供服务，此外还有 20+ 企业在适配测试中，预计未来两周内全部完成上线，总体来说，国内 70% 的企业将基于昇腾向 DeepSeek 靠拢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相较于进口 GPU 方案，昇腾芯片的本地化服务和团队，对部署DeepSeek的效果影响显著。以万卡规模的数据中心为例，MindSpore 工具链的自动并行功能，使分布式训练代码量减少 70%。有某智慧城市项目的实践表明，采用昇腾方案后，AI 推理模块的 TCO（总拥有成本）三年期下降 42%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这么来看， DeepSeek 取得的成绩，只是一个阶段性的胜利。而作为产业链的上游，昇腾担负的风险却在减少，着实赢了个彻底。&lt;/p&gt;</description><link>https://www.infoq.cn/article/jd7HukITcYMO3p5OXDqb</link><guid isPermaLink="false">https://www.infoq.cn/article/jd7HukITcYMO3p5OXDqb</guid><pubDate>Tue, 18 Mar 2025 08:55:07 GMT</pubDate><author>王一鹏</author><category>华为</category><category>芯片&amp;算力</category></item><item><title>如何为预训练LLM添加新token？</title><description>&lt;p&gt;为了使得通用的预训练大模型能够满足专业领域的需求，我们往往会对通用的大模型进行微调。实际上通过微调框架或技术对大模型进行微调之后可能会发现，微调之后的模型其实并没有达到预定的效果。当然这个原因是多方面的，有时候取决于微调的数据集，微调时对模型进行的一些列优化处理等等，但是另外一个不容忽视的问题则是需要在微调之前向预训练LLM的分词器增加一些专业领域的token，以帮助LLM在微调过程中提高对数据集的理解能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文的主要目的是介绍往LLM分词器中新增token的原因和基本方法，同时也会介绍在云原生的场景下如何将该操作集成到LLM微调的流程当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么需要在预训练LLM中添加新token&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对预训练LLM进行微调之前，通常会在预训练LLM分词器中添加额外的新token。这样做有以下几个目的：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;1.&amp;nbsp;引入领域特定词汇&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果需要在特定领域（例如医学、法律、金融）进行微调，则需要引入一些基础模型无法识别的新术语、缩写或技术术语。添加新token 有助于模型更好地理解和生成与该领域相关的文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;2.&amp;nbsp;处理词汇表之外的单词&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预训练LLM 使用分词算法（如 BPE 或 WordPiece），然而有些词可能会被分解为低效的子词。因此添加额外的标记可以提高文本生成的效率和准确性。例如，`ChatGPT` 可能会拆分为 Chat 和 GPT，从而导致语义碎片化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;3.&amp;nbsp;适应多语言扩展&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果针对原始训练数据中没有很好覆盖的新语言进行微调，则为唯一的单词或字符添加额外的标记可以提高LLM的微调性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;4.&amp;nbsp;引入自定义格式标记、指令调整和提示词工程&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果需要在微调数据中引入特定结构（例如，markdown 格式、XML 标签或对话角色的占位符），新token可以帮助预训练LLM更有效地学习所需的结构。除此之外，一些预训练LLM使用特殊标记（如 &lt;question&gt;、&lt;answer&gt; 或 &lt;context&gt;）来指导推理响应。这些额外的token有助于根据结构化输入调节模型。在这些情况下，最好引入特殊token，而不是通用token。本文中的示例只包含了添加通用token的例子。&lt;/context&gt;&lt;/answer&gt;&lt;/question&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综上所述，添加额外的token 有助于预训练的 LLM 在微调过程中更有效地适应特定领域的要求，从而减少对大量训练数据的需求。这也是平衡微调效果和计算成本的关键技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;实施此技术的步骤有哪些？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基本上，通过transformers 包实现额外 token 的添加有如下 4 个步骤：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;1.&amp;nbsp;修改分词器&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;a)&amp;nbsp;加载分词器&amp;nbsp;(例如从&amp;nbsp;Hugging Face’s transformers 库中加载)；&lt;/p&gt;&lt;p&gt;b)&amp;nbsp;添加新的token到分词器的词汇表中；&lt;/p&gt;&lt;p&gt;有关向分词器添加新token的更多详细信息，请参阅&lt;a href=&quot;https://www.infoq.cn/article/1Tao46HoQGF4Rj37R7I2#transformers.PreTrainedTokenizer.add_tokens&quot;&gt;new_tokens&lt;/a&gt;&quot;&amp;nbsp;和&lt;a href=&quot;https://www.infoq.cn/article/1Tao46HoQGF4Rj37R7I2#transformers.PreTrainedTokenizer.add_special_tokens&quot;&gt;special_tokens&lt;/a&gt;&quot;；&lt;/p&gt;&lt;p&gt;c)&amp;nbsp;调整模型中嵌入层的大小以适应新的token。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要调用resize_token_embeddings 来调整模型的嵌入矩阵的大小以匹配其分词器。此外，请参阅&lt;a href=&quot;https://www.infoq.cn/article/1Tao46HoQGF4Rj37R7I2#transformers.PreTrainedModel.resize_token_embeddings&quot;&gt;此处&lt;/a&gt;&quot;了解更多详细信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;2.&amp;nbsp;在模型的嵌入层中正确初始化新token 的嵌入权重&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新的token被添加到分词器中，其索引从当前词汇表的长度开始，并且在应用分词算法之前将被隔离，因此这些新的token和来自分词算法词汇表的token不会以相同的方式处理。除此以外，默认情况下，模型嵌入层中新token的嵌入权重是随机初始化的，这可能会导致最初的训练不稳定或性能不佳。因此，一般在每加入一个新的token之前，事先计算当前模型嵌入层中所有token的平均嵌入权重（average embedding weight），以此作为当前新加入token的嵌入权重。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;3.&amp;nbsp;保存并重新加载更新后的分词器和模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;添加新token后，保存更新的分词器和模型以供将来使用。当然，后面需要加载分词器和模型的时候需要与保存时候的路径相对应地加载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;4.&amp;nbsp;使用添加的token对预训练 LLM 进行微调&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果新token代表专业领域的概念，则在相关数据集上微调模型至关重要。使用包含这些新token实例的数据集并训练模型以正确理解其用法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据以上描述，我完成在GitHub 完成了一个示例程序&lt;a href=&quot;https://github.com/zhl-llm/AddExtraTokens2LLM&quot;&gt;AddExtraTokens2LLM&lt;/a&gt;&quot;。读者克隆该例子后可直接在您的设备上启动运行，欢迎尝试并提出宝贵意见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;云原生场景下如何添加新的token操作集成至LLM的微调流程&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论我们选择何总微调技术或者框架，预训练LLM微调的流程大致如下图所示。红色虚线框的内容包含的就是特定领域的token 表，我们均需要在正式微调模型前将其加入到模型的分词器词库和模型的embedding层中，并且该操作应该在微调模型加载前，更靠前于导入LLM微调的数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/c6/ca/c618e350d7291c3aa1f4e2a93532aaca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结合云原生的场景下，在预训练LLM微调前该如何为其模型添加新的token？对于这个问题其答案取决于多个方面的考量，比如用什么技术框架对模型进行微调，微调的集群是单集群还是多集群等。笔者综合遇到的大部分场景，将这此集成过程抽象成如下三种情况：Pod Level的模型微调，Node Level的模型微调和Cluster Level的模型微调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Pod Level的模型微调，这是所有情况中最简单的场景，一般情况下其微调的时间不会太长，技术也不会太复杂。笔者对此的解决方案是可以将微调的任务放到Pod的常规containers中去实现，然后将添加新的token的任务放入Pod的Init Containers 中。熟悉 Kubernetes Init Containers机制的读者应该明白，Init Containers可以保证其任务在所有常规的containers启动之前完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Node Level的模型微调，这种场景通常情况下会涉及各种分布式微调框架技术的使用，比如基于Ray的大语言模型微调。除此以外，Node Level也可以出现多集群微调的场景，为了简单起见，笔者只考虑单集群的 Node Level的模型微调。而对此的解决方案基本上会定义微调模型的流程，此目的自然是为了实现对微调过程的控制干预。对于使用Ray等框架来说可能相对比较简单，因为用户只需要将给模型添加token的工作封装成 Ray script (Driver)，然后再微调之前通过 Ray Job API 事先提交一个Ray Job即可。如果想要做得更通用灵活，笔者的建议则是为模型的微调实现一个Kubernetes Operator，通过定义Kubernetes CRD来实现模型微调流程的扭转。这些流程节点笔者总结如下：1. 从模型下载。2. 为模型添加新token并保存模型。3. 重新加载处理后的模型。4. 导入微调数据集（可能包含微调数据集的处理，可以添加子流程）。5. 模型微调（需要包含调度，自动扩缩容和断点续训等处理机制）。6. 微调后模型保存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Cluster Level的模型微调，这种场景基本上属于很大规模的模型微调，需要多集群的协作是不可避免的，因此多集群的解决方案是必不可少的。事实上笔者到目前为止并没有遇到这种场景，所以也只能提出一些理论的解决方案。首先需要选择合适的多集群解决方案方。其次要实现多集群Task Discovery（任务发现）的能力。此能力应该是实现各种关键任务的基础，包括本文主要提及的为模型添加token的任务。最后，需要把各种任务及其输出串联起来，形成完整的流程拓扑。鉴于笔者没有实践过，因此不再赘述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然写本文的主要目的是为了介绍“如何为预训练LLM添加新token？”，但是笔者抛砖引玉，也相信读者能从这一隅之地出发，为基于云原生场景下，解决大语言模型服务推理部署，微调，训练等场景提出更加通用优秀的解决方案，为业界分享更多的实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;关于作者&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张怀龙，曾就职于阿尔卡特朗讯、百度、IBM、英特尔等知名公司担任高级开发职位，拥有 16 年的技术研发经验。作为 Istio 社区的维护者，他专注于云原生微服务，并在云原生与 LLM 技术的交叉领域进行创新。他为 OpenVINO、Kserve 等社区做出贡献，致力于云原生场景下的 LLM 推理，是 OPEA（企业 AI 开放平台）社区的开发者和维护者。作者还曾在 KubeCon、ServiceMeshCon、IstioCon、GOTC、InfoQ/Qcon 和 GOSIM 等会议上发表演讲。&lt;/p&gt;</description><link>https://www.infoq.cn/article/1Tao46HoQGF4Rj37R7I2</link><guid isPermaLink="false">https://www.infoq.cn/article/1Tao46HoQGF4Rj37R7I2</guid><pubDate>Tue, 18 Mar 2025 08:27:25 GMT</pubDate><author>张怀龙</author><category>AI&amp;大模型</category></item><item><title>云 IaaS 虚拟化技术在 AI 时代的创新与挑战 ｜QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;百度智能云基础公有云部主任架构师应茹已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6293&quot;&gt;《云 IaaS 虚拟化技术在 AI 时代的创新与挑战》&lt;/a&gt;&quot;的主题分享。大模型技术的迅速发展和AI应用的繁荣，对云 IaaS 基础设施提出了新的要求。IaaS 的单机虚拟化技术，基于其计算与设备的虚拟化能力，为云上应用提供极致性能与弹性的计算实例，在这样的背景下，百度智能云也面向 AI 原生应用做了诸多探索和实践，比如基于大模型性能和效率的评价视角进行端到端技术栈优化，面向 Serverless 训推场景推出安全容器虚拟化形态，以及面向 AI 数据安全场景落地训推一体的 AI 机密计算实例等等，以支撑 AI 原生应用在云上的高效运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/52287c118d896eb02265f67b35571922.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应茹于2013 年加入百度，先后负责大数据集群分布式调度器、云 IaaS 虚拟机控制面、存储虚拟化数据面、单机虚拟化组件与内核的研发工作。她在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲基于 DPU 的全资源售卖虚拟化架构由大模型评价视角驱动的虚拟化技术栈优化Serverless 场景 AI 安全容器AI 机密计算实例您认为，这样的技术在实践过程中有哪些痛点？全资源售卖架构下无法达到 100% 持平裸金属的性能表现安全容器在弹性与企业级特性之间需要有所取舍机密计算实例在安全与性能之间需要由所取舍，性能优化之路道阻且长演讲亮点整体虚拟化架构从传统架构向基于DPU的全资源售卖架构转变，各 DPU 厂商的在设备模拟方面的实现方案各有差异，虚拟化架构需要清晰划分边界高效对接、纳管同时推动设备虚拟化往统一的形态去发展AI 智算视角的虚拟化技术迭代，如何利用通算场景积累的优势，同时又面向智算去做演进和优化。听众收益理解云计算 IaaS 虚拟化架构的演进状态在 AI Infra 蓬勃发展的背景中，IaaS 虚拟化架构可以做什么，做了什么，都有什么样的收益&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/rlAx7gsUsMGwASW4g69F</link><guid isPermaLink="false">https://www.infoq.cn/article/rlAx7gsUsMGwASW4g69F</guid><pubDate>Tue, 18 Mar 2025 08:16:02 GMT</pubDate><author>QCon全球软件开发大会</author><category>百度</category><category>云原生</category><category>AI&amp;大模型</category></item><item><title>英特尔CEO陈立武薪酬曝光，最高可达5亿元；裁员10%不发年终奖、HR和员工互殴？抖音回应；哈佛大学等美名校冻结招聘 |AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;部分公司纷纷“反内卷”，每周4.5工作制引讨论&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有媒体报道，大疆开始反内卷，一到21点，主管和HR分三轮赶人下班，禁止员工加班。上海区域更直截了当，办公楼到21点准时关灯。根据媒体实践观察，20：40分时，大疆办公区域仍灯火通明，陆续有少量员工从办公楼走出；到了20：50分，办公楼小部分区域开始熄灯，迎来了下班高峰；21：00，大部分楼层已关灯。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3 月 10 日消息，有市场消息称，美的从这周起开始提倡各部门领导严谨控制加班，规定 18:20 不允许有人还在公司加班，同时也禁止了员工就餐后再返回工位继续加班的现象。到目前为止，一到下班时间，HR 就开始挨着部门催促大家抓紧时间下班。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，联想集团发布文章《这里的夜晚静悄悄》，称“大厂反内卷文化看联想集团”。联想集团表示，反对996，大部分人都能准时下班，在联想并不是什么新鲜事，也是联想人的共识。联想集团还称“不打卡，灵活办公，不会形式主义地要求几点下班。但这不等于鼓励‘躺平’，因为我们一直相信真正的竞争力来自科技创新，而非无意义的内耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;两会结束后，从办公大楼的清理到PPT的废弃，各大公司的“反内卷”活动日益激烈，其中一批浙江的民营企业更是推出了每周4.5天工作制。对于4.5天工作制的呼声，一线蓝领劳动者与大多企业白领员工的态度形成鲜明对比。对于一线工人而言，4.5天工作制听起来很美好，操作起来却面临困难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;传京东算法全员将进行30%普调涨薪&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，在小红书和脉脉上等平台上均有网友爆料，京东全体算法岗喜提30%普调涨薪（广告 / 搜推 / 算法交易部全覆盖），4月1号开始生效。据内部员工透露，Leader 本周已经口头通知，这次涨薪是全员覆盖，一个不落。近三年来，京东已经连续六轮上调员工薪酬，不断优化薪资体系。据公开数据可知，2024届京东算法岗薪资总包大概在36.5k，按照超75%的涨幅算，2025届算法岗薪资可能达到64k。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;字节裁员10%不发年终奖、HR和员工互殴？抖音官方辟谣&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月14日，抖音黑板报官方辟谣称，近期，部分社交平台上出现了“字节裁员10%”“未给年终奖”“HR和员工互殴被打进医院”等大量不实信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;抖音方面表示，经核实，相关内容皆为谣言，系2023年1月类似谣言的变体，公司并未大量裁员，也不存在HR与员工因“裁员”“年终奖”发生肢体冲突的情况，“互殴”“薅头发”等描述纯属捏造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其还表示，此前，公司已针对多名造谣用户发起维权诉讼，并获得法院胜诉生效判决，相关造谣或传谣的用户也已公开道歉。“请大家注意甄别网络信息，对于类似发布不实信息影响公司正常经营的行为，我们将依法维权。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7fafe7c1ed635ce4afa2e5cdfdfeb5ee.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;传腾讯向英伟达采购数十亿元规模H20芯片&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月13日，据媒报道，腾讯公司正在加速大模型应用的推进，近期为此向英伟达采购一批新芯片，为向腾讯按时交付订单，英伟达H20芯片短期出现供不应求的情况。一位接近英伟达的人士表示，腾讯的这笔订单金额约合几十亿元量级。一位阿里云人士表示，据其了解，腾讯短期内采购大量H20主要是为了应对微信接入DeepSeek的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，英伟达CEO黄仁勋将于周二在GTC大会上揭晓新一代AI芯片平台Rubin架构。这家全球市值前三的科技巨头延续了以杰出科学家命名的传统，此次新品以发现暗物质的天文学家薇拉·鲁宾（Vera Rubin）命名，标志着其持续通过产品命名向科技界多元群体致敬——尽管当前美国特朗普政府正在削减多元化、公平与包容（DEI）计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;英特尔CEO陈立武薪酬曝光，最高可达5亿元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3 月 13 日消息，当地时间周三，英特尔宣布任命现年 65 岁的陈立武为下一任首席执行官，自 3 月 18 日起生效。值得一提的是，陈立武与前任 CEO 基辛格在扭亏为盈的策略上发生冲突，于去年 8 月卸任英特尔董事会成员，意见相左之处包括员工规模、代工策略和官僚主义文化。后续，他将重新加入英特尔董事会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据彭博社报道，英特尔公司周五公布了为其新任CEO陈立武制定的薪酬方案。如果陈立武能够在未来几年实现绩效目标，他的薪酬可达到大约6900万美元（约合5亿元人民币）。据悉，陈立武的薪酬包括 100 万美元基础工资、200% 绩效奖金、1440 万美元长期股权奖励、1700 万美元股票绩效奖励（五年内兑现）、960 万美元股票期权和 2500 万美元新员工期权奖励。陈立武同意上任后 30 天内购买 2500 万美元英特尔股票。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更多可查看：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TikTok的8个评分等级曝光&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据《商业内幕》报道，目前正值TikTok的绩效考核季，一些表现不佳的员工面临两个选择：要么接受绩效改善计划(PIP)，要么拿着离职补偿金走人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，字节跳动对TikTok员工的绩效考核一年两次，按照八个等级对员工进行评估，从“杰出”(outstanding)到“不达标”(failed)。四名来自亚洲和美国办公室的现任及前任TikTok员工证实了这一评估体系的存在。不过一名美国员工表示，他们见过的另一种评估格式在措辞上略有不同(如形容词或词序)，但意思一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该评估体系依据三个主要标准对员工进行考核：工作成果(Output)、领导力原则(Leadership Principles)和字节范(ByteStyle)。字节范是字节跳动的企业文化，内涵包括“追求极致、务实敢为、开放谦逊、坦诚清晰、始终创业、多元兼容”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a6/a6e71cdd3ba161bc3053b523854e2e88.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果员工得到“M-”(未完全达到期望)或“I”(有待改进)的评分，这可能会导致他们被纳入绩效改进计划(PIP)，或者拿着离职补偿金走人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;哈佛大学等多家美国名校冻结招聘教职员工&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月10日，哈佛大学宣布冻结招聘教职员工，原因是特朗普政府可能削减联邦资金，导致学校面临资金困难。宾夕法尼亚大学、斯坦福大学、麻省理工学院、北卡罗来纳州立大学及哥伦比亚大学医学院等多家名校近期也全部或部分冻结招聘。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;特朗普政府大砍科研资金，还威胁要切断允许“非法抗议”的学校的联邦资金，3月7日已经取消了哥伦比亚大学的4亿美元联邦拨款，美国高校面临巨大压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;哈佛大学总统艾伦·加伯（Alan Garber）在一封给教职员工的邮件中宣布，哈佛大学将立即暂停所有教职员工的招聘，原因是“由快速变化的联邦政策驱动的重大财政不确定性”。尽管哈佛拥有超过530亿美元的捐赠基金，但仍然需要失去政府拨款做准备，冻结招聘教职员工是无奈之举，预计至少持续到本学期结束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了哈佛大学，还有多家美国名校宣布了类似措施。宾夕法尼亚大学近日宣布实施招聘冻结，并审查资本支出，以应对联邦资金削减的不确定性。斯坦福大学2月26日宣布冻结员工招聘，不过暂时不包括教职员工、临时工和学生工人。麻省理工学院（MIT）2月14日宣布对所有非必要职位实施招聘冻结。哥伦比亚大学医学院2月16日宣布暂停招聘和其他支出。北卡罗来纳州立大学2月14日宣布暂停所有教职员工的招聘活动。西北大学和华盛顿州立大学等也在考虑控制成本，可能推出类似招聘冻结措施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;200万人等候使用的Manus，曾因出价低婉拒了字节收购&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月12日晚间消息，开发出Manus的中国创业公司Monica宣布，7天来，Manus使用申请等候名单增加到200万人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，Monica在2024年年初曾经与字节进行过一轮收购谈判。当时字节出价3000万美金，但因为远远达不到创始人肖弘的心理价位，这次谈判最终不欢而散。接近Monica内部的从业者表示，字节收购的逻辑是以3000万美金的价格收购团队，将其团队和产品整合到豆包体系中，但是Monica进入字节后就会跟字节大模型战略绑定，就丧失了Monica产品上的独特优势。这也是肖弘及其团队不看好这场收购的主要原因。Monica的最新融资于2024年年底close，目前公司估值接近一亿美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;日前，3月10日，通过查找/opt/.manus/下的文件，网友@jianxliao成功获取Manus沙盒运行代码，并据此发现Manus的基座模型。对于此次“被动”披露，Manus 联合创始人季逸超发文回应称：沙盒代码的“泄露”并非意外或漏洞，而是Manus设计的一部分。每个对话都有独立的沙盒环境，与其他会话完全隔离。用户可以通过Manus界面直接进入沙盒。沙盒中的代码仅用于接收来自Agent的命令，Multi-Agent实现是Manus的关键特性之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前Manus处于“神秘”状态，外界主要通过产品体验与复刻产品，倒推猜测其底座模型。此次Manus终于揭开谜底，季逸超在社交平台评论区回复网友称：Manus目前使用的是Claude与阿里旗下不同的Qwen微调模型。团队早期只能使用Claude 3.5 Sonnet v1，因此需要大量辅助模型。季逸超表示，目前来看Claude 3.7非常有潜力，正在内部测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;字节火山引擎、阿里阿里通义实验室相关负责人离职&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月12日，据报道，原字节跳动AI大将、火山引擎AI解决方案负责人骆怡航于近日加入生数科技，担任CEO一职，全面负责公司研发、产品、商业化及团队管理工作。在加入生数科技之前，骆怡航担任字节跳动火山引擎AI应用产品线一号位，汇报火山引擎总裁，全权负责产品线的战略、产品和商业化。据悉，火山引擎AI应用产品线由骆怡航从0组建，涵盖多个传统AI及大模型应用产品，管理规模数百人，服务全球多个行业及国家的近万家客户，该产品线当前是火山引擎的重点产品线之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里通义实验室语音团队负责人鄢志杰已于近期离职，离职时间为2月15日，按照以前的P序列（原阿里内部职级体系）比照，其职级为P10。鄢志杰曾在语音及文本识别领域顶级学术期刊及会议发表多篇论文，长期担任语音领域顶级学术会议及期刊的专家评审，并拥有多项美国及PCT专利。此番从阿里离职之后，鄢志杰的下一站尚不明朗，但行业普遍预测这位技术大牛，或将投身AI创业大潮。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;苹果Siri主管：AI功能一再跳票“丑陋而令人尴尬”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;苹果公司负责Siri虚拟助手的高级主管向员工表示，Siri关键AI功能的推迟发布“丑陋而令人尴尬”，而在技术尚未准备好之前就对外宣传的决定更是雪上加霜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据彭博新闻，担任苹果高级总监的Robby Walker在Siri部门的一次闭门全体会议上发表了上述言论，他直言团队正经历一段糟糕的时期。Walker还表示，目前尚不清楚这些增强功能究竟何时能够推出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;分析认为，这番坦率的讨论暴露了苹果在人工智能领域的困境。目前，Siri智能化水平落后于竞品，已成为苹果AI挑战的象征。而苹果的困境在上周爆发，当时公司公开承认关键功能将被无限期推迟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在全体会议上，Walker表示，他的团队成员可能会因功能推迟而感到愤怒、失望、精疲力竭和尴尬。公司一直在努力争取今年春天推出相关技术，但知情人士对媒体表示，现在这些功能最早也要等到明年才能推出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;特朗普花8万美元选购一辆红色特斯拉支持马斯克，马斯克再捐1亿&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，当地时间3月11日，美国总统特朗普在白宫外选购了一辆全新的特斯拉Model S红色电动汽车，以此来表明他对马斯克电动汽车公司的支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;报道称，由于马斯克带领的政府效率部正大幅削减联邦开支，精简联邦政府机构，特斯拉在美国国内面临大量批评反对的声音。报道称，包括Model S、Cybertruck和Model Y等多种特斯拉电动汽车于当天中午抵达白宫外，最终特朗普选择了Model S，“我喜欢那款，也想要同样的颜色，我会给开一张支票，我不想要折扣。”他说。这场活动暂时拉住了特斯拉的股价颓势。当日收盘特斯拉上涨3.8%，但难掩其年内累计超40%的暴跌，市值较去年11月高位蒸发超1500亿美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而马斯克近日向美国总统特朗普的顾问团队发出信号，他希望向特朗普政治团队控制的组织捐款1亿美元。《纽约时报》称，一名白宫工作人员，甚至只是兼职，为支持“老板”的议程做出如此巨额的政治捐款，这是闻所未闻的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Transformer 架构重大改进：华人科学家刘壮联合何恺明、Yann LeCun整活&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;来自Meta AI的华人科学家刘壮团队，联合AI大神何恺明、图灵奖得住Yann LeCun等的最新论文证明了：Transformer 模型可以不用Normalization（归一化）层也能达到甚至超越现有性能，论文已经被CVPR 2025接收。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Normalization层在现代神经网络中几乎是标配，但刘壮团队证明，可以用参数化的 tanh() 代替正则化层来训练深度网络。团队灵感来自一个观察：Layer Normalization 在 Transformer 中，经常会产生类似 tanh 函数的 S 型输入输出映射。也就是说，LayerNorm 实际上也在做类似“挤压”的操作。直接用 tanh 函数性能不输，而且速度更快，成本更低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;具体可以查看：&lt;/p&gt;&lt;p&gt;论文:&amp;nbsp;&lt;a href=&quot;http://arxiv.org/abs/2503.10622&quot;&gt;http://arxiv.org/abs/2503.10622&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;代码和网站:&amp;nbsp;&lt;a href=&quot;http://jiachenzhu.github.io/DyT/&quot;&gt;http://jiachenzhu.github.io/DyT/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;D-Wave 宣称实现量子霸权：20 分钟破解超算百万年难题&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;D-Wave 公司宣布实现 “量子霸权”，其 Advantage 退火量子计算机使用 1200 量子位，在 20 分钟内完成了传统计算机需近百万年才能完成的磁性材料特性模拟任务。这一突破被 D-Wave 首席执行官誉为 “量子计算领域的圣杯”。量子计算机基于量子比特构建，能在药物研发、网络安全等领域进行复杂计算。D-Wave 专攻量子退火技术，已为多家客户提供量子优化方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管有科学家质疑经典算法优化后也能完成类似模拟，D-Wave 反驳称质疑方未能复现其研究中的完整晶格结构模拟。业界更倾向于使用 “量子优势” 概念，强调解决实际商业问题的性价比优势，而 D-Wave 坚持 “量子霸权” 表述。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;网信办等四部门印发《人工智能生成合成内容标识办法》，9 月起施行&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了促进人工智能健康发展，规范人工智能生成合成内容标识，保护公民、法人和其他组织合法权益，维护社会公共利益，国家互联网信息办公室、工业和信息化部、公安部、国家广播电视总局制定了《人工智能生成合成内容标识办法》（下称《标识办法》），自 2025 年 9 月 1 日起施行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;《标识办法》明确，人工智能生成合成内容标识主要包括显式标识和隐式标识两种形式，显式标识是指在生成合成内容或者交互场景界面中添加的，以文字、声音、图形等方式呈现并可以被用户明显感知到的标识；隐式标识是指采取技术措施在生成合成内容文件数据中添加的，不易被用户明显感知到的标识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;《标识办法》提出，按照《互联网信息服务算法推荐管理规定》、《互联网信息服务深度合成管理规定》、《生成式人工智能服务管理暂行办法》相关要求开展人工智能生成合成内容标识活动的，应当符合《标识办法》相关要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;《标识办法》要求，服务提供者提供的生成合成服务属于《互联网信息服务深度合成管理规定》第十七条第一款情形的，应当按照要求对生成合成内容添加显式标识；服务提供者应当按照《互联网信息服务深度合成管理规定》第十六条的规定，在生成合成内容的文件元数据中添加隐式标识；提供网络信息内容传播服务的服务提供者应当采取技术措施，规范生成合成内容传播活动。&lt;/p&gt;&lt;p&gt;《标识办法》强调，任何组织和个人不得恶意删除、篡改、伪造、隐匿本办法规定的生成合成内容标识，不得为他人实施上述恶意行为提供工具或者服务，不得通过不正当标识手段损害他人合法权益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;李飞飞团队发布 “保姆” 机器人：倒垃圾刷马桶，Switch 手柄轻松控&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;斯坦福大学李飞飞团队开发了一款名为 BRS 的家用机器人，能够完成倒垃圾、整理衣物、清洁马桶等家务，核心组件成本控制在 500 美元以内。BRS 的成功得益于三大核心控制技术的突破，通过 BEHAVIOR-1K 测试基准确定，包括 1000 项日常家庭活动。BRS 的机械手臂协同工作，具备精准导航能力，执行器操作范围广。研究团队在五项家庭任务中评估 BRS，结果显示其高效实用。数据收集是研发中的挑战，团队开发了 JoyLo 接口，利用 Nintendo Switch 手柄实现全身控制，降低成本并推动技术普及。BRS 的推出标志着低成本机器人技术在家庭自动化领域取得重大进展，展现了市场潜力和智能化生活方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;谷歌推出号称“单GPU运行最强模型”Gemma 3&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间3月12日，谷歌宣布推出Gemma 3，首次引入多模态能力，支持视觉-语言输入和文本输出。该模型可处理长达128k token的上下文窗口，理解140多种语言。Gemma 3提供四种参数规模（1B、4B、12B、27B），既包含可针对特定场景微调的预训练模型，也包含经过通用指令优化的版本。谷歌方面表示，该模型是可在单个GPU或TPU上运行的性能最强大模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;日前，谷歌 DeepMind 还推出了两款新型 AI 模型，旨在帮助机器人完成更多现实世界中的任务。其中一款名为 Gemini Robotics，是一款视觉语言行动模型，能够使机器人在没有进行过专门训练的情况下理解新的情境。Gemini Robotics 基于谷歌最新版本的 AI 旗舰模型——Gemini 2.0。谷歌 DeepMind 机器人部门高级总监 Carolina Parada 曾表示，Gemini Robotics 依托 Gemini 的多模态世界理解能力，通过加入物理行动的新模态，将其应用到现实世界中。另一款则是 Gemini Robotics-ER（具象推理）模型，公司称其为一种先进的视觉语言模型，能够“理解复杂且动态的世界”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 发布新工具，推动 AI 智能体从“回答问题”跨越到“执行任务”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3 月 12 日消息，OpenAI 正在发布一个新的 Responses API，为开发者提供构建模块，以创建能够搜索网络、挖掘文件和代表他们在 计算机上执行任务的代理。Responses API 附带一个基于 ChatGPT 用于搜索的相同模型构建的网络搜索工具，允许开发人员在使用 GPT-4o 和 GPT-4o mini 时从网络上获取实时信息和引用。它还有一个计算机使用功能，同样使用公司的 Operator 模型来代表用户执行任务。Responses API 还有一个搜索大量文件的工具，OpenAI 称它可以帮助客户支持人员筛选常见问题，或帮助法律助理查找以前的案件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了 Responses API，OpenAI 还发布了 Agents SDK，并将其描述为开发人员&quot;协调&quot;人工智能代理工作流程的一种方式。&quot;OpenAI API 团队的产品经理 Nikunj Handa 介绍说：&quot;Responses API 就像是使用模型和工具来完成特定工作的原子单元。Agent SDK 是让多个原子单元协同工作，以解决更复杂的任务。这将使开发人员更容易管理他们的所有代理，并确保他们朝着一个目标工作。&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;日前，3月11日，OpenAI还发布了最新研究，用CoT（思维链）监控的方式，可以阻止大模型胡说八道、隐藏真实意图等恶意行为，同时也是监督超级模型的有效工具之一。据悉，OpenAI使用了最新发布的前沿模型o3-mini作为被监控对象，并以较弱的GPT-4o模型作为监控器。测试环境为编码任务，要求AI在代码库中实现功能以通过单元测试。结果显示，CoT监控器在检测系统性“奖励黑客”行为时表现卓越，召回率高达95%，远超仅监控行为的60%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;一键创建 AI 聊天机器人，微软新增高度可自定义模版&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3 月 10 日消息，微软发文，宣布为旗下Microsoft.Extensions.AI.NET 库引入一款 AI 聊天机器人应用模板，其作用正如其名，就是为了便于开发者快速构建 AI 聊天机器人应用，相应模板基于 Blazor 架构，开发者可以利用终端命令在 VS / VS Code 中安装使用相应模板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，该模板的一大亮点在于支持自定义数据聊天功能，开发者可以直接将 PDF 等文件添加到项目中，系统会自动建立索引并构建向量数据库，使聊天机器人能够理解并引用这些内容进行互动。此外，模板支持本地向量存储和 Azure AI Search，满足不同规模和需求的应用场景。&lt;/p&gt;&lt;p&gt;微软强调，相应模板具有高度可扩展性。开发者可借助Microsoft.Extensions.AI.NET 库本身为聊天机器人增加自定义功能，例如通过新增 C# 函数，让聊天机器人整合天气资讯信息或其它第三方服务，从而进一步提升应用实用性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软同时表示，其计划陆续扩展模板类型，持续发展 .NET 的 AI 生态，同时微软还将与 Semantic Kernel 等团队合作，提供更多与语义分析相关的解决方案。目前开发者可以通过官方 GitHub 页面或.NET AI 社区提供反馈，帮助微软不断改进模板功能，满足开发者实际需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;富士康推出自研AI大模型FoxBrain&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月11日消息，全球最大电子产品代工企业富士康宣布，该公司已自主研发推出首款繁体中文AI大语言模型FoxBrain。模型现已具备数据分析、决策辅助、文书协作、数学、推理解题与代码生成等功能，后续将对外开源分享。富士康鸿海研究院人工智能研究所所长栗永徽称，该模型”采用高效训练策略，专注于训练过程优化而非盲目堆砌算力”。在研发过程中，其团队使用120块英伟达H100显卡，耗时约四周完成FoxBrain的训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为苹果公司iPhone主要组装商，富士康已公布FoxBrain部分参数，并表示将在3月中旬英伟达年度技术大会上披露更多细节。该模型基于Meta开放的大语言模型Llama 3.1架构开发，尤其在数学与逻辑推理方面展现卓越能力。但富士康坦言，其模型性能与DeepSeek的蒸馏模型还有些微差距，但已接近世界先进水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智元发布家务机器人基座大模型 GO-1&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3 月 13 日，智元机器人正式推出其首个通用具身基座大模型 ——Genie Operator-1（简称 GO-1）。根据智元机器人的官方介绍，GO-1大模型通过学习大量人类视频，展示了其在执行多项家务任务上的出色表现，如递送水杯、制作餐点和迎接客人等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术性能方面，GO-1的表现相较于现有模型有显著提升，其平均成功率提高了32%。经过测试，GO-1在一些特定任务如倒水、清理桌面和补充饮料等方面，展现出了更加优异的能力。智元机器人表示，这一进展得益于新提出的 Vision-Language-Latent-Action（ViLLA）架构，这一架构能有效利用高质量数据集和大量视频数据，以增强模型的泛化能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;机器保姆来了！越疆科技首款“灵巧操作+直膝行走”具身智能人形机器人亮相&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月11日上午，被称为“协作机器人第一股”的越疆科技发布了全球首款“灵巧操作+直膝行走”具身智能人形机器人。这款机器人不仅具备工业级精细操作能力，还搭载了自研的神经驱动灵巧操作系统（NDS）和仿人直膝行走系统（AWS）。在应用场景方面，这款人形机器人可以自主完成诸如做早餐（倒牛奶、水果摆盘、烤面包、烤培根）、企业客户接待（倒咖啡、送文件、取快递）等任务。此外，它还适用于面向数以千计用工的车厂组装备料环节、咖啡店制饮多台设备的流程操作、连锁药店夜间取药等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/45/45d770f8efe14a7997cb5af405e668eb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3 月 14 日，知情人士称，苹果公司正在计划推出一项新的AirPods功能，可以让耳机将面对面的对话实时翻译成另一种语言。这项功能将作为今年晚些时候AirPods软件升级的一部分推出，将与即将推出的苹果移动设备操作系统iOS 19捆绑在一起。3 月 13 日，阿里巴巴宣布，全新夸克基于阿里通义领先的推理及多模态大模型，全面升级为无边界的“AI 超级框”。与对话式 AI 不同，夸克将深度思考、深度搜索等执行整合到一个极简“AI 超级框”内，一站式满足用户需求。3 月 12 日，谷歌联合创始人 Larry Page 宣布创立全新 AI 公司 Dynatomics，将把 AI 技术应用到制造业中。据悉，Page 已联系了一批工程师，正合作开发能够优化物体制造任务的 AI。他们的计划是通过 AI 来优化产品设计，之后将其交由工厂生产。报道称，此项目由 Page 旗下电动飞机公司 Kittyhawk 的首席技术官 Chris Anderson 领导。3 月 11 日，继此前宣布支持 DeepSeek 后，亚马逊云科技再次宣布在 Amazon Bedrock 上线完全托管的满血版 DeepSeek-R1，首次将该模型作为完全托管服务推出，进一步扩展了客户在 Amazon Bedrock 中使用 DeepSeek-R1 及其蒸馏版本的方式。3 月 11 日，Manus 官方微博宣布，与阿里通义千问团队正式达成战略合作。双方将基于通义千问系列开源模型，在国产模型和算力平台上实现 Manus 的全部功能。&lt;/p&gt;</description><link>https://www.infoq.cn/article/REshEcfDIffzAZhvlkYa</link><guid isPermaLink="false">https://www.infoq.cn/article/REshEcfDIffzAZhvlkYa</guid><pubDate>Tue, 18 Mar 2025 08:13:32 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>大模型助力研发团队管理提效：从「人仰马翻」到「事半功倍」 ｜QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;腾讯高级工程师毕鸣一已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6336&quot;&gt;《从「人仰马翻」到「事半功倍」：大模型助力研发团队管理提效》&lt;/a&gt;&quot;的主题分享。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在“降本增效”和“大模型技术高速发展”的大背景下，如何借助大模型的能力落地项目降本的目标，激发起技术团队的工作热情和提高工作效率成为了当前各个厂商探索的热点方向，也是一个业界急需突破的难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次演讲结合真实项目场景，从技术管理、项目管理、团队管理、业务管理等多个管理纬度进行了大模型赋能的实践，探索出了基于“降低认知负担”、“高效信息流转”、“减少管理成本”、“智能决策优先级”的一系列实践案例，希望能给各位听众带来提高各个维度管理效率最佳实践的启示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a2/a27d6e533d020496557af903382cd3ca.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;毕鸣一现为腾讯 RUM 项目技术负责人；先后负责过手机 QQ、QIM、TIM，腾讯会议等项目的性能稳定性测试及测试工具平台的研发工作；当前负责腾讯云终端、前端和跨端性能稳定性监控产品的研发，作为技术负责人和项目管理者，持续主导监控平台的技术建设、架构升级和项目迭代管理；具有多年的技术管理经验和 ToB 场景下公有云、私有云、专有云交付经验。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 研发效率的“沼泽”真实故事案例痛点分析及数据统计解决方案及AI赋能场景需求开发的“沼泽”客户支持的“沼泽”问题排障的“沼泽”思考总结大模型赋能与研发效能的关系大模型赋能与技术管理的关系2. 信息传递的“黑洞”真实故事案例痛点分析及数据统计解决方案及 AI 赋能场景需求提出者和需求文档间的信息熵“衰减需求实现成本与需求评审的信息熵“侵蚀”智能代码审查和文档更新，恢复信息熵总结思考大模型赋能与项目效率的关系大模型赋能对项目管理的影响3. 轻组织形态的“困境”真实故事案例 1 及痛点低成本人力的“水杯困境”解决方案 1 及 AI 赋能场景AI 构建团队内部新人知识库真实故事案例 2 及痛点小团队的“管理困境”解决方案 2 及 AI 赋能场AI 辅助 OKR/BSC 制定和对齐AI 自动生成日周月汇报利用大模型分析团队沟通记录总结思考水杯理论与一专多能小团队管理的本质与 AI 的角色团队管理与大模型赋能的关系4. 优先级决策的“迷雾”真实故事案例痛点分析及数据统计解决方案及 AI 赋能场景数据分析系统，提供基础数据支持数据分析系统 + 大模型，智能识别优先级思考总结识别问题的成本与大模型的专长数据分析系统与大模型的关系业务管理与大模型赋能的关系您认为，这样的技术在实践过程中有哪些痛点？人员培训上的大模型赋能模型可以尝试项目管理上还有一些细节可以大模型赋能（发布评审、测试评审）演讲亮点真实团队案例分析真实大模型实践解决方案具有通用性听众收益技术管理、项目管理、团队管理、业务管理上的通用痛点分析与解决方案推荐各个维度管理与 AI 大模型赋能的关系、定位和能够共赢的点的深度思考&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3185660f6af5c63bcc24b3af58f1779.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/q9zW68loUiorX7HhH85K</link><guid isPermaLink="false">https://www.infoq.cn/article/q9zW68loUiorX7HhH85K</guid><pubDate>Tue, 18 Mar 2025 04:25:40 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category><category>研发效能</category></item><item><title>Cohere发布多语言Command A模型：仅需两张GPU即可服务全球企业</title><description>&lt;p&gt;整理 ｜ 华卫、核子可乐&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;日前，加拿大AI初创公司Cohere发布了其最新生成式AI模型Command A，据称专为企业应用场景设计而成。据了解，Cohere由2017年开启大语言模型革命的transformer论文作者之一Aidan Gomez与他的两位多伦多大学校友Ivan Zhang和Nick Frosst联合创立。除此之外，该公司旗下非营利子公司Cohere for AI还于本月初发布了名为Aya Vision的开源多语言视觉模型（仅供研究）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，作为2024年3月首次亮相的Command-R及后续Command R+大模型的继任者，Command A以Cohere在检索增强生成（RAG）、外部工具以及企业AI效率领域的研发成果为基础，主要强调以更快速度完成计算并交付答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;比Command-R更进一步&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当初Command-R于2024年首次亮相时，就曾引入一系列关键创新，如优级RAG性能、更好的知识检索效果与更低的AI部署成本。该模型很快获得企业青睐，并被整合至甲骨文、Nation、Scale AI、埃森哲及麦肯锡等公司的商业解决方案当中。不过Menlo Ventures在2024年11月发布的企业采用调查报告中指出，Cohere在企业领域的市场份额仅为3%，远低于OpenAI的34%、Anthropic的24%乃至Mistral等小型初创厂商的5%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今，为了吸引更多企业的加入，Command A进一步拓展了这些功能。根据官方介绍，新版本：&lt;/p&gt;&lt;p&gt;在商业、STEM及编码任务中等同或超越了OpenAI的GPT-4o与DeepSeek-V3。可仅依托两张GPU（A100或H100）运行，相较于其他需要多达32张GPU模型在效率上实现了显著提升。可实现更快的token生成速度，每秒可生成156个token——相当于GPT-4o的1.75倍，DeepSeek-V3的2.4倍。降低延迟，首token生成时间为6500毫秒，优于GPT-4o的7460毫秒与DeepSeek-V3的14740毫秒。增强多语言AI能力，改进了对阿拉伯语方言的支持并扩展支持23种全球语言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cohere通过Command A延续了其企业优先战略，确保此模型能够无缝集成至业务环境当中。Command A的核心特性包括：&lt;/p&gt;&lt;p&gt;高级检索增强生成（RAG）：为企业应用程序提供可验证的高精度响应结果。使用代理式工具：与企业工具相集成以支持复杂的工作流程。North AI平台集成：与Cohere的North AI平台配合使用，允许企业使用安全的企业级AI智能体自动执行任务。可扩展性与成本效率：私有部署的成本比API访问低50%。支持多种语言，在阿语支持方面表现出色。Command A的一大突出特点，是它能够对全球23种最常用语言生成准确响应，包括经过改进的阿拉伯方言处理能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，速度是企业AI部署中的关键因素，Command A的设计目标正是以超越竞争对手的速度交付生成结果。100K上下文请求的token流速度为：每秒73个token（相比之下，GPT-4o为每秒38个token，DeepSeek-V3则为每秒32个token）。首次token生成速度更快：与其他大模型相比，Command A的响应速度明显更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基准测试结果中，Command A在使用阿拉伯语响应英语提示词的准确率为98.2%——高于DeepSeek-V3的94.9%与GPT-4o的92.2%。它在方言一致性方面的表现似乎也明显优于竞争对手，ADI2得分为24.7，远高于GPT-4o的15.9与DeepSeek-V3的15.7。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;凭借更快的速度、更低的硬件要求加上扩展后的多语言功能，Command A将自身定位成GPT-4o及DeepSeek-V3等模型的有力替代选项——请注意，这里列出的均是经典的大语言模型，而非最近掀起行业热潮的新兴推理模型。与能够支持12.8万个token上下文长度（即大模型在一次输入/输出交换中可以处理的信息量，12.8万token相当于一本300页的小说）的前身不同，Command A将上下文长度增加了一倍，达到25.6万个token（相当于600页文本），同时提高了整体效率与生产应用就绪水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;行业反响&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，Command A已在Cohere平台上正式上线，并在Hugging Face上基于Creative Commons Attribution Non Commercial 4.0 International (CC-by-NC 4.0)许可证提供开放权重，但仅供研究使用。后续还将面向广泛云服务商提供支持方案。输入token：每百万个2.5美元；输出token：每百万个10.00美元。可根据要求提供私有与本地部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多位AI研究人员及Cohere团队成员表达了自己对于Command A的赞赏之情。Cohere公司预训练专家Dwaraknath Ganesan在X上发帖表示：“很高兴能够展示我们过去几个月间倾力研究的成果！Command A非常出色，只需两张H100 GPU即可部署！256K上下文长度、经过扩展的多语言支持、代理式工具使用……我们对此深感自豪。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cohere公司AI研究员Pierre Richemond补充道，“Command A是我们全新打造的GPT-4o/DeepSeek V3级别、开放权重111B模型，可支持256K上下文长度，且针对企业用例的运行效率进行了优化。”凭借更快的运行速度、更大的上下文窗口、更好的多语言处理能力以及更低的部署成本，它将针对企业需求为现有AI模型提供强大的替代选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cohere在其开发者文档中指出，“Command A非常健谈。在默认情况下，该模型为交互式设计，并针对对话进行了优化。就是说它的输出内容很长，而且会使用markdown来高亮显示代码。要覆盖此机制，开发人员可在前置词中要求模型仅提供答案，且不使用markdown或代码块标记。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由此看来，Command A有望成为那些预算有限、但又亟需建立AI优势的企业，以及快速响应类应用场景（例如金融、医疗、医学、科学和法律）下的理想模型选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://venturebeat.com/ai/cohere-targets-global-enterprises-with-new-highly-multilingual-command-a-model-requiring-only-2-gpus/&quot;&gt;https://venturebeat.com/ai/cohere-targets-global-enterprises-with-new-highly-multilingual-command-a-model-requiring-only-2-gpus/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/a3l5odEkQTsKDOTq2UgM</link><guid isPermaLink="false">https://www.infoq.cn/article/a3l5odEkQTsKDOTq2UgM</guid><pubDate>Tue, 18 Mar 2025 03:21:09 GMT</pubDate><author>华卫,核子可乐</author><category>AI&amp;大模型</category></item><item><title>支付宝多模态应用实验室研究员李宇明确认出席QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支付宝多模态应用实验室研究员李宇明已确认出席并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6379&quot;&gt;《EchoMimic：多模态大模型驱动下的生成式数字人技术与应用》&lt;/a&gt;&quot;的主题分享。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着多模态大模型的不断发展，生成式数字人的技术融合趋势也日益明显。通过结合视觉、语音和自然语言等多种模态数据，生成式数字人可以更加完整地呈现出真实世界中的人的行为和交流方式。这种技术融合趋势将进一步推动生成式数字人在虚拟现实、增强现实、人机交互等领域的广泛应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;EchoMimic 是支付宝多模态应用实验室发布并开源的数字人技术项目，仅需输入一张参考图像、一段音频及一段手势序列，即可生成高质量人物动画视频，同时确保半身数字人与音频内容之间的协调。EchoMimic V1 论文中稿人工智能领域顶级国际会议 AAAI 2025，EchoMimic V2 论文中稿世界国际计算机视觉与模式识别会议 CVPR 2025。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本演讲将围绕 EchoMimic 系列开源生成式数字人项目，介绍生成式数字人领域最新进展、详细讲解 EchoMimic 背后的技术细节、以及生成式数字人相关应用场景，及该领域后续研究思路与方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李宇明现任支付宝多模态应用实验室研究员，是香港城市大学电子工程系博士。曾任 MINIEYE，腾讯自动驾驶实验室高级研究员。先后从事自动驾驶，量化交易，人脸攻防安全及 AIGC 等算法研发工作，在国际知名期刊会议发表论文 30 余篇，申请发明专利 20 余项，以核心成员参与完成省部级课题 3 项。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 传统数字人与生成式数字人技术背景传统数字人技术介绍生成式数字人技术介绍2. EchoMimic（基于语音驱动的人像动画生成） 背后的技术技术细节与亮点实验结果分析3. 应用场景探索生成式数字人结合大语言模型的实时交互生成式数字人结合音乐生成模型的 AI 创作生成式数字人结合商品的直播带货4. 总结与展望生成式数字人存在的问题和挑战生成式数字人开发新范式您认为，这样的技术在实践过程中有哪些痛点？高质量人物相关数据获取、训练和推理效率、生成数字人自然度和真实性演讲亮点生成式数字人领域的技术路线，最新进展，以及与多模态大模型应用结合趋势EchoMimic 系列生成式数字人开源项目的技术细节生成式数字人领域后续研究方向听众收益了解生成式数字人领域最新进展了解 EchoMimic 系列生成式数字人开源项目技术细节了解生成式数字人相关应用场景，及该领域后续研究思路与方法&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3185660f6af5c63bcc24b3af58f1779.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/kRZ8JpQrWeuyU9tb92tq</link><guid isPermaLink="false">https://www.infoq.cn/article/kRZ8JpQrWeuyU9tb92tq</guid><pubDate>Tue, 18 Mar 2025 02:26:58 GMT</pubDate><author>QCon全球软件开发大会</author><category>阿里巴巴</category><category>AI&amp;大模型</category></item><item><title>谷歌被爆正与联发科合作开发下一代AI芯片</title><description>&lt;p&gt;美国当地时间3月18日，据The Information周一援引参与该项目的人士的话报道，Alphabet 旗下谷歌公司正准备与联发科合作开发下一代人工智能芯片张量处理单元 (Tensor Processing Units)，该芯片将于明年生产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;消息一出，博通股价下跌，反映出投资者担心谷歌订单可能会减少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌从很早之前就一直与博通合作开发 TPU。然而，近两年双方的关系却逐渐恶化，有报道称谷歌正在寻找替代方案。其中一个原因是博通的强硬谈判。与博通相比，联发科每张芯片的收费更低。最重要的是，它与全球最大芯片代工厂台积电有着密切的关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，据报道，谷歌目前并未终止与博通的合作。然而，双方在定价问题上的分歧影响了谷歌与联发科的合作决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌、联发科和博通尚未立即回应路透社的置评请求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与英伟达一样，谷歌早在2015年开始就设计了自己的 AI 服务器芯片。在内部使用约一年后，谷歌于 2016 年发布了首款张量处理单元人工智能芯片。谷歌的这款 TPU 芯片是其 AI 战略的核心，为内部 AI 研发、云计算以及谷歌搜索、YouTube 和 Gemini AI 模型等服务提供支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年，该公司在 TPU 上花费了约 60 亿至 90 亿美元，但它仍然是 Nvidia 最大的客户之一，已订购了价值超过 100 亿美元的 Nvidia 旗舰 Blackwell 芯片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种方法让谷歌在人工智能竞赛中获得了竞争优势，因为它减少了对英伟达的依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年年底，谷歌推出了第六代 TPU，旨在为自己和云客户提供英伟达芯片的替代品，英伟达芯片是业内最受欢迎的处理器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究公司 Omdia 根据博通去年的 AI 半导体收入目标，得出谷歌去年在 TPU 上的支出为 60 亿到 90 亿美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 The Information 报道，尽管谷歌正在努力内部开发更多 AI 芯片，但预计仍将依赖博通和联发科等外部合作伙伴进行芯片的生产、封装和质量测试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.reuters.com/technology/artificial-intelligence/google-preparing-partner-with-taiwans-mediatek-next-ai-chip-information-reports-2025-03-17/&lt;/p&gt;</description><link>https://www.infoq.cn/article/XyKjBBX2Z84wVsi9GBQP</link><guid isPermaLink="false">https://www.infoq.cn/article/XyKjBBX2Z84wVsi9GBQP</guid><pubDate>Tue, 18 Mar 2025 02:20:58 GMT</pubDate><author>李冬梅</author><category>Google</category><category>芯片&amp;算力</category></item><item><title>Hugging Face 发布了高效的跨 GPU 大语言模型训练指南</title><description>&lt;p&gt;Hugging Face 发布了 《超大规模实战指南：在 GPU 集群上训练大语言模型（LLMs）》，这是一份开源指南，详细探讨了跨 GPU 集群进行大语言模型训练的方法和技术。该指南基于使用多达 512 个 GPU 进行的超过 4000 次扩缩实验，重点是优化吞吐量、GPU 利用率和训练效率。其目标是为从事大规模模型训练的研究人员和工程师提供实用的指导，提供可复现的基准测试、实现细节和性能优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;指南涵盖了扩展 LLM 训练所必需的各种并行策略。数据并行（Data Parallelism，DP） 使多个 GPU 能同时处理不同批次的数据，而张量并行（Tensor Parallelism，TP） 则通过在 GPU 之间分配模型权重来平衡内存使用和计算负载。流水线并行（Pipeline parallelism，PP） 将模型拆分为多个分布在不同 GPU 上的段，使得模型的不同部分可以并发处理。此外，该指南还探讨了上下文并行（Context parallelism，CP），这是一种提高可扩展性的新兴技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内存管理是该指南特别关注的另一个关键主题，它解决了诸如内存限制和优化技术之类的挑战。激活重计算被引入以作为减少内存消耗的方法，该方法通过在需要时重新计算中间激活而不是存储它们。梯度累积则被强调为一种在不超过内存限制的情况下实现更大有效批量的方法，从而可以提高训练的稳定性和效率。这些技术对于训练超过单个 GPU 内存容量的 LLM 至关重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该指南还提供了广泛的基准测试分析见解，展示了实证测试在优化训练配置中的重要性。通过测试各种配置来确定批处理的大小、模型架构和使用的 GPU 数量之间的最佳平衡。有效的基准测试有助于提高训练速度、资源分配和计算效率，这对于大规模训练是至关重要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;GPU 之间的通信开销是影响训练效率的另一个因素。该指南讨论了通过将通信与计算重叠来减少空闲 GPU 时间的方法，例如在反向传递期间使用全归约（all-reduce）操作。还探索了优化网络带宽和最小化同步延迟的策略，以提高整体训练的性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于该指南的帖子反映了人们对这本开源指南的兴奋和赞赏。Hugging Face 的研究负责人 Leandro von Werra 在发布该指南时，分享道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;学习如何使用 5D 并行、ZeRO、快速内核、计算 / 通信重叠和瓶颈，通过理论、交互式图表和 4000 多个扩缩实验以及音频来训练自己的 DeepSeek-V3 模型&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;人工智能开发人员 Denis Redozubov 则发布道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有一些非常酷的内容，比如一个计算 transformer 模型内存分解的小部件。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，该指南还谈到了 LLM 训练的未来方向，预计硬件和软件方面的进步将继续塑造该领域。对优化通信、减少内存开销和改进并行技术的研究有望进一步提高可扩展性和效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作者介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Daniel Dominguez 是 AWS 合作伙伴网络公司 SamXLabs 的管理合伙人。他在为初创公司和财富 500 强公司开发软件产品方面拥有超过 13 年的经验。Daniel 拥有华盛顿大学的机器学习专业学位。他热衷于利用人工智能和云计算来创建创新的解决方案。作为机器学习层的 AWS 社区构建者，Daniel 致力于分享知识并推动软件产品的创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2025/03/huggingface-ultra-scale-playbook/&quot;&gt;https://www.infoq.com/news/2025/03/huggingface-ultra-scale-playbook/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5iuJHHReWPVgNaZSjqvt</link><guid isPermaLink="false">https://www.infoq.cn/article/5iuJHHReWPVgNaZSjqvt</guid><pubDate>Tue, 18 Mar 2025 02:00:00 GMT</pubDate><author>作者：Daniel Dominguez</author><category>自然语言处理</category></item><item><title>Step-Video 开源模型：视频生成基础模型的最新进展、挑战与未来展望｜QCon北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶跃星辰Tech Fellow段楠已确认出席并发表题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6367&quot;&gt;Step-Video 开源模型：视频生成基础模型的最新进展、挑战与未来展望&lt;/a&gt;&quot;》的主题分享，本演讲将围绕 Step-Video 系列开源模型，介绍视频生成基础模型的最新进展，包括文生视频和图生视频等任务。此外，本报告还将总结现有视频生成模型面临的主要挑战，并和大家讨论未来可能的发展发向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;段楠博士现任阶跃星辰 Tech Fellow ，带领研究团队构建以语言和视频为中心的多模态基础模型。此前，他曾任微软亚洲研究院资深首席研究员、自然语言计算团队研究经理（ 2012 年至 2024 年）。段博士是中国科学技术大学和西安交通大学兼职博导，天津大学兼职教授。主要从事自然语言处理、代码智能、多模态基础模型、智能体等研究。他在本次会议的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 视频生成背景和现状视频生成从简单的帧插值逐渐演变为复杂的多模态生成任务多模态融合（文本、图像、音频等）、大规模预训练模型的应用逐渐成为趋势2. 视频生成 SoTA 模型：Step-Video技术亮点实验结果：性能对比、案例分析视频生成目前面临的挑战3. 关于未来的讨论更强大的多模态融合实时生成您认为，这样的技术在实践过程中有哪些痛点？高质量数据的获取、训练和推理效率、物理规律的遵循演讲亮点SoTA 模型的技术创新与架构优势系统优化与高效训练听众收益了解该领域最新进展&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3185660f6af5c63bcc24b3af58f1779.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/GG1rFmrlwBzFfO1Opv4v</link><guid isPermaLink="false">https://www.infoq.cn/article/GG1rFmrlwBzFfO1Opv4v</guid><pubDate>Mon, 17 Mar 2025 10:43:55 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category></item><item><title>王兴兴自曝做机器人从200块钱“手搓板”开始！各机器人企业疯狂秀肌肉，你pick谁?</title><description>&lt;p&gt;整理 ｜华卫&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，宇树科技创始人、首席执行官兼首席技术官王兴兴发表的名为《献给春天的演讲：相信》演讲内容在各大平台刷屏了。官方透露，“90 后”的他，和团队研发的四足机器人目前已占全球近 7 成销售份额，人形机器人也已位列全球出货量第一。据了解，今年春晚被大家熟知的跳秧歌的“人形机器人”、前不久上热搜的“花 30 余万元买回来、租出去日赚 8000 元”的机器人都来自宇树科技。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除宇树之外，我们也注意到，今年以来人形机器人领域的产品和创新技术层出不穷，众擎的“全球首例前空翻人形机器人”、稚晖君在创业两年后带来的自研机器人灵犀 X2、李飞飞团队不到 500 美元实现成本的家庭任务行为机器人套件，都被认为指明了人形机器人的更多发展方向与前景，同时这些产品们也或将展开激烈的市场竞争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;人形机器人“卖起来了”?&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;宇树科技在春晚出圈后没多久，就在京东线上首发开售了 H1 和 G1 人形机器人，其中 G1 起售价 9.9 万元、H1 起售价 65 万元，都快速售罄。而最让其火爆“出圈”的，就是其灵活性了。今年 1 月，在 G1 亮相以来的首个应用方案 Unitree G1-Comp 中，人形机器人都能踢足球了，射门、奔跑、转向、转圈圈都不在话下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上个月，宇树机器人宣布旗下人形机器人算法再次升级， 发布视频中的 G1 机器人展示了各类常见的武术动作，一套“中国功夫”下来，直接登上了热搜榜。3 月 4 日，王兴兴又发布了一段 G1 机器人完成了包括“720 度回旋踢”等连续武打动作并保持平衡的抗干扰演示视频。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;曾因在深圳街头散步而走红的众擎机器人，最被热议的则是运动能力。2 月 23 日，众擎一款人形机器人 PM01 完成“全球首例机器人前空翻特技”，引起不少讨论。据介绍，PM01 具有机械步态和拟人自然步态两种行走模式，拟人程度与运动表现媲美在去年 10 月发布的旗舰机型 SE01。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，众擎机器人主要面向的是 B 端市场。SE01 的主定位为工业机器人，PM01 则对外提供商业版与教育版，将在今年 3 月以 8.8 万元的统一价格发售。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比宇树和众擎在灵活性和运动方面的肢体能力展现，稚晖君的灵犀 X2 似乎更侧重于交互。如今不仅是 B 站 up 主“稚晖君”还是智元机器人创始人兼 CTO 的彭志辉称，X2 是第一台真正具备复杂交互能力的灵动机器人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，这款机器人集运动、交互、作业三方面的功能于一体，且比目前大部分机器人都更具“生命感”。智元机器人团队为其开发了一套基于 Diffusion 的生成式动作引擎，让 X2 拥有了和人自然交互的能力；又借助大语言模型，为 X2 训练了定制的多模态交互大模型硅光动语，光就是视觉、动是动作、语是语音。通过边缘侧大脑端到端的模型架构以及大量工程优化，X2 拥有毫秒级交互反应，能通过人类的面部表情和语音语调精准判断情感状态，并做出相应的回应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，X2 使用了大量创新，包括把机器人的硬件系统抽象出了一系列可复用的核心组件，包括小脑控制器 Xyber-Edge、域控制器 Xyber-DCU、智能电源管理系统 Xyber-BMS，以及核心关节模组 Powerflow 等。同时，灵犀 X2 使用了抗摔的柔性材料，全身拥有 28 个自由度，未使用任何并联结构。并且，X2 尤其擅长运动方面，能骑平衡车、滑板车、自行车。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作业能力方面，X2 拥有简单任务的泛化能力。按照智元的“终极期待，这款机器人的生产力价值能够让“机器人吉祥三保”——保安、保姆以及保洁成为现实。许多网友感慨，从 X2 身上看到了家用机器人、情感陪伴机器人的未来，并评价这款机器人“可以卖起来了”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;斯坦福李飞飞团队最新发布的行为机器人套件（BEHAVIOR Robot Suite ，简称为 BRS），是一个用于各种家庭任务中全身操作的综合框架。BRS 通过两项关键创新解决了硬件和学习方面的挑战：用于构建经济高效的全身遥控界面的通用框架 JoyLo 和全身视觉运动注意力策略 WB-VIMA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，该团队在 Galaxea R1 机器人上实现了 JoyLo，整个系统由 3D 可打印的臂连杆、低成本 Dynamixel 电机和现成的 Joy-Con 控制器组成，总成本低于 500 美元，且模块化设计便于更换组件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;WB-VIMA 是一种模仿学习算法，旨在通过利用机器人固有的运动学层次结构来建模全身动作。其背后的一个关键理念是，机器人关节之间存在很强的相互依赖性——上游关节（如躯干）的微小运动可能导致下游关节（如末端执行器）产生较大位移。为确保所有关节的精确协调，WB-VIMA 将下游组件的动作预测条件设定为上游组件的动作预测，从而实现更同步的全身动作。此外，WB-VIMA 使用自注意力动态聚合多模态观察，使其能够学习表达策略，同时减轻对本体感受输入的过度拟合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该团队在五个具有代表性的家庭任务上评估 BRS，包括清洁房屋、清洁厕所、倒垃圾、放置物品到架子上和晾晒衣服。结果表明，BRS 适用于各种家庭任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，现在人形机器人这么火，但除技术性能方面的突破外，落地和普及还面临许多尚未解决的情况，其中有一项问题很少被提到，即它的续航和充电方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据了解，目前业界的人形机器人通常使用由圆柱形锂离子电池组成的电池组。2 月 20 日，高工机器人产业研究所所长卢瀚宸表示：“人形机器人目前大部分厂商的续航都是 2 小时以内，应该说还有很大的提升空间。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如美国位于俄勒冈州的 Agility Robotics 公司研发的 Digit 双足人形机器人，能够搬运约 30 磅重的物品，但每次充满电后的工作时间仍只有一个半到两个小时，而充电则需要 50 分钟到一小时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《机器人报告》的编辑 Steve LeVine 指出：“大多数人形机器人运行时间仅有差不多一个半小时，然而，充电时间则需要长达一个小时。这迫使工厂或仓库必须为每项任务配备两台或更多机器人轮换使用。这是企业在实际运营中采用人形机器人的最大障碍。他们必须找到延长电池续航时间的解决方案。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;王兴兴：奇迹也有“算法”，从 200 块钱的“手搓版”开始&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但总的来说，现在人形机器人领域的产品正处于一个“百花齐放”阶段，正如王兴兴在最新演讲中所提出的：几百年的技术积累到了临界点，我们能做更强的 AI、更好的机器人。对于机器人的应用未来，王兴兴这样形容，“机器人可以很大，大到移山填海；机器人也可以很小，小到进入血管消灭癌细胞。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下为王兴兴演讲全文：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你相信吗？我身边这台机器人在不久的将来会和我们一样灵敏，甚至成为我们的好帮手，我相信。我是王兴兴，这是我和团队研发的宇树机器人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和很多人想象的不一样，我不是“学霸”，尤其是英语，虽然我一直很努力，但高中 3 年只及格过 3 次，甚至有人说：这个小孩好像比其他人笨一点。我也曾因此自我怀疑、充满焦虑，但，我有自己的热爱，我把所有的课余时间都用来做“小发明”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;10 岁用废旧纸板做风力小车，15 岁用废旧铁皮做微型发动机、做航模，19 岁我想做个机器人试试，但没有一个人看好。我不相信这世上有不可能的事，没有设备就用手工小钻头锉刀和剪刀，没有资金就买 9 块钱的零件找没人要的边角料。最后，我只用 200 块钱就“手搓”了一个小的双足机器人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/18/18faa2827b78a4949e057aa37cd7a570.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/4e/4ee9c9339d6c5c9076267c335e84e0d1.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那一刻，我再次明白，奇迹也有“算法”，做成事没有那么难，就是把“不可能”三个字拆解成三百个、三千个技术步骤和参数，然后一一攻克。后来，我遇到了更多难题。刚创业的时候，我四处碰壁。开始量产机器人的时候，周围更是充满质疑。现在做这个，有什么用？海外实验室烧几亿美元才能做的事你凭什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我听过一句话，有些人只有看见了才相信，而有些人因为相信所以看见。前进的路是一个个“迷思”被打破的过程，我相信别人能做的事我们也能做，还能做得更好。为了这个信念，我们没有一天敢懈怠持续学习、不断尝试。前进的路也是一次次“快与慢”、“长与短”的抉择身处，“快时代”我宁可“慢”一点也不去走捷径、抄近路，我还是愿意下笨功夫坚持自主研发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我相信坚持做难而正确的事，时代不会辜负长期主义者。如今我当初做的那个颤颤巍巍的机器人，经过不断迭代，已经成为全球行业出货量最大的机器人。常有人问我机器人的未来会怎么样，我无法给出准确答案。机器人可以很大，大到移山填海；机器人也可以很小，小到进入血管消灭癌细胞。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我可以肯定的是，我们这一代人幸运地遇到了前所未有的机遇。几百年的技术积累到了临界点，我们能做更强的 AI、更好的机器人、更酷的游戏、更火的电影。我相信我们真的可以。最后，我想借用电影《哪吒 2》的台词与你共勉：难道你还想改变这世界？我想试试！我们一起试试！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MjAxNDM4MA==&amp;amp;mid=2666927087&amp;amp;idx=1&amp;amp;sn=f18333f0eb8ff91d3331121f0554918c&amp;amp;scene=21#wechat_redirect&quot;&gt;https://mp.weixin.qq.com/s/6NTqdQ-yzvddjb0hcxBJOw&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://b23.tv/l1CDzel&quot;&gt;https://b23.tv/l1CDzel&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://behavior-robot-suite.github.io/&quot;&gt;https://behavior-robot-suite.github.io/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2503.05652v1&quot;&gt;https://arxiv.org/pdf/2503.05652v1&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/60e0XvVNy8HO4vJ2F4Wr</link><guid isPermaLink="false">https://www.infoq.cn/article/60e0XvVNy8HO4vJ2F4Wr</guid><pubDate>Mon, 17 Mar 2025 10:06:47 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>老黄 5090 都被初创公司虐了？印度 CEO 用 20 人团队让芯片快10 倍、功耗大砍近 80%！网友：等英伟达收购</title><description>&lt;p&gt;整理 | 华卫、核子可乐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在图形处理器（GPU）领域，英伟达、AMD 和英特尔占据主导地位已有一段时间了。虽然中国还有其他相关企业，但他们要打入美国市场一直以来都困难重重。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，一家美国 GPU 初创公司 Bolt Graphics 发布了专为游戏、渲染和超级计算机模拟等用例设计的 Zeus GPU 平台。据了解，Bolt Graphics 没有仅仅致力于打造低端显卡并寄希望于扩大规模，而是巧妙地解决了高端 GPU 计算方面的一个特定难题。该公司表示，其 Zeus GPU 不仅支持可升级内存与内置以太网接口等，而且在路径追踪工作负载方面的性能表现比英伟达 GeFOrce RTX 5090 快 10 倍左右。根据 Bolt Graphics 的数据，280 张 RTX 5090 GPU 的算力只需 28 张 Zeus GPU 即可实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/56/56e5ef91a6295d0b5f1985c473c2781d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据了解，Bolt Graphics 是一家成立不到 6 年的初创公司，由印度人 Darwesh Singh 在 2020 年创立，该公司在领英的主页上显示共有 20 位员工。该公司在 2021 年获得了第一轮融资，随后很快又于 2022 年获得了第二轮融资，专注于电影、模拟和游戏中的硬件加速光线追踪技术，目标是在解决模拟和 3D 图形等繁重任务的性能问题同时降低功耗。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创立 Bolt Graphics 前，Darwesh Singh 从事了十年的数据中心和云环境设计工作。多年来，他从事过从安装机架到为大型企业设计先进数据中心的各种工作。2014 年，Darwesh 凭借创新精神，在目睹了电影视觉效果的冗长渲染时间后，开发出了硬件加速光线追踪解决方案。这一突破为他于 2020 年创立的 Bolt Graphics 公司奠定了基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于 Zeus GPU 现在所公布的性能情况，有网友调侃道：“这家公司将在‘3、2、1’的倒计时结束后被英伟达收购。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;速度快了，功耗更低、显存可扩展？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与当今许多处理器一样，Zeus 同样依赖于多芯片设计。据介绍，入门款 Zeus 1c26-032 配备单一处理单元以及 32 GB LPRDDR5X 内存，传输带宽为 273 GB/ 秒，可使用双 SO-DIMM（速度为 80 GB/ 秒）和最高 128 GB 的 DDR5 内存。Zeus GPU 还搭配有 I/O 芯片，该芯片包含一个 400 GbE/800 GbE 的 QSFP-DD 端口、两个使用 CXL 3.0 协议的 PCIe Gen5 x16 插槽（可实现多卡间的高效内存共享）以及一个用于 BMC 的 GbE 端口。该 GPU 芯片以 256 GB/ 秒的速率与其 I/O 芯片连接。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/0b/0bb8188c04aab1d29afabe891d764443.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Zeus 单芯片架构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更高端的 Zeus 2c26-064/128 则使用双 Zeus 处理单元、一个 I/O 芯片，且支持 64 GB 或 128 GB 的 LPDDR5X 内存。最强大的旗舰版本 Zeus 4c26-256 则集成了四个处理单元、四个 I/O 芯片、256 GB LPDDR5X 以及最高 2 TB 的 DDR5 内存容量。四芯片版的 Zeus 不再以 GPU 卡的形式存在，而是直接作为服务器交付。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/89/8977f02eaf81fa5cd3e40c1740728339.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;四芯片版 Zeus 的架构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与优先考虑带宽的高端 GPU 不同，Bolt Graphics 显然更重视内存容量的绝对数值，希望借此处理更大的渲染与模拟数据集。另值得一提的是，从该公司展示的 Zeus 性能表现表格图中可以看到，Zeus 的 DDR5 内存还带有 SO-DIMMs 后缀，这代表它是支持可插拔的。也就意味着，这块显卡是可以通过插入多条 DDR5 内存来扩展显存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/0e/0efeeda57fc669ba15879196acbd135d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，内置的 400 GbE 及 800 GbE 端口可实现联网 GPU 之间的更快数据传输，这表明 Zeus 显然是以数据中心作为主要应用场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/ab/ab62e808245f274f225db6aeae98f59c.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Zeus 高性能计算模拟用例&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Zeus vs RTX 5090&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据该公司介绍，高质量渲染、实时路径追踪与计算是 Zeus 关注的重点领域，因此即使是入门级 Zeus 1c26-32，也能提供比英伟达 GeForce RTX 5090 更高的 FP64 计算性能（高达 5 TFLOPS，远高于后者的 1.6 TFLOPS），路径追踪性能也高得多（77 Gigarays，远高于后者的 32 Gigarays）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;功耗&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Zeus 还拥有比英伟达这款旗舰级产品更大的片上缓存（高达 128 MB，后者为 96 MB），且运行功耗更低（120W，后者高达 575W），约是 RTX 5090 的 21%，这使其在模拟、路径追踪和离线渲染等领域更高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此前，RTX 5090 曾因高功耗而受到争议，其相比 RTX 4090 性能提升有限，但功耗却多出 125W，对电源的要求较高。原本许多用户都希望，英伟达 RTX 50 系列能更注重效率而不是继续提高功耗，特别是考虑到 RTX 4090 相比 350W 的 RTX 3090 已经是一次大幅跃升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;价格&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，四芯片版的 Zeus 虽然功耗低于 RTX 5090，但价格可能更贵——从数据来看，除了 FP32 和 FP16 运算之外，四芯片版的 Zeus 在所有工作负载方面都能胜过英伟达的这款旗舰级游戏显卡，这凸显出 Zeus 或并不打算以传统游戏画面渲染为主要卖点。RTX 5090 推出时就因高昂价格引发讨论，1999 美元（合人民币约 14647 元）的 GPU 定价对普通玩家来说不是一笔不小的数目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该公司表示，四芯片版本针对电磁场建模、光子学研究和快速傅里叶变换（FFT）计算进行了优化。凭借更大的内存池加上对于外部存储的较低依赖，Zeus 有望提高大规模模拟的运行速度。当然，前提是它的这套混合内存子系统在所有工作负载上都能高效运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/6f/6f31e10bfce088e65fe33bb610e15920.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Zeus 电磁波模拟输出用例&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 及传统渲染&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RTX 5090 在 AI 工作负载中似乎仍然占据主导地位，其 FP16 算力达到 105 TFLOPS、INT8 算力达到 1637 TFLOPS，而单芯片 Zeus 的这两项指标分别只有 10 FP16 TFLOPS 与 614 INT8 TFLOPS。如果 Zeus 可以进行传统渲染，那么 1c26-32 也只拥有 10 FP32 TFLOPS 性能，似乎远无法与 RTX 5090 的 105 TFOPS 相抗衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 Bolt Graphics 还推出了 Glowstick 路径追踪渲染引擎功能，这可能是种很有前途的内部实时渲染解决方案。传统的渲染工作流程往往需要较长的处理时间才能实现结果可视化，而 Zeus 则大大减少了这种延迟，因此更适用于专业的可视化应用场景。与现有解决方案相比，Bolt Graphics 声称其在单芯片版本上的性能提高了 2.5 倍，且使用多张 GPU 时性能还会更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统图形处理方面，但目前还不清楚 Zeus 究竟会提供怎样的性能。与现代消费级显卡相比，Zeus 公布的矩阵吞吐量确实看似平平无奇，更不用说与数据中心级别的显卡相比了。单块功率限制为 700W 的英伟达图形处理器 Blackwell B200，可提供 60 TFLOPS 的着色器 FP32 运算能力、30 TFLOPS 的 FP64 密集矩阵运算能力和 1.8 PetaFLOS 的稀疏 TF32 运算能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;全面投产的挑战&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Zeus 的出现似乎代表着一种突破，但目前 Zeus 仍在纯模拟环境下运行，因此以上所有性能声明均未通过实际硬件验证。Bolt Graphics 方面表示，其首批开发套件将于 2025 年底上市，全面投产则要等到 2026 年底，期间软件开发者将可充分试用这款硬件。如果 Zeus 真能兑现承诺，则很有可能成为科学计算、路径追踪与离线渲染等应用场景下的重要替代方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要注意的是，由于 Zeus 针对的是路径追踪渲染技术以及计算工作负载，因此它被猜测可能没有传统的固定功能 GPU 硬件，如纹理单元（TMU）及光栅操作单元（ROP）。尽管如此，每张 Zeus GPU 都配备一个 DisplayPort 2.1a 和一个 HDMi 2.1b 输出端口。但该公司则专门解释称，Zeus 配备了 TMU 和 ROP 引擎，且优化工作仍在持续进行，目前尚未披露任何规格。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 GPU 中，ROP 负责将 3D 图形数据转换为 2D 像素表示，是图形渲染过程中的重要一步，对最终输出阶段起着关键作用。据英伟达介绍，RTX 5090 配备了 176 个 ROP 单元。但前不久被曝，有部分用户到手的 RTX 5090 存在 ROP 数量不足的缺陷。要知道，ROP 的缺失将带来许多明显的游戏体验影响，包括游戏帧率下降、延迟增加、抗锯齿性能降低等。英伟达当时对此的解释是生产问题，并表示故障卡的数量占比不到 0.5%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外， 软件支持无疑将成为决定 Zeus GPU 成功与否的关键因素，毕竟单凭硬件功能其并不足以与 AMD 和英伟达等老牌厂商展开竞争。 与英伟达的 CUDA 和 AMD 的 ROCm 不同，Bolt Graphics 的 Zeus 缺乏成熟且得到广泛采用的软件生态系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据已发布的演示文稿，与依赖专有指令集的 AMD、英特尔和英伟达 GPU 不同，Bolt Graphics 的 Zeus 依赖于开源的 RISC-V ISA，其采用了一种开源无序通用 RVA32 标量核心，同时与 FP64 算术逻辑单元（ALU）及 RVV 1.0（RISC-V 扩展版 1.0）配合使用，能够处理 8 位、16 位、32 位乃至 64 位的数据类型，还配备了针对加速科学工作负载所设计的其他专有扩展功能。基于 RISC_V 架构，Zeus 可以使用现有的开源工具和库，但若无强大的开发者支持，其采用可能会受到限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，目前还不清楚 Zeus 是否支持行业标准框架，如 OpenCL、Vulkan 和 CUDA 转换层——这些框架对于 GPU 产品在高性能计算（HPC）工作负载领域的推广至关重要。如果 Bolt Graphics 能够提供强大的开发者工具、优秀的编译器支持以及同 Linux HPC 环境的兼容性，Zeus 确实有望成为科学计算与渲染领域的一位强大参与者。但无论如何，与英伟达成熟生态系统的“艰苦”竞争仍然不可避免。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;驱动程序也将是另一大潜在影响因素——即便是像英特尔这样的科技大厂，往往也需要很长时间才能解决驱动程序带来的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简而言之，关于 Zeus GPU 家族还有太多的未解之谜和需面临的挑战。Zeus 目前才刚刚完成模拟运行测试，实体硬件计划于今年晚些时候推出。它会如何处理传统渲染、路径追踪以及 AI？我们还须拭目以待。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tomshardware.com/pc-components/gpus/startup-claims-its-zeus-gpu-is-10x-faster-than-nvidias-rtx-5090-bolts-first-gpu-coming-in-2026&quot;&gt;https://www.tomshardware.com/pc-components/gpus/startup-claims-its-zeus-gpu-is-10x-faster-than-nvidias-rtx-5090-bolts-first-gpu-coming-in-2026&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/&quot;&gt;https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;声明：本文为 AI前线整理，不代表平台观点，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HPZgWlpt27447fyzstrK</link><guid isPermaLink="false">https://www.infoq.cn/article/HPZgWlpt27447fyzstrK</guid><pubDate>Mon, 17 Mar 2025 10:03:54 GMT</pubDate><author>华卫,核子可乐</author><category>芯片&amp;算力</category></item><item><title>微博多模态 AI 应用实践分享｜QCon 北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京盛大召开，大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;微博增值业务研发中心总经理吴侃已确认出席本次大会，并在&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;「多模态大模型及应用」&lt;/a&gt;&quot;专题论坛分享《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6392&quot;&gt;以微驭博：从多模态感知到数智化转型&lt;/a&gt;&quot;》。在当前数智化转型的浪潮中，企业普遍面临着成本与收益难以平衡的困境，传统模式也难以兼顾规模化和个性化需求。而多模态 AI 技术凭借对多源数据的综合处理能力，为提升业务效率带来了新的可能，大模型能力的提升更是降低了 AI 应用门槛。吴侃此次演讲将结合微博及其他行业的实际应用案例，深入探讨如何借助多模态 AI 技术为传统业务注入创新活力，提升企业数据的综合利用价值，同时还会分析如何平衡大模型技术优势与实际应用中的成本和风险，确保企业获得可持续的投资回报。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴侃在行业内经验丰富，曾任职于硅谷，随后深度参与了微博从创立至今长达十六年的发展历程。他一直专注于技术落地、商业模式以及创新应用的实践，还参与了国家十四五项目和国家自然科学基金项目，在相关领域积累了深厚的见解。本次会议中，他的详细演讲内容如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;演讲提纲1. 多模态大模型技术的发展脉络与趋势从单模态的感知到跨模态的综合理解，反映了 AI 技术在认知层面的深刻变革AI 正逐步实现从简单信号识别、规则到深层语义理解、意图感知和决策的本质飞跃多模态的边云结合的计算协同协作优化&amp;nbsp;2. 多模态 AI 赋能企业数智化转型的路径与实践多模态内容理解在微博的部分应用：整合文本、图像、视频和音频等多模态数据，实现舆情监测与内容智能管理分类整理微博多模态素材，构建个人知识库以提升信息管理使用效率图像与视频分析理解图像内容与事件的“坑”&amp;nbsp;3. 融合多模态生成式 UI 与 SaaS+AI 创新交付模式从传统的图形用户界面（GUI）与对话式界面（CUI）逐渐转向融合交互界面的发展趋势通过融合生成式 UI 快速实现与现有业务系统之间的整合，显著提升用户操作体验与业务效率建立在多个智能体实时协作与通信的基础之上的数据共享、任务分配和协作机制您认为，这样的技术在实践过程中有哪些痛点？数智化转型时面临技术与实际业务需求脱节的问题，对于实际业务的 knowhow 不够深入，导致方案无法落地商业角度考虑大模型落地往往是投入大，收益甚微，“大炮打蚊子”商业项目和企业落地要求确定性，而大模型“幻觉”造成方案达不到预期指标演讲亮点全新视角看待多模态大模型技术落地如何寻找场景，平衡精度与成本Agent 协同融合，数据共享机制拓展应用生态听众收益理解企业如何以多模态大模型为抓手，在规模化落地过程中有效权衡模型准确性与幻觉风险之间的实践经验与心得了解新时代交互界面的最新实践，帮助企业高效、低门槛地实现业务系统的智能化升级与融合获得企业数字化转型过程中结合自身业务特点科学规划多模态多 Agent AI 落地路径的方法论和实操经验，有效避免数据治理、成本控制等方面的常见“坑”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，本次大会还精心策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有 100 多位资深专家现场分享前沿技术洞察和一线实践经验。现在报名可享 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lcNt3PcCeLvZY2UuvFB4</link><guid isPermaLink="false">https://www.infoq.cn/article/lcNt3PcCeLvZY2UuvFB4</guid><pubDate>Mon, 17 Mar 2025 09:52:34 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category></item><item><title>零一万物全面拥抱DeepSeek，李开复：今年第一季度收入接近去年全年</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;“零一万物聚焦ToB的商业模式从一开始就非常健康，去年有超一亿元收入，今年第一季度的收入就接近了去年全年的收入，所以零一万物ToB业务的运营模式是非常良性的。”李开复说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;李开复表示，过去整个大模型行业ToB会遇到挑战，一方面是因为中国还没有经历所谓的“ChatGPT时刻”，模型赋能后企业能获得的价值也不见得足够大。另一方面，过去是模型厂商大幅砍价去争招标，最后厂商都赚不到很多钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“今天的差别是，企业客户都觉醒了，有了DeepSeek Moment之后，很多企业对大模型赋能自身业务抱着很大的希望，也愿意在自己的公司去尝试接入，并且用在更核心的业务场景，所以我认为，未来会有更多的企业下决心拥抱大模型。”李开复表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年7月，零一万物首度对外亮相后相继发布了Yi系列大模型。去年 5 月，零一内部对 Scaling Law 的边际收益递减进行推演，最终决定放弃训练原定万亿参数的超大模型 Yi-X-Large，而转为训练更轻量化、更具商业落地前景的 MoE（混合专家）模型 Yi-Lightning。到了2024年底，零一宣布进行针对 to B 智能化市场的战略转型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年 1 月，零一万物与阿里云成立产业大模型联合实验室，正式宣布聚焦参数适中、性能领先、推理速度快、推理成本低的轻量化模型，以产业大模型主助力商业落地。2 月，零一万物与苏州高新区联合成立的产业大模型基地正式授牌，聚焦制造、金融等领域的产业大模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3月17日，零一万物在To B业务上更进一步，正式发布万智企业大模型一站式平台，并宣布可提供企业级 DeepSeek 部署定制解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;李开复表示，很多模型似乎都由模型厂商自己提供类似的平台和解决方案。DeepSeek是一个非常厉害的技术团队，他们还是希望继续沉浸在技术开发中，这也对今天很多企业在DeepSeek落地时会面临一些挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对企业使用DeepSeek遇到的部署难、应用难、高端模型用户定制难和数据问题，万智企业大模型一站式平台为企业规划了DeepSeek 落地“三步走”步骤，并提供了针对性的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在基座模型部署方面，零一万物整合了 DeepSeek、Qwen、Yi 等国内 Top3 模型，推出了“安全部署”方案。对于算力储备薄弱的企业，零一万物将联合头部硬件厂商推出软硬集成式一体机方案，预装高性能 GPU，内置 DeepSeek 全系列模型，部署周期缩短至小时级，帮助企业快速上手 671B 参数的满血增强版的 DeepSeek-R1 模型；对于已具备算力硬件基础设施的企业，零一万物同样可实现 DeepSeek 全系列模型的快速部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;完成模型部署后，具备工具调用能力、自主执行任务的专家级 Agent，成为推动模型落地的关键。“DeepSeek + 联网搜索”支持实时数据，赋能决策和业务流；“DeepSeek + 知识库RAG”来构建企业专属知识大脑；“DeepSeek + 智能体 Agent”驱动业务自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，零一万物支持行业定制，企业可以基于自身数据库对 DeepSeek-R1 进行模型微调，使之符合垂直领域的业务需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;零一万物企业解决方案技术负责人王猛表示，定制化开发在过去确实是一个长周期重交付的事情，但AI时代就不是这样了。AI coding能力越来越强，AI怎么做交付也有了比较大的改变。“AI可以帮助我们完成很多交付，我们很多交付代码都是AI来写，效率非常高，这也是对行业的根本性颠覆。”另外，整个产品中台有很强的扩展性，比如客户喜欢用Dify就用Dify，喜欢用LangFlow就可以用LangFlow，团队在全平台做了很多功能解耦来保证扩展性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;“不会再做万亿以上超大参数模型”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这次发布会上，零一万物COO &amp;nbsp;Anita（黄蕙雯）明确表示“不会再做万亿以上超大参数模型，因为我们做不起。所以今年对我们来说非常重要的，就是仰望星空、脚踏实地。AGI终有一天会来到，我们也会伴随着行业一起去迎接AGI的来到，但我们现阶段最关注的是能否真正把AI推到市场上，让市场不只歌颂AI，而是真正能用上AI，这是我们现阶段的战略选择。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;零一的团队组织也重新做了调整和聚焦。Anita透露，当前零一万物的模型每天还在运维。现在因为面向企业服务，所以主要做中规模尺寸、性价比特别高的模型。其中，性价比一定要结合优异的Infra能力，还有模型本身训练的持续优化，零一目前维持在MoE的路线上还在做V2。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;零一万物如今聚焦在ToB商业化上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;李开复表示，零一做ToB产品的优势在于积淀。“我们是有完整的软硬结合的解决方案，我们自己做大模型，在如何微调、如何对应数据库、如何做机器训练和数据配比上经验丰富。我们真的能够帮上厂商，因为这些大模型的知识可能难以由系统集成商和一体机提供商来提供。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“大厂也有大模型经验，我非常尊重，但大厂主要业务还是在做云，而且定价比较昂贵，所以零一万物提供跟大厂一样好的产品，这是我们唯一的产品，会帮你深度做私有化部署，而不是想卖一个云的解决方案给你，另外我们的定价比大厂来得较低，所以我们有自己独特的优势。”李开复补充道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年零一万物做的最大的调整是：不再执着于只推Yi 模型，会聆听市场的变化和需求，市场上有很好的模型，也有很具性价比的模型可以选择，所以零一采取“开放模型”的策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“作为第一个全面拥抱DeepSeek模型的六小虎，零一万物以开放的态度选择‘模型自由’，除了自研 Yi 模型，更开放企业客户选用市场上性价比佳的优异模型。”李开复认为，“未来的大模型的行业竞争将不再单指模型性能的比拼，更关乎从中台到应用的能力，即模型能否快速响应场景需求、基于中台构建行业应用。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/mS8uUawFsT6zvaEeq5tF</link><guid isPermaLink="false">https://www.infoq.cn/article/mS8uUawFsT6zvaEeq5tF</guid><pubDate>Mon, 17 Mar 2025 09:37:26 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>微软亚太人工智能黑带团队技术总经理曾臻确认出席 QCon 北京并发表主题演讲</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京盛大召开。此次大会以 “智能融合，引领未来” 为主题，汇聚各领域技术先锋与创新者，共促行业发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;微软亚太人工智能黑带团队技术总经理曾臻（Vickie）已确认出席，并在Keynote主论坛发表题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6385&quot;&gt;Al Vision Shape the Future&lt;/a&gt;&quot;》的主题演讲。在 AI 技术飞速发展的当下，我们正迈向新时代。而 AI 的下半场将由哪些核心技术引领成为关键问题。本次演讲将深入探讨 AI 未来的发展方向，重点聚焦智能体（Agentic AI）、推理模型（Reasoning Models）以及模型微调（Fine-Tuning）这三个关键领域。曾臻（Vickie）还将分享微软在 AI Vision 领域的最新研究成果，以及这些技术如何塑造未来智能应用的蓝图。无论是 AI 领域的从业者，还是对未来技术充满好奇的探索者，这场演讲都能带来前沿视角和深刻洞见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;曾臻（Vickie）目前带领微软全球人工智能亚太黑带专家团队，支持大中华区和日本客户基于 Azure AI 服务的人工智能场景创新与实践。在加入微软之前，她曾就职于阿里云、Couchbase、IBM 等国内外大型科技或互联网企业，拥有丰富的跨行业现代化数据平台建设经验，以及运用云原生、大数据和人工智能技术辅助行业客户落地数字化转型项目的经验。她擅长技术管理，带领团队实践业务驱动的技术领导力，是科技行业出色的女性技术管理者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/13/1396ec18cc215a27a04208e3e1fdae5c.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，杭州久痕科技CEO汪源博士、北京智源人工智能研究院副院长兼总工程师林咏华、清华大学副研究员/无问芯穹联合创始人颜深根、Coupang 研发 VP郭东白、新浪微博首席科学家及新技术研发负责人张俊林也已确认出席QCon北京站主论坛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/TjJH8Ch3rW83EYpsFwkF</link><guid isPermaLink="false">https://www.infoq.cn/article/TjJH8Ch3rW83EYpsFwkF</guid><pubDate>Mon, 17 Mar 2025 07:44:01 GMT</pubDate><author>QCon全球软件开发大会</author><category>微软</category><category>AI&amp;大模型</category></item><item><title>云计算巨头合作开发全新 Kubernetes 资源管理工具</title><description>&lt;p&gt;谷歌云、亚马逊云科技和微软 Azure 联合宣布了一个新的开源项目 Kube Resource Orchestrator（kro，读作 “crow”）。该项目试图对 Kubernetes 资源的分组和部署方式进行标准化，使平台团队可以更轻松地部署工作负载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该文指出，Kubernetes 没有提供一种可供平台团队使用的原生方法，让他们能够创建自定义资源组供开发团队使用，许多组织都是使用 Helm 或 Kustomize 等客户端模板工具，或构建自己的自定义 Kubernetes 控制器。事实证明，这些方法往往维护成本高昂，非专业人员难以有效使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有了 kro，你就可以将应用程序及其依赖项组合成单个资源，方便终端用户使用。—— Abdelfettah Sghiouar 和 Nic Slattery&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kro 的核心创新是引入了&amp;nbsp;自定义资源&amp;nbsp;ResourceGraphDefinition&amp;nbsp;。Kro 将 Kubernetes 部署及其依赖项都封装到了单个 API 中，支持自定义终端用户界面，仅暴露供非平台工程师使用的参数。这种屏蔽隐藏了 Kubernetes 和云提供商 API 端点的复杂性，那在部署上下文中并没有用处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/85/85afab6f4b7a3232749fae162e999e9b.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该文大致介绍了两个真实的 kro 应用案例。在第一个场景中，平台工程师使用 kro 为组织成员提供自助服务访问权限，使他们可以创建 Google Kubernetes 引擎（GKE）集群，并且预先配置好了管理工作负载、策略和安全设置。第二个示例演示了 DevOps 工程师如何为 Web 应用程序创建可重复使用的定义，封装从部署和服务到监控代理和云存储的所有必要的资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于当前云提供商提供的那些用于管理云资源的 Kubernetes 扩展，Kro 可与它们无缝集成。其中包括：AWS Kubernetes 控制器（ACK）、Google 配置连接器（KCC）和 Azure 服务操作员（ASO）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kro 可实现标准化、可重用的服务模板，促进不同项目和环境间的一致性，而且完全是 Kubernetes 原生的 。Kro 仍处于早期开发阶段。文中写道：“作为一个尚处于早期阶段的项目，kro 还不能用于生产，但我们仍然鼓励你在自己的 Kubernetes 开发环境中进行测试。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AKS 工程博客上，Bridget Kromhout 和 Matthew Christopher 发表了一篇 文章，从微软的角度简要介绍了 kro 项目。对于这个旨在简化资源管理的 Kubernetes 原生工具，该文强调了微软 Azure 与 亚马逊云科技以及谷歌云三方的合作。Kromhout 和 Christopher 还提供了特定于 Azure 的实施案例，并强调了社区参与的机会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们以客户和云原生社区的需求为中心，提供一个无论你在哪里运行 K8s 集群都能有效使用的工具。—— Matthew Christopher 和 Bridget KromhoutKro&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网站上提供的演练教程深入浅出地解释了 kro 的工作原理，说明了 kro 如何创建 ResourceGraphDefinition 。它首先生成一个有向无环图（DAG），用于理解定义的依赖关系、验证它们并确定正确的部署顺序。然后，它会在 Kubernetes 集群中为该资源新建一个自定义资源定义（CRD）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社区中有些评论认为，kro 有能力增强或取代其他成熟的工具，如 Crossplane（一个开源的 CNCF 项目，可让用户使用 Kubernetes 协调云资源）和 Helm（用于定义、安装和升级 Kubernetes 应用程序的软件包管理器）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 DevOps Toolkit 频道的一个 YouTube 视频中，Viktor Farcic 讨论了 kro 的发布。他还探讨了 kro 对 Crossplane 的影响。最初，Farcic 对 kro 简化云资源组合的潜力感到兴奋，并成功创建了一个简单的应用定义，生成了正确的 Kubernetes 资源。但是，Farcic 发现，对于比较复杂的场景，如涉及有条件资源创建和数据库集成，就会产生许多问题，像缺失默认值和所有者引用，以及资源组的更改无法正确地传播到现有资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还指出，将 YAML 用于命令式结构并不理想，在一种并非为其设计的格式中添加更多的逻辑可能会导致 “恶果”。对于 Crossplane 社区来说，最重要的是，Farcic 对其用途提出了质疑，因为 kro 与现有工具的功能存在重叠。他指出，“kro 的功能与不久前创建的其他工具差不多，没有任何令人信服的改进。”虽然 kro 似乎语法更简单，模板更少，但他说，它目前只提供了 Crossplane 的一小部分功能，还不是一个可行的替代品，尤其是 Crossplane 支持多种语言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在一篇题为 “Helm 杀手终于出现了吗？”的博文中，来自 Parity 公司的 Wilson Spearman 指出，Helm 的架构在管理依赖关系、处理 CRD 升级和管理生命周期等方面存在着根本性的限制，而 kro 的成功之处在于其语法更人性化、更易读。Spearman 最后预测，Helm 将继续为开源软件和小型组织提供服务，而 kro 则将在大型企业中占据主导地位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kro 项目已发布在 GitHub 上，由谷歌、亚马逊云科技和微软团队共同拥有，并欢迎社区为其开发做出贡献。项目网站上有全面的文档和示例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2025/02/kube-resource-orchestrator/&quot;&gt;https://www.infoq.com/news/2025/02/kube-resource-orchestrator/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/3e5BZcY6oRKDByH8Z5IB</link><guid isPermaLink="false">https://www.infoq.cn/article/3e5BZcY6oRKDByH8Z5IB</guid><pubDate>Mon, 17 Mar 2025 07:00:00 GMT</pubDate><author>作者：Matt Saunders In</author><category>云原生</category></item><item><title>Agent下的知识管理新思考｜杭州久痕科技 CEO 汪源博士确认出席 QCon 北京</title><description>&lt;p&gt;2025 年 4 月 10 - 12 日，&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing?utm_source=infoqweb&amp;amp;utm_medium=artad&quot;&gt;QCon 全球软件开发大会&lt;/a&gt;&quot;将在北京盛大召开。大会以 “智能融合，引领未来” 为主题，将汇聚各领域的技术先行者以及创新实践者，为行业发展拨云见日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杭州久痕科技 CEO 汪源博士已确认出席，并发表题为&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6401&quot;&gt;《Agent 元年，关于知识管理的新思考》&lt;/a&gt;&quot;的主题演讲。今年被公认为 Agent 元年，Manus 的火爆引发了不少人的恐慌，大家似乎担心工作很快会被 AI 取代。在本次分享中，汪源博士将汇集权威研究，客观分析知识工作的自动化前景，探讨知识工作自动化思路。同时，他还会回顾知识管理领域的发展历程，分享自己对该领域发展的思路和见解，并介绍久痕科技在相关方面所做的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;汪源博士目前主要研发面向全球市场的智能办公助手产品。此前，他担任网易集团副总裁、杭州研究院执行院长，还是 CCF CTO Club 创始成员、中国软件行业协会智能应用服务分会副主任、浙江软件行业协会副理事长。他荣获过浙江省有突出贡献青年科技人才、万人计划青年拔尖人才和杭州市杰出青年人才等荣誉，承担过 6 项省部级以上科技项目，获得省部级以上科技进步奖特等奖和一等奖各 1 项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/39/394bfdb9dc23c363aeb44406b79db556.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除此之外，微软亚太人工智能黑带团队技术总经理曾臻（Vickie）、北京智源人工智能研究院副院长兼总工程师林咏华、清华大学副研究员/无问芯穹联合创始人颜深根、Coupang 研发 VP郭东白、新浪微博首席科学家及新技术研发负责人张俊林也已确认出席QCon北京站主论坛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次大会还策划了&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1777&quot;&gt;多模态大模型及应用&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1766&quot;&gt;AI 驱动的工程生产力&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1754&quot;&gt;面向 AI 的研发基础设施&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1776&quot;&gt;不被 AI 取代的工程师&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1769&quot;&gt;大模型赋能 AIOps&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1761&quot;&gt;云成本优化&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1768&quot;&gt;Lakehouse 架构演进&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/track/1763&quot;&gt;越挫越勇的大前端&lt;/a&gt;&quot;等专题，届时将有来自不同行业、不同领域、不同企业的100+资深专家在QCon北京现场带来前沿技术洞察和一线实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在报名即可以享受 9 折优惠，单张门票立省 680 元，详情可扫码或联系票务经理 18514549229 咨询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8b4ca81cc61c526d8590d33c2853b422.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Gr7TRFKQKbvCoqv4vmXv</link><guid isPermaLink="false">https://www.infoq.cn/article/Gr7TRFKQKbvCoqv4vmXv</guid><pubDate>Mon, 17 Mar 2025 04:06:50 GMT</pubDate><author>QCon全球软件开发大会</author><category>AI&amp;大模型</category></item></channel></rss>