<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>ModelScope魔搭社区-DevPress官方社区</title><link>https://community.modelscope.cn</link><atom:link href="http://rss.wudifeixue.com/modelscope/community" rel="self" type="application/rss+xml"></atom:link><description>ModelScope魔搭社区 DevPress官方社区-ModelScope旨在打造下一代开源的模型即服务共享平台，为泛AI开发者提供灵活、易用、低成本的一站式模型服务产品，让模型应用更简单。 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)</description><generator>RSSHub</generator><webMaster>i@diygod.me (DIYgod)</webMaster><language>en</language><image><url>https://g.alicdn.com/sail-web/maas/0.8.10/favicon/128.ico</url><title>ModelScope魔搭社区-DevPress官方社区</title><link>https://community.modelscope.cn</link></image><lastBuildDate>Wed, 19 Mar 2025 21:25:37 GMT</lastBuildDate><ttl>5</ttl><item><title>ModelScope魔搭25年3月发布月报</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/bf09e4f4d2dc4bf3904ca3e10f7c4af8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;在这个春天里，小鲸鱼的DeepSeek-R1系列在模型社区掀起的巨大浪潮尚未平息，我们又迎来了千问的QwQ-32B正式版本，社区在Reasoning模型上的热情还在升温。除此之外，业界其他模型在过去一&lt;/blockquote&gt;   &lt;p&gt;本月上新&lt;/p&gt;

&lt;p&gt;在这个春天里，小鲸鱼的DeepSeek-R1系列在模型社区掀起的巨大浪潮尚未平息，我们又迎来了千问的QwQ-32B正式版本，社区在Reasoning模型上的热情还在升温。除此之外，业界其他模型在过去一个月里持续的、高强度的开源开放，为广大开发者带来一波波新的惊喜。但过去的一个月也不是专属于大语言模型的。我们携手各个模型团队，迎来了Wan2.1，StepFun，Hunyuan等系列中的新模型发布，为视频生成领域，注入了新鲜的活力。&lt;/p&gt;

&lt;p&gt;为了更好的服务广大的社区开发者，我们对ModelScope的模型存储和下载等服务做了整体优化，来更支撑广大开发者的热情，以及因此剧增的社区下载流量。同时ModelScope的开源工具链生态，也针对Reasoning模型所需的训练微调，评估，推理等全链路需求提供支持，并对Wan 2.1等视频生成模型的推理和训练，进行了全方位的优化。我们希望这些开源工具链，能支持广大开发者更方便地把模型真正用起来。我们的AIGC专区，也在用户体验升级持续快速迭代的同时，第一时间就引入了基于Wan2.1 视频模型提供的视频生成、训练定制等能力，满足广大开发者对于高质量内容创作和技术探索的需求，欢迎大家到AIGC专区体验。ModelScope的Paper板块，则引入了AI解读论文的能力，方便大家探索模型背后的技术细节。&lt;/p&gt;

&lt;p&gt;在刚结束不久的GDC（全球开发者大会）上，我们和众多的开发者做了线下面基，感谢大家对社区一直以来的支持！这是一个模型发展真正日新月异的时代，ModelScope团队非常有幸和大家一起见证社区模型技术一日千里的发展，我们也持续致力于平台服务的完善，以及我们的开源生态的打造。今天准时带来三月份ModelScope社区Release Notes，请大家查收。也欢迎广大开发者通过各种渠道，为大家共同的社区的建设与发展，提供反馈与建议。&lt;/p&gt;

&lt;p&gt;· 站点更新&lt;/p&gt;

&lt;p&gt;1. 模型下载速度优化：为了在社区巨大的流量下，依旧能持续为广大开发者提供稳定高速的模型下载，我们对后端模型服务和存储做了整体的优化和升级，当前已全面上线。更多的优化还在路上～～&lt;/p&gt;

&lt;p&gt;2. 论文板块升级：让模型和背后的技术解读更近一点！ModelScope的Paper板块新增了“AI读论文”能力，帮助大家在我们的ModelScope社区上，不仅能迅速找到模型相关的研究与论文，而且能更方便的通过论文对模型和相关技术的细节，进行深入的理解。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;750&quot; src=&quot;https://devpress.csdnimg.cn/5af34e3c36484b99865757b95ebbf756.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;/p&gt;&lt;p&gt;3. AIGC专区&lt;/p&gt;


&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;首页内容瀑布流改版：专区的首页整体内容布局，切换为更直观的瀑布流设计，使浏览更加便捷和高效。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;608&quot; src=&quot;https://devpress.csdnimg.cn/4c198409fc104ae4b24068c06ade9a71.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;/p&gt;&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;支持视频生成&amp;amp;训练功能：基于Wan2.1视频模型，推出视频生成与训练能力，并支持用户在社区内分享自己生成的创意视频作品。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;


&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1182&quot; src=&quot;https://devpress.csdnimg.cn/620e1821708140fd9834bfb6177b5480.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;903&quot; src=&quot;https://devpress.csdnimg.cn/a0410bdac9f74aec97a8390345cbf1bf.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;553&quot; src=&quot;https://devpress.csdnimg.cn/5b2a4a8d556d4916ac7ac51185629f2d.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;关联模型可视化：在作品详情页中增加了关联模型的可视化展示，方便直接查看作品背后的模型信息，便于理解和评估。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;作品区排序筛选：支持根据最新或热度对作品反馈进行排序筛选，帮助大家更快找到想要的灵感和创意。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;525&quot; src=&quot;https://devpress.csdnimg.cn/65749bc49acf40ae8b996f9b7bb5d475.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;/p&gt;&lt;p&gt;4. 合集广场等页面的加载速度进行了整体的优化，同时对模型等元素页面关联合集的逻辑进行修订，确保高质量的合集能优先展示给开发者。&lt;/p&gt;


&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;802&quot; src=&quot;https://devpress.csdnimg.cn/ea5ef504835d43a9bf81bcb4903c457a.png&quot; width=&quot;696&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
&lt;/p&gt;&lt;p&gt;5. 模型系谱血缘优化上线，保证更准确的显示，包括AIGC专区的模型。&lt;/p&gt;


&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;360&quot; src=&quot;https://devpress.csdnimg.cn/273b2c75eab14580b5293b0321a192e0.png&quot; width=&quot;610&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;6. 模型介绍页面的代码准确性提升：对于ModelScope上的模型，依托transformers/diffusers/peft等库的接入的代码，对模型卡片上的代码自动转写逻辑进行了完善。整体范例代码的可用性有了较好的提升。&lt;/p&gt;

&lt;p&gt;7. 创空间Gradio完成5.20.1新版本支持并上线。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;258&quot; src=&quot;https://devpress.csdnimg.cn/40218a7ead584d2c8d4ad1af3635e5fa.png&quot; width=&quot;632&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;8. Langflow 创空间迭代：在魔搭Langflow创空间使用模板快速创建工作流时，对模板工作流中的模型节点默认设置为 ModelScope 节点，方便社区用户快捷使用魔搭推理API-Inference构建工作流应用；同时支持了一键发布工作流为独立的Playground创空间，方便应用更好地被分享、体验。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;570&quot; src=&quot;https://devpress.csdnimg.cn/e2ba3ccfd9af48d3a623362802bb77f3.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;· 开源SDK与工具链更新&lt;/p&gt;

&lt;p&gt;1. 模型的命令行下载提供了单独的token入口，支持一条命令完成需要鉴权的模型下载。原生的pipeline提供了对于LLM更完善的支持。对于即将发布的ModelScope国际站上资源的下载，提供了更好的支持。&lt;/p&gt;

&lt;p&gt;2.SWIFT 3.2版本发布：针对Reasoning模型训练的需求，提供了一系列围绕GRPO训练的支持。包括LLM和多模态模型的GRPO在单节点和多节点上的训练，模型权重的分批加载，ORM支持，以及GRPO训练过程中需要的灵活多样的推理配置。添加了embedding模型的训练支持，并完善了sampler的能力。在infer方面，对于接入的多种引擎提供了更完善的功能对接。&lt;/p&gt;

&lt;p&gt;3.EvalScope更新到0.13，支持了对于Reasoning模型思考效率的评估，并针对推理模型特点，支持对模型输出进行后处理后的评测，以及对reasoning_content字段的特殊处理等等。引入LLM-as-a-Judge的评测方式，支持大模型打分评估。添加AIME25, MuSR, ProcessBench, SuperGPQA, SimpleQA, Chinese SimpleQA, LiveCodeBench等一系列新的benchmark。&lt;/p&gt;

&lt;p&gt;4.DiffSynth在第一时间全面支持了Wan2.1系列模型的显存优化、FP8 量化、多种训练方式等能力，并提供了对多种attention机制的支持。发布后DiffSynth-Studio项目再次上榜GitHub Trending #2，感谢广大开发者的支持。&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d90e21d649b06b61d0a713.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d90e21d649b06b61d0a713.html</guid><pubDate>Tue, 18 Mar 2025 06:09:37 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>论文推荐：R1-Omni、VisualPRM、4D LangSplat、Vision-R1、GoT</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/bb6038a7d2634559bf467e818e46849d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;简要介绍：由复旦大学、上海AI实验室等机构提出了首个统一多模态理解和生成的奖励模型UnifiedReward。该工作构建了大规模人类偏好数据集，包含图像和视频生成/理解任务，并利用该模型进行自动构建高&lt;/blockquote&gt;   &lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;180&quot; src=&quot;https://devpress.csdnimg.cn/eee5dffee2884576a58ac7c57850093c.gif&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;作者：InternLM、Qwen 等 LLM每周一览热门论文版，顶会投稿选题不迷惘。快来看看由「机智流」和「ModelScope」社区推荐的上周「多模态」方向的热门论文吧！&lt;/p&gt;

&lt;h2&gt;Unified Reward Model for Multimodal Understanding and Generation&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124332&lt;/p&gt;

&lt;p&gt;简要介绍：由复旦大学、上海AI实验室等机构提出了首个统一多模态理解和生成的奖励模型UnifiedReward。该工作构建了大规模人类偏好数据集，包含图像和视频生成/理解任务，并利用该模型进行自动构建高质量偏好对数据，最终通过DPO优化视觉模型。实验结果表明，联合学习评估多样化视觉任务可以带来显著的相互益处。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;312&quot; src=&quot;https://devpress.csdnimg.cn/6405245f10e349d0b8a19134ccb5c936.png&quot; width=&quot;504&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126628&lt;/p&gt;

&lt;p&gt;简要介绍：提出了Generation Chain-of-Thought (GoT)，这是一种将推理能力融入视觉生成和编辑的新范式。该工作将常规的文本到图像生成和编辑转变为推理引导框架，分析语义关系和空间排列。研究团队构建了包含900万样本的大规模GoT数据集，并开发了一个统一框架，整合了Qwen2.5-VL的推理能力和扩散模型的生成能力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;602&quot; src=&quot;https://devpress.csdnimg.cn/2203b5284fdc426986491ca50f6bef06.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125186&lt;/p&gt;

&lt;p&gt;简要介绍：该研究介绍了MMDiag，一个多轮多模态对话数据集，通过精心设计的规则和GPT辅助生成，特点是问题之间、问题与图像之间以及不同图像区域之间的强相关性。同时提出了DiagNote，一个具备多模态定位和推理能力的MLLM，包含两个相互作用的模块(Deliberate和Gaze)，在多轮对话中执行思维链和注释。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;552&quot; src=&quot;https://devpress.csdnimg.cn/d8d0995681724ede880d3ebc6101b43e.png&quot; width=&quot;480&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124176&lt;/p&gt;

&lt;p&gt;简要介绍：这项工作首次将可验证奖励强化学习(RLVR)应用于全模态大型语言模型用于情感识别，这是一项视觉和音频模态都起关键作用的任务。研究利用RLVR优化Omni模型，在三个关键方面显著提升其性能：推理能力、情感识别准确性和泛化能力。这种方法不仅提高了模型在分布内数据上的整体性能，还在分布外数据集评估中展示了更强的鲁棒性。&lt;/p&gt;

&lt;h2&gt;VisualPRM: An Effective Process Reward Model for Multimodal Reasoning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126372&lt;/p&gt;

&lt;p&gt;简要介绍：由清华大学、上海AI实验室等机构提出了VisualPRM，一个拥有8B参数的多模态过程奖励模型，它通过Best-of-N评估策略显著提升了现有多模态大语言模型的推理能力。该模型改进了三种类型、四种不同规模的MLLM的推理性能，即使应用于InternVL2.5-78B这样的高能力模型，也在七项多模态推理基准上获得了5.9点的提升。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;282&quot; src=&quot;https://devpress.csdnimg.cn/7f8ae00d2d4947c8b0bbdb16ea0b2779.png&quot; width=&quot;538&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;UniF^2ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125793&lt;/p&gt;

&lt;p&gt;简要介绍：研究提出了UniF^2ace，这是第一个专为精细人脸理解和生成设计的统一多模态模型(UMM)。该模型通过两种互补的扩散技术和两级专家混合架构，在特制的数据集上进行训练。团队构建了大规模人脸数据集UniF^2ace-130K，包含13万图像-文本对和100万问答对，涵盖广泛的人脸属性，并建立了离散扩散分数匹配和掩码生成模型之间的理论联系。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;324&quot; src=&quot;https://devpress.csdnimg.cn/a4ff9d0ab20b45b29b3631bd423f48f6.png&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126311&lt;/p&gt;

&lt;p&gt;简要介绍：这项工作提出了4D LangSplat，一种学习4D语言场的方法，以便在动态场景中处理时间敏感或时间无关的开放词汇查询。该方法绕过了从视觉特征学习语言场的传统途径，而是直接从多模态大语言模型(MLLM)生成的对象级视频描述文本中学习。研究提出了多模态对象级视频提示方法，生成详细、时间一致的高质量描述，并使用大语言模型将这些描述编码为高质量句子嵌入。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;454&quot; src=&quot;https://devpress.csdnimg.cn/968eaddd3fce48ee9d62e50d33e2b9ba.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124832&lt;/p&gt;

&lt;p&gt;简要介绍：受DeepSeek-R1-Zero成功通过强化学习(RL)激发LLM推理能力的启发，该研究探索了如何利用RL提升多模态大语言模型(MLLM)的推理能力。研究者提出了Vision-R1，通过利用现有MLLM和DeepSeek-R1通过模态桥接和数据过滤构建了20万多模态思维链数据集作为冷启动初始化数据，并提出了渐进思维抑制训练(PTST)策略来减轻冷启动后过度思考的优化挑战。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;336&quot; src=&quot;https://devpress.csdnimg.cn/b9851748907a484dbc2be5e9c5930c04.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126288&lt;/p&gt;

&lt;p&gt;简要介绍：研究团队提出了VisualWebInstruct，一种利用搜索引擎创建多样化高质量数据集的新方法，涵盖数学、物理、金融、化学等多个学科。从精心挑选的3万种子图像开始，通过Google图像搜索识别包含类似图像的网站，收集并处理了来自70万唯一URL源的HTML。通过内容提取、过滤和合成管道，构建了约90万问答对的数据集，其中40%为视觉QA对，其余为文本QA对。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;380&quot; src=&quot;https://devpress.csdnimg.cn/22fcb48409db45f69fca3e70ad61b658.png&quot; width=&quot;548&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125368&lt;/p&gt;

&lt;p&gt;简要介绍：由华中科技大学等机构提出了OmniMamba，这是首个基于线性架构的多模态生成模型，通过统一的下一个令牌预测范式同时生成文本和图像。该模型充分利用Mamba-2的高计算和内存效率，将其功能从文本生成扩展到多模态生成。为解决现有统一模型的数据效率问题，提出了两项关键创新：解耦词汇表来引导特定模态生成，以及用于参数高效适应的任务特定LoRA。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;468&quot; src=&quot;https://devpress.csdnimg.cn/85e288af3818480eb05106486048a80b.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d90ccab8d50678a24da4bb.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d90ccab8d50678a24da4bb.html</guid><pubDate>Tue, 18 Mar 2025 06:03:54 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>魔搭社区模型速递（3.9-3.15）</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/5b43d9ea4ba6453db42500a259774d25.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;魔搭ModelScope本期社区进展：1066个模型，153个数据集，125个创新应用，13篇内容&lt;/blockquote&gt;   &lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;608&quot; src=&quot;https://devpress.csdnimg.cn/ade0d9d9873043318c0a5cf809f1c6f3.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;🙋魔搭ModelScope本期社区进展：&lt;/p&gt;

&lt;p&gt;📟1066个模型：CSM-1b、R1-Omni-0.5B、Gemma-3系列、OLMo-2-0325-32B-Instruct等；&lt;/p&gt;

&lt;p&gt;📁153个数据集：SmallThoughts、s1-m_beta、codeforces-cots等；&lt;/p&gt;

&lt;p&gt;🎨125个创新应用：ImageCaculate、MelQCD、模型费用计算器等；&lt;/p&gt;

&lt;p&gt;📄 13篇内容：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;有效的思考：模型思考效率评测&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;10分钟评测实验，鉴别大模型是真智能or伪学霸&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;驱动“超真人”虚拟助手Maya的实时语音对话模型CSM-1b开源！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;线上共学 | Mac本地玩转大模型&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;R1-Omni开源！多模态模型+RLVR，让各模态作用清晰可见&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;QwQ-32B，支持Function Call的推理模型，深度思考Agent的时代来了！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;今日热门论文推荐：CoSTAast、Transformers without Normalization、VisualPRM&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;今日热门论文推荐：Seedream、LMM-R1、YuE、Gemini Embedding&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;今日热门论文推荐：MM-Eureka、FedRand、EasyControl、FEA-Bench、SurveyForge&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;今日热门论文推荐：EuroBERT、S2S-Arena、R1-Searcher&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;解读 | 金融长上下文基准测试FailSafeQA：解锁金融领域LLM真实的审慎性和容错性&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;“同西游，见万相” 主题LoRA风格挑战赛来袭！万元奖金池+猫超卡+限定周边来赢！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;20万悬赏AI美妆！欧莱雅美妆科技黑客松2025重磅来袭&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;01.精选模型&lt;/h2&gt;

&lt;p&gt;CSM-1b&lt;/p&gt;

&lt;p&gt;Sesame团队开源语音生成模型 CSM-1b，可根据文本和音频输入生成 RVQ 音频代码。其基于Llama架构构建，通过多模态骨干网络处理文本与音频输入，结合音频解码器生成RVQ编码的高质量语音。该模型支持端到端多模态学习，可结合对话历史生成连贯自然的语音，适用于实时交互场景如语音助手，延迟极低，当前开源版本主要针对英语场景优化。&lt;/p&gt;

&lt;p&gt;模型地址：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/sesameAILabs/csm-1b&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;step1：拉取代码&lt;/p&gt;

&lt;p&gt;本次拉取的代码为基于gradio开发的版本，模型托管在modelscope上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://oauth2:Lj_V_qf8NsjT2RoCksjr@www.modelscope.cn/studios/sesameAILabs/csm-1b.git&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;step2：安装依赖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd csm-1b
pip install -r requirements.txt&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;step3：运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 声明两个环境变量
export MS_TOKEN=&#39;xxxx&#39; # MS_TOKEN 从https://www.modelscope.cn/my/myaccesstoken 获取
export WATERMARK_KEY=&quot;123 234 111 222 11&quot;

cd csm-1b
python app.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;542&quot; src=&quot;https://devpress.csdnimg.cn/8832c6268d8a4fe0bec2cbe5e7b1e1f8.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;Gemma-3系列&lt;/p&gt;

&lt;p&gt;Google DeepMind团队升级Gemma正式发布Gemma 3系列。Gemma 3是多模态大模型，处理文本和图像输入并生成文本输出，预训练变体和指令调整变体都有开放权重。Gemma 3 具有一个 128K 的大上下文窗口，支持 140 多种语言，并且比以前的版本提供更多大小。Gemma 3 模型非常适合各种文本生成和图像理解任务，包括问答、总结和推理。它们相对较小的尺寸使其能够部署在资源有限的环境中，例如笔记本电脑、台式机或自己的云基础设施。&lt;/p&gt;

&lt;p&gt;模型地址：&lt;/p&gt;

&lt;p&gt;google/gemma-3-1b-it https://modelscope.cn/models/LLM-Research/gemma-3-1b-it google/gemma-3-1b-pt https://modelscope.cn/models/LLM-Research/gemma-3-1b-pt google/gemma-3-4b-it https://modelscope.cn/models/LLM-Research/gemma-3-4b-it google/gemma-3-4b-pt https://modelscope.cn/models/LLM-Research/gemma-3-4b-pt google/gemma-3-12b-it https://modelscope.cn/models/LLM-Research/gemma-3-12b-it google/gemma-3-12b-pt https://modelscope.cn/models/LLM-Research/gemma-3-12b-pt google/gemma-3-27b-it https://modelscope.cn/models/LLM-Research/gemma-3-27b-it google/gemma-3-27b-pthttps://modelscope.cn/models/LLM-Research/gemma-3-27b-pt&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;环境安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM
import torch
from modelscope import snapshot_download

model_id = snapshot_download(&quot;LLM-Research/gemma-3-1b-it&quot;)

#quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = Gemma3ForCausalLM.from_pretrained(
    model_id
).eval()

tokenizer = AutoTokenizer.from_pretrained(model_id)

messages = [
    [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are a helpful assistant.&quot;},]
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Write a poem on Hugging Face, the company&quot;},]
        },
    ],
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
).to(model.device)


with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=64)

outputs = tokenizer.batch_decode(outputs)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R1-Omni-0.5B&lt;/p&gt;

&lt;p&gt;R1-Omni 是由通义实验室推出的业界首次将带有可验证奖励（RLVR）的强化学习应用于全模态大型语言模型的研究工作，专注于情感识别任务，是一个视觉和音频模态都起关键作用的任务，以验证结合RLVR与全模态模型的潜力。研究揭示了几个关键见解：&lt;/p&gt;

&lt;p&gt;1、增强的推理能力：R1-Omni展示了卓越的推理能力，使人们能够更清晰地理解视觉和音频信息如何贡献于情感识别。 2、提升的理解能力：与SFT相比，RLVR在情感识别任务上的表现显著提高。 3、更强的泛化能力：RLVR模型显示出明显更好的泛化能力，特别是在分布外场景中表现出色。&lt;/p&gt;

&lt;p&gt;模型合集链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/models/iic/R1-Omni-0.5B&lt;/p&gt;

&lt;h2&gt;02.数据集推荐&lt;/h2&gt;

&lt;p&gt;SmallThoughts&lt;/p&gt;

&lt;p&gt;开放综合推理数据集，涵盖数学、科学、代码和谜题。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/datasets/AI-ModelScope/SmallThoughts&lt;/p&gt;

&lt;p&gt;s1-m_beta&lt;/p&gt;

&lt;p&gt;S1-M 数据集 （Beta） 是一个开源的 TI2T 推理数据集，用于训练 S1-M 模型 （Beta），使其具有 “先思考，后响应” 的范式。S1-M 数据集（Beta）中的提示和图片来自两个开源数据集：align-anything 和 multimodal-open-r1-8k-verified，分别占 49.62% 和 50.38%，旨在平衡模型的通用能力和数学能力。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/datasets/PKU-Alignment/s1-m_beta&lt;/p&gt;

&lt;p&gt;codeforces-cots&lt;/p&gt;

&lt;p&gt;CodeForces-CoTs 是一个大规模数据集，用于在竞争性编程任务中训练推理模型。它由 10k 个 CodeForces 问题组成，最多 4 个 DeepSeek R1 生成的推理跟踪。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/datasets/open-r1/codeforces-cots&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;03.精选应用&lt;/h2&gt;

&lt;p&gt;CSM-1b&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/sesameAILabs/csm-1b&lt;/p&gt;

&lt;p&gt;ImageCaculate&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/crwsr124/ImageCaculate&lt;/p&gt;

&lt;p&gt;MelQCD&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/iic/MelQCD&lt;/p&gt;

&lt;p&gt;模型费用计算器&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/h2000c820y/Model_Cost_Calculator&lt;/p&gt;

&lt;h2&gt;04.社区精选文章&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501627&amp;amp;idx=1&amp;amp;sn=a0d8961bcbdceac0c01191f45a6bf175&amp;amp;scene=21#wechat_redirect&quot;&gt;有效的思考：模型思考效率评测&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501650&amp;amp;idx=1&amp;amp;sn=c05c479bcbe74045736e9b2b231a03e9&amp;amp;scene=21#wechat_redirect&quot;&gt;驱动“超真人”虚拟助手Maya的实时语音对话模型CSM-1b开源！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501650&amp;amp;idx=2&amp;amp;sn=06fb29a7c0236b6d6aab9829c90e5ea7&amp;amp;scene=21#wechat_redirect&quot;&gt;10分钟评测实验，鉴别大模型是真智能or伪学霸&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501593&amp;amp;idx=1&amp;amp;sn=8bf3ca7906af3080b0e8229c04a9dc48&amp;amp;scene=21#wechat_redirect&quot;&gt;线上共学 | Mac本地玩转大模型&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501454&amp;amp;idx=1&amp;amp;sn=2b30bf32cfe9ab3dd25512d7d5438d0d&amp;amp;scene=21#wechat_redirect&quot;&gt;QwQ-32B，支持Function Call的推理模型，深度思考Agent的时代来了！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501480&amp;amp;idx=1&amp;amp;sn=912b988f959b2bc710b66f30a17abb5f&amp;amp;scene=21#wechat_redirect&quot;&gt;R1-Omni开源！多模态模型+RLVR，让各模态作用清晰可见&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501543&amp;amp;idx=2&amp;amp;sn=984f4c47e254d3074ed24ab86dd7eaa5&amp;amp;scene=21#wechat_redirect&quot;&gt;今日热门论文推荐：Seedream、LMM-R1、YuE、Gemini Embedding&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501652&amp;amp;idx=1&amp;amp;sn=2ceebce989b94579797cee7d2718576b&amp;amp;scene=21#wechat_redirect&quot;&gt;今日热门论文推荐：CoSTAast、Transformers without Normalization、VisualPRM&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501521&amp;amp;idx=2&amp;amp;sn=3e3cbca91b56b535c30e4d5a7da0b9d0&amp;amp;scene=21#wechat_redirect&quot;&gt;今日热门论文推荐：MM-Eureka、FedRand、EasyControl、FEA-Bench、SurveyForge&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501480&amp;amp;idx=2&amp;amp;sn=c256100b105d35dc4db9b628576532d1&amp;amp;scene=21#wechat_redirect&quot;&gt;今日热门论文推荐：EuroBERT、S2S-Arena、R1-Searcher&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501454&amp;amp;idx=2&amp;amp;sn=2fb39f816b82eb4799672d3e7dc5962e&amp;amp;scene=21#wechat_redirect&quot;&gt;解读 | 金融长上下文基准测试FailSafeQA：解锁金融领域LLM真实的审慎性和容错性&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501543&amp;amp;idx=1&amp;amp;sn=bd76d088455bce6824585f158bf6d4be&amp;amp;scene=21#wechat_redirect&quot;&gt;20万悬赏AI美妆！欧莱雅美妆科技黑客松2025重磅来袭&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501521&amp;amp;idx=1&amp;amp;sn=1e07ba89634a99c15b3bf359cf6afd1e&amp;amp;scene=21#wechat_redirect&quot;&gt;“同西游，见万相” 主题LoRA风格挑战赛来袭！万元奖金池+猫超卡+限定周边来赢！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;hr&gt; </description><link>https://community.modelscope.cn/67d90bfa6670175f993869b2.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d90bfa6670175f993869b2.html</guid><pubDate>Tue, 18 Mar 2025 06:00:26 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>本周 AI Benchmark 方向论文推荐</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/185703ad408e4d4681c1fad3cc1e48ab.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;由北京大学和微软亚洲研究院的魏李等人提出的 FEA-Bench，是一个专为评估大型语言模型（LLMs）在代码库级别进行增量开发能力的基准测试。它从 83 个 GitHub 仓库中收集了 1,401 个&lt;/blockquote&gt;   &lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;180&quot; src=&quot;https://devpress.csdnimg.cn/a7a4aa7e02f644958023eb5b5aa7cd3b.gif&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;作者：InternLM、Qwen 等 LLM每周一览 AI Benchmark，AI 发展方向不迷惘。快来看看「机智流」、「ModelScope」和「司南评测集社区」推荐的 AI BenchMark 相关论文吧！！！&lt;/p&gt;

&lt;h2&gt;FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125249&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由北京大学和微软亚洲研究院的魏李等人提出的 FEA-Bench，是一个专为评估大型语言模型（LLMs）在代码库级别进行增量开发能力的基准测试。它从 83 个 GitHub 仓库中收集了 1,401 个任务实例，专注于新功能的实现。研究表明，即使是先进的 LLMs 在此任务中的表现仍远低于预期，揭示了仓库级代码开发的重大挑战。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;356&quot; src=&quot;https://devpress.csdnimg.cn/b3617bfba842429290fb03ff8eaa5d3b.png&quot; width=&quot;964&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;WritingBench: A Comprehensive Benchmark for Generative Writing&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124147&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由阿里巴巴集团和中国人民大学的宁吴等人提出的 WritingBench，是一个全面评估 LLMs 生成写作能力的基准测试，覆盖 6 个核心写作领域和 100 个子领域。它引入了查询相关的动态评估框架，使模型能生成特定实例的评估标准。实验显示，7B 参数模型在数据策划能力上接近 SOTA 水平，验证了框架的高效性。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;816&quot; src=&quot;https://devpress.csdnimg.cn/864139ffd80244ffbf2027c89c64537f.png&quot; width=&quot;906&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124656&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由耶鲁大学的向如唐等人提出的 MedAgentsBench，是一个针对复杂医疗推理的基准测试，评估 LLMs 在多步骤临床推理、诊断和治疗规划中的表现。从七个医疗数据集构建了 862 个任务，结果显示 DeepSeek R1 和 OpenAI o3 等模型在复杂任务中表现突出，为医疗 AI 研究提供了新方向。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;870&quot; src=&quot;https://devpress.csdnimg.cn/04d1ce583a834fc49c2c728a1663ec63.png&quot; width=&quot;964&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Do I look like a &lt;code&gt;cat.n.01&lt;/code&gt;&amp;nbsp;to you? A Taxonomy Image Generation Benchmark&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126535&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由 Skoltech 和汉堡大学的维克多·莫斯科夫列茨基等人提出，该基准测试探索文本到图像模型生成分类学概念图像的可行性。包含 WordNet 概念的全面评估显示，Playground-v2 和 FLUX 在不同指标中表现优异，揭示了自动化结构化数据生成的新潜力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;924&quot; src=&quot;https://devpress.csdnimg.cn/c981baf9d4b1401f918839094bcbbdd8.png&quot; width=&quot;928&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large Vision-Language Models in Fact-Seeking Question Answering&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125193&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由中国人民大学的 Yanling Wang 等人提出的 VisualSimpleQA，是一个多模态事实寻求 QA 基准测试，支持对大型视觉语言模型（LVLMs）的视觉与语言能力进行解耦评估。它引入难度标准并提取了 VisualSimpleQA-hard 子集，实验显示即使 GPT-4o 在复杂任务中正确率仅 30%+，凸显改进空间。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;630&quot; src=&quot;https://devpress.csdnimg.cn/2ede925315464f2a9e46f64743b72909.png&quot; width=&quot;960&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://huggingface.co/papers/2503.05860&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由 Roham Koohestani 等人提出的工作通过审查 173 项研究，识别了 204 个 AI4SE 基准测试，并分析其局限性。他们开发了 BenchScout 搜索工具和 BenchFrame 增强方法，以 HumanEval 为例推出 HumanEvalNext，显著提升了评估难度和质量。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;322&quot; src=&quot;https://devpress.csdnimg.cn/c0bea241a5c5453ebcb9f705630b845b.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/123554&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由北京大学的 Jia Li 等人提出的 LONGCODEU，是一个长代码理解基准测试，从四个方面（8 个任务）评估 LCLMs 的能力。实验表明，当代码长度超 32K 时，模型性能急剧下降，远低于其宣称的 128K-1M 上下文窗口，为软件工程优化提供了洞见。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;504&quot; src=&quot;https://devpress.csdnimg.cn/4c3052090138465483d3386dad64f7b9.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124404&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由澳大利亚国立大学的 Yan Yang 等人提出的 ProBench，是一个开放式多领域专家任务基准测试，包含 4,000 个高质量样本，覆盖 10 个领域和 56 个子领域。实验显示，即使最佳开源模型与专有模型接近，视觉感知和高级推理仍具挑战性。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;560&quot; src=&quot;https://devpress.csdnimg.cn/bb78f4ecb9044fdda91d6e63b7b5ce9f.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;MinorBench: A hand-built benchmark for content-based risks for children&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126387&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由新加坡政府科技局的 Shaun Khoo 等人提出的 MinorBench，是一个手工构建的基准测试，评估 LLMs 在拒绝儿童不安全查询方面的能力。通过中学案例研究，结果显示主流 LLMs 在儿童安全合规性上差异显著，强调了定制化 AI 的重要性。&lt;/p&gt;

&lt;hr&gt;
&lt;p&gt;本周的 AI Bench 研究成果涵盖了代码生成、医疗推理、图像生成和儿童安全等多个领域。这些基准测试不仅揭示了当前模型的局限，也为未来研究指明了方向。让我们共同期待 AI 技术在这些前沿领域的突破吧！🌟&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d90a2a6670175f99386978.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d90a2a6670175f99386978.html</guid><pubDate>Tue, 18 Mar 2025 05:52:42 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>论文推荐：CoSTAast、Transformers without Normalization</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/ac7b7f11c1aa445d891c0c299152779c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;由马里兰大学团队提出的CoSTA*，针对多轮图像编辑任务设计了一种成本敏感的工具路径代理。该工作结合大语言模型（LLM）的子任务规划与A搜索算法，构建了一个高效的工具选择路径，不仅降低了计算成本，还提&lt;/blockquote&gt;   &lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;180&quot; src=&quot;https://devpress.csdnimg.cn/aa42a57f7bd54f69b30b1189de8eec31.gif&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;作者：InternLM、Qwen 等 LLM每日一览热门论文版，顶会论文投稿选题不迷惘。快来看看「机智流」和「ModelScope」社区推荐的热门论文吧！&lt;/p&gt;

&lt;p&gt;作者：InternLM、Qwen 等 LLM每日一览热门论文版，顶会论文投稿选题不迷惘。快来看看「机智流」和「ModelScope」社区推荐的热门论文吧！&lt;/p&gt;

&lt;h2&gt;CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://huggingface.co/papers/2503.10613&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由马里兰大学团队提出的CoSTA*，针对多轮图像编辑任务设计了一种成本敏感的工具路径代理。该工作结合大语言模型（LLM）的子任务规划与A搜索算法，构建了一个高效的工具选择路径，不仅降低了计算成本，还提升了图像编辑质量。通过视觉语言模型评估子任务输出，CoSTA能在失败时快速调整路径，并在全新多轮图像编辑基准测试中超越现有最佳模型。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;368&quot; src=&quot;https://devpress.csdnimg.cn/6a94bd1bf20b4893ba8fe5c119fce814.png&quot; width=&quot;510&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Transformers without Normalization&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126286&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由Meta FAIR、NYU等团队联手推出的这项研究，挑战了规范化层在Transformer中的必要性。他们提出了Dynamic Tanh（DyT），一种简单操作替代规范化层，不仅保持甚至提升了模型性能，还无需过多超参数调整。实验验证了DyT在视觉与语言任务中的广泛适用性，为神经网络设计提供了新思路。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;270&quot; src=&quot;https://devpress.csdnimg.cn/5ab92094dc51475eb09f213bd4b48a09.png&quot; width=&quot;664&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Charting and Navigating Hugging Face&#39;s Model Atlas&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://huggingface.co/papers/2503.10633&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
耶路撒冷希伯来大学团队打造了Hugging Face模型图谱，试图为数百万公开神经网络绘制导航蓝图。该工作通过可视化模型演化与趋势，预测模型属性，并在未文档区域利用结构先验填补空白，为大规模模型库研究提供了新工具与数据集。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;384&quot; src=&quot;https://devpress.csdnimg.cn/45266bf375654c9984c4f89be8df1cec.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126368&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
复旦大学等机构提出的D²PO，通过双重偏好优化联合训练状态预测与动作选择，提升了大型视觉语言模型在实体任务规划中的能力。结合树搜索机制自动收集数据，该方法在VoTa-Bench上显著超越GPT-4o，展现了更高的成功率与执行效率。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;498&quot; src=&quot;https://devpress.csdnimg.cn/f324dc3da7dd45e1b834c4754ea3a9f3.png&quot; width=&quot;384&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126588&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
KAIST团队揭示了文本到图像扩散模型的新漏洞，提出了“无声品牌攻击”。通过在训练数据中隐秘注入品牌标志，该方法让模型在无触发词的情况下生成含标志的图像，成功率高且不影响图像质量，引发了对数据安全的深思。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;222&quot; src=&quot;https://devpress.csdnimg.cn/e1b4aa34b10c4a2680620fa649ce58c3.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;CoRe^2: Collect, Reflect and Refine to Generate Better and Faster&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126707&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
香港科技大学（广州）团队推出的CoRe²，通过“收集-反思-精炼”三阶段范式，提升了文本到图像生成的速度与质量。该方法适用于多种扩散模型与自回归模型，在多项基准测试中表现优异，且与Z-Sampling集成时进一步提升性能。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;172&quot; src=&quot;https://devpress.csdnimg.cn/205b0b6a06234ee29b802e6dfce0dc42.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126628&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
香港中文大学等团队提出了Generation Chain-of-Thought（GoT），通过语言推理链指导图像生成与编辑。结合Qwen2.5-VL与新型语义-空间引导模块，该框架在生成质量与编辑精度上均有突破，并支持交互式调整，开启了推理驱动视觉合成的新方向。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;602&quot; src=&quot;https://devpress.csdnimg.cn/f4c4fd83e8934ae49cb0ed731a728293.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;VisualPRM: An Effective Process Reward Model for Multimodal Reasoning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/126372&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
上海AI实验室等机构开发的VisualPRM，一个8B参数的多模态过程奖励模型，通过Best-of-N策略提升了MLLM的推理能力。搭配VisualPRM400K数据集与VisualProcessBench基准，该模型在七个推理任务中表现出色，甚至对InternVL2.5-78B也有显著提升。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;282&quot; src=&quot;https://devpress.csdnimg.cn/b88123af41e947fab062202bd470ae8e.png&quot; width=&quot;538&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125491&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
罗切斯特大学与Adobe Research合作推出的OmniPaint，将对象移除与插入视为相互依存的过程。通过CycleFlow训练与全新CFD指标，该框架实现了高保真编辑，保留场景几何与物理效应，成为对象导向编辑的新标杆。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;334&quot; src=&quot;https://devpress.csdnimg.cn/346585688d6640b78bf05d0c8d39872a.png&quot; width=&quot;810&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Shifting Long-Context LLMs Research from Input to Output&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/123598&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由清华大学等团队倡议的研究转向，强调长输出生成在LLM中的重要性。该文指出当前研究过于聚焦输入处理，忽略了长篇写作、规划等任务的需求，呼吁开发专为高质量长输出设计的模型，潜力巨大。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;434&quot; src=&quot;https://devpress.csdnimg.cn/b9f6edb540d74e90b98b576f41d616f8.png&quot; width=&quot;480&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d906226670175f99386922.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d906226670175f99386922.html</guid><pubDate>Tue, 18 Mar 2025 05:35:30 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>驱动“超真人”虚拟助手Maya的实时语音对话模型CSM-1b开源！</title><description>&lt;blockquote&gt;3月14日，创造出病毒级虚拟助手 Maya 的Sesame团队开源了他们的语音生成模型 CSM-1b，可根据文本和音频输入生成 RVQ 音频代码。这意味着，我们每个人都可以0成本拥有一个真正的AI伴侣&lt;/blockquote&gt;   &lt;p&gt;3月14日，创造出病毒级虚拟助手 Maya 的Sesame团队开源了他们的语音生成模型 CSM-1b，可根据文本和音频输入生成 RVQ 音频代码。这意味着，我们每个人都可以0成本拥有一个真正的AI伴侣了，甚至可以自己动手搭建、测试和改进模型。&lt;/p&gt;

&lt;p&gt;模型尺寸：Sesame训练了三种模型大小，由backbone和解码器大小划分：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;Tiny：1B backbone，100M 解码器&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;Small：3B backbone，250M 解码器&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;Medium：8B backbone，300M 解码器&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;本次Sesame开源的模型是tiny版本 CSM-1b，开源模型在魔搭社区和HuggingFace都可以下载到。一个微调版的CSM可以在Sesame官网博客文章中展示的交互式语音演示中体验。&lt;/p&gt;

&lt;p&gt;模型：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/sesameAILabs/csm-1b&lt;/p&gt;

&lt;p&gt;CSM-1B体验：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/sesameAILabs/csm-1b/summary&lt;/p&gt;

&lt;p&gt;微调版CSM体验（Maya效果）：&lt;/p&gt;

&lt;p&gt;Sesame官网：https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice&lt;/p&gt;

&lt;h3&gt;01.会话语音模型&lt;/h3&gt;

&lt;p&gt;模型结构&lt;/p&gt;

&lt;p&gt;CSM 是一种直接对 RVQ 标记进行操作的多模态文本和语音模型，使用了两个自回归变换器，在第零个码本处拆分变换器。第一个多模态主干网络处理交错的文本和音频以对第零个码本进行建模。第二个音频解码器对每个码本使用不同的线性头，并对剩余的 N – 1 个码本进行建模，以根据主干网络的表示重建语音。解码器比主干网络小得多，从而能够实现低延迟生成，同时保持模型的端到端。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;843&quot; src=&quot;https://devpress.csdnimg.cn/88b8a3ca0f824ebebb23189992539253.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;CSM 模型推理过程&lt;/p&gt;

&lt;p&gt;这两个转换器都是 Llama 架构的变体。文本标记通过 Llama 标记器生成，而音频则使用 Mimi（一种 split-RVQ 标记器）进行处理，以 12.5 Hz 的频率每帧生成一个语义码本和 N – 1 个声学码本。训练样本的结构为文本和音频的交替交错模式，说话者身份直接编码在文本表示中。&lt;/p&gt;

&lt;p&gt;计算摊销&lt;/p&gt;

&lt;p&gt;这种设计在训练过程中带来了巨大的基础设施挑战。音频解码器自回归处理有效批量大小为 B × S 和 N 个码本，即使模型很小，这种高内存负担也会减慢训练速度、限制模型扩展并阻碍快速实验。&lt;/p&gt;

&lt;p&gt;为了应对这些挑战，该团队提出一种计算摊销方案，该方案可以缓解内存瓶颈，同时保持完整 RVQ 码本的保真度。音频解码器仅在音频帧的随机 1/16 子集上进行训练，而第零码本则在每个帧上进行训练。使用这种方法时，在训练过程中没有观察到音频解码器损失的明显差异。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;665&quot; src=&quot;https://devpress.csdnimg.cn/83e9c701ed664df59c3a073f029aeea8.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;摊销训练过程&lt;/p&gt;

&lt;p&gt;效果评估&lt;/p&gt;

&lt;p&gt;评估套件从四个关键方面衡量模型性能：文本忠实度、上下文利用率、韵律和延迟，从客观和主观指标两个方面进行全面的评估。&lt;/p&gt;

&lt;p&gt;客观指标&lt;/p&gt;

&lt;p&gt;传统基准，例如词错误率（WER）和说话人相似度（SIM），已经饱和，而包括 CSM 在内的现代模型现在在这些指标上实现了接近人类的表现。&lt;/p&gt;

&lt;p&gt;词语错误率（Word Error Rate）和说话人相似度（Speaker Similarity）测试的客观指标结果显示，指标已饱和（与人类表现相匹配）。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;670&quot; src=&quot;https://devpress.csdnimg.cn/0b03a20319bf4a6593b632553fe0c7c0.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;为了更好地评估发音和上下文理解，Sesame引入了一套新的基于语音转录的基准。&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;通过同形异义词消歧理解文本： 评估模型是否正确发音具有相同拼写的不同单词（例如，“lead” /lɛd/ 与“lead” /liːd/）。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;通过发音连续一致性理解音频： 评估模型是否能保持多轮语音中具有多种发音变体的特定单词的发音一致性。一个例子是“route”（/raʊt/ 或 /ruːt/），它会根据说话者所在地区和上下文而变化。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;同形异义词消歧（Homograph Disambiguation）和发音一致性（Pronunciation Consistency）测试的客观指标结果，衡量每个模型正确发音的准确率百分比。下图比较了三种模型大小的客观指标结果。从同形异义词准确率和发音一致性的评估结果观察到，模型越大，性能越佳。这个结果支持了我们的假设，即缩放可以增强更真实语音的合成。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;524&quot; src=&quot;https://devpress.csdnimg.cn/88a9343f443a4b24b3fe95cd3538f0d3.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;主观指标&lt;/p&gt;

&lt;p&gt;Sesame团队使用Expresso数据集评估 CSM-Medium 生成的语音的自然度和韵律适宜性。向人类评估者展示成对的音频样本 - 一个由模型生成，另一个是真实的人类录音。听众根据 7 分偏好量表对生成的样本进行评分。&lt;/p&gt;

&lt;p&gt;具体来说，在第一个 CMOS 研究中，提供了没有语境的生成音频和人类音频样本，并要求听众“选择哪种演绎更像人类语音”。在第二个 CMOS 研究中，提供了前 90 秒的音频和文本语境，并要求听众“选择哪种演绎更像对话的延续”。八十人受雇参与评估，平均每人对 15 个示例进行评分。&lt;/p&gt;

&lt;p&gt;下图为Expresso 数据集上的主观评价结果。&lt;/p&gt;

&lt;p&gt;无上下文：听众在不了解上下文的情况下选择“哪种演绎感觉更像人类语音” 。&lt;/p&gt;

&lt;p&gt;上下文：听众在了解音频和文本上下文的情况下选择“哪种演绎感觉更适合继续对话”。&lt;/p&gt;

&lt;p&gt;50:50 的胜负比表明听众没有明确的偏好。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;594&quot; src=&quot;https://devpress.csdnimg.cn/6b61818903094d0f83df354c5684078f.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;上图显示了两项研究中真实人类录音与 CSM 生成的语音样本的胜率。在没有对话语境的情况下（顶部），人类评估者对生成的语音和真实语音没有明显的偏好，这表明自然度已经饱和。然而，当包含语境时（底部），评估者始终青睐原始录音。这些发现表明，在对话语音生成中，生成的语音和人类的韵律之间仍然存在明显的差距。&lt;/p&gt;

&lt;h3&gt;02.最佳实践&lt;/h3&gt;

&lt;p&gt;魔搭社区开发者第一时间部署体验了这个模型，本部分教你如何一步步在魔搭平台的notebook上运行CSM-1b模型。&lt;/p&gt;

&lt;p&gt;step1: 申请魔搭免费实例&lt;/p&gt;

&lt;p&gt;notebook地址：https://www.modelscope.cn/my/mynotebook&lt;/p&gt;

&lt;p&gt;点击terminal，打开终端&lt;/p&gt;

&lt;p&gt;step2：克隆代码&lt;/p&gt;

&lt;p&gt;本次拉取的代码为基于gradio开发的版本，模型托管在modelscope上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://oauth2:Lj_V_qf8NsjT2RoCksjr@www.modelscope.cn/studios/sesameAILabs/csm-1b.git&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;step3：安装依赖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd csm-1b
pip install -r requirements.txt&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;step4：运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 声明两个环境变量
export MS_TOKEN=&#39;xxxx&#39; # MS_TOKEN 从https://www.modelscope.cn/my/myaccesstoken 获取
export WATERMARK_KEY=&quot;123 234 111 222 11&quot;

cd csm-1b
python app.py&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;542&quot; src=&quot;https://devpress.csdnimg.cn/d8ae1cc80d5a4a579b17713ebefb155a.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;Github：&lt;/p&gt;

&lt;p&gt;https://github.com/SesameAILabs/csm?tab=readme-ov-file&lt;/p&gt;

&lt;p&gt;点击链接阅读原文，直达体验&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/sesameAILabs/csm-1b&quot;&gt;csm-1b&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d904d43b685529b70d6e4e.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d904d43b685529b70d6e4e.html</guid><pubDate>Tue, 18 Mar 2025 05:29:56 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>有效的思考：模型思考效率评测</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/f5bba92f25b540fda2f9654e564eed5f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;随着大语言模型的迅速发展，模型的推理能力得到了显著提升。特别是长推理模型（Long Reasoning Models），如OpenAI的o1、DeepSeek-R1、QwQ-32B和Kimi K1.5&lt;/blockquote&gt;   &lt;h2&gt;00.前言&lt;/h2&gt;

&lt;p&gt;随着大语言模型的迅速发展，模型的推理能力得到了显著提升。特别是长推理模型（Long Reasoning Models），如OpenAI的o1、DeepSeek-R1、QwQ-32B和Kimi K1.5等，因其展现出类似人类的深度思考能力而备受关注。这些模型通过长时间推理（Inference-Time Scaling），能够在解码阶段不断思考并尝试新的思路来得到正确的答案。&lt;/p&gt;

&lt;p&gt;然而，随着研究的深入，科研人员发现这些模型在推理过程中存在两个极端问题：Underthinking（思考不足） 和Overthinking（过度思考） ：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;Underthinking（思考不足）[^1] 现象指的是模型在推理过程中频繁地进行思路跳转，反复使用“alternatively”、“but wait”、“let me reconsider”等词，如下图所示，模型无法将注意力集中在一个正确的思路上并深入思考，从而得到错误答案。这种现象类似于人类的&quot;注意力缺陷多动障碍&quot;，影响了模型的推理质量。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;592&quot; src=&quot;https://devpress.csdnimg.cn/3f3c03f2863142878fdc15829846d472.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;Overthinking（过度思考）[^2] 现象则表现为模型在不必要的情况下生成过长的思维链，浪费了大量的计算资源。例如下图所示，对于简单的&quot;2+3=？&quot;这样的问题，某些长推理模型可能会消耗超过900个token来探索多种解题策略。尽管这种思维链策略对于复杂问题的解答非常有帮助，但在应对简单问题时，反复验证已有的答案和进行过于宽泛的探索显然是一种计算资源的浪费。[^3]&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1006&quot; src=&quot;https://devpress.csdnimg.cn/53c5c49a952e4cf5aacb5160f48abca5.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;这两种现象都凸显了一个关键问题：如何在保证答案质量的同时，提高模型的思考效率？换句话说，我们希望模型能够在尽可能短的输出中获取正确的答案。基于这样的背景，我们在EvalScope项目引进了EvalThink组件，希望提供一个便捷的工具，来评估不同模型的思考效率。本文中，我们使用MATH-500数据集为例，衡量包括DeepSeek-R1-Distill-Qwen-7B在内的推理模型的思考效率，从模型推理token数、首次正确token数、剩余反思token数、token效率、子思维链数量和准确率六个维度来评估模型的表现，让我们一起开始吧。&lt;/p&gt;

&lt;h3&gt;01.安装依赖&lt;/h3&gt;

&lt;p&gt;首先，安装EvalScope (https://github.com/modelscope/evalscope) 模型评估框架：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install &#39;evalscope&#39; -U&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;02.评测模型&lt;/h2&gt;

&lt;p&gt;下面我们将开始正式评测，整个过程分为两个主要步骤：&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;
	&lt;p&gt;模型推理评估：使用EvalScope框架让模型在MATH-500数据集上进行推理。该数据集包含500个数学问题，每个问题由一个数学表达式和对应答案组成，难度从1级(简单)到5级(复杂)不等。这一步将获得模型对每个问题的推理结果，以及整体答题正确率。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;模型思考效率评测：使用EvalScope框架中的EvalThink组件对模型的输出进行深入分析，从模型思考长度、子思维链数量、token效率等维度进一步评测模型的思考效率。&lt;/p&gt;
	&lt;/li&gt;&lt;/ol&gt;

&lt;h3&gt;模型推理&lt;/h3&gt;

&lt;p&gt;准备评测模型&lt;/p&gt;

&lt;p&gt;首先，我们需要通过OpenAI API兼容的推理服务接入模型能力，以进行评测。值得注意的是，EvalScope也支持使用transformers进行模型推理评测，详细信息可参考EvalScope文档 https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html。&lt;/p&gt;

&lt;p&gt;除了将模型部署到云端，再通过OpenAI-API兼容的接口接入外，也可以选择在本地使用vLLM、ollama等框架直接启动模型。这些推理框架能够很好地支持并发多个请求，从而加速评测过程。推理类的模型输出通常包含较长的思维链，输出token数量往往超过1万。使用高效的推理框架部署模型可以显著提高推理速度。&lt;/p&gt;

&lt;p&gt;下面以DeepSeek-R1-Distill-Qwen-7B为例，介绍如何使用vLLM部署模型：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VLLM_USE_MODELSCOPE=True CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B  --served-model-name DeepSeek-R1-Distill-Qwen-7B --trust_remote_code --port 8801&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用EvalScope评测模型&lt;/p&gt;

&lt;p&gt;运行以下命令，即可让模型在MATH-500数据集上进行推理，并获得模型在每个问题上的输出结果，以及整体答题正确率：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from evalscope import TaskConfig, run_task

task_config = TaskConfig(
    api_url=&#39;http://0.0.0.0:8801/v1/chat/completions&#39;,  # 推理服务地址
    model=&#39;DeepSeek-R1-Distill-Qwen-7B&#39;,  # 模型名称 (需要与部署时的模型名称一致)
    eval_type=&#39;service&#39;,  # 评测类型，SERVICE表示评测推理服务
    datasets=[&#39;math_500&#39;],  # 数据集名称
    dataset_args={&#39;math_500&#39;: {&#39;few_shot_num&#39;: 0, &#39;subset_list&#39;: [&#39;Level 1&#39;, &#39;Level 2&#39;, &#39;Level 3&#39;, &#39;Level 4&#39;, &#39;Level 5&#39;]}},  # 数据集参数
    eval_batch_size=32,  # 发送请求的并发数
    generation_config={
        &#39;max_tokens&#39;: 20000,  # 最大生成token数，建议设置为较大值避免输出截断
        &#39;temperature&#39;: 0.6,  # 采样温度 (deepseek 报告推荐值)
        &#39;top_p&#39;: 0.95,  # top-p采样 (deepseek 报告推荐值)
        &#39;n&#39;: 1,  # 每个请求产生的回复数量
    },
)
run_task(task_config)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果如下，可以看到模型在每个等级问题上的模型回答正确率：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+-----------------------------+-----------+---------------+----------+-------+---------+---------+
| Model                       | Dataset   | Metric        | Subset   |   Num |   Score | Cat.0   |
+=============================+===========+===============+==========+=======+=========+=========+
| DeepSeek-R1-Distill-Qwen-7B | math_500  | AveragePass@1 | Level 1  |    43 |  0.9535 | default |
+-----------------------------+-----------+---------------+----------+-------+---------+---------+
| DeepSeek-R1-Distill-Qwen-7B | math_500  | AveragePass@1 | Level 2  |    90 |  0.9667 | default |
+-----------------------------+-----------+---------------+----------+-------+---------+---------+
| DeepSeek-R1-Distill-Qwen-7B | math_500  | AveragePass@1 | Level 3  |   105 |  0.9587 | default |
+-----------------------------+-----------+---------------+----------+-------+---------+---------+
| DeepSeek-R1-Distill-Qwen-7B | math_500  | AveragePass@1 | Level 4  |   128 |  0.9115 | default |
+-----------------------------+-----------+---------------+----------+-------+---------+---------+
| DeepSeek-R1-Distill-Qwen-7B | math_500  | AveragePass@1 | Level 5  |   134 |  0.8557 | default |
+-----------------------------+-----------+---------------+----------+-------+---------+---------+&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;模型思考效率评估&lt;/h3&gt;

&lt;p&gt;在获取模型的推理结果后，我们便可以着手评估其思考效率。在开始之前，我们需要介绍评估过程中涉及的几个关键指标：模型推理token数、子思维链数量以及token效率。&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;模型推理token数（Reasoning Tokens）：这是指模型在推理过程中生成的长思维链的token数量。对于O1/R1类型的推理模型，该指标表示&amp;lt;/think&amp;gt;标志前的token数量。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;首次正确token数（First Correct Tokens）：模型推理过程中，从起始位置到第一个可以识别为正确答案位置的token数。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;剩余反思token数（Reflection Tokens）：从第一个正确答案位置到推理结束的token数。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;子思维链数量（Num Thought）：这一指标表示模型在推理过程中生成的不同思维路径的数量。具体来说，是通过统计模型生成的标志词（如alternatively、but wait、let me reconsider等）的出现次数来计算的。这反映了模型在推理中切换思路的频率。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;token效率（Token Efficiency）：指的是首次正确token数与模型推理token总数的比值，其计算公式如下：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;103&quot; src=&quot;https://devpress.csdnimg.cn/0dae35497c2a48f1bde3876ce313bf23.png&quot; width=&quot;326&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;其中，为问题数量，是从模型回复开始到第一个可识别为正确答案部分的token数量，是模型的思考长度。如果模型答案错误，则为0。该指标值越高，表示模型有效思考的比例越高。&lt;/p&gt;

&lt;p&gt;在本评估框架中，我们参考ProcessBench的构建方式，使用额外的模型Qwen2.5-72B-Instruct来检测推理过程中最早出现的正确答案位置。为了实现这一点，我们首先将模型输出分解为多个步骤，对每个步骤进行编号，然后使用Qwen2.5-72B-Instruct模型对这些步骤进行检验，找出第一个正确答案的token位置。我们实现了三种分解策略：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;separator：使用\n\n标志进行分解。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;keywords：使用标志词（如alternatively、but wait、let me reconsider）进行分解。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;llm：去除回复中的\n标志，使用LLM重写回复并插入\n\n标志进行分解。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;下面以Qwen2.5-72B-Instruct作为检测模型进行评估，只需运行以下命令，即可启动评估并获取结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from evalscope.third_party.thinkbench import run_task

judge_config = dict(  # 配置Judge模型服务
    api_key=&#39;EMPTY&#39;,
    base_url=&#39;http://0.0.0.0:8801/v1&#39;,
    model_name=&#39;Qwen2.5-72B-Instruct&#39;,
)

model_config = dict(
    report_path = &#39;./outputs/2025xxxx&#39;,  # 上一步模型推理结果路径
    model_name = &#39;DeepSeek-R1-Distill-Qwen-7B&#39;,  # 模型名称
    tokenizer_path = &#39;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&#39;,  # 模型tokenizer路径，用于计算token数量
    dataset_name = &#39;math_500&#39;,  # 上一步数据集名称
    subsets = [&#39;Level 1&#39;, &#39;Level 2&#39;, &#39;Level 3&#39;, &#39;Level 4&#39;, &#39;Level 5&#39;],  # 上一步数据集子集
    split_strategies=&#39;separator&#39;,  # 推理步骤分割策略，可选值为separator、keywords、llm
    judge_config=judge_config
)

max_tokens = 20000  # 筛选token数量小于max_tokens的输出，用于提升评测效率
count = 200  # 每个子集筛选count条输出，用于提升评测效率

# 评测模型思考效率
run_task(model_config, output_dir=&#39;outputs&#39;, max_tokens=max_tokens, count=count)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果如下，可以看到模型在每个等级问题上的模型推理token数、首次正确token数、剩余反思token数、token效率、子思维链数量和准确率：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;720&quot; src=&quot;https://devpress.csdnimg.cn/5f04839dd482496c97060c55e356b9e8.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图1：DeepSeek-R1-Distill-Qwen-7B思考效率&lt;/p&gt;

&lt;p&gt;使用相同的方法，我们还对另外4个款推理模型QwQ-32B、QwQ-32B-Preview、DeepSeek-R1、DeepSeek-R1-Distill-Qwen-32B，以及一个非推理模型Qwen2.5-Math-7B-Instruct（将模型输出的所有token视为思考过程），以便观察不同类型模型的表现。具体结果整理如下：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;720&quot; src=&quot;https://devpress.csdnimg.cn/91118bb71f934e4593fbbdd1e7362486.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图2：6个模型思考效率对比&lt;/p&gt;

&lt;p&gt;通过分析这些折线图，我们可以得出一些有趣的结论：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;问题难度与模型表现：随着问题难度的增加，大多数模型的正确率呈现下降趋势，但QwQ-32B和DeepSeek-R1表现出色，在高难度问题上仍保持很高的准确率，其中QwQ-32B在最高难度级别上取得了最佳表现。同时，所有模型的输出长度都随问题难度增加而增加，这表明模型在解答更复杂问题时需要更长的&quot;思考时间&quot;，与Inference-Time Scaling现象相符。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;O1/R1类推理模型的表现：&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;对于O1/R1推理模型，随着问题难度的提升，虽然输出长度稳定变长，但token的效率也有所提高（DeepSeek-R1从36%增长到54%，QwQ-32B从31%增长到49%）。这表明推理类型的模型，在更复杂的问题上其token的消耗会更加“物有所值”。而在相对简单的问题中，可能更多的存在不必须的token浪费：即使在简单问题上也可能不必要的对答案进行反复验证。其中QwQ-32B输出的token数量相较其他模型更多，这使得该模型在Level 5这种高难度问题上也能保持较高正确率，但另一方面也说明该模型可能存在过度分析的问题。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;我们观察到一个有趣的现象：在难度Level 4及以下的问题中，DeepSeek系列的三个模型在子思维链数量上的变化相对稳定。然而，在最具挑战性的Level 5难度下，子思维链的数量会突然大幅增加。这可能是因为对于所测试的这些模型来说，Level 5的问题带来了极大的挑战，需要经过多轮尝试和思考才能找到解决方案。相比之下，QwQ-32B和QwQ-32B-Preview在思维链数量的增长上表现得更加均匀。这个现象可能反映了它们在处理复杂问题时的不同策略和能力。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;非推理模型的表现：在处理高难度数学问题时，非推理模型Qwen2.5-Math-7B-Instruct的准确率显著下降。此外，由于缺乏深入的思考过程，该模型的输出数量仅为推理模型的三分之一。从图中不难看出，像Qwen2.5-Math-7B-Instruct这样的专门训练的数学模型在普通问题上的解题速率和资源消耗方面优于通用推理模型，但随着问题难度的提升，由于深入思考过程的缺乏所导致的模型性能下降的现象会更加明显，即存在明显的&quot;天花板&quot;效应。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;03.Tips&lt;/h2&gt;

&lt;p&gt;在撰写这篇最佳实践的过程中，我们积累了一些心得与大家分享：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;关于思考效率评测指标的定义：&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;本文借鉴了文献[1]和文献[2]对“过度思考”（overthinking）和“思考不足”（underthinking）的定义，简化了Outcome Efficiency这一指标，提出了token效率指标。然而，这个指标主要关注生成token数量这一方面，未能涵盖模型思考过程的全部细节。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;子思维链数量的计算采用了启发式方法，即通过预定义一些常见关键词来识别。需要注意的是，不同模型可能需要不同的关键词集合，以准确捕捉其思考方式。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;关于指标的适用范围：&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;目前，这些指标主要在数学推理数据集上应用，因此可能无法全面反映模型在其他应用场景下的表现。此外，由于DeepSeek-R1-Distill-Qwen-7B是基于Qwen2.5-7B-Math数学模型蒸馏而来，其本身在数学能力上可能具备更大的优势。这在我们的MATH-500数据集上会有倾斜性的占优。在例如开放性问答或其他需要创造性回复的场景中，这些指标可能不够充分。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;关于token效率指标的计算：&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;在实现过程中，我们依赖一个额外的Judge模型来判断模型推理步骤的正确性。参考ProcessBench[^4]的工作，这一任务对现有模型来说颇具挑战性，通常需要一个能力较强的模型来进行判断。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;如果Judge模型做出错误判断，可能会影响到token效率指标的准确性，这也意味着在选择Judge模型时需格外谨慎。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;04.总结&lt;/h2&gt;

&lt;p&gt;本文基于MATH-500数据集，对包括QwQ-32B、DeepSeek-R1在内的几个主流Reasoning模型进行了推理效率的评测，从&quot;Token efficiency&quot;和&quot;Accuracy&quot;的视角，我们得到几个值得探讨的结论：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;模型的“深入思考”能力，与模型的性能表现有明显的相关关系，更高难度的问题，需要“更深入”的思考过程&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;对于Reasoning效率评测，本文基于“overthinking”和“underthinking”的相关工作，探讨如何定义可量化的评测指标，以及在EvalScope框架上的工程实现&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;模型Reasoning效率的评测，对GRPO和SFT训练过程有非常重要的参考意义，有助于开发出&quot;更高效&quot;且能根据问题难度&quot;自适应推理&quot;的模型&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;05.参考文献&lt;/h2&gt;

&lt;p&gt;[^1]: Wang, Y. et al. Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs. Preprint at https://doi.org/10.48550/arXiv.2501.18585 (2025).&lt;/p&gt;

&lt;p&gt;[^2]: Chen, X. et al. Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs. Preprint at https://doi.org/10.48550/arXiv.2412.21187 (2025).&lt;/p&gt;

&lt;p&gt;[^3]: Think Less, Achieve More: Cut Reasoning Costs by 50% Without Sacrificing Accuracy. https://novasky-ai.github.io/posts/reduce-overthinking/.&lt;/p&gt;

&lt;p&gt;[^4]: Zheng, C. et al. ProcessBench: Identifying Process Errors in Mathematical Reasoning. Preprint at https://doi.org/10.48550/arXiv.2412.06559(2024).&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d90452b8d50678a24da2f5.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d90452b8d50678a24da2f5.html</guid><pubDate>Tue, 18 Mar 2025 05:27:46 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>今日热门论文推荐：TPDiff、Block Diffusion、Reangle-A-Video、GTR</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/329e340c4a2243b89c04e94d277214e3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;由新加坡国立大学Show Lab的Lingmin Ran和Mike Zheng Shou提出，TPDiff是一个创新的视频扩散模型框架，针对视频生成的高计算需求问题，通过分阶段逐步提高帧率优化了训练和&lt;/blockquote&gt;   &lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;180&quot; src=&quot;https://devpress.csdnimg.cn/91772577235a41c7a55eb46eeeda4afc.gif&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;作者：InternLM、Qwen 等 LLM每日一览热门论文版，顶会投稿选题不迷惘。来看看由「机智流」和「ModelScope」社区推荐的今日热门论文吧～&lt;/p&gt;

&lt;h2&gt;TPDiff: Temporal Pyramid Video Diffusion Model&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/125911&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;由新加坡国立大学Show Lab的Lingmin Ran和Mike Zheng Shou提出，TPDiff是一个创新的视频扩散模型框架，针对视频生成的高计算需求问题，通过分阶段逐步提高帧率优化了训练和推理效率。核心贡献包括提出“时间金字塔”方法和阶段式扩散训练策略，实验表明训练成本降低50%，推理效率提升1.5倍。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;918&quot; src=&quot;https://devpress.csdnimg.cn/57487f487b944a6a88c9b37eaa4577e6.png&quot; width=&quot;1068&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/126168&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;由Marianne Arriola等人提出，Block Diffusion结合自回归和扩散模型的优势，推出了一种支持灵活长度生成的高效语言模型。通过块状扩散设计和优化的训练算法，该模型在语言建模基准上刷新了扩散模型的性能纪录，支持任意长度序列生成。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;436&quot; src=&quot;https://devpress.csdnimg.cn/a97d15421f254a92b1f5e9aed88648c0.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Reangle-A-Video: 4D Video Generation as Video-to-Video Translation&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/126077&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;KAIST AI的Hyeonho Jeong等人提出了Reangle-A-Video，将多视角视频生成任务重构为视频到视频的翻译问题。无需大规模4D数据集，该方法利用现有图像和视频扩散模型，通过多视角运动学习和一致性引导生成同步多视角视频，超越了现有方法。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;562&quot; src=&quot;https://devpress.csdnimg.cn/e13770e445e747d28b0af346501e486b.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/125459&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;清华大学Tong Wei等人提出了GTR框架，解决强化学习训练视觉-语言模型（VLM）代理时出现的“思维崩溃”问题。通过自动纠正器指导推理过程，该方法显著提升了LLaVA-7b在复杂视觉任务中的表现，成功率提升3-5倍。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;492&quot; src=&quot;https://devpress.csdnimg.cn/fbfc6aef1cf44507897f33be3d795f1e.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/125961&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;耶路撒冷希伯来大学的Itay Chachy等人提出了RewardSDS，通过奖励加权采样增强得分蒸馏采样（SDS）的对齐能力。该方法在文本到图像、2D编辑和3D生成任务中表现出色，提升了生成质量和用户意图对齐度。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;438&quot; src=&quot;https://devpress.csdnimg.cn/163467ac27ba4d34a49af277c6df2a35.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/123851&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;耶路撒冷希伯来大学的Shahar Levy等人研究了检索增强生成（RAG）中多文档处理的独立挑战。实验表明，在固定上下文长度下，文档数量增加会导致LLM性能下降高达10%，揭示了多文档处理的新难题。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;470&quot; src=&quot;https://devpress.csdnimg.cn/ecb04984212f45a3a828708f5e21a614.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Motion Anything: Any to Motion Generation&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/125267&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;由ANU的Zeyu Zhang等人提出的Motion Anything是一个多模态运动生成框架，通过注意力掩码建模实现对关键帧和动作的精细控制。还推出了包含2153组文本-音乐-舞蹈数据的TMD数据集，FID提升15%。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;564&quot; src=&quot;https://devpress.csdnimg.cn/e2e699a805a04b968f8eacb50fbf81ae.png&quot; width=&quot;1120&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Quantizing Large Language Models for Code Generation: A Differentiated Replication&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/125103&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;Alessandro Giagnorio等人对代码生成LLM的量化进行了扩展研究，测试了高达34B参数的模型和2位极致量化技术。结果显示4位量化可减少70%内存占用而不损性能，代码特定数据集在极低位量化时表现更优。&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;WildIFEval: Instruction Following in the Wild&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/125130&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;耶路撒冷希伯来大学的Gili Lior等人推出了WildIFEval，一个包含12K真实用户多约束指令的大规模数据集。实验显示，随着约束数量增加，所有LLM性能均下降，揭示了复杂指令跟随的改进空间。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;896&quot; src=&quot;https://devpress.csdnimg.cn/0b53e4ea3985449fb82e91b7f54cadeb.png&quot; width=&quot;954&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/126245&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;/p&gt;

&lt;p&gt;新加坡国立大学Show Lab的Kevin Qinghong Lin等人提出了VLog，通过生成式检索和叙述词汇表革新视频理解。基于GPT-2，该模型实现高效、上下文准确的视频叙述，处理长视频速度提升10倍。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;838&quot; src=&quot;https://devpress.csdnimg.cn/9fe415e2d4a942468cc724d8f9d3fe09.png&quot; width=&quot;1058&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d903a86670175f9938689f.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d903a86670175f9938689f.html</guid><pubDate>Tue, 18 Mar 2025 05:24:56 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>线上共学 | Mac本地玩转大模型</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/d87ac94f10a3441ca76c19897da15c60.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;本文介绍如何在Mac本地部署和使用大模型，包括基础运行、多模态扩展、交互优化、知识增强、定制进化等技术链路，并提供Ollama、Stable Diffusion、LM-Studio等工具的详细操作指南&lt;/blockquote&gt;   &lt;h1&gt;主题内容简介&lt;/h1&gt;

&lt;p&gt;🌟【从零构建本地AI工作流：打造属于你的智能助手生态】&lt;/p&gt;

&lt;p&gt;本次分享将带你探索本地化AI部署的完整技术链路，构建一个安全可控、功能强大的个人智能助手系统：&lt;/p&gt;

&lt;p&gt;1️⃣ 基础筑基：从本地部署开源大模型出发，本地简单运行大模型&lt;br&gt;
2️⃣ 多模态扩展：接入Stable Diffusion等文生图模型，实现文生图功能&lt;br&gt;
3️⃣ 交互优化：基于LM-Studio构建个人知识助手，实现智能对话与记忆存储&lt;br&gt;
4️⃣ 知识增强：通过LangFlow搭建RAG应用，连接本地知识库与网络资源，打造定制化AI助手&lt;br&gt;
5️⃣ 定制进化：运用MLX框架进行领域适配微调，让通用模型拥有专属风格&lt;br&gt;
6️⃣ 系统融合：借力Open Interpreter打通操作系统API，实现文件管理、数据分析等自动化办公场景&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;💡本地部署优势：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;全链路隐私保护：数据不出本地&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;低成本定制方案：mac M系列芯片即可运行&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;可扩展架构：模块化设计自由组合&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;适合追求数据安全、需要定制化AI能力的技术探索者，开启属于你的智能生产力革命！🚀&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3&gt;环境搭建&lt;/h3&gt;

&lt;p&gt;虚拟环境准备&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;使用Python 3内置的模块venv来创建一个虚拟环境（也可以使用conda），使用source命令激活虚拟环境。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;python3 -m venv testsource test/bin/activate&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;虚拟环境可以为不同的项目维护独立的依赖关系和版本，避免库之间的冲突&lt;/p&gt;

&lt;p&gt;下载依赖库文件&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;下载Langflow、MLX等相关库&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;pip install langflow  
pip install mlx-lmpip install transformers  
pip install torchpip install numpy 
pip install modelscope&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;🖥️首先，我们可以使用Ollama、stable-diffusion-webui等工具直接运行大模型，进行简单对话或图片生成&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;ollama 本地部署运行大模型&lt;/h1&gt;

&lt;h2&gt;下载ollama工具&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;点击ollama官网一键下载 本地镜像包（https://modelscope.oss-cn-beijing.aliyuncs.com/releases/Ollama-darwin.zip）。下载后自动启用，或输入以下命令：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ollama&amp;nbsp;serve&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;如果运行ollama serve后出现类似下面的提示，说明已经可以正常使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error:&amp;nbsp;listen&amp;nbsp;tcp&amp;nbsp;127.0.0.1:11434:&amp;nbsp;bind:&amp;nbsp;address&amp;nbsp;already&amp;nbsp;in&amp;nbsp;use&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;选择模型&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;在魔搭社区的模型库中选择合适的GGUF模型：(网站链接：)&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;444&quot; src=&quot;https://devpress.csdnimg.cn/fe231b8e3e1e48be9ac2634c3163e520.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;开始运行&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;复制模型路径（如：Qwen/Qwen2.5-0.5B-Instruct-GGUF），替换下面命令中的{model path}，然后进行运行：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ollama run modelscope.cn/{model path}
例如：ollama run modelscope.cn/Qwen/Qwen2.5-0.5B-Instruct-GGUF&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;运行后，就可以直接使用大模型进行对话啦&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;344&quot; src=&quot;https://devpress.csdnimg.cn/e9be61aa1d464694bd3b09ccf00883f3.png&quot; width=&quot;1068&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;可以通过ollama list命令看到已经有的模型：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ollama list&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;78&quot; src=&quot;https://devpress.csdnimg.cn/d3d3dc323e9c4453a7c3a3356b38ead3.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;stable-diffusion-webui框架本地运行文生图模型&lt;/h1&gt;

&lt;h2&gt;环境准备&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;克隆stable-diffusion-webui库&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;git&amp;nbsp;clone&amp;nbsp;https://github.com/AUTOMATIC1111/stable-diffusion-webui.git&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;运行及配置&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;运行stable-diffusion-webui&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;cd stable-diffusion-webui  
./webui.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;可能的报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OSError:&amp;nbsp;Can&#39;t&amp;nbsp;load&amp;nbsp;tokenizer&amp;nbsp;for&amp;nbsp;&#39;openai/clip-vit-large-patch14&#39;.&amp;nbsp;If&amp;nbsp;you&amp;nbsp;were&amp;nbsp;trying&amp;nbsp;to&amp;nbsp;load&amp;nbsp;it&amp;nbsp;from&amp;nbsp;&#39;https://huggingface.co/models&#39;,&amp;nbsp;make&amp;nbsp;sure&amp;nbsp;you&amp;nbsp;don&#39;t&amp;nbsp;have&amp;nbsp;a&amp;nbsp;local&amp;nbsp;directory&amp;nbsp;with&amp;nbsp;the&amp;nbsp;same&amp;nbsp;name.&amp;nbsp;Otherwise,&amp;nbsp;make&amp;nbsp;sure&amp;nbsp;&#39;openai/clip-vit-large-patch14&#39;&amp;nbsp;is&amp;nbsp;the&amp;nbsp;correct&amp;nbsp;path&amp;nbsp;to&amp;nbsp;a&amp;nbsp;directory&amp;nbsp;containing&amp;nbsp;all&amp;nbsp;relevant&amp;nbsp;files&amp;nbsp;for&amp;nbsp;a&amp;nbsp;CLIPTokenizer&amp;nbsp;tokenizer.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;如果出现以上报错信息，说明当前网络无法从huggingface上下载文件，可以从魔搭上下载所需的模型，命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;modelscope&amp;nbsp;download&amp;nbsp;--model&amp;nbsp;AI-ModelScope/clip-vit-large-patch14&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;打开调用这个模型的代码位置，将路径换成刚才下载的文件路径：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;116&quot; src=&quot;https://devpress.csdnimg.cn/3d57312e8b074d0795da67587925c0d3.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;536&quot; src=&quot;https://devpress.csdnimg.cn/e2a7ae789e3b4317acadeebacbdb4c2d.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;此时运行./webui.sh ，就不会出现报错，自动打开本地网页界面&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;下载模型并使用&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;打开魔搭社区diffusion模型库：，选择合适的模型，在模型文件中下载模型权重（ckpt或safetensors格式）&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;478&quot; src=&quot;https://devpress.csdnimg.cn/174cc875b266452f9df782256d64efa1.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;将下载好的模型保存在：stable-diffusion-webui/models/Stable-diffusion路径下&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;210&quot; src=&quot;https://devpress.csdnimg.cn/dd0265b1da034288b03a51f4626e5c3c.png&quot; width=&quot;336&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;此时在左上角刷新后就会显示可以使用的模型：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;412&quot; src=&quot;https://devpress.csdnimg.cn/13dd8603f3ae49529cff90c73b43c45e.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;接下来就可以使用各种功能进行图片生成啦：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;526&quot; src=&quot;https://devpress.csdnimg.cn/7dcd8be0618c41e88b6e510ec9190543.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;ComfyUI使用工作流形式灵活搭建生图功能&lt;/h1&gt;

&lt;h2&gt;下载ComfyUI代码框架并运行&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI
pip install -r requirements.txt
python main.py&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;h2&gt;下载生图模型&lt;/h2&gt;

&lt;p&gt;使用命令行下载stable-diffusion-v1-5：（将path/to换成你本地的路径）&lt;/p&gt;

&lt;p&gt;或者直接在魔搭社区https://www.modelscope.cn/models?name=stable%20diffusion&amp;amp;page=1进行下载&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;将模型下载到ComfyUI/models/checkpoints目录下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;modelscope&amp;nbsp;download&amp;nbsp;--model&amp;nbsp;AI-ModelScope/stable-diffusion-v1-5&amp;nbsp;v1-5-pruned-emaonly.safetensors&amp;nbsp;--local_dir&amp;nbsp;path/to/ComfyUI/models/checkpoints&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样下载lora模型到ComfyUI/models/loras目录下&lt;/p&gt;

&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;h2&gt;具体演示&lt;/h2&gt;

&lt;p&gt;使用模版工作流，在checkpoint组建中选择下载好的stable-diffusion-v1-5模型，点击执行即可生成图片&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;在clip文本编辑中可以修改提示词以及反向提示词；在k采样器中可以修改迭代步数等一些参数&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468909&quot;&gt;https://live.csdn.net/v/468909&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;添加lora模型后，可以生成更有风格化的图片（可以在模型强度中修改lora模型的风格程度）&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468910&quot;&gt;https://live.csdn.net/v/468910&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;🖥️想要更美观的交互界面？试试LM-studio&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;LM-studio本地大模型个人助手&lt;/h1&gt;

&lt;h2&gt;下载LM-studio及大模型&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;点击LM-studio官网一键下载（本地镜像：https://modelscope.oss-cn-beijing.aliyuncs.com/releases/LM-Studio-0.3.10-6-arm64.dmg）&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;下载后自动启用，并下载模型存入指定目录，下载命令：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;modelscope&amp;nbsp;download&amp;nbsp;--model&amp;nbsp;Qwen/Qwen2.5-0.5B-Instruct-GGUF&amp;nbsp;--local_dir&amp;nbsp;/Users/mingjiangao/lmstudio/models/Publisher/Repository/qwen2.5-0.5b&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;存入的local_dir目录与LM-studio中的模型路径相同即可，注意需要Publisher/Repository/两层目录&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;396&quot; src=&quot;https://devpress.csdnimg.cn/373cb801db6c499b810e6f795e4d5481.png&quot; width=&quot;816&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;开始使用&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;在Chat界面选择下载好的模型&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;464&quot; src=&quot;https://devpress.csdnimg.cn/3e69606011e343c98a61269e89063943.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;开始聊天吧！&lt;img alt=&quot;&quot; height=&quot;708&quot; src=&quot;https://devpress.csdnimg.cn/afd620a5f2ca4d8882ca314225f5ac87.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;如果想要使用工作流的形式搭建功能更强大的大模型应用，可以使用Langflow工具&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;langflow搭建RAG应用&lt;/h1&gt;

&lt;h2&gt;准备模型&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;使用ollama pull命令下载一个一个文本嵌入模型，用来构建RAG知识库：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;ollama&amp;nbsp;pull&amp;nbsp;modelscope.cn/Embedding-GGUF/nomic-embed-text-v1.5-GGUF:latest&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;本地运行langflow&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;运行langflow：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;pip&amp;nbsp;install&amp;nbsp;langflowpython -m langflow run&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;运行成功后，会出现以下的提示及本地网址：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;244&quot; src=&quot;https://devpress.csdnimg.cn/6c2968efd7df47f58c92f83a6dc99d4c.png&quot; width=&quot;834&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;搭建工作流&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;打开给出的本地网址http://127.0.0.1:7861，点击New Flow，创建一个空的工作流：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;348&quot; src=&quot;https://devpress.csdnimg.cn/728e542ff45044e0986fc60eea7a1bfd.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;添加最简单的输入输出逻辑，分别添加Chat Input、Chat output以及ollama模型块，在ollama模型块中选择已经上传的qwen模型，将输入输出块与模型块相连，点击右上角的playground即可运行&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468911&quot;&gt;https://live.csdn.net/v/468911&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;添加提示词，个性化AI服务：添加prompt块，在内容中加入需要的提示词，并添加question接口，将我们的输入添加在提示词后&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468912&quot;&gt;https://live.csdn.net/v/468912&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;添加本地知识库，获得更加专业的回答：创建一个数据库，将知识库的文件/链接等输入到数据库中，连接准备好的embedding模型和查询输入，将数据库检索的结果转换为text格式。在提示词中添加知识库接口，将检索结果输入到这个接口上。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468913&quot;&gt;https://live.csdn.net/v/468913&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然后，我们就可以开始进行对话啦，可以看到此时大模型的知识库已经包含我们所上传的内容：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;626&quot; src=&quot;https://devpress.csdnimg.cn/a378c9c9e77c427e9c6a2816ac77e65b.png&quot; width=&quot;906&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;🖥️对已有的模型不满意，想要更个性化专业化的模型？试试MLX框架进行大模型微调吧&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;MLX框架本地大模型微调&lt;/h1&gt;

&lt;h2&gt;下载模型&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;调用ModelScope平台提供的下载工具的命令，从ModelScope模型库中下载需要的模型（--model参数表示下载的模型名称；--local_dir表示会把模型下载到这个路径的文件夹下）&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;modelscope&amp;nbsp;download&amp;nbsp;--model&amp;nbsp;&#39;Qwen/Qwen2.5-0.5B-Instruct&#39;&amp;nbsp;--local_dir&amp;nbsp;&#39;qwen2.5-0.5B&#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;微调大模型&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;下载mlx代码库：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/ml-explore/mlx-examples.git&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;找到lora/data目录下的train.jsonl文件，修改为我们自己的训练集，以可爱幽默风格为例：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;404&quot; src=&quot;https://devpress.csdnimg.cn/b103759eaf1542d7a270edd5feddf77f.png&quot; width=&quot;1122&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&quot;prompt&quot;: &quot;为什么冬天要穿棉衣？&quot;, &quot;completion&quot;: &quot;呜呜呜～因为小雪花会偷偷钻进衣服里，穿上厚厚的棉衣就像被温暖的小熊抱抱一样暖和！&quot;}{&quot;prompt&quot;: &quot;怎么让猫咪喜欢你？&quot;, &quot;completion&quot;: &quot;喵喵喵～拿小鱼干当见面礼，然后轻轻摸摸它的头，它就会蹭蹭你啦！&quot;}{&quot;prompt&quot;: &quot;怎样才能快速入睡？&quot;, &quot;completion&quot;: &quot;呼噜噜～想象自己躺在云朵上，数着绵羊跳过月亮，很快就能进入甜甜的梦乡啦！&quot;}{&quot;prompt&quot;: &quot;怎么记住朋友的生日？&quot;, &quot;completion&quot;: &quot;叮咚咚～在手机里设置小闹钟提醒，这样就不会忘记给小伙伴庆祝生日啦！&quot;}{&quot;prompt&quot;: &quot;怎样能让头发长得快？&quot;, &quot;completion&quot;: &quot;唰唰唰～多吃蔬菜水果，多喝水，还可以跟头发说悄悄话，让它快点长高高哦！&quot;}{&quot;prompt&quot;: &quot;为什么夏天要吃冰淇淋？&quot;, &quot;completion&quot;: &quot;丝丝丝～因为冰淇淋能赶走炎热的小恶魔，带来清凉的感觉，超级舒服哒！&quot;}{&quot;prompt&quot;: &quot;如何让自己的房间变得整洁？&quot;, &quot;completion&quot;: &quot;嗖嗖嗖～把所有东西都放进魔法盒子里（收纳箱），房间瞬间就干净整齐啦！&quot;}{&quot;prompt&quot;: &quot;怎么让自己更开心？&quot;, &quot;completion&quot;: &quot;嘻嘻嘻～看一部搞笑的动画片，或者和好朋友一起玩耍，快乐马上来敲门啦！&quot;}{&quot;prompt&quot;: &quot;怎样做作业更快？&quot;, &quot;completion&quot;: &quot;刷刷刷～集中注意力，像小火箭一样冲向题目，做完后别忘了给自己一个小奖励哦！&quot;}{&quot;prompt&quot;: &quot;为什么读书很重要？&quot;, &quot;completion&quot;: &quot;呀呀呀～因为书本是知识的小宝库，每读一本书就像是找到了一颗闪亮的宝石呢！&quot;}{&quot;prompt&quot;: &quot;为什么睡觉要盖被子？&quot;, &quot;completion&quot;: &quot;小兔子要暖和才能蹦蹦跳跳，盖上被子就像爸爸妈妈抱着小兔子一样温暖！&quot;}{&quot;prompt&quot;: &quot;怎么让眼睛不累？&quot;, &quot;completion&quot;: &quot;眨巴眨巴～多看看远处的绿树，给眼睛做个小按摩，它们就会很开心啦！&quot;}{&quot;prompt&quot;: &quot;怎样才能交到好朋友？&quot;, &quot;completion&quot;: &quot;嘻嘻嘻～分享自己的小零食，一起玩耍聊天，很快就能找到志同道合的好朋友啦！&quot;}{&quot;prompt&quot;: &quot;为什么要做运动？&quot;, &quot;completion&quot;: &quot;动一动～蹦蹦跳跳能让身体变得更加强壮，像小超人一样充满活力哦！&quot;}{&quot;prompt&quot;: &quot;如何让心情变好？&quot;, &quot;completion&quot;: &quot;哼哼歌～找小伙伴倾诉一下心事，或者画一幅彩色的心情图，快乐就会回来找你啦！&quot;}{&quot;prompt&quot;: &quot;怎么知道天气好不好？&quot;, &quot;completion&quot;: &quot;瞧一瞧～打开窗户看看太阳公公是不是在微笑，听听风儿姐姐的声音就知道啦！&quot;}{&quot;prompt&quot;: &quot;为什么不能挑食？&quot;, &quot;completion&quot;: &quot;咕噜咕噜～各种食物都有自己的魔法力量，多吃不同的食物才能变得更强壮哦！&quot;}{&quot;prompt&quot;: &quot;怎样保护牙齿健康？&quot;, &quot;completion&quot;: &quot;刷刷刷～每天早晚用小牙刷给牙齿们洗澡，不让蛀虫小坏蛋有机会捣乱！&quot;}{&quot;prompt&quot;: &quot;怎么选择喜欢的书？&quot;, &quot;completion&quot;: &quot;翻一翻～找到让你心跳加速、眼睛放光的故事，那就是你的最爱啦！&quot;}{&quot;prompt&quot;:&amp;nbsp;&quot;为什么需要喝水？&quot;,&amp;nbsp;&quot;completion&quot;:&amp;nbsp;&quot;咕咚咕咚～水是生命的小溪流，喝够水身体里的小细胞们才能欢快地跳舞哦！&quot;}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;进入lora目录，开始微调，注意--model参数后换成你本地模型文件夹的实际目录，使用默认的训练参数，更多的训练参数可以参考&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;cd mlx-examples/lora
mlx_lm.lora --model path/to/qwen2.5-0.5B --train --data ./data&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;在训练结束后后，会在lora目录下生成微调后的模型适配器权重文件目录adapters&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;200&quot; src=&quot;https://devpress.csdnimg.cn/04b91de2a75b40c592d4735ab0262108.png&quot; width=&quot;348&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;模型生成并验证&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;通过mlx_lm.fuse命生成新模型，命名为“qwen2.5-0.5B-new”&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;mlx_lm.fuse&amp;nbsp;--model&amp;nbsp;path/to/qwen2.5-0.5B&amp;nbsp;--adapter-path&amp;nbsp;adapters&amp;nbsp;--save-path&amp;nbsp;qwen2.5-0.5B-new&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;通过简单的问题来验证微调后的模型效果：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model path/to/model --prompt &quot;Your promp&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;可以看到，微调之后的回答十分可爱幽默：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;336&quot; src=&quot;https://devpress.csdnimg.cn/411f76f1a1fa4af6ba47fc56bf8e8bb7.png&quot; width=&quot;2216&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;112&quot; src=&quot;https://devpress.csdnimg.cn/645846474be64060a856364cff28cf4d.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;🖥️不止于对话，想用大模型完成更多任务？&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;open-interpreter框架本地操作电脑&lt;/h1&gt;

&lt;h2&gt;创建新的虚拟环境&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;新建一个虚拟环境，避免与Langflow库之间产生冲突。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;python3 -m venv computer_use
source computer_use/bin/activate&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;下载open-interpreter库&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;pip install open-interpreter&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;运行代码完成指令&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;新建Python文件，使用魔搭免费模型推理AP，运行下面的代码（将Your-SDK-Token替换为魔搭SDK令牌）：&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;from interpreter import interpreter
interpreter.llm.api_base = &quot;https://api-inference.modelscope.cn/v1&quot;
interpreter.llm.api_key = &quot;Your-SDK-Token&quot;
interpreter.llm.model = &quot;openai/Qwen/Qwen2.5-Coder-32B-Instruct&quot;
interpreter.chat(&quot;Can you help me create a.txt in Desktop&quot;)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d3ec3f807ce562bfe05956.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d3ec3f807ce562bfe05956.html</guid><pubDate>Fri, 14 Mar 2025 08:43:43 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>20万悬赏AI美妆！欧莱雅美妆科技黑客松2025重磅来袭</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/3aec1e3534f042fbaace390703bbf01d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;DeepSeek 的风甚至吹到了美妆区。近日，一小红书博主跟足 DeepSeek 指示上妆，意外打造出「石矶娘娘」妆效，引发全网围观。一场看似无厘头的跨界实验，实则揭示了美妆与科技源远流长的共生关系：&lt;/blockquote&gt;   &lt;p&gt;DeepSeek 的风甚至吹到了美妆区。近日，一小红书博主跟足 DeepSeek 指示上妆，意外打造出「石矶娘娘」妆效，引发全网围观。一场看似无厘头的跨界实验，实则揭示了美妆与科技源远流长的共生关系：经济史上的「口红效应」本质就是技术迭代与消费心理的精准契合，而今 AI 又在重构美妆逻辑。&lt;/p&gt;

&lt;p&gt;在美妆科技的赛道上，全球第一大美妆集团——欧莱雅正在加速引领「美妆科技」，致力于创造未来之美。作为 CES 消费电子展的常驻先锋，其亮出的黑科技让人目不暇接：仅需五分钟，AI 皮肤分析仪就能测出肌肤年龄，提供护肤建议、生成式 AI 顾问为你量身定制美妆建议、上妆辅助器可以协助手臂活动能力受限人士顺利上妆。在研发领域，生成式 AI 正协助欧莱雅更快找到下一个玻色因。基于生成式 AI 的品牌定制模型能够识别集团多个品牌独特的视觉元素，赋能营销创意。&lt;/p&gt;

&lt;p&gt;当 AI 与美妆融合，欧莱雅深知其中还蕴含诸多挑战和机遇。非技术人员如何被海量业务数据轻松赋能？大模型的「幻觉输出」与内容合规风险亟待解决。而在创新前沿，未被开垦的 AI 应用场景仍如星海般璀璨。&lt;/p&gt;

&lt;p&gt;在此背景下，由全球第一大美妆集团——欧莱雅发起，中国青少年发展基金会、阿里云、机器之心、魔搭社区等顶尖机构和企业支持的欧莱雅集团首届美妆科技黑客松大赛【科技大 FUN 颂】正式启动，诚邀全国开发者通过智能体技术创新，共同探索美妆行业的数字化未来。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;608&quot; src=&quot;https://devpress.csdnimg.cn/d43212c3c9c74e38beb93ba88687e776.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;报名通道现已全面开启，让我们携手突破想象边界，见证科技赋予美的全新定义。&lt;/p&gt;

&lt;p&gt;一、四个赛道比什么？&lt;/p&gt;

&lt;p&gt;本次赛事开辟「创智体验家」、「内容鉴查官」、「数据解构师」、「风控守护神」四个赛道，聚焦美妆科技创新的 4 个主要领域，即：AI 美妆解决方案、AI 内容审核、商业智能分析、AI 的安全可靠。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1033&quot; src=&quot;https://devpress.csdnimg.cn/751caa5430cf4b98857d30c89fb9ec27.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;365&quot; src=&quot;https://devpress.csdnimg.cn/6e7af48cc99a480e806ba029ccf7c485.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;四大赛道具体说明可扫描官网二维码。&lt;/p&gt;

&lt;p&gt;二、丰厚奖金&amp;amp;产业资源，撬动职业未来&lt;/p&gt;

&lt;p&gt;对于参赛者来说，这不仅是一场数十小时的脑力竞技，更是开启职业发展的多维钥匙。除了不同阶段设置丰厚的奖金，欧莱雅中国还整合了顶尖产业资源推动赛事，连接学术前沿与产业实践。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1461&quot; src=&quot;https://devpress.csdnimg.cn/45c09a676f134563bf12fa08b030ab01.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1941&quot; src=&quot;https://devpress.csdnimg.cn/55a73a7346074bd5a322d83fedf0fb11.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;三、把握赛程关键节点&lt;/p&gt;

&lt;p&gt;大赛以团队形式报名，面向全国范围内全日制在校大学生的本科生、硕士研究生及博士研究生。比赛的关键节点如下。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1032&quot; src=&quot;https://devpress.csdnimg.cn/b51682b1dec84b78948f770d765c85e3.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;大赛报名：2025 年 3 月 11 日— 2025 年 2025 年 5 月 9 日 21:00（ UTC+8 ）&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;作品提交：2025 年 4 月 1 日 — 2025 年 5 月 23 日 21:00（ UTC+8 ）&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;人气争夺战：自选手提交作品之日起 — 2025 年 5 月 26 日 21:00（ UTC+8 ）&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;参与方式：将作品按要求提交至创空间。&lt;/p&gt;

&lt;p&gt;评选标准：将依据作品的获赞数量，从高到低依次遴选出人气前 16 支参赛队伍，并授予相应的奖金及奖品以资鼓励。&lt;/p&gt;

&lt;p&gt;名单公布：人气争夺战获胜队伍名单将于 2025 年 5 月 28 日在大赛官网公布。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;824&quot; src=&quot;https://devpress.csdnimg.cn/d5b69791c2ef420199fe06ae064de5f7.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;晋级决赛名单公布：2025 年 5 月 30 日&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;评选标准：将由专业评审团从作品创意性（商业前景）、技术难度（代码部分）、完成度（呈现方式，运行速度，需求匹配程度）、社会价值以及 AI 合规性的维度进行综合评审。经过全面评估，作品成绩综合排名位于前 15 且通过代码审核的参赛队伍将晋级至决赛。&lt;/p&gt;

&lt;p&gt;名单公布：晋级决赛的队伍名单将于 2025 年 5 月 30 日在获奖名单公布区公示直至 2025 年 6 月 2 日 12:00。&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;训练营 Workshop：2025 年 6 月&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;晋级决赛的 15 支队伍将参与为期 1 天的专项 Workshop，每队配备专属导师，自 Workshop 起至决赛前提供阶段性 1 对 1 辅导，选手可随时与导师沟通优化作品。另设产学研专家现场指导环节，针对作品需求提供技术攻坚支持。具体时间、地点及议程后续公布。&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;决赛：2025 年 7 月初（ WAIC 期间）&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;线下决赛将以现场作品调优以及路演答辩的形式展开，在作品调优阶段，组委会将安排专家技术宣讲、专家答疑环节。其中，技术宣讲将围绕比赛中选手可能会遇到的共性问题展开，拟邀如欧莱雅、阿里云、魔搭社区等一线专家进行宣讲；答疑与优化环节则针对选手作品提供更具针对性的分享与答疑；最后还将包含答辩技巧培训，帮助选手更好地呈现作品亮点与技术优势。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;471&quot; src=&quot;https://devpress.csdnimg.cn/bd2d022096e34c08b1434e8f650cd125.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;欧莱雅中国期待与每位怀揣梦想、追求卓越的开发者在这场激动人心的技术角逐中相聚！&lt;/p&gt;

&lt;p&gt;点击链接，直达赛题介绍&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/brand/view/BeautyTechathon&quot;&gt;https://www.modelscope.cn/brand/view/BeautyTechathon&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d393bbd649b06b61ce03ac.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d393bbd649b06b61ce03ac.html</guid><pubDate>Fri, 14 Mar 2025 02:26:03 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>今日热门论文推荐：Seedream、LMM-R1、YuE、Gemini Embedding</title><description>&lt;blockquote&gt;由Cohere、SEACrowd等机构联手打造，SEA-VL是一个面向东南亚地区的多文化视觉-语言数据集，填补了AI模型在该地区文化细微差别理解上的空白。该工作通过众包、爬取和生成三种方式收集了128&lt;/blockquote&gt;   &lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;作者：InternLM、Qwen 等 LLM 每日一览热门论文版，顶会投稿选题不迷惘。快来看看由「机智流」和「ModelScope」社区推荐的今日热门论文吧！&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;180&quot; src=&quot;https://devpress.csdnimg.cn/f07aa77ab3f7453b945f8a9ed623eb3e.gif&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125634&lt;/p&gt;

&lt;p&gt;简要介绍：由Cohere、SEACrowd等机构联手打造，SEA-VL是一个面向东南亚地区的多文化视觉-语言数据集，填补了AI模型在该地区文化细微差别理解上的空白。该工作通过众包、爬取和生成三种方式收集了128万张文化相关图像，结合本地贡献者确保数据的高质量和多样性，推动了更具包容性的AI发展。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/b6668f585f2f45eb96c132c48667fde0.jpg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125170&lt;/p&gt;

&lt;p&gt;简要介绍：由东南大学等机构提出的LMM-R1，通过两阶段规则强化学习（RL）框架增强了3B参数多模态大模型的推理能力。该方法先通过文本数据强化基础推理，再推广至多模态任务，在Qwen2.5-VL-Instruct-3B上实现多模态和文本基准提升4.83%和4.5%，为数据高效的推理优化提供了新思路。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/fa856fcb31a84291898645c2fa71bd6f.png&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;YuE: Scaling Open Foundation Models for Long-Form Music Generation&lt;/p&gt;

&lt;p&gt;论文链接：https://huggingface.co/papers/2503.08638&lt;/p&gt;

&lt;p&gt;简要介绍：HKUST和MAP团队推出了YuE，一种基于LLaMA2架构的开源音乐生成模型，专注于长篇歌词到歌曲生成。YuE能生成长达5分钟的音乐，保持歌词对齐和音乐连贯性，支持风格迁移和多语言扩展，在音乐性和声乐敏捷性上媲美甚至超越部分专有系统。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/51aa55c0ee44436db23145bc6e35e957.jpg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/124803&lt;/p&gt;

&lt;p&gt;简要介绍：由Hedra Inc.和北京大学等合作开发的MagicInfinite，是一种基于扩散Transformer的框架，可生成无限长度的说话视频，支持多种角色风格和多模态控制。该工作通过3D全注意力机制和两阶段学习策略，实现高效推理和高保真动画，已公开上线供体验。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/9327083c29ff42aaae749e11177b4ee5.jpg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;UniF^2ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125793&lt;/p&gt;

&lt;p&gt;简要介绍：由北京大学等机构提出的UniF^2ace，是首个专为细粒度人脸理解和生成设计的统一多模态模型。基于自建的130K人脸数据集，该模型结合扩散技术和混合专家架构，在理解和生成任务中均超越现有模型，推动了人脸领域的AGI研究。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/b44617d5949044d5a7b4dab8a0cdd817.jpg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125042&lt;/p&gt;

&lt;p&gt;简要介绍：由CMU和Hugging Face团队开发的MRT（Meta Reinforcement Fine-Tuning），将测试时计算优化形式化为元强化学习问题，通过最小化累积遗憾提升LLM推理性能。在数学推理任务中，MRT比传统RL方法性能提升2-3倍，token效率提高1.5倍。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/9574429192914f75a81ab12dabd65a6d.jpg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125511&lt;/p&gt;

&lt;p&gt;简要介绍：字节跳动Seed Vision团队推出的Seedream 2.0，是一款中英双语图像生成模型，解决现有模型在中文文化理解和文本渲染上的不足。集成自研LLM和多阶段优化，该模型在提示跟随、美学和结构正确性上达到SOTA，已应用于多个平台。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://devpress.csdnimg.cn/e8063891cc084febb1d4445abbef0110.jpg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125357&lt;/p&gt;

&lt;p&gt;简要介绍：由浙江大学和蚂蚁集团合作的SegAgent，通过模仿人类标注轨迹探索MLLM的像素级理解能力。提出HLMAT任务，将分割建模为多步决策过程，SegAgent在无需额外解码器的情况下实现高精度分割，支持掩码精炼等扩展任务。&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;

&lt;p&gt;Gemini Embedding: Generalizable Embeddings from Gemini&lt;/p&gt;

&lt;p&gt;论文链接：https://modelscope.cn/papers/125362&lt;/p&gt;

&lt;p&gt;简要介绍：谷歌Gemini Embedding团队基于Gemini LLM开发了一种通用嵌入模型，支持多语言和代码任务。在MMTEB基准上，该模型大幅超越前SOTA，展示了对250+语言的强大适应性，适用于分类、检索等多种下游任务。&lt;/p&gt;

&lt;hr&gt;
&lt;p&gt;结语：今天的盘点涵盖了从文化数据集到多模态推理、音乐生成等多个领域的突破性研究，每篇论文都展现了AI技术的最新进展。你最看好哪篇？欢迎留言讨论！🌟 下期见！&lt;/p&gt;

&lt;p&gt;-- 完 --&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;欢迎在「机智流」公众号后台回复「cc」，加入机智流大模型交流群，与我们一起探索 AI 与人类潜能的未来，一起共赴 AI 浪潮！&lt;/p&gt;
&lt;/blockquote&gt;
 </description><link>https://community.modelscope.cn/67d390f03b685529b70ace86.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d390f03b685529b70ace86.html</guid><pubDate>Fri, 14 Mar 2025 02:14:08 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>R1-Omni开源！多模态模型+RLVR，让各模态作用清晰可见</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/349f8ecf3b244da6bd5a662991edd5f6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;随着 DeepSeek R1 的推出，强化学习在大模型领域的潜力被进一步挖掘。Reinforcement Learning with Verifiable Reward (RLVR) 方法的出现，为多&lt;/blockquote&gt;   &lt;p&gt;随着 DeepSeek R1 的推出，强化学习在大模型领域的潜力被进一步挖掘。Reinforcement Learning with Verifiable Reward (RLVR) 方法的出现，为多模态任务提供了全新的优化思路，无论是几何推理、视觉计数，还是经典图像分类和物体检测任务，RLVR 都展现出了显著优于传统监督微调（SFT）的效果。&lt;/p&gt;

&lt;p&gt;然而，现有研究多聚焦于 Image-Text 多模态任务，尚未涉足更复杂的全模态场景。基于此，通义实验室团队探索了 RLVR 与视频全模态模型的结合，并提出了 R1-Omni 模型。&lt;/p&gt;

&lt;p&gt;论文：&lt;/p&gt;

&lt;p&gt;https://arxiv.org/abs/2503.05379&lt;/p&gt;

&lt;p&gt;Github：&lt;/p&gt;

&lt;p&gt;https://github.com/HumanMLLM/R1-Omni&lt;/p&gt;

&lt;p&gt;模型：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/iic/R1-Omni-0.5B&lt;/p&gt;

&lt;h2&gt;01.模型介绍&lt;/h2&gt;

&lt;p&gt;模型训练的两个阶段&lt;/p&gt;

&lt;p&gt;冷启动阶段：奠定基础推理能力&lt;/p&gt;

&lt;p&gt;为了保证RLVR阶段训练的平稳性，该团队使用一个组合数据集进行微调，使其初步掌握多模态情感识别任务中的推理能力。该组合数据集是一个由 580 条视频数据组成的组合数据集，其中包括来自 Explainable Multimodal Emotion Reasoning (EMER) 数据集的 232 条样本，以及来自 HumanOmni 数据集的 348 条样本。&lt;/p&gt;

&lt;p&gt;EMER数据集的数据格式如下：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;785&quot; src=&quot;https://devpress.csdnimg.cn/a5397ead2d2d403f92df203935db902e.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;这一阶段确保了模型在进入 RLVR 阶段前已具备一定基础能力，从而提升后续训练的效率与稳定性。&lt;/p&gt;

&lt;p&gt;RLVR阶段：推理与泛化能力双重提升&lt;/p&gt;

&lt;p&gt;基于冷启动阶段初始化的模型，通过RLVR的方式训练，同时利用视频和音频的多模态数据优化情感识别任务。该阶段通过强化学习与可验证奖励机制，进一步优化了模型的推理能力和泛化性能。&lt;/p&gt;

&lt;p&gt;RLVR的第一个关键组件是策略模型（policy model），该模型处理由视频帧和相应音频流组成的多模态输入数据，并生成一组候选响应。每个响应都附带详细的推理，展示了模型如何整合视觉和听觉信息从而得出预测的详细过程。&lt;/p&gt;

&lt;p&gt;第二个关键组件是奖励函数，策略模型生成的这些候选响应使用可验证的奖励函数（reward function）进行评估。RLVR训练框架中用到的奖励函数受DeepSeekR1的启发，将奖励分成了两个部分，精确率奖励（accuracy reward）和格式奖励（format reward），这两部分共同形成最终的奖励R：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;118&quot; src=&quot;https://devpress.csdnimg.cn/3f79fade8cf2424890c428bf0b3fd558.png&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;通过联合两部分奖励，该奖励函数不仅鼓励模型生成正确的预测，同时保证输出是结构化的，并且和我们预设的格式一致。&lt;/p&gt;

&lt;p&gt;实验表明，RLVR 不仅让音频和视频信息的作用更加透明，还显著提升了模型在情绪识别任务中的关键指标。此外，R1-Omni 在分布外测试中表现出色，充分展现了其在复杂场景下的强大泛化能力。&lt;/p&gt;

&lt;p&gt;模型效果对比&lt;/p&gt;

&lt;p&gt;为了验证 R1-Omni 的性能，我们将其与原始的 HumanOmni-0.5B 模型、冷启动阶段的模型以及在 MAFW 和 DFEW 数据集上有监督微调的模型进行了对比。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;540&quot; src=&quot;https://devpress.csdnimg.cn/943ca8b7e8d44a0081fa73df72d9982c.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;实验结果显示，在同分布测试集（DFEW 和 MAFW）上，R1-Omni 相较于原始基线模型平均提升超过 35%，相较于 SFT 模型在 UAR 上的提升高达 10% 以上。在不同分布测试集（RAVDESS）上，R1-Omni 同样展现了卓越的泛化能力，WAR 和 UAR 均提升超过 13%。这些结果充分证明了 RLVR 在提升推理能力和泛化性能上的显著优势。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;326&quot; src=&quot;https://devpress.csdnimg.cn/9f2621c7430d489597021a8ee642fbf5.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;02.模型效果&lt;/h2&gt;

&lt;p&gt;R1-Omni 的一大亮点在于其透明性（推理能力）。通过 RLVR 方法，音频信息和视频信息在模型中的作用变得更加清晰可见。&lt;/p&gt;

&lt;p&gt;比如，在情绪识别任务中，R1-Omni 能够明确展示哪些模态信息对特定情绪的判断起到了关键作用。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468588&quot;&gt;https://live.csdn.net/v/468588&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;51&quot; src=&quot;https://devpress.csdnimg.cn/14b7a8f7d68342a0bef2d2d671c14061.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;337&quot; src=&quot;https://devpress.csdnimg.cn/2bdddb9f9f98483cb2d8ccdb2f091983.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468589&quot;&gt;https://live.csdn.net/v/468589&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;53&quot; src=&quot;https://devpress.csdnimg.cn/aea2714c90514bbfbf0ff72da6514212.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;525&quot; src=&quot;https://devpress.csdnimg.cn/9d05bdc9be23457f869018581c62ccac.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;这种透明性不仅帮助我们更好地理解模型的决策过程，也为未来的研究提供了重要参考方向。未来，我们期待 R1-Omni 在更多复杂场景中发挥作用，为多模态任务的研究与应用开辟新的道路。&lt;/p&gt;

&lt;p&gt;点击阅读原文，即可跳转模型~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/iic/R1-Omni-0.5B&quot;&gt;R1-Omni-0.5B&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d24c4c6670175f99352f8b.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d24c4c6670175f99352f8b.html</guid><pubDate>Thu, 13 Mar 2025 03:09:00 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>“同西游，见万相” 主题LoRA风格挑战赛来袭！万元奖金池+猫超卡+限定周边来赢！</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/3fe94a35487a4776a4d2322bc12b2974.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;在通义万相Wan2.1的文生视频模型的基础上训练LoRA模型，以《西游记》的经典人物或故事为背景，描绘一个创作者心中的西游场景。参赛者根据自己的创意自定义故事内容，选择用任意的视觉风格和叙事手法进行演&lt;/blockquote&gt;   &lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;8008&quot; src=&quot;https://devpress.csdnimg.cn/6bfcd1e86c334552bab6fc32af1de8ff.png&quot; width=&quot;1408&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d248cbb8d50678a24a5d33.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d248cbb8d50678a24a5d33.html</guid><pubDate>Thu, 13 Mar 2025 02:54:03 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>今日热门论文推荐：EuroBERT、S2S-Arena、R1-Searcher</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/a6e502cfab804960a740cb71ad0f033a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;由俄罗斯多家机构提出的RuCCoD，聚焦于俄语临床编码自动化的可行性研究。俄语作为生物医学资源有限的语言，该工作构建了一个包含超过1万实体和1500+独特ICD码的电子健康记录数据集，并测试了BERT&lt;/blockquote&gt;   &lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;作者：InternLM、Qwen 等 LLM每日一览热门论文版，顶会投稿选题不迷惘。来看看机智流和modelscope社区今日推荐的热门论文吧。&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;RuCCoD: Towards Automated ICD Coding in Russian&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/121670&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由俄罗斯多家机构提出的RuCCoD，聚焦于俄语临床编码自动化的可行性研究。俄语作为生物医学资源有限的语言，该工作构建了一个包含超过1万实体和1500+独特ICD码的电子健康记录数据集，并测试了BERT、LLaMA和RAG等模型。实验表明，使用自动预测编码训练的模型显著优于医生手动标注，提升了准确性。这一成果为资源匮乏语言的临床效率和数据精度提供了宝贵洞见。&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Unified Reward Model for Multimodal Understanding and Generation&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124332&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由复旦大学、上海AI实验室等机构联手打造的UnifiedReward，提出了首个统一的多模态理解与生成评估奖励模型。针对现有任务特定模型的局限性，该模型通过联合学习图像和视频任务，利用大规模人类偏好数据集，实现了成对排序和逐点评分。实验证明，其通过直接偏好优化（DPO）显著提升了视觉模型性能，展现了跨任务协同增益的潜力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;426&quot; src=&quot;https://devpress.csdnimg.cn/c27b933236bf478bb32306af2df264f9.png&quot; width=&quot;672&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;EuroBERT: Scaling Multilingual Encoders for European Languages&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124319&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由CentraleSupélec、里斯本大学等机构研发的EuroBERT，重新审视了多语言编码器的发展，推出了一组覆盖欧洲及全球广泛使用语言的模型。EuroBERT在检索、分类和回归任务中超越现有模型，支持高达8192 token的序列，且无需位置嵌入。团队公开了模型及训练框架，为多语言研究注入新活力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;301&quot; src=&quot;https://devpress.csdnimg.cn/450145952e1749398a46dff33b0677a4.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with Paralinguistic Information&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124126&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由香港中文大学（深圳）提出的S2S-Arena，是首个关注语音到语音（S2S）指令遵循能力并融入副语言信息的基准测试。包含154个样本，覆盖4个领域21个任务，该研究揭示了现有语音模型在理解输入副语言信息上的潜力，但生成相应音频仍具挑战性，为未来多模态语音模型设计提供了方向。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;756&quot; src=&quot;https://devpress.csdnimg.cn/8ff6b9df00044639b3a15cd7e851a9fb.png&quot; width=&quot;651&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124263&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由KAIST和DeepAuto.ai提出的Sketch-of-Thought（SoT），通过认知启发的推理范式优化大语言模型的推理效率。SoT引入三种自适应范式减少76%的token使用，同时保持甚至提升数学和多跳推理的准确性。其代码已开源，展示了高效推理的巨大潜力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;594&quot; src=&quot;https://devpress.csdnimg.cn/cf478b9111af40679bbf14877a7214b7.png&quot; width=&quot;618&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;R1-Zero&#39;s &quot;Aha Moment&quot; in Visual Reasoning on a 2B Non-SFT Model&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124102&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由加州大学洛杉矶分校等机构提出的R1-Zero，在2B非SFT模型上首次复现了多模态推理中的“顿悟时刻”。基于Qwen2-VL-2B，通过强化学习，该模型在CVBench上提升约30%准确率，超越SFT设置，揭示了非监督模型在视觉推理中的潜力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;641&quot; src=&quot;https://devpress.csdnimg.cn/7805931774414184873c8456d4e3c932.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Forgetting Transformer: Softmax Attention with a Forget Gate&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/122938&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由Mila及蒙特利尔大学提出的Forgetting Transformer（FoX），通过在softmax注意力中引入遗忘门，增强了Transformer的长上下文建模能力。FoX在语言建模和下游任务中表现出色，且兼容FlashAttention，开源代码进一步推动了研究。&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124030&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由中国人民大学提出的R1-Searcher，通过两阶段强化学习框架提升大语言模型的搜索能力。无需预训练或蒸馏，该方法使模型自主调用外部搜索系统，显著超越传统RAG方法，甚至挑战GPT-4o-mini，展现了强大的泛化性。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;657&quot; src=&quot;https://devpress.csdnimg.cn/de8c3dd91dcd4fa4bf84bc30be381e5e.png&quot; width=&quot;993&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;SafeArena: Evaluating the Safety of Autonomous Web Agents&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124153&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
SafeArena由多机构合作推出，是首个评估网络代理恶意使用的基准测试。包含250个安全和250个有害任务，覆盖五大危害类别。测试显示GPT-4o等模型对恶意请求的遵从率高达34.7%，凸显了网络代理安全对齐的迫切需求。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;409&quot; src=&quot;https://devpress.csdnimg.cn/c43f0ba5e3a2416fbbe1feb5df0ad3db.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124052&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由香港中文大学、腾讯ARC Lab等提出的VideoPainter，推出了双流架构实现任意长度视频修复和编辑。通过轻量级上下文编码器和ID重采样技术，该模型在390K+片段的VPData上训练，展现了卓越的视频质量和一致性。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;596&quot; src=&quot;https://devpress.csdnimg.cn/6b1c3183e8154c35812947b7dbd20a55.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;p&gt;结语：&lt;br&gt;
今天的热点论文推荐到此结束！从俄语编码自动化到视频修复创新，每一篇论文都为AI领域带来了新的火花。🔥 您最看好哪项研究？欢迎留言讨论，一起期待AI的更多突破！🚀&lt;/p&gt;

&lt;p&gt;-- 完 --&lt;/p&gt;

&lt;p&gt;欢迎访问 https://chat.intern-ai.org.cn/&lt;/p&gt;

&lt;p&gt;和书生·浦语一起读论文&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d2482a6670175f99352a13.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d2482a6670175f99352a13.html</guid><pubDate>Thu, 13 Mar 2025 02:51:22 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>今日热门论文推荐：MM-Eureka、FedRand、EasyControl、FEA-Bench</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/c1e2e97d1b3d4adc93483ecee5c9e7fa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;由Skolkovo科技学院等机构提出的这项研究，聚焦于人工智能文本检测（ATD）的可解释性提升。利用Sparse Autoencoders（SAE）从Gemma-2-2b模型中提取特征，该工作揭示了现&lt;/blockquote&gt;   &lt;p&gt;作者：InternLM、Qwen 等 LLM每日一览热门论文版，顶会投稿选题不迷惘。快来看看由「机智流」和「ModelScope」社区推荐的今日热门论文吧！&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/123418&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由Skolkovo科技学院等机构提出的这项研究，聚焦于人工智能文本检测（ATD）的可解释性提升。利用Sparse Autoencoders（SAE）从Gemma-2-2b模型中提取特征，该工作揭示了现代大语言模型（LLM）与人类文本的差异，尤其是在信息密集领域，展现了独特的写作风格，为ATD提供了新的见解和方法。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;468&quot; src=&quot;https://devpress.csdnimg.cn/6a80bc5217dd45a9b2ea2f8fe0176d49.png&quot; width=&quot;654&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F65d72536-bcd3-49aa-9fa2-918fd8681c7f.png&amp;amp;pos_id=yOJLZ9Xa&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125314&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
中国人民大学等团队提出了SEAP，一种无需训练的剪枝方法，通过选择性地保留与任务相关的参数，降低大语言模型推理成本。实验表明，SEAP在50%剪枝率下超越WandA和FLAP超20%，展现了高效性和可扩展性，为优化大规模LLM开辟了新路径。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;362&quot; src=&quot;https://devpress.csdnimg.cn/31315d82f49249f6be2424f2186cbb8e.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2Feb88e145-601f-4e99-9e29-65997fefb645.png&amp;amp;pos_id=OyILD1Fm&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125149&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
上海AI实验室等机构推出了MM-Eureka，将基于规则的大规模强化学习（RL）扩展到多模态推理。该模型在多模态数学任务中展现了视觉“顿悟时刻”，数据效率远超传统方法，且开源了完整pipeline，推动了多模态推理研究。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;522&quot; src=&quot;https://devpress.csdnimg.cn/c910f4742de747029cef5e9141e0e18f.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2Fca723aca-b78d-40c4-8986-83f3577bc5b1.png&amp;amp;pos_id=DsOs0Avg&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125186&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
北京大学等团队提出了MMDiag数据集和DiagNote模型，旨在提升多模态大语言模型（MLLM）在多轮对话中的推理能力。受人类视觉处理启发，DiagNote通过“深思”和“注视”模块协同工作，显著提升了多模态信息整合能力。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;735&quot; src=&quot;https://devpress.csdnimg.cn/1a572082f68e4af2aa6ff82b998f45b0.png&quot; width=&quot;639&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F743fedae-ee30-411f-9e91-0898e449856f.png&amp;amp;pos_id=CuH2tbbr&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;Automated Movie Generation via Multi-Agent CoT Planning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124513&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
新加坡国立大学Show Lab提出了MovieAgent，通过多智能体Chain of Thought（CoT）规划实现自动化电影生成。给定剧本和角色库，该框架能生成连贯的多场景长视频，显著减少人工干预，开创了电影生成新范式。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;323&quot; src=&quot;https://devpress.csdnimg.cn/01ddc89e500947aca7f2b4d26fd1f303.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F9de6199a-7587-4950-9124-24d7a3e458e0.png&amp;amp;pos_id=aWXAcD19&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124596&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
由韩国研究团队开发的FedRand，通过随机选择LoRA子参数更新，增强了联邦学习（FL）的隐私保护。仅将非私有参数上传至服务器，该方法有效抵御成员推理攻击，同时保持高准确率。&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124672&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
Tiamat AI等团队提出了EasyControl，为Diffusion Transformer（DiT）引入高效灵活的控制框架。通过轻量级LoRA模块和因果注意力机制，该方法提升了图像生成的效率和多样性，适用于多种应用场景。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F6768d1a9-cfec-4171-8606-be106345a8a9.png&amp;amp;pos_id=4Dr2py2C&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;650&quot; src=&quot;https://devpress.csdnimg.cn/e8053c5cbd164ddba949f2585b83fa85.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/125249&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
微软亚洲研究院和北京大学合作推出了FEA-Bench，首个评估LLM在代码库级别增量开发能力的基准。基于83个GitHub仓库的PR数据，该基准揭示了LLM在特征实现任务中的挑战，推动了自动化软件工程研究。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;237&quot; src=&quot;https://devpress.csdnimg.cn/5bd41b3c117549dd9ab634c9e6e1160c.png&quot; width=&quot;642&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F67c447c6-5ca5-4f09-8d3a-0385ed2a08db.png&amp;amp;pos_id=RvTwuIhJ&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/124998&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
华中科技大学等团队提出了AlphaDrive，将强化学习（RL）和推理能力融入视觉-语言模型（VLM），优化自动驾驶规划。引入GRPO奖励和两阶段训练策略，该方法显著提升了规划性能和安全性。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;498&quot; src=&quot;https://devpress.csdnimg.cn/7d148e31d9a142448db7117cd3058faf.png&quot; width=&quot;690&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F0b31fb87-b2bd-483b-9582-afe103d6cda5.png&amp;amp;pos_id=DYHYmD99&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;h2&gt;SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing&lt;/h2&gt;

&lt;p&gt;论文链接：&lt;br&gt;
https://modelscope.cn/papers/123886&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;简要介绍：&lt;br&gt;
上海AI实验室等机构推出了SurveyForge，通过启发式大纲生成和记忆驱动的内容优化，自动化生成高质量综述论文。搭配SurveyBench基准，该框架在结构和引用质量上超越了AutoSurvey等现有工作。&lt;/p&gt;

&lt;p&gt;核心图片：&lt;br&gt;
&lt;img alt=&quot;&quot; height=&quot;573&quot; src=&quot;https://devpress.csdnimg.cn/abc9403202eb4693abb43a15f2cff8b1.png&quot; width=&quot;648&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;299&quot; src=&quot;https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fresouces.modelscope.cn%2Frace%2Fimage%2F0f3958d8-c54a-4332-ad3a-42f715103e32.png&amp;amp;pos_id=HIRCr4X2&quot; width=&quot;534&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
&lt;p&gt;今热门论文推荐就到这里结束啦，这些论文展现了AI在文本检测、隐私保护、自动驾驶等领域的最新突破，每一项都值得深入探索。喜欢这期内容？别忘了点赞、分享和关注我们，明天见！🌟&lt;/p&gt;

&lt;p&gt;-- 完 --&lt;/p&gt;

&lt;p&gt;欢迎在「机智流」公众号后台回复「cc」，加入机智流大模型交流群，与我们一起探索 AI 与人类潜能的未来，一起共赴 AI 浪潮！&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67d2479cd649b06b61cd5f9d.html</link><guid isPermaLink="false">https://community.modelscope.cn/67d2479cd649b06b61cd5f9d.html</guid><pubDate>Thu, 13 Mar 2025 02:49:00 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>QwQ-32B，支持Function Call的推理模型，深度思考Agent的时代来了！</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/227270e267ee4a93b28f8f8cd0bc2d43.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;近期，Qwen 发布了 QwQ-32B - 一个在许多基准测试中性能可与 DeepSeek-R1 相媲美的推理模型。&lt;/blockquote&gt;   &lt;h3&gt;00.前言&lt;/h3&gt;

&lt;p&gt;近期，Qwen 发布了 QwQ-32B - 一个在许多基准测试中性能可与 DeepSeek-R1 相媲美的推理模型。QwQ在推理模型中集成了调用工具的能力，使其能够在使用工具的同时进行批判性思考，并根据反馈调整推理过程。这样的能力使得QwQ能够很好在Agentic System中使用。本文介绍如何通过vLLM和SgLang结合QwQ-32B，搭建OpenAI格式的聊天API，并与外部函数结合来拓展模型的更多功能。&lt;/p&gt;

&lt;p&gt;tools是OpenAI的Chat Completion API中的一个可选参数，可用于提供函数调用规范（function specifications）。这样做的目的是使模型能够生成符合所提供的规范的函数参数格式。同时，API 实际上不会执行任何函数调用。开发人员需要使用模型输出来执行函数调用。&lt;/p&gt;

&lt;p&gt;vLLM和SgLang均支持OpenAI-API的tool参数。通过tool参数以及其中的函数调用规范，QwQ将能决定何时调用什么样的函数，以及怎么调用函数。&lt;/p&gt;

&lt;p&gt;注：本文测试用例参考OpenAI cookbook：https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models&lt;/p&gt;

&lt;p&gt;本文主要包含以下两个个部分：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;模型部署：使用vLLM，SgLang和QwQ，通过设置参数，部署支持Function call的聊天API接口。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;生成函数参数：指定一组函数并使用 API 生成函数参数。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt;01.模型部署&lt;/h3&gt;

&lt;p&gt;模型文件下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;modelscope download --model=Qwen/QwQ-32B --local_dir ./QwQ-32B&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;环境安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install vllm
pip install &quot;sglang[all]&amp;gt;=0.4.3.post2&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vLLM部署命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vllm serve /ModelPath/QwQ-32B \
--port 8000 \
--reasoning-parser deepseek_r1 \
--max_model_len 4096 \
--enable-auto-tool-choice \
--tool-call-parser hermes&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sglang部署命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path /ModelPath/QwQ-32B --port 3001 --host 0.0.0.0 --tool-call-parser qwen25&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;模型调用&lt;/h3&gt;

&lt;p&gt;使用OpenAI的API格式调用本地部署的QwQ模型&lt;/p&gt;

&lt;p&gt;单轮对话&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from openai import OpenAI
 
# 设置 OpenAI 的 API 密钥和 API 基础 URL 使用 vLLM 的 API 服务器。
openai_api_key = &quot;EMPTY&quot;
openai_api_base = &quot;http://localhost:8000/v1&quot;
 
client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)
 
# 使用流式输出（stream=True）
chat_response = client.chat.completions.create(
    model=&quot;path/to/QwQ-32B&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;}],
    stream=True  # 启用流式响应
)
 
# 处理流式输出
contents = []
for e in chat_response:
    # print(e.choices[0].delta.content,end=&quot;&quot;)
    contents.append(e.choices[0].delta.content)
print(&quot;&quot;.join(contents))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;137&quot; src=&quot;https://devpress.csdnimg.cn/99f06f0e37144b9fa39a1e8be1e1af24.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;多轮对话&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from openai import OpenAI
import os

# 初始化OpenAI客户端
client = OpenAI(
    api_key = &quot;empty&quot;,
    base_url=&quot;http://localhost:8000/v1&quot;
)

reasoning_content = &quot;&quot;  # 定义完整思考过程
answer_content = &quot;&quot;     # 定义完整回复
is_answering = False   # 判断是否结束思考过程并开始回复

messages = []
conversation_idx = 1
while True:
    print(&quot;=&quot;*20+f&quot;第{conversation_idx}轮对话&quot;+&quot;=&quot;*20)
    conversation_idx += 1
    user_msg = {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input(&quot;请输入你的消息：&quot;)}
    messages.append(user_msg)
    # 创建聊天完成请求
    completion = client.chat.completions.create(
       model=&quot;path/to/QwQ-32B&quot;,# 此处以 qwq-32b 为例，可按需更换模型名称
        messages=messages,
        stream=True
    )
    print(&quot;\n&quot; + &quot;=&quot; * 20 + &quot;思考过程&quot; + &quot;=&quot; * 20 + &quot;\n&quot;)
    for chunk in completion:
        # 如果chunk.choices为空，则打印usage
        if not chunk.choices:
            print(&quot;\nUsage:&quot;)
            print(chunk.usage)
        else:
            delta = chunk.choices[0].delta
            # 打印思考过程
            if hasattr(delta, &#39;reasoning_content&#39;) and delta.reasoning_content != None:
                print(delta.reasoning_content, end=&#39;&#39;, flush=True)
                reasoning_content += delta.reasoning_content
            else:
                # 开始回复
                if delta.content != &quot;&quot; and is_answering is False:
                    print(&quot;\n&quot; + &quot;=&quot; * 20 + &quot;完整回复&quot; + &quot;=&quot; * 20 + &quot;\n&quot;)
                    is_answering = True
                # 打印回复过程
                print(delta.content, end=&#39;&#39;, flush=True)
                answer_content += delta.content
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: answer_content})
    print(&quot;\n&quot;)
    # print(&quot;=&quot; * 20 + &quot;完整思考过程&quot; + &quot;=&quot; * 20 + &quot;\n&quot;)
    # print(reasoning_content)
    # print(&quot;=&quot; * 20 + &quot;完整回复&quot; + &quot;=&quot; * 20 + &quot;\n&quot;)
    # print(answer_content)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;553&quot; src=&quot;https://devpress.csdnimg.cn/a125dcf184954fda88785e7d056f7d0a.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;02.使用工具&lt;/p&gt;

&lt;p&gt;首先，定义模型调用函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from openai import OpenAI
 
# 设置 OpenAI 的 API 密钥和 API 基础 URL 使用 vLLM 的 API 服务器。
openai_api_key = &quot;EMPTY&quot;
openai_api_base = &quot;http://localhost:8000/v1&quot;
MODEL = &quot;path/to/QwQ-32B&quot;
 
client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

def chat_completion_request(messages, tools=None, tool_choice=None, model=MODEL):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice=&quot;auto&quot;,
        )
        return response
    except Exception as e:
        print(&quot;Unable to generate ChatCompletion response&quot;)
        print(f&quot;Exception: {e}&quot;)
        raise&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，我们定义一些实用工具，用于调用聊天完成 API 以及维护和跟踪对话状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def pretty_print_conversation(messages):
    role_to_color = {
        &quot;system&quot;: &quot;red&quot;,
        &quot;user&quot;: &quot;green&quot;,
        &quot;assistant&quot;: &quot;blue&quot;,
        &quot;function&quot;: &quot;magenta&quot;,
    }
    
    for message in messages:
        if message[&quot;role&quot;] == &quot;system&quot;:
            print(colored(f&quot;system: {message[&#39;content&#39;]}\n&quot;, role_to_color[message[&quot;role&quot;]]))
        elif message[&quot;role&quot;] == &quot;user&quot;:
            print(colored(f&quot;user: {message[&#39;content&#39;]}\n&quot;, role_to_color[message[&quot;role&quot;]]))
        elif message[&quot;role&quot;] == &quot;assistant&quot; and message.get(&quot;function_call&quot;):
            print(colored(f&quot;assistant: {message[&#39;function_call&#39;]}\n&quot;, role_to_color[message[&quot;role&quot;]]))
        elif message[&quot;role&quot;] == &quot;assistant&quot; and not message.get(&quot;function_call&quot;):
            print(colored(f&quot;assistant: {message[&#39;content&#39;]}\n&quot;, role_to_color[message[&quot;role&quot;]]))
        elif message[&quot;role&quot;] == &quot;function&quot;:
            print(colored(f&quot;function ({message[&#39;name&#39;]}): {message[&#39;content&#39;]}\n&quot;, role_to_color[message[&quot;role&quot;]]))&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;03.工具定义&lt;/h3&gt;

&lt;p&gt;这里假设了一个天气 API，并设置了一些函数规范和它进行交互。将这些函数规范传递给 Chat API，以便模型可以生成符合规范的函数参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tools = [
    {
        &quot;type&quot;: &quot;function&quot;,
        &quot;function&quot;: {
            &quot;name&quot;: &quot;get_current_weather&quot;,
            &quot;description&quot;: &quot;Get the current weather&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;location&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,
                    },
                    &quot;format&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                        &quot;description&quot;: &quot;The temperature unit to use. Infer this from the users location.&quot;,
                    },
                },
                &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;],
            },
        }
    },
    {
        &quot;type&quot;: &quot;function&quot;,
        &quot;function&quot;: {
            &quot;name&quot;: &quot;get_n_day_weather_forecast&quot;,
            &quot;description&quot;: &quot;Get an N-day weather forecast&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;location&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,
                    },
                    &quot;format&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                        &quot;description&quot;: &quot;The temperature unit to use. Infer this from the users location.&quot;,
                    },
                    &quot;num_days&quot;: {
                        &quot;type&quot;: &quot;integer&quot;,
                        &quot;description&quot;: &quot;The number of days to forecast&quot;,
                    }
                },
                &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;, &quot;num_days&quot;]
            },
        }
    },
]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们向模型询问当前的天气情况，它将会反问，希望获取到进一步的更多的参数信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;messages = []
messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;hi ，can you tell me what&#39;s the weather like today&quot;})
chat_response = chat_completion_request(
    messages, tools=tools
)
print(chat_response)
assistant_message = chat_response.choices[0].message
messages.append(assistant_message)
assistant_message&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦我们通过对话提供缺失的参数信息，模型就会为我们生成适当的函数参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I&#39;m in Glasgow, Scotland.&quot;})
chat_response = chat_completion_request(
    messages, tools=tools
)
assistant_message = chat_response.choices[0].message
messages.append(assistant_message)
assistant_message&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过不同的提示词，我们可以让它反问不同的问题以获取函数参数信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;messages = []
messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;can you tell me, what is the weather going to be like in Glasgow, Scotland in next x days&quot;})
chat_response = chat_completion_request(
    messages, tools=tools
)
assistant_message = chat_response.choices[0].message
messages.append(assistant_message)
assistant_message
messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;5 days&quot;})
chat_response = chat_completion_request(
    messages, tools=tools
)
chat_response.choices[0]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并行函数调用&lt;/p&gt;

&lt;p&gt;支持一次提问中，并行调用多次函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;messages = []
messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;what is the weather going to be like in San Francisco and Glasgow over the next 4 days&quot;})
chat_response = chat_completion_request(
    messages, tools=tools, model=MODEL
)

assistant_message = chat_response.choices[0].message.tool_calls
assistant_message&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;点击阅读原文 ，跳转模型合集&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/collections/QwQ-32B-0f1806b8a8514a&quot;&gt;QwQ-32B&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67cf9e1fd649b06b61cc1669.html</link><guid isPermaLink="false">https://community.modelscope.cn/67cf9e1fd649b06b61cc1669.html</guid><pubDate>Tue, 11 Mar 2025 02:21:19 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>解读 | 金融长上下文基准测试FailSafeQA：解锁金融领域LLM真实的审慎性和容错性</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/3a2080a4bb984cbfa76575616a0b6be7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;近年来，大型语言模型（LLMs）在金融领域的应用如火如荼，从风险分析到客户服务，它们正逐步改变行业的游戏规则。然而，这些模型是否真的足够“靠谱”？面对复杂的金融数据和多变的用户输入，它们还能保持精准和&lt;/blockquote&gt;   &lt;p&gt;近年来，大型语言模型（LLMs）在金融领域的应用如火如荼，从风险分析到客户服务，它们正逐步改变行业的游戏规则。然而，这些模型是否真的足够“靠谱”？面对复杂的金融数据和多变的用户输入，它们还能保持精准和稳健吗？&lt;/p&gt;

&lt;p&gt;近日，由一家在美国加州于 2023 年成立的人工智能公司 Writer Inc 的研究团队打造的 FailSafeQA 横空出世，为我们提供了一个全新的视角，去审视这些智能工具的真实实力。&lt;/p&gt;

&lt;p&gt;今天，我们以第三方观察者的身份，带你深入了解这项突破性的金融长上下文基准测试。它不仅挑战了24个主流模型的极限，还揭示了它们在“用户友好性”和“防幻觉”之间的微妙平衡。准备好了吗？让我们一探究竟！&lt;/p&gt;

&lt;p&gt;Expect the Unexpected: FailSafe Long Context QA for Finance https://arxiv.org/pdf/2502.06329 https://modelscope.cn/papers/114717&lt;/p&gt;

&lt;h2&gt;🚀 什么是 FailSafeQA？一窥金融领域的“硬核”测试&lt;/h2&gt;

&lt;p&gt;FailSafeQA 是一项专为金融场景设计的长上下文问答基准测试，由 Kiran Kamble、Melisa Russak 等专家领衔开发。它聚焦于两大核心场景：查询失败（Query Failure） 和 上下文失败（Context Failure），通过六种不同的输入变体，全面检验 LLM 在真实世界中的表现。&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;查询失败 模拟用户输入中的拼写错误、不完整查询和非专业术语，考验模型的“容错”能力。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;上下文失败 通过上传劣质OCR文档、无关文件甚至空文件，测试模型是否会“胡编乱造”。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;研究团队基于美国上市公司 10-K 年报（长达25k tokens）构建了数据集，并利用 Meta Llama 3.1 405B 等先进模型生成测试样本。最终，他们用 Qwen2.5-72B-Instruct 作为“裁判”，为24个模型打分，评估其 稳健性（Robustness）、上下文依赖性（Context Grounding） 和 合规性（Compliance）。&lt;/p&gt;

&lt;h2&gt;🔥 亮点揭秘：模型表现大比拼&lt;/h2&gt;

&lt;p&gt;FailSafeQA 的测试结果让人眼前一亮，也让人深思。以下是几个关键发现：&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;
	&lt;p&gt;Palmyra-Fin-128k-Instruct 以 0.81 的合规性得分拔得头筹。它在提供准确回答和避免幻觉之间取得了最佳平衡，但仍有 17% 的测试案例未能保持稳健预测。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;OpenAI o3-mini 是稳健性冠军，得分高达 0.90。然而，它在 41% 的案例中出现了幻觉，显示出“过于自信”的短板。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;上下文依赖性 是所有模型的普遍痛点。特别在缺失上下文时，大多数模型宁愿“胡说八道”也不愿拒绝回答，只有少数如 Qwen 和 Palmyra-Fin 能稳住阵脚。&lt;/p&gt;
	&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;613&quot; src=&quot;https://devpress.csdnimg.cn/e2a8ca3270e84b5e94b82a29888ccd67.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图1：FailSafeQA 的评估框架，展示了查询失败和上下文失败的六种测试场景，考验模型的稳健性和防幻觉能力。&lt;/p&gt;

&lt;h2&gt;🌟 为何 FailSafeQA 值得关注？&lt;/h2&gt;

&lt;p&gt;这项基准测试不仅是一次技术较量，更是为金融行业敲响了警钟。随着 LLM 被广泛用于自动化决策，模型的可靠性变得至关重要。FailSafeQA 的独特之处在于：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;真实性 它模拟了用户可能犯的各种“失误”，如拼写错误或上传错误文件，贴近实际应用场景。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;透明性 数据集和评估标准已公开，任何人都可以在 HuggingFace或者魔搭社区 上获取，助力研究复现和改进。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;前瞻性 通过引入“合规性”指标，它为未来开发更安全的金融 LLM 提供了方向。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;563&quot; src=&quot;https://devpress.csdnimg.cn/fd4c6e134d874ae29c5ebc8a87eca340.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图2：模型在稳健性和合规性上的表现对比，Palmyra-Fin-128k-Instruct 以 0.80 的上下文依赖性得分领跑。&lt;/p&gt;

&lt;h2&gt;🚀 下一步：金融 LLM 的进化之路&lt;/h2&gt;

&lt;p&gt;FailSafeQA 的结论发人深省：即使是顶尖模型，在长上下文金融任务中仍有显著的改进空间。研究指出，文本生成任务（如博客撰写）比简单问答更容易引发幻觉，或许未来的解决之道在于“先提取信息，再生成内容”的分步策略。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;407&quot; src=&quot;https://devpress.csdnimg.cn/8e77221e36f04d058df93a25fe8533f5.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图3：模型的合规性和稳健性在不同查询类型（问答（QA）和文本生成（TG））上的对比，（左）在所有模型中，鲁棒性的下降在文本生成（TG）中比在问答（QA）任务中更为突出。（右）类似的说法也适用于上下文依赖性 —— 当模型被要求生成文本（例如，博客文章）时，它更有可能忽略相关信息的缺乏并捏造细节。对于所有模型，基于错误的文档（不相关的上下文）拒绝回答比处理空上下文（例如，由于文档上传失败）更容易。&lt;/p&gt;

&lt;p&gt;对于金融从业者和技术爱好者来说，这项工作不仅揭示了 LLM 的潜力与局限，还为构建更值得信赖的智能工具指明了方向。你是否也期待看到更“靠谱”的金融 AI？不妨去 魔搭社区 或者 HuggingFace 下载数据集，自己动手试试看！&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/datasets/LLM-Research/FailSafeQA&lt;/p&gt;

&lt;h2&gt;🎯 结语：挑战已至，你敢接招吗？&lt;/h2&gt;

&lt;p&gt;FailSafeQA 用数据告诉我们：LLM 在金融领域的征途才刚刚开始。从 Palmyra-Fin 的稳健表现到 OpenAI o3-mini 的“冒险”尝试，每一个结果都在提醒我们——技术虽强大，但信任需谨慎。🔥 你准备好加入这场探索了吗？让我们一起期待更智能、更安全的金融未来！&lt;/p&gt;

&lt;p&gt;想了解更多细节？欢迎阅读原文：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/papers/114717&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67cf9da7d649b06b61cc165c.html</link><guid isPermaLink="false">https://community.modelscope.cn/67cf9da7d649b06b61cc165c.html</guid><pubDate>Tue, 11 Mar 2025 02:19:19 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>魔搭社区模型速递（3.2-3.8）</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/4e837b6ac5e245a587a13e9f6599cbd6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;魔搭ModelScope本期社区进展：1340个模型,220个数据集,91个创新应用, 8篇内容&lt;/blockquote&gt;   &lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;608&quot; src=&quot;https://devpress.csdnimg.cn/403823044efe49f98a1fb9caca8fc090.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;🙋魔搭ModelScope本期社区进展：&lt;/p&gt;

&lt;p&gt;📟1340个模型：通义千问 QwQ-32B、HunyuanVideo-I2V、CogView4-6B、Phi-4系列模型等；&lt;/p&gt;

&lt;p&gt;📁220个数据集：Big-Math-RL-UNVERIFIED、msmarco-msmarco-MiniLM-L6-v3、KodCode-V1等；&lt;/p&gt;

&lt;p&gt;🎨91个创新应用：CogView4、QwQ-32B-Demo、推理模型大作战（QwQ-32B vs DeepSeek-R1）等；&lt;/p&gt;

&lt;p&gt;📄 8篇内容：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;腾讯开源HunyuanVideo-I2V图生视频模型+LoRA训练脚本，社区部署、推理实战教程来啦！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;QwQ-32B开源！更小尺寸，仅1/20参数性能比肩满血R1&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;微软Phi-4系列开源：多模态与文本处理的创新突破&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;打造跨语言智能工具与应用，“万卷·丝路”专项课题开放申请&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;CogView4开源发布！智谱AI文生图模型支持任意长度双语输入，汉字生成能力突出，可商用！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;CLIPer：开创性框架提升CLIP空间表征，实现开放词汇语义分割突破&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;高效部署通义万相Wan2.1：ComfyUI文生/图生视频实战，工作流直取！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;高效部署通义万相Wan2.1：使用Gradio搭建WebUI体验实战&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;01.精选模型&lt;/h2&gt;

&lt;p&gt;通义千问 QwQ-32B&lt;/p&gt;

&lt;p&gt;QwQ-32B是通义千问团队最新开源的推理模型，以320亿参数实现媲美更大模型的性能，其亮点包括：&lt;/p&gt;

&lt;p&gt;推理能力突出 ：数学测试（AIME24）与DeepSeek-R1持平，编程（LiveCodeBench）接近其水平，通用能力在LiveBench等评测中更超越DeepSeek-R1；&lt;/p&gt;

&lt;p&gt;分阶段强化学习 ：初期通过数学答案校验与代码执行反馈优化，后期结合通用奖励模型训练，兼顾多领域性能；&lt;/p&gt;

&lt;p&gt;轻量化部署 ：支持单卡（如3090/M4 Max）运行，降低硬件门槛；&lt;/p&gt;

&lt;p&gt;开源标杆价值 ：以1/20的参数规模（对比DeepSeek-R1）验证强化学习的效率提升，为开发者提供高性价比选择。&lt;/p&gt;

&lt;p&gt;模型合集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/collections/QwQ-32B-0f1806b8a8514a&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;Qwen/QwQ-32B&quot;

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = &quot;How many r&#39;s are in the word \&quot;strawberry\&quot;&quot;
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于模型更多部署、推理、微调实战教程详见：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501370&amp;amp;idx=1&amp;amp;sn=3d391feb659fce99a2787879771f1578&amp;amp;scene=21#wechat_redirect&quot;&gt;QwQ-32B开源！更小尺寸，仅1/20参数性能比肩满血R1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;HunyuanVideo-I2V图生视频模型&lt;/p&gt;

&lt;p&gt;腾讯混元重磅发布并开源HunyuanVideo-I2V图生视频模型。该模型基于HunyuanVideo文生视频基础模型，利用基础模型先进的视频生成能力，将应用扩展到图像到视频的生成任务，还同步开源了LoRA训练代码，用于定制化特效生成，可创建更有趣的视频效果。&lt;/p&gt;

&lt;p&gt;研究团队采用图像潜在连接技术，重建参考图像信息并纳入视频生成过程，同时使用预训练的Decoder-Only架构多模态大语言模型作为文本编码器，增强模型对输入图像语义内容的理解，实现图像与文本描述信息的深度融合。&lt;/p&gt;

&lt;p&gt;模型地址：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/models/AI-ModelScope/HunyuanVideo-i2v&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;提供本地推理运行HunyuanVideo-I2V方案，硬件要求如下&lt;/p&gt;

&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;模型&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;分辨率&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;GPU显存峰值&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;HunyuanVideo-I2V&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;720p&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;60GB&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;需配备支持CUDA的NVIDIA GPU&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;测试环境为单卡80G GPU&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;最低要求: 720p分辨率需至少60GB显存&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;推荐配置: 建议使用80GB显存GPU以获得更佳生成质量&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;测试操作系统：Linux&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;克隆代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/tencent/HunyuanVideo-I2V
cd HunyuanVideo-I2V&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
pip install ninja
pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;混元图生视频包括三个模型，基础模型hunyuan-video-i2v-720p和两个文本编码器（text_encoder_i2v，text_encoder_2）。模型下载后默认放在HunyuanVideo-I2V/ckpts文件夹下，文件结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HunyuanVideo-I2V
  ├──ckpts
  │  ├──README.md
  │  ├──hunyuan-video-i2v-720p
  │  │  ├──transformers
  │  │  │  ├──mp_rank_00_model_states.pt
  ├  │  ├──vae
  ├  │  ├──lora
  │  │  │  ├──embrace_kohaya_weights.safetensors
  │  │  │  ├──hair_growth_kohaya_weights.safetensors
  │  ├──text_encoder_i2v
  │  ├──text_encoder_2
  ├──...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;魔搭上可以下载到这三个模型，下载命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd HunyuanVideo-I2V

# 下载基础模型
modelscope download --model AI-ModelScope/HunyuanVideo-I2V --local_dir ./ckpts

# 下载文本编码器MLLM
modelscope download --model AI-ModelScope/llava-llama-3-8b-v1_1-transformers --local_dir ./ckpts/text_encoder_i2v

# 下载文本编码器CLIP
modelscope download --model AI-ModelScope/clip-vit-large-patch14 --local_dir ./ckpts/text_encoder_2&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd HunyuanVideo-I2V

python3 sample_image2video.py \
    --model HYVideo-T/2 \
    --prompt &quot;A man with short gray hair plays a red electric guitar.&quot; \
    --i2v-mode \
    --i2v-image-path ./assets/demo/i2v/imgs/0.png \
    --i2v-resolution 720p \
    --video-length 129 \
    --infer-steps 50 \
    --flow-reverse \
    --flow-shift 17.0 \
    --seed 0 \
    --use-cpu-offload \
    --save-path ./results&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;耗时：50步，生成1280*704分辨率5秒的视频，A100，大概需要50分钟&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;30&quot; src=&quot;https://devpress.csdnimg.cn/83e1e742db6d4064bb27a1bf0ff13237.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;显存占用：约60G&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;124&quot; src=&quot;https://devpress.csdnimg.cn/2424431bae0e40bf919897571e553641.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;关于模型更多部署、推理实战教程详见：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501410&amp;amp;idx=1&amp;amp;sn=a1d1306c056943142dd9bcd2d30ac584&amp;amp;scene=21#wechat_redirect&quot;&gt;腾讯开源HunyuanVideo-I2V图生视频模型+LoRA训练脚本，社区部署、推理实战教程来啦！&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CogView4-6B&lt;/p&gt;

&lt;p&gt;智谱AI正式发布并开源了最新的图像生成模型CogView4，该模型具备复杂语义对齐和指令跟随能力，支持任意长度中英双语输入，可生成任意分辨率图像，文字生成能力出色，尤其在汉字生成方面表现突出。&lt;/p&gt;

&lt;p&gt;在DPG-Bench基准测试中综合评分排名第一，达到开源文生图模型的SOTA水平。模型采用二维旋转位置编码、Flow-matching扩散生成建模、多阶段训练策略等技术，突破了传统模型在文本长度和图像分辨率上的限制，适合国内广告、短视频等领域的创意需求，且也是首个遵循 Apache 2.0协议开源的图像生成模型。&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/models/ZhipuAI/CogView4-6B&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;安装依赖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/diffusers.git&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from diffusers import CogView4Pipeline
from modelscope import snapshot_download
import torch

model_dir = snapshot_download(&quot;ZhipuAI/CogView4-6B&quot;)
pipe = CogView4Pipeline.from_pretrained(model_dir, torch_dtype=torch.bfloat16)

# Open it for reduce GPU memory usage
pipe.enable_model_cpu_offload()
pipe.vae.enable_slicing()
pipe.vae.enable_tiling()

prompt = &quot;A vibrant cherry red sports car sits proudly under the gleaming sun, its polished exterior smooth and flawless, casting a mirror-like reflection. The car features a low, aerodynamic body, angular headlights that gaze forward like predatory eyes, and a set of black, high-gloss racing rims that contrast starkly with the red. A subtle hint of chrome embellishes the grille and exhaust, while the tinted windows suggest a luxurious and private interior. The scene conveys a sense of speed and elegance, the car appearing as if it&#39;s about to burst into a sprint along a coastal road, with the ocean&#39;s azure waves crashing in the background.&quot;
image = pipe(
    prompt=prompt,
    guidance_scale=3.5,
    num_images_per_prompt=1,
    num_inference_steps=50,
    width=1024,
    height=1024,
).images[0]

image.save(&quot;cogview4.png&quot;)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phi-4系列&lt;/p&gt;

&lt;p&gt;微软最新开源的Phi-4系列模型，包括Phi-4-multimodal和Phi-4-mini。Phi-4-multimodal是一个5.6B参数的多模态语言模型，能同时处理语音、视觉和文本，为创建具有上下文感知能力的应用程序提供新可能；Phi-4-mini则是一个3.8B参数的紧凑模型，专为提高速度和效率设计，在基于文本的任务中表现出色。&lt;/p&gt;

&lt;p&gt;模型合集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/collections/phi-4-4ce2630c1b664f&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;Phi-4-mini-instruct推理代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from vllm import LLM, SamplingParams

llm = LLM(model=&quot;LLM-Research/Phi-4-mini-instruct&quot;, trust_remote_code=True)

messages = [
 {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful AI assistant.&quot;},
 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Can you provide ways to eat combinations of bananas and dragonfruits?&quot;},
 {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.&quot;},
 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What about solving an 2x + 3 = 7 equation?&quot;},
]

sampling_params = SamplingParams(
 max_tokens=500,
 temperature=0.0,
)

output = llm.chat(messages=messages, sampling_params=sampling_params)
print(output[0].outputs[0].text)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phi-4-multimodal-instruct推理代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import requests
import torch
import os
import io
from PIL import Image
import soundfile as sf
from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from urllib.request import urlopen
from modelscope import snapshot_download


# Define model path
model_path = snapshot_download(&quot;LLM-Research/Phi-4-multimodal-instruct&quot;)

# Load model and processor
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    device_map=&quot;cuda&quot;, 
    torch_dtype=&quot;auto&quot;, 
    trust_remote_code=True, 
    #attn_implementation=&#39;flash_attention_2&#39;,
).cuda()

# Load generation config
generation_config = GenerationConfig.from_pretrained(model_path)

# Define prompt structure
user_prompt = &#39;&amp;lt;|user|&amp;gt;&#39;
assistant_prompt = &#39;&amp;lt;|assistant|&amp;gt;&#39;
prompt_suffix = &#39;&amp;lt;|end|&amp;gt;&#39;

# Part 1: Image Processing
print(&quot;\n--- IMAGE PROCESSING ---&quot;)
image_url = &#39;https://www.ilankelman.org/stopsigns/australia.jpg&#39;
prompt = f&#39;{user_prompt}&amp;lt;|image_1|&amp;gt;What is shown in this image?{prompt_suffix}{assistant_prompt}&#39;
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Prompt\n{prompt}&#39;)

# Download and open image
image = Image.open(requests.get(image_url, stream=True).raw)
inputs = processor(text=prompt, images=image, return_tensors=&#39;pt&#39;).to(&#39;cuda:0&#39;)

# Generate response
generate_ids = model.generate(
    **inputs,
    max_new_tokens=1000,
    generation_config=generation_config,
)
generate_ids = generate_ids[:, inputs[&#39;input_ids&#39;].shape[1]:]
response = processor.batch_decode(
    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Response\n{response}&#39;)

# Part 2: Audio Processing
print(&quot;\n--- AUDIO PROCESSING ---&quot;)
audio_url = &quot;https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac&quot;
speech_prompt = &quot;Transcribe the audio to text, and then translate the audio to French. Use &amp;lt;sep&amp;gt; as a separator between the original transcript and the translation.&quot;
prompt = f&#39;{user_prompt}&amp;lt;|audio_1|&amp;gt;{speech_prompt}{prompt_suffix}{assistant_prompt}&#39;
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Prompt\n{prompt}&#39;)

# Downlowd and open audio file
audio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))

# Process with the model
inputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors=&#39;pt&#39;).to(&#39;cuda:0&#39;)

generate_ids = model.generate(
    **inputs,
    max_new_tokens=1000,
    generation_config=generation_config,
)
generate_ids = generate_ids[:, inputs[&#39;input_ids&#39;].shape[1]:]
response = processor.batch_decode(
    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Response\n{response}&#39;)&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;更多Phi-4系列模型微调实战教程，详见：&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501359&amp;amp;idx=1&amp;amp;sn=f49132bceba83e4a89192eba4f7a96af&amp;amp;scene=21#wechat_redirect&quot;&gt;微软Phi-4系列开源：多模态与文本处理的创新突破&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;02.数据集推荐&lt;/p&gt;

&lt;p&gt;Big-Math-RL-UNVERIFIED&lt;/p&gt;

&lt;p&gt;Big-Math-RL-UNVERIFIED 是一个专注于数学问题的高质量数据集，包含超过25万道数学题目及其可验证的解题过程，专为强化学习（Reinforcement Learning, RL）在语言模型中的应用而设计。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/datasets/SynthLabsAI/Big-Math-RL-UNVERIFIED&lt;/p&gt;

&lt;p&gt;msmarco-msmarco-MiniLM-L6-v3&lt;/p&gt;

&lt;p&gt;msmarco-MiniLM-L6-v3是一个基于 Sentence-Transformers 的预训练语言模型，专为文本嵌入和语义相似性任务设计。它在 MS MARCO数据集上进行了微调，适用于信息检索、文本匹配和语义搜索等任务。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/sentence-transformers/msmarco-MiniLM-L6-v3&lt;/p&gt;

&lt;p&gt;KodCode-V1&lt;/p&gt;

&lt;p&gt;KodCode 是最大的全合成开源数据集，为编码任务提供可验证的解决方案和测试。它包含 12 个不同的子集，涵盖各个领域（从算法到包特定的知识）和难度级别（从基本编码练习到面试和竞争性编程挑战）。KodCode 专为监督式微调 （SFT） 和 RL 优化而设计。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/datasets/AI-ModelScope/KodCode-V1&lt;/p&gt;

&lt;h2&gt;03.精选应用&lt;/h2&gt;

&lt;p&gt;CogView4&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/ZhipuAI/CogView4&lt;/p&gt;

&lt;p&gt;QwQ-32B-Demo&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/Qwen/QwQ-32B-Demo&lt;/p&gt;

&lt;p&gt;推理模型大作战（QwQ-32B vs DeepSeek-R1）&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/AI-ModelScope/QwQ-32B-vs-DeepSeek-R1&lt;/p&gt;

&lt;h2&gt;04.社区精选文章&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501410&amp;amp;idx=1&amp;amp;sn=a1d1306c056943142dd9bcd2d30ac584&amp;amp;scene=21#wechat_redirect&quot;&gt;腾讯开源HunyuanVideo-I2V图生视频模型+LoRA训练脚本，社区部署、推理实战教程来啦！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501370&amp;amp;idx=1&amp;amp;sn=3d391feb659fce99a2787879771f1578&amp;amp;scene=21#wechat_redirect&quot;&gt;QwQ-32B开源！更小尺寸，仅1/20参数性能比肩满血R1&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501359&amp;amp;idx=1&amp;amp;sn=f49132bceba83e4a89192eba4f7a96af&amp;amp;scene=21#wechat_redirect&quot;&gt;微软Phi-4系列开源：多模态与文本处理的创新突破&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501359&amp;amp;idx=2&amp;amp;sn=418d9a75193ea2bcceda85529f6b4c4a&amp;amp;scene=21#wechat_redirect&quot;&gt;打造跨语言智能工具与应用，“万卷·丝路”专项课题开放申请&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501332&amp;amp;idx=1&amp;amp;sn=d561179c349789e0b069b3581ce3f840&amp;amp;scene=21#wechat_redirect&quot;&gt;CogView4开源发布！智谱AI文生图模型支持任意长度双语输入，汉字生成能力突出，可商用！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501332&amp;amp;idx=2&amp;amp;sn=7c372aaf57ba2998670fd1167c93576c&amp;amp;scene=21#wechat_redirect&quot;&gt;CLIPer：开创性框架提升CLIP空间表征，实现开放词汇语义分割突破&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501306&amp;amp;idx=1&amp;amp;sn=15c8d846a2f777924b994a216c44cb9c&amp;amp;scene=21#wechat_redirect&quot;&gt;高效部署通义万相Wan2.1：ComfyUI文生/图生视频实战，工作流直取！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501306&amp;amp;idx=2&amp;amp;sn=1620df93ef849e3cc02c6582b4cf0221&amp;amp;scene=21#wechat_redirect&quot;&gt;高效部署通义万相Wan2.1：使用Gradio搭建WebUI体验实战&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;
 </description><link>https://community.modelscope.cn/67ce591c807ce562bfdda3b7.html</link><guid isPermaLink="false">https://community.modelscope.cn/67ce591c807ce562bfdda3b7.html</guid><pubDate>Mon, 10 Mar 2025 03:14:36 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>腾讯开源HunyuanVideo-I2V图生视频模型+LoRA训练脚本，社区部署、推理实战教程来啦！</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/684b9578b9af402ca02fdcd5cbabddc4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;继阿里的通义万相wan2.1模型之后，腾讯混元又出大招，重磅发布HunyuanVideo-I2V图生视频模型。&lt;/blockquote&gt;   &lt;h2&gt;01.引言&lt;/h2&gt;

&lt;p&gt;继阿里的通义万相wan2.1模型之后，腾讯混元又出大招，重磅发布HunyuanVideo-I2V图生视频模型。该模型基于HunyuanVideo文生视频基础模型，利用基础模型先进的视频生成能力，将应用扩展到图像到视频的生成任务。混元研究团队还同步开源了LoRA训练代码，用于定制化特效生成，可创建更有趣的视频效果。&lt;/p&gt;

&lt;p&gt;开源内容：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;HunyuanVideo-I2V的推理代码&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;HunyuanVideo-I2V的模型权重&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;LoRA训练脚本&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;代码仓库：&lt;/p&gt;

&lt;p&gt;https://github.com/Tencent/HunyuanVideo-I2V&lt;/p&gt;

&lt;p&gt;模型地址：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/models/AI-ModelScope/HunyuanVideo-i2v/&lt;/p&gt;

&lt;h2&gt;02.整体架构&lt;/h2&gt;

&lt;p&gt;为利用HunyuanVideo强大的视频生成能力，研究团队采用图像潜在连接技术来有效地重建参考图像信息，并将其纳入视频生成过程。&lt;/p&gt;

&lt;p&gt;由于使用预训练的Decoder-Only架构多模态大语言模型（MLLM）作为文本编码器，可用于显著增强模型对输入图像语义内容的理解能力，并实现图像与文本描述信息的深度融合。具体而言，输入图像经MLLM处理后生成语义图像tokens，这些tokens与视频隐空间tokens拼接，实现跨模态的全注意力计算。&lt;/p&gt;

&lt;p&gt;整个系统架构旨在最大化图像与文本模态的协同效应，确保从静态图像生成连贯的视频内容。该集成不仅提升了生成视频的保真度，还增强了模型对复杂多模态输入的解析能力。整体架构如下图所示：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1199&quot; src=&quot;https://devpress.csdnimg.cn/eff0d0d2cc9b4fd0849c4737c2f7128d.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;03.本地推理实践&lt;/h2&gt;

&lt;p&gt;运行要求&lt;/p&gt;

&lt;p&gt;下表展示了运行HunyuanVideo-I2V模型（batch size=1）生成视频的硬件要求：&lt;/p&gt;

&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;模型&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;分辨率&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;GPU显存峰值&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;HunyuanVideo-I2V&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;720p&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;60GB&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;需配备支持CUDA的NVIDIA GPU&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;测试环境为单卡80G GPU&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;最低要求: 720p分辨率需至少60GB显存&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;推荐配置: 建议使用80GB显存GPU以获得更佳生成质量&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;测试操作系统：Linux&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;克隆代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/tencent/HunyuanVideo-I2V
cd HunyuanVideo-I2V&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
pip install ninja
pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;模型下载&lt;/p&gt;

&lt;p&gt;混元图生视频包括三个模型，基础模型hunyuan-video-i2v-720p和两个文本编码器（text_encoder_i2v，text_encoder_2）。模型下载后默认放在HunyuanVideo-I2V/ckpts文件夹下，文件结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HunyuanVideo-I2V
  ├──ckpts
  │  ├──README.md
  │  ├──hunyuan-video-i2v-720p
  │  │  ├──transformers
  │  │  │  ├──mp_rank_00_model_states.pt
  ├  │  ├──vae
  ├  │  ├──lora
  │  │  │  ├──embrace_kohaya_weights.safetensors
  │  │  │  ├──hair_growth_kohaya_weights.safetensors
  │  ├──text_encoder_i2v
  │  ├──text_encoder_2
  ├──...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;魔搭平台上可以下载到这三个模型，下载命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd HunyuanVideo-I2V

# 下载基础模型
modelscope download --model AI-ModelScope/HunyuanVideo-I2V --local_dir ./ckpts

# 下载文本编码器MLLM
modelscope download --model AI-ModelScope/llava-llama-3-8b-v1_1-transformers --local_dir ./ckpts/text_encoder_i2v

# 下载文本编码器CLIP
modelscope download --model AI-ModelScope/clip-vit-large-patch14 --local_dir ./ckpts/text_encoder_2&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd HunyuanVideo-I2V

python3 sample_image2video.py \
    --model HYVideo-T/2 \
    --prompt &quot;A man with short gray hair plays a red electric guitar.&quot; \
    --i2v-mode \
    --i2v-image-path ./assets/demo/i2v/imgs/0.png \
    --i2v-resolution 720p \
    --video-length 129 \
    --infer-steps 50 \
    --flow-reverse \
    --flow-shift 17.0 \
    --seed 0 \
    --use-cpu-offload \
    --save-path ./results&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;耗时：50步，生成1280*704分辨率5秒的视频，A100，大概需要50分钟&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;30&quot; src=&quot;https://devpress.csdnimg.cn/8072fba826924d22bc4046614dbb4e9c.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;显存占用：约60G&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;124&quot; src=&quot;https://devpress.csdnimg.cn/96eec8ad90ea4a10890538720b6e6dbe.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;测试case：&lt;/p&gt;

&lt;p&gt;提示词：A man with short gray hair plays a red electric guitar.&lt;/p&gt;

&lt;p&gt;输入的图片：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;512&quot; src=&quot;https://devpress.csdnimg.cn/1876a58d7db94c34a1fba9287680c160.png&quot; width=&quot;928&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/468004&quot;&gt;https://live.csdn.net/v/468004&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;💡写图生视频模型提示词(prompt)的建议：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;使用简短的提示：为了有效地引导模型的生成，请保持提示简短且直截了当。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;包含关键元素：一个结构良好的提示应包括：&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;主体：指定视频的主要焦点。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;动作：描述正在发生的运动或活动。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;背景（可选）：设置视频的场景。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;镜头（可选）：指示视角或视点。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;避免过于详细的提示：冗长或高度详细的提示可能会导致视频输出中出现不必要的转场。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;04.ComfyUI推理实践&lt;/h2&gt;

&lt;p&gt;ComfyUI官方在第一时间支持了混元的图生视频模型，小编也带大家动手玩玩混元的图生视频工作流。&lt;/p&gt;

&lt;p&gt;不久前通义万相Wan2.1发布时，&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501306&amp;amp;idx=1&amp;amp;sn=15c8d846a2f777924b994a216c44cb9c&amp;amp;scene=21#wechat_redirect&quot;&gt;魔搭社区出过一篇实战教程，如果你是小白同学，请先预习一下这一篇，其他同学请直接开始。&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501306&amp;amp;idx=1&amp;amp;sn=15c8d846a2f777924b994a216c44cb9c&amp;amp;scene=21#wechat_redirect&quot;&gt;《&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501306&amp;amp;idx=1&amp;amp;sn=15c8d846a2f777924b994a216c44cb9c&amp;amp;scene=21#wechat_redirect&quot;&gt;高效部署通义万相Wan2.1：ComfyUI文生/图生视频实战，工作流直取！&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501306&amp;amp;idx=1&amp;amp;sn=15c8d846a2f777924b994a216c44cb9c&amp;amp;scene=21#wechat_redirect&quot;&gt;》&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;更新ComfyUI、下载模型、打开ComfyUI服务后，拖入工作流就可以直接运行。&lt;/p&gt;

&lt;p&gt;更新ComfyUI&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ComfyUI
git pull
git status # 确认本地代码是否与master分支一致&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载模型&lt;/p&gt;

&lt;p&gt;魔搭平台上模型下载地址：https://modelscope.cn/models/Comfy-Org/HunyuanVideo_repackaged/files&lt;/p&gt;

&lt;p&gt;模型下载命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ComfyUI/models
modelscope download --model Comfy-Org/HunyuanVideo_repackaged --local_dir .&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;模型下载好后分别把模型挪到ComfyUI的对应目录，文件结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── clip_vision/
│   └── llava_llama3_vision.safetensors
├── text_encoders/
│   ├── clip_l.safetensors
│   ├── llava_llama3_fp16.safetensors
│   └── llava_llama3_fp8_scaled.safetensors
├── vae/
│   └── hunyuan_video_vae_bf16.safetensors
└── diffusion_models/
    └── hunyuan_video_image_to_video_720p_bf16.safetensors
mv split_files/clip_vision/llava_llama3_vision.safetensors  clip_vision/
mv split_files/text_encoders/* text_encoders/
mv split_files/vae/hunyuan_video_vae_bf16.safetensors vae/
mv split_files/diffusion_models/hunyuan_video_* diffusion_models/&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动ComfyUI&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ComfyUI
python main.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行示例工作流&lt;/p&gt;

&lt;p&gt;将示例robot工作流的json文件拖入ComfyUI，点击执行即可运行工作流&lt;/p&gt;

&lt;p&gt;robot图生视频工作流文件：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/notebook/share/ipynb/b8a3efeb/robot.ipynb&lt;/p&gt;

&lt;p&gt;输入图片：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1024&quot; src=&quot;https://devpress.csdnimg.cn/a103b3a98a4b4df993212c6c45ce478a.png&quot; width=&quot;1024&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;549&quot; src=&quot;https://devpress.csdnimg.cn/bae92197aebd46e3b3ee6bddb022fa9c.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;显存与耗时&lt;/p&gt;

&lt;p&gt;测试了两种尺寸分辨率的显存占用与耗时数据，如下表：&lt;/p&gt;

&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;分辨率&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;显存峰值&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;生成时间&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;512*512&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;42G&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;～1min&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;1024*1024&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;48G&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;～5min&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;如遇到显存不足可以把clip2切换为fp8版本。另外ComfyUI官方满血版视频生成速度比官方推理代码快太多了，强烈推荐直接使用ComfyUI进行推理！&lt;/p&gt;

&lt;p&gt;相关链接：https://github.com/Tencent/HunyuanVideo-I2V/tree/main?tab=readme-ov-file&lt;/p&gt;

&lt;p&gt;点击链接即可跳转模型链接~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/AI-ModelScope/HunyuanVideo-i2v/&quot;&gt;tencent/HunyuanVideo-I2V 腾讯混元图生视频&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67ce58b5b8d50678a24873f8.html</link><guid isPermaLink="false">https://community.modelscope.cn/67ce58b5b8d50678a24873f8.html</guid><pubDate>Mon, 10 Mar 2025 03:12:53 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>QwQ-32B开源！更小尺寸，仅1/20参数性能比肩满血R1</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/0386fcb2ebe745e5a4713c683741fd67.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;QwQ-32B 在一系列基准测试中进行了评估，测试了数学推理、编程能力和通用能力。&lt;/blockquote&gt;   &lt;h2&gt;01.模型介绍&lt;/h2&gt;

&lt;p&gt;今天，通义千问开源了推理模型QwQ-32B&lt;/p&gt;

&lt;p&gt;QwQ-32B 在一系列基准测试中进行了评估，测试了数学推理、编程能力和通用能力。以下结果展示了 QwQ-32B 与其他领先模型的性能对比，包括 DeepSeek-R1-Distilled-Qwen-32B、DeepSeek-R1-Distilled-Llama-70B、o1-mini 以及原始的 DeepSeek-R1。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;609&quot; src=&quot;https://devpress.csdnimg.cn/66a13c16995147ea842636ca5df0c8b8.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;在测试数学能力的 AIME24 评测集上，以及评估代码能力的 LiveCodeBench 中，千问 QwQ-32B 表现与DeepSeek-R1相当，远胜于 o1-mini 及相同尺寸的R1 蒸馏模型；在由Meta首席科学家杨立昆领衔的“最难LLMs评测榜” LiveBench、谷歌等提出的指令遵循能力IFEval评测集、由加州大学伯克利分校等提出的评估准确调用函数或工具方面的BFCL测试中，千问 QwQ-32B 的得分均超越了 DeepSeek- R1。&lt;/p&gt;

&lt;p&gt;大规模强化学习&lt;/p&gt;

&lt;p&gt;研究团队在冷启动的基础上开展了大规模强化学习。在初始阶段，特别针对数学和编程任务进行了 RL 训练。与依赖传统的奖励模型（reward model）不同，研究团队通过校验生成答案的正确性来为数学问题提供反馈，并通过代码执行服务器评估生成的代码是否成功通过测试用例来提供代码的反馈。&lt;/p&gt;

&lt;p&gt;研究团队发现在 RL 扩展过程中，随着训练轮次的推进，这两个领域中的性能均表现出持续的提升。&lt;/p&gt;

&lt;p&gt;在第一阶段的 RL 过后，研究团队增加了另一个针对通用能力的 RL。此阶段使用通用奖励模型和一些基于规则的验证器进行训练。研究团队发现，通过少量步骤的通用 RL，可以提升其他通用能力，同时在数学和编程任务上的性能没有显著下降。&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/collections/QwQ-32B-0f1806b8a8514a&lt;/p&gt;

&lt;p&gt;体验空间：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/Qwen/QwQ-32B-Demo&lt;/p&gt;

&lt;h2&gt;02.模型推理&lt;/h2&gt;

&lt;p&gt;Transformers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;Qwen/QwQ-32B&quot;

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = &quot;How many r&#39;s are in the word \&quot;strawberry\&quot;&quot;
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;魔搭API-Inference直接调用&lt;/h3&gt;

&lt;p&gt;魔搭平台的API-Inference，也第一时间为QwQ-32B模型提供了支持。魔搭的用户可通过API调用的方式直接使用该模型。代码范例在模型页面(https://www.modelscope.cn/models/Qwen/QwQ-32B) 右侧API-Inference入口可以直接获取。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;685&quot; src=&quot;https://devpress.csdnimg.cn/508c1cd0a545428494b5e10181044407.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;实现通过API接口直接调用QwQ-32B：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;2095&quot; src=&quot;https://devpress.csdnimg.cn/7c92c65ac87245779de1d6d368d223cf.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Ollama本地拉起&lt;/h3&gt;

&lt;p&gt;通过Ollama与魔搭平台的整合，开发者也可以直接在本地的Ollama环境，直接运行QwQ-32B模型：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ollama run modelscope.cn/Qwen/QwQ-32B-GGUF&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更详细的调用选项和使用方法可参考文档：https://modelscope.cn/docs/models/advanced-usage/ollama-integration，具体运行结果如下：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;756&quot; src=&quot;https://devpress.csdnimg.cn/a5bbacc658f842d38976df51e3a68045.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h3&gt;模型微调&lt;/h3&gt;

&lt;p&gt;ms-swift已经支持了QwQ-32B的训练到部署。ms-swift是魔搭社区官方提供的大模型与多模态大模型训练部署框架。&lt;/p&gt;

&lt;p&gt;ms-swift开源地址：&lt;/p&gt;

&lt;p&gt;https://github.com/modelscope/ms-swift&lt;/p&gt;

&lt;p&gt;我们展示对QwQ-32B进行微调的demo，并给出自定义数据集的格式。&lt;/p&gt;

&lt;p&gt;在开始微调之前，请确保您的环境已准备妥当。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pip install git+https://github.com/modelscope/ms-swift.git

git clone https://github.com/modelscope/ms-swift.git
cd ms-swift
pip install -e .&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先我们使用QWQ-32B蒸馏部分数据，保持其思考的能力，将蒸馏的数据保存在本地路径：qwq-32b-distill.jsonl。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1 \
swift infer \
    --model Qwen/QwQ-32B \
    --infer_backend vllm \
    --val_dataset &#39;AI-ModelScope/alpaca-gpt4-data-zh#1000&#39; &#39;AI-ModelScope/alpaca-gpt4-data-en#1000&#39; \
    --gpu_memory_utilization 0.9 \
    --max_model_len 32768 \
    --max_new_tokens 8192 \
    --result_path qwq-32b-distill.jsonl \
    --tensor_parallel_size 2 \
    --disable_custom_all_reduce true&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;微调脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NPROC_PER_NODE=2 \
CUDA_VISIBLE_DEVICES=0,1 \
swift sft \
    --model Qwen/QwQ-32B \
    --train_type lora \
    --dataset &#39;qwq-32b-distill.jsonl&#39; \
              &#39;&amp;lt;your-dataset-path&amp;gt;&#39; \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 8 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 5 \
    --logging_steps 5 \
    --max_length 4096 \
    --output_dir output \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --model_author swift \
    --model_name swift-robot \
    --deepspeed zero2&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自定义数据集可以参考以下格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是个有用无害的助手&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;告诉我明天的天气&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&amp;lt;think&amp;gt;\n...&amp;lt;/think&amp;gt;\n\n明天天气晴朗&quot;}]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是个有用无害的数学计算器&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;1+1等于几&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&amp;lt;think&amp;gt;\n...&amp;lt;/think&amp;gt;\n\n等于2&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;再加1呢&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&amp;lt;think&amp;gt;\n...&amp;lt;/think&amp;gt;\n\n等于3&quot;}]}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;训练显存占用：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;204&quot; src=&quot;https://devpress.csdnimg.cn/e301c9873fcb47ccafb71e0b584f82c1.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;训练完成后，使用以下命令对训练后的权重进行推理，这里的`--adapters`需要替换成训练生成的last checkpoint文件夹。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream true \
    --max_new_tokens 2048 \
    --infer_backend pt&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推送模型到ModelScope：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 \
swift export \
    --adapters output/vx-xxx/checkpoint-xxx \
    --push_to_hub true \
    --hub_model_id &#39;&amp;lt;your-model-id&amp;gt;&#39; \
    --hub_token &#39;&amp;lt;your-sdk-token&amp;gt;&#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;点击链接即可跳转模型合集~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/collections/QwQ-32B-0f1806b8a8514a&quot;&gt;QwQ-32B&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67ca52c2d649b06b61c98271.html</link><guid isPermaLink="false">https://community.modelscope.cn/67ca52c2d649b06b61c98271.html</guid><pubDate>Fri, 07 Mar 2025 01:58:26 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>微软Phi-4系列开源：多模态与文本处理的创新突破</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/b9f7ced7142142fcad3d2dc7ec5f316c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;微软近期推出 Phi-4-multimodal 和 Phi-4-mini，这些模型是 Microsoft Phi 系列小型语言模型 (SLM) 中的最新模型。Phi-4-multimodal 能够同时&lt;/blockquote&gt;   &lt;h2&gt;00.前言&lt;/h2&gt;

&lt;p&gt;微软近期推出 Phi-4-multimodal 和 Phi-4-mini，这些模型是 Microsoft Phi 系列小型语言模型 (SLM) 中的最新模型。Phi-4-multimodal 能够同时处理语音、视觉和文本，为创建创新且具有上下文感知能力的应用程序开辟了新的可能性。另一方面，Phi-4-mini 在基于文本的任务方面表现出色，以紧凑的形式提供高精度和可扩展性。&lt;/p&gt;

&lt;h2&gt;01.什么是 Phi-4-multimodal？&lt;/h2&gt;

&lt;p&gt;Phi-4-multimodal 是Phi系列的第一个多模态语言模型，标志着微软人工智能发展的一个新里程碑。Phi-4-multimodal是一个5.6B参数模型，它将语音、视觉和文本处理无缝集成到一个统一的架构中。&lt;/p&gt;

&lt;p&gt;通过利用先进的跨模态学习技术，该模型可实现更自然、更情境感知的交互，使设备能够同时理解和推理多种输入模态。无论是解释口头语言、分析图像还是处理文本信息，它都能提供高效、低延迟的推理，同时优化设备执行并降低计算开销。&lt;/p&gt;

&lt;h3&gt;专为多模式体验而构建&lt;/h3&gt;

&lt;p&gt;Phi-4-multimodal 是一个混合了 LoRA 的单一模型，包括语音、视觉和语言，所有这些都在同一个表示空间内同时处理。结果是一个能够处理文本、音频和视觉输入的单一统一模型——无需复杂的管道或针对不同模态的单独模型。&lt;/p&gt;

&lt;p&gt;Phi-4-multimodal 采用一种可提高效率和可扩展性的新架构。它整合了更大的词汇量以改进处理能力，支持多语言功能，并将语言推理与多模式输入相结合。所有这些都是在强大、紧凑、高效的模型中实现的，该模型适合部署在设备和边缘计算平台上。&lt;/p&gt;

&lt;p&gt;该模型代表了 Phi 系列模型的进步，在小巧的封装中提供增强的性能。无论您是在寻找移动设备还是边缘系统上的高级 AI 功能，Phi-4-multimodal 都能提供高效且多功能的高性能选项。&lt;/p&gt;

&lt;h3&gt;解锁新功能&lt;/h3&gt;

&lt;p&gt;Phi-4-multimodal 的功能范围和灵活性不断扩大，为希望以创新方式利用 AI 力量的应用开发者、企业和行业开辟了令人兴奋的新可能性。多模态 AI 的未来已然到来，它已准备好改变您的应用。&lt;/p&gt;

&lt;p&gt;Phi-4-multimodal 能够同时处理视觉和音频。下表显示了当视觉内容的输入查询是合成语音时，在图表/表格理解和文档推理任务中的模型质量。与其他现有的能够将音频和视觉信号作为输入的最先进的全向模型相比，Phi-4-multimodal 在多个基准测试中实现了更强大的性能。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://pica.zhimg.com/80/v2-1e9307268da53a470a83ed638858a17a_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;图 1：Phi-4-多模式音频和视觉基准。&lt;/p&gt;

&lt;p&gt;Phi-4-multimodal 在语音相关任务中表现出了卓越的能力，成为多个领域领先的开放模型。它在自动语音识别 (ASR) 和语音翻译 (ST) 方面都优于 WhisperV3 和 SeamlessM4T-v2-Large 等专业模型。它是少数几个成功实现语音摘要并达到与 GPT-4o 模型相当的性能水平的开放模型之一。该模型在语音问答 (QA) 任务上与 Gemini-2.0-Flash 和 GPT-4o-realtime-preview 等接近的模型存在差距，因为模型尺寸较小导致保留事实 QA 知识的能力较弱。正在开展工作以在下一次迭代中改进此功能。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-c07cb54cff9fef9dc387a107eac68d23_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;图 2：Phi-4 多模式语音基准。&lt;/p&gt;

&lt;p&gt;Phi-4-multimodal 仅具有 5.6B 个参数，但在各种基准测试中都表现出了卓越的视觉能力，最显著的是在数学和科学推理方面取得了优异的表现。尽管规模较小，但该模型在一般多模态能力（如文档和图表理解、光学字符识别 (OCR) 和视觉科学推理）方面仍保持着竞争性的表现，与 Gemini-2-Flash-lite-preview/Claude-3.5-Sonnet 等接近的模型相当或超过它们。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-f6a51683cad86d0812e68b8c75f38633_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;图 3：Phi-4 多模式视觉基准。&lt;/p&gt;

&lt;h2&gt;02.什么是 Phi-4-mini？&lt;/h2&gt;

&lt;p&gt;Phi-4-mini是一个 3.8B 参数模型和一个密集的解码器专用转换器，具有分组查询注意、200,000 个词汇表和共享输入输出嵌入，专为提高速度和效率而设计。尽管体积小巧，但它在基于文本的任务（包括推理、数学、编码、指令跟踪和函数调用）中的表现仍然优于大型模型。它支持多达 128,000 个标记的序列，具有高准确度和可扩展性，使其成为高级 AI 应用程序的强大解决方案。&lt;/p&gt;

&lt;p&gt;为了了解模型质量，下图Phi-4-mini 与一系列基准上的模型进行了比较，如图 4 所示。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://pic1.zhimg.com/80/v2-dcf27d0ccb0c2ef26a8bdd579d9ed2a0_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;图 4：Phi-4-mini 语言基准。&lt;/p&gt;

&lt;p&gt;函数调用、指令跟踪、长上下文和推理是强大的功能，使 Phi-4-mini 等小型语言模型能够访问外部知识和功能，尽管其容量有限。通过标准化协议，函数调用允许模型与结构化编程接口无缝集成。当用户发出请求时，Phi-4-Mini 可以推理查询，识别并调用具有适当参数的相关函数，接收函数输出，并将这些结果合并到其响应中。这创建了一个可扩展的基于代理的系统，通过明确定义的函数接口将模型连接到外部工具、应用程序编程接口 (API) 和数据源，可以增强模型的功能。以下示例使用 Phi-4-mini 模拟智能家居控制代理。&lt;/p&gt;

&lt;h2&gt;03.定制和跨平台&lt;/h2&gt;

&lt;p&gt;由于尺寸较小，Phi-4-mini 和 Phi-4-multimodal 模型可用于计算受限的推理环境。这些模型可以在设备上使用，尤其是在使用 ONNX Runtime 进一步优化以实现跨平台可用性时。它们较低的计算需求使它们成为一种成本更低且延迟更低的选择。较长的上下文窗口可以接收和推理大量文本内容（文档、网页、代码等）。Phi-4-mini 和 multimodal 表现出强大的推理和逻辑能力，使其成为分析任务的理想选择。它们的小尺寸也使微调或定制更容易、更实惠。下表显示了使用 Phi-4-multimodal 进行微调的场景示例。&lt;/p&gt;

&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;任务&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;基础模型&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;微调模型&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;计算&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;英语至印尼语的语音翻译&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;17.4&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;35.5&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;3小时，16 A100&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;医学视觉问答&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;47.6&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;56.7&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;5 小时，8 A100&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;04.模型结构&lt;/h2&gt;

&lt;p&gt;Phi-4-Mini和Phi-4-Multimodal共享相同的语言模型backbone。Phi-4-Mini由32个transformers layers组成，隐藏状态大小为3,072，并绑定了输入/输出embedding，与Phi-3.5相比，显着降低了内存消耗，同时提供了更广泛的词汇表覆盖范围。每个transformers块包括基于组查询注意力 (GQA) 的注意力机制，其优化用于长上下文生成的key和value存储器 (KV高速缓存) 使用。具体来说，该模型采用24个query头和8个key/value head，将KV缓存消耗减少到其标准大小的3分之1。Phi-4-Mini型号使用tokenizer o200k base tiktoken，词汇量为200,064，旨在更有效地支持多语言和多模式输入和输出。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-692e757996b2fed5926d19dc571f8062_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;为了实现特定于模态的功能，多模态模型通常需要对基本语言模型进行微调，这通常会降低其原始语言能力。为了解决这个问题，llamam-vision采用了受Flamingo启发的策略，在保留核心语言模型的同时添加了额外的交叉注意层。但是，与完全微调的模型相比，这种方法将导致视觉语言基准测试的性能降低。为了填补性能差距，NVLM进一步探索了一种混合框架，采用了高质量文本SFT数据的联合监督微调。然而，这种方法只检查有限的语言基准，没有解决SFT之后通常需要的额外培训阶段。Phi-4-Multimodal体系结构采用LoRAs设计的混合，以支持不同的多模态用例。训练不同的lora以处理不同模态之间的相互作用。&lt;/p&gt;

&lt;h2&gt;05.模型推理&lt;/h2&gt;

&lt;p&gt;Phi-4-mini-instruct推理代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from vllm import LLM, SamplingParams

llm = LLM(model=&quot;LLM-Research/Phi-4-mini-instruct&quot;, trust_remote_code=True)

messages = [
 {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful AI assistant.&quot;},
 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Can you provide ways to eat combinations of bananas and dragonfruits?&quot;},
 {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.&quot;},
 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What about solving an 2x + 3 = 7 equation?&quot;},
]

sampling_params = SamplingParams(
 max_tokens=500,
 temperature=0.0,
)

output = llm.chat(messages=messages, sampling_params=sampling_params)
print(output[0].outputs[0].text)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phi-4-multimodal-instruct推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import requests
import torch
import os
import io
from PIL import Image
import soundfile as sf
from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from urllib.request import urlopen
from modelscope import snapshot_download


# Define model path
model_path = snapshot_download(&quot;LLM-Research/Phi-4-multimodal-instruct&quot;)

# Load model and processor
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    device_map=&quot;cuda&quot;, 
    torch_dtype=&quot;auto&quot;, 
    trust_remote_code=True, 
    #attn_implementation=&#39;flash_attention_2&#39;,
).cuda()

# Load generation config
generation_config = GenerationConfig.from_pretrained(model_path)

# Define prompt structure
user_prompt = &#39;&amp;lt;|user|&amp;gt;&#39;
assistant_prompt = &#39;&amp;lt;|assistant|&amp;gt;&#39;
prompt_suffix = &#39;&amp;lt;|end|&amp;gt;&#39;

# Part 1: Image Processing
print(&quot;\n--- IMAGE PROCESSING ---&quot;)
image_url = &#39;https://www.ilankelman.org/stopsigns/australia.jpg&#39;
prompt = f&#39;{user_prompt}&amp;lt;|image_1|&amp;gt;What is shown in this image?{prompt_suffix}{assistant_prompt}&#39;
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Prompt\n{prompt}&#39;)

# Download and open image
image = Image.open(requests.get(image_url, stream=True).raw)
inputs = processor(text=prompt, images=image, return_tensors=&#39;pt&#39;).to(&#39;cuda:0&#39;)

# Generate response
generate_ids = model.generate(
    **inputs,
    max_new_tokens=1000,
    generation_config=generation_config,
)
generate_ids = generate_ids[:, inputs[&#39;input_ids&#39;].shape[1]:]
response = processor.batch_decode(
    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Response\n{response}&#39;)

# Part 2: Audio Processing
print(&quot;\n--- AUDIO PROCESSING ---&quot;)
audio_url = &quot;https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac&quot;
speech_prompt = &quot;Transcribe the audio to text, and then translate the audio to French. Use &amp;lt;sep&amp;gt; as a separator between the original transcript and the translation.&quot;
prompt = f&#39;{user_prompt}&amp;lt;|audio_1|&amp;gt;{speech_prompt}{prompt_suffix}{assistant_prompt}&#39;
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Prompt\n{prompt}&#39;)

# Downlowd and open audio file
audio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))

# Process with the model
inputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors=&#39;pt&#39;).to(&#39;cuda:0&#39;)

generate_ids = model.generate(
    **inputs,
    max_new_tokens=1000,
    generation_config=generation_config,
)
generate_ids = generate_ids[:, inputs[&#39;input_ids&#39;].shape[1]:]
response = processor.batch_decode(
    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(f&#39;&amp;gt;&amp;gt;&amp;gt; Response\n{response}&#39;)&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;模型微调&lt;/h3&gt;

&lt;p&gt;ms-swift已经支持了Phi-4-multimodal-instruct的微调，包括图像和音频。ms-swift是魔搭社区官方提供的大模型与多模态大模型训练部署框架。&lt;/p&gt;

&lt;p&gt;ms-swift开源地址：&lt;/p&gt;

&lt;p&gt;https://github.com/modelscope/ms-swift&lt;/p&gt;

&lt;p&gt;我们将展示可运行的微调demo，并给出自定义数据集的格式。&lt;/p&gt;

&lt;p&gt;在开始微调之前，请确保您的环境已准备妥当。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pip install git+https://github.com/modelscope/ms-swift.git

git clone https://github.com/modelscope/ms-swift.git
cd ms-swift
pip install -e .&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;微调脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model LLM-Research/Phi-4-multimodal-instruct \
    --dataset &#39;AI-ModelScope/LaTeX_OCR:human_handwrite#20000&#39; \
    --train_type dummy \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 16 \
    --eval_steps 200 \
    --save_steps 200 \
    --save_total_limit 5 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;训练显存占用：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-528ce6a43b2f530a83dc442df596b2e5_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;添加图片注释，不超过 140 字（可选）&lt;/p&gt;

&lt;p&gt;如果要使用自定义数据集进行训练，你可以参考以下格式，并指定`--dataset &amp;lt;dataset_path&amp;gt;`。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;浙江的省会在哪？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;浙江的省会在杭州。&quot;}]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&amp;lt;image&amp;gt;&amp;lt;image&amp;gt;两张图片有什么区别&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;前一张是小猫，后一张是小狗&quot;}], &quot;images&quot;: [&quot;/xxx/x.jpg&quot;, &quot;/xxx/x.png&quot;]}
{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&amp;lt;audio&amp;gt;语音说了什么&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;今天天气真好呀&quot;}], &quot;audios&quot;: [&quot;/xxx/x.mp3&quot;]}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;训练完成后，使用以下命令对训练后的权重进行推理，这里的`--adapters`需要替换成训练生成的last checkpoint文件夹。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream false \
    --max_batch_size 1 \
    --load_data_args true \
    --max_new_tokens 2048&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推送模型到ModelScope：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 \
swift export \
    --adapters output/vx-xxx/checkpoint-xxx \
    --push_to_hub true \
    --hub_model_id &#39;&amp;lt;your-model-id&amp;gt;&#39; \
    --hub_token &#39;&amp;lt;your-sdk-token&amp;gt;&#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;点击链接即可跳转模型体验~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/collections/phi-4-4ce2630c1b664f&quot;&gt;phi-4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c90c3e3b685529b705af46.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c90c3e3b685529b705af46.html</guid><pubDate>Thu, 06 Mar 2025 02:45:18 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>打造跨语言智能工具与应用，“万卷·丝路”专项课题开放申请</title><description>&lt;blockquote&gt;随着共建“一带一路”进入高质量发展阶段，全球开发者对于多语言模型训练的需求不断增长，上海AI实验室联合大模型语料数据联盟成员发布了“万卷·丝路”多语言预训练语料库，为多语言大模型训练提供高质量数据支撑&lt;/blockquote&gt;   &lt;p&gt;随着共建“一带一路”进入高质量发展阶段，全球开发者对于多语言模型训练的需求不断增长，上海AI实验室联合大模型语料数据联盟成员发布了“万卷·丝路”多语言预训练语料库，为多语言大模型训练提供高质量数据支撑，助力全球开发者构建跨语言智能工具与应用。（点击了解：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxMDc0MDU5Mg==&amp;amp;mid=2247549665&amp;amp;idx=1&amp;amp;sn=162dc8cb8d7088f405e061f7fd9c079d&amp;amp;scene=21#wechat_redirect&quot;&gt;多语言语料库“万卷·丝路”发布，AI赋能共建“一带一路”&lt;/a&gt;）现面向全社会开放多语言课题合作及应用案例征集，并提供丰富、多维的资源支持，欢迎大家报名。&lt;/p&gt;

&lt;h2&gt;01. 多语言语料库研究合作&lt;/h2&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;383&quot; src=&quot;https://devpress.csdnimg.cn/41943b30c5b149f3898fec1ec5693cdd.png&quot; width=&quot;900&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;“万卷·丝路”多语言语料库发布以来，吸引众多开发者积极下载、使用。围绕该语料库的开源应用，上海人工智能实验室联合大模型语料数据联盟，对外开放专项课题合作，提供多元化的项目支持，寻找那些能够深化多语种语料库在关键领域的应用，如多语言模型微调与性能优化、跨语言企业出海智能工具研发等。&lt;/p&gt;

&lt;p&gt;欢迎基础模型研发类企业、NLP算法/多语言算法研究员、多语种模型应用开发者、开发团队加入。&lt;/p&gt;

&lt;p&gt;专题课题参考&lt;/p&gt;

&lt;p&gt;围绕多语种语料库的建设、应用与发展趋势，聚焦语言教育、翻译实践、人工智能等多领域的应用需求，按照“创新驱动、应用导向”的原则，选取多语种语料库的构建、处理、分析、应用等“小切口”主题，广泛征集创新性研究成果。&lt;/p&gt;

&lt;p&gt;研究方向参考：&lt;/p&gt;

&lt;p&gt;1. 多语言模型微调与性能优化&lt;/p&gt;

&lt;p&gt;2. 多语言智能助手应用开发&lt;/p&gt;

&lt;p&gt;3. 跨语言企业出海智能工具研发&lt;/p&gt;

&lt;p&gt;4. 多语言大模型评价与优化机制研究&lt;/p&gt;

&lt;p&gt;5. 多语言语料库建设与开放&lt;/p&gt;

&lt;p&gt;6. 多语言对话模型对齐评测基准构建&lt;/p&gt;

&lt;p&gt;7. 多语言语料的自动标注与生成工具开发&lt;/p&gt;

&lt;p&gt;8. 多语言智能教育系统的研究与开发&lt;/p&gt;

&lt;p&gt;支持内容&lt;/p&gt;

&lt;p&gt;基于上海人工智能实验室与大模型语料数据联盟在数据处理与体系化能力建设的积淀，“万卷·丝路”多语言语料库专项课题为入选团队提供多维度的支持：&lt;/p&gt;

&lt;p&gt;● 顶级项目支持（科研经费、算力资源、数据工具、宣传等资源）&lt;/p&gt;

&lt;p&gt;● 优秀成果应用转化&lt;/p&gt;

&lt;p&gt;● 技术直播讲师邀约&lt;/p&gt;

&lt;p&gt;● 大模型语料数据联盟卓越贡献者证书&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;参与对象&lt;/p&gt;

&lt;p&gt;欢迎各类机构或个人参加，包括但不限于：&lt;/p&gt;

&lt;p&gt;● 基础模型研发类企业&lt;/p&gt;

&lt;p&gt;● NLP算法/多语言算法研究员&lt;/p&gt;

&lt;p&gt;● 多语种模型应用开发者&lt;/p&gt;

&lt;p&gt;● AI 应用开发团队&lt;/p&gt;

&lt;p&gt;征集时间&lt;/p&gt;

&lt;p&gt;专项课题申请截止日期为2025年6月20日。&lt;/p&gt;

&lt;p&gt;（更多细则，可点击文末阅读原文查看）&lt;/p&gt;

&lt;p&gt;主办单位&lt;/p&gt;

&lt;p&gt;上海人工智能实验室、大模型语料数据联盟、北京外研在线数字科技有限公司&lt;/p&gt;

&lt;p&gt;报名方式&lt;/p&gt;

&lt;p&gt;如果你正在开展“万卷·丝路”多语言语料库的相关应用研究，或是渴望加入多语种语料算法优化、模型开发专属社群，与全球开发者共同开展技术交流，欢迎扫码填报“万卷·丝路”使用情况，与我们一起，以人工智能推动跨文化交流与业务拓展。&lt;/p&gt;

&lt;p&gt;👇&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;256&quot; src=&quot;https://devpress.csdnimg.cn/42566869baea46e890b90ec6d9f743da.png&quot; width=&quot;256&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;更多交流请添加小助手微信&lt;/p&gt;

&lt;p&gt;获取更多高质量、大规模、安全可信的数据集&lt;/p&gt;

&lt;p&gt;沟通更多关于数据集的合作事项&lt;/p&gt;

&lt;p&gt;👇&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1426&quot; src=&quot;https://devpress.csdnimg.cn/eaf2c805a4164783baa459db991fdf8d.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;02.“万卷·丝路”多语言预训练语料库介绍&lt;/h2&gt;

&lt;p&gt;“万卷·丝路”是国内规模最大、领域覆盖最全面的高质量多语言预训练语料库，数据总规模超1.2TB，Token总数超过300B，处于国际领先水平。首期开源的语料库包含泰语、俄语、阿拉伯语、韩语和越南语5个子集，每个子集均突破150GB数据规模，通过创新的七维分类体系（历史沿革、商贸经济、专业知识库、文化传承、新闻时政等）和32个垂直领域标签，系统构建起兼具语言特征与文化特质的数据集。开发者可根据具体需求检索数据，满足从基础研究到产业应用的多样化需求。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;经严格评测验证，&quot;万卷·丝路&quot;展现出显著的模型赋能效应：基于7B参数基础模型训练，实现综合性能跃升52.3%；在700亿参数的大模型训练中，仍保持12.8%的性能增益。值得注意的是，依托”万卷·丝路”，使轻量化模型在多语言处理领域展现出超越大模型的卓越表现。&lt;/p&gt;

&lt;p&gt;数据集下载&lt;/p&gt;

&lt;p&gt;“万卷·丝路”已发布至大模型语料数据联盟开源数据服务指定平台OpenDataLab及各大开发者社区，全球开发者均可通过以下开源链接下载：&lt;/p&gt;

&lt;p&gt;OpenDataLab:&lt;/p&gt;

&lt;p&gt;https://opendatalab.com/applyMultilingualCorpus&lt;/p&gt;

&lt;p&gt;ModelScope:&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/datasets?Tags=text-classification&amp;amp;dataType=text&amp;amp;organization=OpenDataLab&amp;amp;page=1&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c90b8b6670175f9930b022.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c90b8b6670175f9930b022.html</guid><pubDate>Thu, 06 Mar 2025 02:42:19 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>CogView4开源发布！智谱AI文生图模型支持任意长度双语输入，汉字生成能力突出，可商用！</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/6627d3cefc2e47deb784c33f44f43b03.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;今天智谱AI正式发布并开源了最新的图像生成模型——CogView4。&lt;/blockquote&gt;   &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/4a1f15e7-0a34-4f5e-9e12-3e7de153e005.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;这幅图像融合了中国古典艺术与现代元素，灵感源自北宋画家王希孟的《千里江山图》。画面展现了一幅壮丽的山水长卷，青绿山水技法使得山峦起伏、江河浩渺，色彩层次丰富，细节精致入微。在这如诗如画的江山美景之上，巧妙地浮现出一个墨色淋漓的毛笔字“CogView4”，字体苍劲有力，墨迹浓淡相宜，仿佛是古代文人墨客在欣赏美景时即兴挥毫留下的印记。“CogView4”这几个字与周围的山水景致相得益彰，既不突兀也不失和谐，反而增添了一种跨越时空的对话感。整个画面既有古典山水的韵味，又融入了现代科技感的元素，呈现出一种独特的艺术张力，让人在欣赏传统美学的同时，也能感受到现代创意的碰撞与融合。&lt;/p&gt;

&lt;p&gt;今天智谱AI正式发布并开源了最新的图像生成模型——CogView4。　&lt;/p&gt;

&lt;p&gt;该模型具备较强的复杂语义对齐和指令跟随能力，支持任意长度的中英双语输入，能够生成在给定范围内的任意分辨率图像，同时具备较强的文字生成能力。该模型也是首个遵循 Apache 2.0协议开源的图像生成模型。　&lt;/p&gt;

&lt;h1&gt;一、评测&lt;/h1&gt;

&lt;p&gt;DPG-Bench（Dense Prompt Graph Benchmark）是一个用于评估文本到图像生成模型的基准测试，主要关注模型在复杂语义对齐和指令跟随能力方面的表现。　&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;CogView4-6B，其在 DPG-Bench基准测试中的综合评分排名第一，在开源文生图模型中达到 SOTA。　&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/1f67b12b-2919-40be-96a3-e2c7077c9864.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;二、任意长度&amp;amp;任意分辨率&lt;/h1&gt;

&lt;p&gt;CogView4模型实现了任意长度的文本描述和任意分辨率图像的混合训练范式。　&lt;/p&gt;

&lt;p&gt;1、图像位置编码　&lt;/p&gt;

&lt;p&gt;CogView4采用二维旋转位置编码（2D RoPE）来建模图像的位置信息，并通过内插位置编码的方式支持不同分辨率的图像生成任务。　&lt;/p&gt;

&lt;p&gt;2、扩散生成建模　&lt;/p&gt;

&lt;p&gt;模型采用Flow-matching方案进行扩散生成建模，并结合参数化的线性动态噪声规划，以适应不同分辨率图像的信噪比需求。　&lt;/p&gt;

&lt;p&gt;3、架构设计　&lt;/p&gt;

&lt;p&gt;在DiT模型架构上，CogView4延续了上一代的Share-param DiT架构，并为文本和图像模态分别设计独立的自适应LayerNorm层，以实现模态间的高效适配。　&lt;/p&gt;

&lt;p&gt;4、多阶段训练　&lt;/p&gt;

&lt;p&gt;CogView4采用多阶段训练策略，包括基础分辨率训练、泛分辨率训练、高质量数据微调以及人类偏好对齐训练。这种分阶段训练方式不仅覆盖了广泛的图像分布，还确保生成的图像具有高美感并符合人类偏好。　&lt;/p&gt;

&lt;p&gt;5、训练框架优化&lt;/p&gt;

&lt;p&gt;从文本角度，CogView4突破了传统固定token长度的限制，允许更高的token上限，并显著减少了训练过程中的文本token冗余。当训练caption的平均长度在200-300 token时，与固定512 token的传统方案相比，CogView4减少了约50%的token冗余，并在模型递进训练阶段实现了5%-30%的效率提升。　&lt;/p&gt;

&lt;p&gt;从图像角度，混合分辨率训练使模型能够支持较大范围内的任意分辨率生成，极大地提升了创作的自由度。目标分辨率只需满足以下条件：　&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/15d3aaad-0f14-4411-9661-9c767307fb94.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;　&lt;/p&gt;

&lt;p&gt;这两点可以极大提高创作的自由度。　&lt;/p&gt;

&lt;p&gt;示例：超长故事（四格漫画）　&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/b386b834-c1dc-4791-a653-25a55040b37d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;请生成一张图包含四个场景的四格漫画图，采用动漫插画风格的连环画。其中主要出现的角色有：　&lt;/p&gt;

&lt;p&gt;小明：人类男孩，拥有一颗勇敢的心，手持宝剑，穿着简易的战士服装。　&lt;/p&gt;

&lt;p&gt;公主：人类女性，美丽优雅，穿着华丽的公主服饰，被囚禁在怪兽的老巢。　&lt;/p&gt;

&lt;p&gt;国王：人类男性，威严而仁慈，穿着华丽的王者服饰，坐在王国的宝座上。　&lt;/p&gt;

&lt;p&gt;火焰龙：怪兽，全身覆盖着火焰般的鳞片，口吐火焰，体型庞大。　&lt;/p&gt;

&lt;p&gt;黑暗魔王：怪兽，体型巨大，全身笼罩在黑暗中，拥有强大的魔法力量。　&lt;/p&gt;

&lt;p&gt;Scene 1: 小明踏上征程　&lt;/p&gt;

&lt;p&gt;创建一个动漫风格的场景，背景是壮丽的王国庭院。场景中的主要角色是小明（人类男孩，拥有一颗勇敢的心，手持宝剑，穿着简易的战士服装），他正以踏上征程的姿势展现。包括庭院中的花草和远处城堡的细节，晨曦的光照传达出勇敢和决心。质量：杰作，最佳质量，超详细，4k　&lt;/p&gt;

&lt;p&gt;Scene 2: 小明战胜火焰龙　&lt;/p&gt;

&lt;p&gt;创建一个动漫风格的场景，背景是炽热的火山口。场景中的主要角色是小明（人类男孩，拥有一颗勇敢的心，手持宝剑，穿着简易的战士服装），他正处于战胜火焰龙的瞬间。包括火山口的岩石和熔岩的细节，火红的光照传达出激烈和勇气。质量：杰作，最佳质量，超详细，4k　&lt;/p&gt;

&lt;p&gt;Scene 3: 小明与黑暗魔王激战　&lt;/p&gt;

&lt;p&gt;创建一个动漫风格的场景，背景是阴暗的怪兽老巢。场景中的主要角色是小明（人类男孩，拥有一颗勇敢的心，手持宝剑，穿着简易的战士服装），他正处于与黑暗魔王激战的场景中。包括老巢的黑暗和魔法能量的细节，阴沉的光照传达出激烈和紧张。质量：杰作，最佳质量，超详细，4k　&lt;/p&gt;

&lt;p&gt;Scene 4: 小明救出公主　&lt;/p&gt;

&lt;p&gt;创建一个动漫风格的场景，背景是荒废的城堡内部。场景中的主要角色是小明（人类男孩，拥有一颗勇敢的心，手持宝剑，穿着简易的战士服装）和公主（人类女性，美丽优雅，穿着华丽的公主服饰），他们正处于小明救出公主的温馨场景中。包括城堡内部废墟和昏暗光线的细节，温柔的光照传达出感动和救赎。质量：杰作，最佳质量，超详细，4k　&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;三、支持中英文&lt;/h1&gt;

&lt;p&gt;在技术实现上，CogView4将文本编码器从纯英文的T5 encoder 换为具备双语能力的GLM-4 encoder，并通过中英双语图文对进行训练，使CogView4模型具备双语提示词输入能力。　&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;目前来看，CogView4 是首个支持中英双语提示词输入的开源文生图模型，尤其擅长理解和遵循中文提示词，并能在画面中生成汉字。这两点特性更加适合国内广告、短视频等领域广泛的创意需求。　&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/559a0ef1-ca46-4941-9ae0-0525e3ac6e71.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;这幅图像展现了一堵充满朋克风格的墙面，色彩鲜明且富有冲击力。墙面以深黑色为底，上面覆盖着层层叠叠的鲜艳涂鸦，包括尖锐的线条、铆钉图案和闪烁的金属质感贴纸，彰显出反叛与自由的精神。在墙面的中央位置，用粗犷的白色喷漆字体醒目地写着“CogView-4”，字迹边缘带有磨损和溅射的效果，增添了一种街头艺术的粗犷美感。在“CogView-4”下方，同样以白色喷漆书写着“不破不立”四个大字，字体风格与上方呼应，但尺寸稍小，形成视觉上的层次感。这四个字周围环绕着一些细小的涂鸦符号，如星星、骷髅和火焰，进一步强化了朋克文化的标志性元素。墙面的背景中还隐约可见一些裂痕和剥落的油漆，暗示着时间的痕迹和不断变革的力量。整幅画面充满了活力与张力，完美诠释了朋克文化中的反叛精神与创新理念。　&lt;/p&gt;

&lt;h2&gt;体验链接：&lt;/h2&gt;

&lt;p&gt;https://modelscope.cn/studios/ZhipuAI/CogView4&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;四、Apache协议&lt;/h1&gt;

&lt;p&gt;CogView4-6B模型支持Apache2.0协议，后续会陆续增加ControlNet、ComfyUI等生态支持，全套的微调工具包也即将推出。　&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;开源仓库地址：&lt;/p&gt;

&lt;p&gt;* https://github.com/THUDM/CogView4　&lt;/p&gt;

&lt;p&gt;模型仓库：　&lt;/p&gt;

&lt;p&gt;* https://modelscope.cn/models/ZhipuAI/CogView4-6B　&lt;/p&gt;

&lt;p&gt;最新的CogView4模型将于3月13日上线智谱清言（chatglm.cn）。　&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;五、模型推理&lt;/h1&gt;

&lt;p&gt;安装依赖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/diffusers.git&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from diffusers import CogView4Pipeline
from modelscope import snapshot_download
import torch

model_dir = snapshot_download(&quot;ZhipuAI/CogView4-6B&quot;)
pipe = CogView4Pipeline.from_pretrained(model_dir, torch_dtype=torch.bfloat16)

# Open it for reduce GPU memory usage
pipe.enable_model_cpu_offload()
pipe.vae.enable_slicing()
pipe.vae.enable_tiling()

prompt = &quot;A vibrant cherry red sports car sits proudly under the gleaming sun, its polished exterior smooth and flawless, casting a mirror-like reflection. The car features a low, aerodynamic body, angular headlights that gaze forward like predatory eyes, and a set of black, high-gloss racing rims that contrast starkly with the red. A subtle hint of chrome embellishes the grille and exhaust, while the tinted windows suggest a luxurious and private interior. The scene conveys a sense of speed and elegance, the car appearing as if it&#39;s about to burst into a sprint along a coastal road, with the ocean&#39;s azure waves crashing in the background.&quot;
image = pipe(
    prompt=prompt,
    guidance_scale=3.5,
    num_images_per_prompt=1,
    num_inference_steps=50,
    width=1024,
    height=1024,
).images[0]

image.save(&quot;cogview4.png&quot;)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;点击链接即可跳转模型详情~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogView4-6B&quot;&gt;https://modelscope.cn/models/ZhipuAI/CogView4-6B&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c7c9f6d649b06b61c84710.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c7c9f6d649b06b61c84710.html</guid><pubDate>Wed, 05 Mar 2025 03:50:14 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>CLIPer：开创性框架提升CLIP空间表征，实现开放词汇语义分割突破</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/bf583d21491f44f4bcffc01942b99669.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;对比语言-图像预训练（CLIP）在多种图像级任务上表现出强大的零样本分类能力，促使研究行人尝试将CLIP应用于像素级开放词汇语义分割，而无需额外训练。关键在于提升图像级CLIP的空间表征能力，例如，用&lt;/blockquote&gt;   &lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://arxiv.org/abs/2411.13836&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/sunlin449/CLIPer&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;01.论文解读&lt;/h2&gt;

&lt;p&gt;对比语言-图像预训练（CLIP）在多种图像级任务上表现出强大的零样本分类能力，促使研究行人尝试将CLIP应用于像素级开放词汇语义分割，而无需额外训练。关键在于提升图像级CLIP的空间表征能力，例如，用自-自注意力图或基于视觉基础模型的自注意力图替换最后一层的自注意力图。本文提出了一种新颖的分层框架CLIPer，该框架分层提升了CLIP的空间表征能力。&lt;/p&gt;

&lt;p&gt;CLIPer包括浅层融合模块和精细补偿模块。浅层融合模块包括嵌入浅层特征和注意力图以保留空间结构信息，生成具有更好空间一致性的分割图。精细补偿模块利用扩散模型的自注意力图来补偿局部细节。本文在七个分割数据集上进行了实验，并在这些数据集上实现了最先进的性能。在不采用使用滑动窗口的策略下，使用ViT-L模型，CLIPer在VOC和COCO Object任务上的mIoU分别达到69.8%和43.3%，分别高出ProxyCLIP模型9.2%和4.1%。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;486&quot; src=&quot;https://devpress.csdnimg.cn/84c3d57b40b2464e9a0a93ad6a4153dc.png&quot; width=&quot;882&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;486&quot; src=&quot;https://i-blog.csdnimg.cn/img_convert/faa7ae299230bd31fa968876ed146a5c.png&quot; width=&quot;882&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;方法描述&lt;/p&gt;

&lt;p&gt;CLIPer方法有以下几个关键步骤：&lt;/p&gt;

&lt;p&gt;1.最后一层注意力机制的弥补：原始的CLIP图像编码器最后一层缺乏空间特性，CLIPer通过利用浅层空间连续性强的特性，将浅层的多头注意力图直接融合至最后一层中，并去掉了残差连接以及前向传播网络，以提高最终分割的连续性。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2.浅层特征的提取：在每次经过CLIP图像编码时，CLIPer会记录所有Transformer块的特征，并将这些特征送入到修改后的最后一层，并分别与文本特征计算相似度，得到分割结果。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3.精细补偿机制：CLIPer提出将扩散模型中的细节融入到分割中。该模块利用扩散模型中的多头自注意力头蕴含的细节信息，通过矩阵链乘法的形式去优化粗糙的分割结果。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;论文实验&lt;/p&gt;

&lt;p&gt;本文对CLIPer与一些最先进的方法在各种数据集上的表现进行了比较。当使用ViT-B和ViT-L两种框架时，CLIPer几乎在所有这些数据集上都达到了最佳性能。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;620&quot; src=&quot;https://devpress.csdnimg.cn/0205011942f7415fb79ce8b7e81be40d.png&quot; width=&quot;1048&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;620&quot; src=&quot;https://i-blog.csdnimg.cn/img_convert/2a05e9690e610b0d62e58e184047ee9d.png&quot; width=&quot;1048&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;另外，本文还将开放词汇语义分割可以看作是两个方面：类别分类和掩码预测。为了深入展示CLIPer在这两个方面上的优势，本文通过两个实验与其他方法进行了更多比较。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;300&quot; src=&quot;https://devpress.csdnimg.cn/bffce66366204b519031858ec869b48d.png&quot; width=&quot;1048&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;正在上传…重新上传取消&lt;/p&gt;

&lt;p&gt;正在上传…重新上传取消&lt;/p&gt;

&lt;p&gt;正在上传…重新上传取消&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;300&quot; src=&quot;https://i-blog.csdnimg.cn/img_convert/1d4271de5cc7ea094110423329f9f730.png&quot; width=&quot;1048&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;本文第一个实验结果的图像级分类的形式展示了CLIPer的分类能力，通过表二可以得出CLIPer取得更优的mAP、F1、P和R分数，表明在类别分类上表现更好，这对于开放词汇语义分割非常有用。&lt;/p&gt;

&lt;p&gt;本文的第二个实验通过弱监督的设置展示了CLIPer的分割能力，通过表三可以得出，CLIPer在弱监督设置中较其他方式取得更优的mIoU分数，说明具备更好的分割能力。&lt;/p&gt;

&lt;p&gt;对于推理时间，与ClearCLIP相比，CLIPer*具有更快的速度和更高的mIoU。与ProxyCLIP相比，CLIPer*具有更快的速度和可比的mIoU。此外，CLIPer通过细粒度补偿显著提升了CLIPer*的性能。&lt;/p&gt;

&lt;p&gt;本文也有一系列的消融实验。具体包括：&lt;/p&gt;

&lt;p&gt;1. 单独只使用浅层特征融合模块(CLIPer*)，单独只使用精细补偿模块，以及两者同时具备(CLIPer)的情况，发现每一个模块均能单独提升分割结果，并且同时使用两者能更进一步的提升效果。&lt;/p&gt;

&lt;p&gt;2. 进一步的消融浅层特征融合模块，展示了只融合浅层特征，只融合自注意力，以及同时融合两者同同条件下使用q-q,k-k,v-v的对比，得出这两种融合的有效性。&lt;/p&gt;

&lt;p&gt;3. 在精细补偿中，本文也探讨了如何融合扩散模型自注意力图，包括选取其中一个自注意力图，平均所有自注意力图以及将所有自注意力图做矩阵链乘法，得出矩阵链乘法能显著的提升分割精度的结果。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;02.最佳实践&lt;/h2&gt;

&lt;p&gt;CLIPer在魔搭社区上进行了部署，在魔搭社区免费提供的GPU免费算力上可体验CLIPer。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;体验地址：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/sunlin449/CLIPer&lt;/p&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;2295&quot; src=&quot;https://devpress.csdnimg.cn/755651705c7d4c76884102bd6fcbf842.png&quot; width=&quot;1354&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;点击链接即可跳转模型~&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;a href=&quot;https://modelscope.cn/studios/sunlin449/CLIPer&quot;&gt;https://modelscope.cn/studios/sunlin449/CLIPer&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c7c9c23b685529b70510a6.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c7c9c23b685529b70510a6.html</guid><pubDate>Wed, 05 Mar 2025 03:49:22 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>高效部署通义万相Wan2.1：使用Gradio搭建WebUI体验实战</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/bccd0b84687946ef839c72dd483f0951.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;随着通义万相Wan2.1 在社区的热度持续上涨，魔搭创空间的体验Demo（https://modelscope.cn/studios/Wan-AI/Wan-2.1）已经排起长队。&lt;/blockquote&gt;   &lt;p&gt;随着通义万相Wan2.1 在社区的热度持续上涨，魔搭创空间的体验Demo（https://modelscope.cn/studios/Wan-AI/Wan-2.1）已经排起长队。为了提供更好的体验，本文向大家介绍如何使用魔搭社区的免费GPU给自己部署一个独占算力通道的WebUI界面玩转Wan2.1文生视频模型，包会！&lt;/p&gt;

&lt;p&gt;Wan2.1文生视频WebUI界面部署&lt;/p&gt;

&lt;p&gt;1. 以下链接直达脚本教程：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/notebook/share/ipynb/f548cee3/wan_gradio.ipynb&lt;/p&gt;

&lt;p&gt;2. 在Notebook中打开，选择使用GPU打开&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/0afe0028-988d-4798-8cf0-08b8f4738521.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3.进入到notebook页面，按顺序执行提供的脚本教程&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/90b2a9f9-bece-4ac3-9bd3-99f7d83ab4f0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;4. 点击链接进入网站（建议使用谷歌浏览器，兼容性较好）&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/e6f071cf-e439-453a-bffc-adc8d7c3d73d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;5. 这就完成了在魔搭免费GPU算力中部署一个WebUI，独享算力通道玩转Wan2.1 文生视频模型啦！&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/b6145398-9fb6-481b-ba59-50daea72b091.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;LoRA应用&lt;/h2&gt;

&lt;p&gt;1. 如果已有LoRA文件，将其放在&lt;code&gt;DiffSynth-Studio/models/lora/&lt;/code&gt;文件夹里。&lt;/p&gt;

&lt;p&gt;2. 再次点击运行。&lt;/p&gt;

&lt;p&gt;3. 重复文生视频的操作。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/c6d342f0-2fbb-402f-abac-e944df235116.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;脚本中各项参数的作用&lt;/h2&gt;

&lt;p&gt;1. 下载并安装&lt;code&gt;DiffSynth-Studio&lt;/code&gt;，这是魔搭社区的开源Diffusion模型引擎，项目链接：https://github.com/modelscope/DiffSynth-Studio&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/b2fe4f92-51a7-4071-aa51-8695d7792d99.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2.&lt;code&gt; wan2.1-t2v-1.3b&lt;/code&gt;文生视频模型包含3个组件：文本编码器、扩散模型和视频解码器。需要从魔搭模型库中下载3个组件对应的模型文件，并将这些模型文件放置到对应文件夹：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;文本编码器：&lt;code&gt;models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors&lt;/code&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;扩散模型：&lt;code&gt;models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth&lt;/code&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;视频解码器：&lt;code&gt;models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth&lt;/code&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;如果需要更换14b的模型，在截图下更改模型即可&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/c02942c4-836b-4a88-b203-5fa838831563.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;下面就是一些gradio界面代码了，大家可以对照界面参考一下&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/3dbb2f54-953b-45c4-8edc-cc20135cbc1f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/df8684e0-c4cd-432d-b169-100d971353db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c6d6e7807ce562bfd9ff68.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c6d6e7807ce562bfd9ff68.html</guid><pubDate>Tue, 04 Mar 2025 10:33:11 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>高效部署通义万相Wan2.1：ComfyUI文生/图生视频实战，工作流直取！</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/02c7ed95608e45959da7e96e26bfb3a1.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;通义万相Wan2.1开源不到一周，已登顶HuggingFace Model 和 Space 榜双榜首，在HuggingFace和ModelScope平台的累计下载量突破100万次，社区热度持续攀升！为&lt;/blockquote&gt;   &lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501176&amp;amp;idx=1&amp;amp;sn=5105a4f9db9b56ecb978361c2f58357e&amp;amp;scene=21#wechat_redirect&quot;&gt;通义万相Wan2.1开源&lt;/a&gt;不到一周，已登顶HuggingFace Model 和 Space 榜双榜首，在HuggingFace和ModelScope平台的累计下载量突破100万次，社区热度持续攀升！为响应小伙伴们对ComfyUI工作流运行Wan2.1的强烈需求，社区开发者整理了实战教程👇&amp;nbsp;&lt;/p&gt;

&lt;p&gt;本文将手把手教你分别在魔搭免费GPU算力环境、本地环境部署运行ComfyUI工作流，玩转Wan2.1文生视频、图生视频案例实践。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2&gt;01.魔搭Notebook运行ComfyUI文生视频工作流&lt;/h2&gt;

&lt;h3&gt;step1 如何在魔搭中启动Notebook&lt;/h3&gt;

&lt;p&gt;1、打开ModelScope 魔搭社区首页，点击我的Notebook&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;508&quot; src=&quot;https://devpress.csdnimg.cn/815e531861ec4a79bdd3ecdfc5670ae3.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/ef9b3a9f-bbb2-4569-bfa6-80a2703cc97b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;2、魔搭社区免费提供的GPU免费算力上体验，选择方式二启动后点击查看Notebook&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;540&quot; src=&quot;https://devpress.csdnimg.cn/cb565b302e2d420db637d96293b717d0.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;545&quot; src=&quot;https://devpress.csdnimg.cn/d9b5673b2c4b4dfebb184bea6012bebd.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/581f457a-f3db-499a-bfa0-49c522b9d2bd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt; &lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/564a7a3b-f642-4141-bc62-47c3f42ce866.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h3&gt;step2 安装ComfyUI及其依赖&lt;/h3&gt;

&lt;p&gt;1、打开Notebook终端&lt;/p&gt;

&lt;p&gt;在页面中选择Terminal&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;672&quot; src=&quot;https://devpress.csdnimg.cn/398fb3e9747849a7813edc0119167fc3.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/3d44613f-d040-4a0e-97aa-e208ac549086.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;2、克隆ComfyUI仓库&lt;/p&gt;

&lt;p&gt;在Notebook终端中运行如下命令，将官方的ComfyUI的仓库克隆下来：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  git clone https://github.com/comfyanonymous/ComfyUI.git&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/c582fc08-3354-4eac-a108-0ed6f19fbc93.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;140&quot; src=&quot;https://devpress.csdnimg.cn/84528bdab8f2414f939c223803c19959.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;克隆过程中可能遇到这个报错，提示“RPC失败”：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;200&quot; src=&quot;https://devpress.csdnimg.cn/48362ea1033a4600aaef02c0f644bd04.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/ce98dfd8-b8ed-445e-b3bc-615c3db2abf4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;这是因为http协议版本的问题导致网络连接失败，版本降低至1.1即可解决。命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  git config --global http.version HTTP/1.1
  git clone https://github.com/comfyanonymous/ComfyUI.git
  git config --global http.version HTTP/2&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;完成的界面像这样：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;178&quot; src=&quot;https://devpress.csdnimg.cn/025737f26e40484eb9495fdb46812b77.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/ba2b4090-88f1-404c-acd1-70bf4ddd6bc0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3、安装依赖&lt;/p&gt;

&lt;p&gt;使用pip安装运行ComfyUI服务所需的环境依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ComfyUI
pip install -r requirements.txt&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;4、验证安装&lt;/p&gt;

&lt;p&gt;运行以下命令启动ComfyUI服务，测试是否成功安装。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;如果服务成功启动，将在终端中看到http://127.0.0.1:8188的提示&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;261&quot; src=&quot;https://devpress.csdnimg.cn/cb27b1d27ddb4875b2dfa89e563cc951.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/6c8b4e6a-6e17-4293-bb80-7a0e4c63253e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;点击这个链接就可以进入comfyui的界面啦！&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3&gt;step3 文生视频工作流&lt;/h3&gt;

&lt;p&gt;以通义万相wan2.1-t2v-1.3b文生视频模型为例，演示如何运行工作流。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;1、下载模型&lt;/p&gt;

&lt;p&gt;wan2.1-t2v-1.3b文生视频模型包含3个组件，文本编码器、扩散模型和视频解码器。我们需要从魔搭模型库中下载3个组件对应的模型文件，并将这些模型文件放置到对应文件夹：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;文本编码器：&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/text_encoders/umt5_xxl_fp16.safetensors →ComfyUI/models/text_encoders&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;扩散模型&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/diffusion_models/wan2.1_t2v_1.3B_bf16.safetensors → ComfyUI/models/diffusion_models&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;视频解码器&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/vae/wan_2.1_vae.safetensors → ComfyUI/models/vae&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;命令行如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 文本编码器
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/text_encoders/umt5_xxl_fp16.safetensors --local_dir ./models/text_encoders/
# 扩散模型
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/diffusion_models/wan2.1_t2v_1.3B_bf16.safetensors --local_dir ./models/diffusion_models/
# 视频解码器
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/vae/wan_2.1_vae.safetensors --local_dir ./models/vae/&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、启动comfyui&lt;/p&gt;

&lt;p&gt;运行以下命令启动ComfyUI服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;261&quot; src=&quot;https://devpress.csdnimg.cn/383de1efa81546a490a0640d6fbcbfd1.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/6470dded-e3be-4e86-82b8-d7a53504e06a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;点击http://127.0.0.1:8188链接进入comfyui的界面&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;668&quot; src=&quot;https://devpress.csdnimg.cn/b547e5bd05ce4b3c95099748164a4469.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/623c2697-0b06-40d0-bf2b-1d01520a74cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;3、运行文生视频工作流&lt;/p&gt;

&lt;p&gt;ComfyUI启动后会自动打开一个默认的工作流，可以不用管它。我们需要下载wan2.1的示例工作流文件，然后将文件拖入ComfyUI界面。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;wan2.1的示例工作流文件：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/notebook/share/ipynb/5eee8a46/text_to_video_wan.ipynb&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;拖入后在界面中就可以看到工作流的样子，分别点击三个模型的下拉选项，检查一下模型文件是否存在，再点击“执行”开始视频生成。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;604&quot; src=&quot;https://devpress.csdnimg.cn/7284d1aab6594949934a093b10306820.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/528f5bd0-bf59-4ac5-a8ac-77858f337125.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;视频生成完成之后的页面：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/74d68cc2-4357-431e-a3b3-08be4cfa2a44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;554&quot; src=&quot;https://devpress.csdnimg.cn/2654c114c8474251b115063ac1b28e28.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;4、运行日志查看&lt;/p&gt;

&lt;p&gt;终端界面上可以查看实时运行的日志：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;280&quot; src=&quot;https://devpress.csdnimg.cn/68194dc22ead44c0be997823fe4e66e9.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/f50c6379-485f-454f-b8df-b1bf7e36657c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;nvidia-smi命令可以查看显存占用：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;507&quot; src=&quot;https://devpress.csdnimg.cn/a043e6ec0af1486496b94191a88c6f7c.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/0d6f3911-f09c-4d14-95ec-2bf87dac9bdb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h2&gt;02.本地搭建ComfyUI图生视频工作流&lt;/h2&gt;

&lt;p&gt;如果你自己拥有GPU，则可以选择在本地部署工作流。本节以通义万相wan2.1-i2v-14b的图生视频模型为例，教你一步步用命令行运行图生视频的工作流。&lt;/p&gt;

&lt;h3&gt;step1 检查显卡信息&lt;/h3&gt;

&lt;p&gt;要运行万相wan2.1-i2v-14b文生视频ComfyUI工作流，需要高规格的显卡。生成512*512大小的视频，显存大约需要44G；生成1280*720尺寸的视频，显存需要53GB。运行nvidia-smi命令，检查显卡是否符合要求。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;463&quot; src=&quot;https://devpress.csdnimg.cn/ab49d80656ad4b09b40c9bce8f1455f0.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/1bc7c3fe-ed8b-4312-9e91-d883e5511857.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h3&gt;step2 安装ComfyUI及其依赖&lt;/h3&gt;

&lt;p&gt;此步骤同上，请查阅上一节step2。&lt;/p&gt;

&lt;h3&gt;step3 图生视频工作流&lt;/h3&gt;

&lt;p&gt;1、模型下载&lt;/p&gt;

&lt;p&gt;wan2.1-i2v-1.3b文生视频模型包含4个组件，图片编码器、文本编码器、视频扩散模型和视频解码器。我们需要从魔搭模型库中下载4个组件对应的模型文件，并将这些模型文件放置到对应文件夹：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;图片编码器&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/clip_vision/clip_vision_h.safetensors →ComfyUI/models/clip_vision&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;文本编码器&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/text_encoders/umt5_xxl_fp16.safetensors →ComfyUI/models/text_encoders&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;视频扩散模型&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/diffusion_models/wan2.1_i2v_720p_14B_bf16.safetensors → ComfyUI/models/diffusion_models&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;视频解码器&lt;/p&gt;

	&lt;ul&gt;&lt;li&gt;
		&lt;p&gt;split_files/vae/wan_2.1_vae.safetensors → ComfyUI/models/vae&lt;/p&gt;
		&lt;/li&gt;&lt;/ul&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;命令行如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# 图片编码器
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/clip_vision/clip_vision_h.safetensors --local_dir ./models/clip_vision/
# 文本编码器
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/text_encoders/umt5_xxl_fp16.safetensors --local_dir ./models/text_encoders/
# 视频扩散模型
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/diffusion_models/wan2.1_i2v_720p_14B_bf16.safetensors --local_dir ./models/diffusion_models/
# 视频解码器
modelscope download --model Comfy-Org/Wan_2.1_ComfyUI_repackaged --include split_files/vae/wan_2.1_vae.safetensors --local_dir ./models/vae/&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、启动comfyui&lt;/p&gt;

&lt;p&gt;运行以下命令启动ComfyUI服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;261&quot; src=&quot;https://devpress.csdnimg.cn/9ea7b96ddb8948e48a055b332430d7c9.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/96aeea5b-8f9e-44c1-95f0-ae9f2a75fda8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;点击http://127.0.0.1:8188链接就可以进入comfyui的界面&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3、上传工作流运行&lt;/p&gt;

&lt;p&gt;下载wan2.1的图生视频示例工作流文件，然后将文件拖入ComfyUI界面。&lt;/p&gt;

&lt;p&gt;wan2.1的示例工作流文件：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/notebook/share/ipynb/0e5bbc8b/image_to_video_wan_example.ipynb&lt;/p&gt;

&lt;p&gt;拖入后在界面中就可以看到工作流的样子，分别点击三个模型的下拉选项，检查一下模型文件是否存在，再点击“执行”开始视频生成。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;619&quot; src=&quot;https://devpress.csdnimg.cn/489faef7e191446b9a71bcaa755f6bc7.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;https://resouces.modelscope.cn/race/image/da6194dc-a7d8-48ce-8d99-b74d6843ec5c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c6d66f3b685529b7049c73.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c6d66f3b685529b7049c73.html</guid><pubDate>Tue, 04 Mar 2025 10:31:11 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>魔搭社区模型速递（2.16-3.1）</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/9b94f6ac55a04420826149b49b7e477a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;魔搭ModelScope本期社区进展：2621个模型，Ovis2系列模型等，276个数据集，203个创新应用， 12篇技术内容&lt;/blockquote&gt;   &lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;608&quot; src=&quot;https://devpress.csdnimg.cn/e783798c561d44bfb5b752969c007fc9.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;🙋魔搭ModelScope本期社区进展：&lt;/p&gt;

&lt;p&gt;📟2621个模型：通义万相Wan2.1、Step- Video-t2v系列、Step-Audio系列、Moonlight-16B-A3B、TinyR1-32B-Preview、Ovis2系列模型等；&lt;/p&gt;

&lt;p&gt;📁276个数据集：中文基于满血DeepSeek-R1蒸馏数据集、自我认知微调数据集(COT)、NL2SQL思维链数据集等；&lt;/p&gt;

&lt;p&gt;🎨203个创新应用：通义万相Wan2.1视频生成、InspireMusic音乐生成模型、Step-Audio-TTS-3B、olmOCR PDF高精准文本提取等；&lt;/p&gt;

&lt;p&gt;📄 12篇技术内容：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;人人都是应用开发者：AI时代的全栈产品经理实践&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;HumanOmni：首个专注人类中心场景的多模态大模型，视觉与听觉融合的突破！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;无需微调！扩散模型新突破：Attentive Eraser高效稳定移除目标物体&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;强强联合，开源中文DeepSeek-R1蒸馏数据集联合魔搭社区，一起来训练中文版推理模型！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;英特尔显卡+魔搭社区，一行命令快速部署本地大模型&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;通义万相Wan2.1视频模型开源！视频生成模型新标杆，支持中文字效+高质量视频生成&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;Kimi开源Moonlight-16B-A3B：基于Muon优化器的高效大模型，性能与训练效率双突破！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;PromptScope: 一个灵活高效的In-Context Training框架&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;阿里国际Ovis2系列模型开源：多模态大语言模型的新突破&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;SPO来袭：Prompt工程师90%不存在了？AI自动优化时代开启！&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;王炸组合，阶跃星辰SOTA模型Step-Video和Step-Audio模型开源&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;ModelScope魔搭25年2月版本发布月报&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;01.精选模型&lt;/h2&gt;

&lt;p&gt;通义万相Wan2.1系列&lt;/p&gt;

&lt;p&gt;通义万相Wan2.1是阿里通义实验室开源的视频生成模型，包括14B和1.3B两个参数规格的全部推理代码和权重全部开源，同时支持文生视频和图生视频任务，实现了性能与易用性的双重突破：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;SOTA 性能：Wan2.1在多个基准测试中始终优于现有的开源模型和最先进的商业解决方案。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;支持消费级 GPU：T2V-1.3B 型号仅需 8.19 GB VRAM，可兼容几乎所有消费级 GPU。它可在约 4 分钟内（未使用量化等优化技术）在 RTX 4090 上生成 5 秒的 480P 视频。其性能甚至可与一些闭源模型相媲美。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;多项任务：Wan2.1在文本转视频、图像转视频、视频编辑、文本转图像和视频转音频方面表现出色，推动了视频生成领域的发展。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;视觉文本生成：Wan2.1是第一个能够生成中英文文本的视频模型，具有强大的文本生成功能，可增强其实际应用。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;强大的视频 VAE：Wan-VAE提供卓越的效率和性能，可对任意长度的 1080P 视频进行编码和解码，同时保留时间信息，使其成为视频和图像生成的理想基础。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;通义万相2.1-文生视频-1.3B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B&lt;/p&gt;

&lt;p&gt;通义万相2.1-文生视频-14B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B&lt;/p&gt;

&lt;p&gt;通义万相2.1-图生视频-14B-720P&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P&lt;/p&gt;

&lt;p&gt;通义万相2.1-图生视频-14B-480P&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;使用DiffSynth-Studio进行Wan2.1系列模型推理&lt;/p&gt;

&lt;h3&gt;安装&lt;/h3&gt;

&lt;p&gt;通过以下命令可下载并安装 DiffSynth-Studio：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git
cd DiffSynth-Studio
pip install -e .&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;模型下载&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;modelscope download --model Wan-AI/Wan2.1-T2V-1.3B --local_dir ./models/Wan-AI/Wan2.1-T2V-1.3B
modelscope download --model Wan-AI/Wan2.1-T2V-14B --local_dir ./models/Wan-AI/Wan2.1-T2V-14B
modelscope download --model Wan-AI/Wan2.1-I2V-14B-480P --local_dir ./models/Wan-AI/Wan2.1-I2V-14B-480P
modelscope download --model Wan-AI/Wan2.1-I2V-14B-720P --local_dir ./models/Wan-AI/Wan2.1-I2V-14B-720P&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;&amp;lt;1.3B 模型&amp;gt;&lt;/h3&gt;

&lt;p&gt;运行 1.3B 文生视频模型的代码（提示词等参数可在代码文件中进行修改）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python examples/wanvideo/wan_1.3b_text_to_video.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.3B 模型需要 6G 显存即可运行。&lt;/p&gt;

&lt;h3&gt;&amp;lt;14B 模型&amp;gt;&lt;/h3&gt;

&lt;p&gt;运行 14B 文生视频和图生视频模型的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python examples/wanvideo/wan_14b_text_to_video.py
python examples/wanvideo/wan_14b_image_to_video.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多模型玩法详见：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501176&amp;amp;idx=1&amp;amp;sn=5105a4f9db9b56ecb978361c2f58357e&amp;amp;scene=21#wechat_redirect&quot;&gt;通义万相Wan2.1视频模型开源！视频生成模型新标杆，支持中文字效+高质量视频生成&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Moonlight-16B-A3B&lt;/p&gt;

&lt;p&gt;月之暗面团队基于Muon训练了Moonlight-16B-A3B系列模型并开源发布。Moonlight-16B-A3B是一个具备16B参数（激活参数为3B）的专家混合(MoE)模型，使用5.7T个token数据训练得到，该模型模型改进了当前的帕累托前沿，与之前的模型相比，使用更少的训练FLOPs就能实现更好的性能。&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;Moonlight-16B-A3B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/moonshotai/Moonlight-16B-A3B&lt;/p&gt;

&lt;p&gt;Moonlight-16B-A3B-Instruct&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/moonshotai/Moonlight-16B-A3B-Instruct&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;moonshotai/Moonlight-16B-A3B-Instruct&quot;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

prompt = &quot;1+1=2, 1+2=&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;, padding=True, truncation=True).to(model.device)
generated_ids = model.generate(**inputs, max_new_tokens=100)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多关于Muon优化器微调实战教程，详见：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501105&amp;amp;idx=1&amp;amp;sn=bd6f15d7659e7771d2dfd28131f1cd61&amp;amp;scene=21#wechat_redirect&quot;&gt;Kimi开源Moonlight-16B-A3B：基于Muon优化器的高效大模型，性能与训练效率双突破！&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Step-Video-T2V系列&lt;/p&gt;

&lt;p&gt;Step-Video-T2V系列是由阶跃星辰开源的SoTA文本转视频预训练模型，是目前全球参数量最大的视频生成大模型，具有 300 亿个参数，能够生成高达204帧、540P分辨率的高质量视频，具备复杂运动控制能力，可实现精细的动态表现。&lt;/p&gt;

&lt;p&gt;Step-Video-T2V采用高压缩Video-VAE实现16x16空间和8x时间压缩，通过双语文本编码器支持中英输入。模型基于3D全注意力DiT扩散架构，使用Flow Matching训练，将噪声转化为潜在帧，以文本嵌入和时间步为条件。结合视频优化的DPO技术，显著减少伪影，提升视频流畅性与真实感，生成高质量、长时序的多语言适配视频内容。&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;stepvideo-t2v&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/stepfun-ai/stepvideo-t2v&lt;/p&gt;

&lt;p&gt;stepvideo-t2v-turbo&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/stepfun-ai/stepvideo-t2v-turbo&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;h2&gt;单卡体验&lt;/h2&gt;

&lt;p&gt;使用DiffSynth-Studio工具在 80G 显存的单卡 A100 上进行推理&lt;/p&gt;

&lt;p&gt;下载并安装 DiffSynth-Studio：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git
cd DiffSynth-Studio
pip install -e .&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行 Step-Video-T2V 模型：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python examples/stepvideo/stepvideo_text_to_video.py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提示词、帧数等参数可在样例代码 stepvideo_text_to_video.py中进行修改。&lt;/p&gt;

&lt;p&gt;代码链接：https://github.com/modelscope/DiffSynth-Studio/blob/main/examples/stepvideo/stepvideo_text_to_video.py&lt;/p&gt;

&lt;p&gt;更多玩法详见：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247500969&amp;amp;idx=1&amp;amp;sn=64514ee77386c2974e92297bc5d7bfcf&amp;amp;scene=21#wechat_redirect&quot;&gt;王炸组合，阶跃星辰SOTA模型Step-Video和Step-Audio模型开源&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Step-Audio系列&lt;/p&gt;

&lt;p&gt;Step-Audio系列是由阶跃星辰开源的业界首个产品级语音交互模型，支持多语言（如中文、英文、日语）、情感表达（如开心、悲伤）、方言（如粤语、四川话）以及语速、韵律和唱歌（包括RAP、哼唱）的精准控制，核心技术包括：&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;1300亿多模态模型 实现语音识别、语义理解、对话生成等功能&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;高效数据生成链路 通过130B模型生成高质量合成音频数据&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;精细语音控制 支持多种情绪、方言和声乐风格的细粒度调节&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;扩展工具调用 结合RLHF和ToolCall机制提升复杂任务表现&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;采用双码本编码器方案和混合语音解码器，确保语音清晰度与自然度，后训练阶段通过SFT和RLHF优化情感、语速等控制能力，在主流测试集中性能领先，尤其在HSK-6评测中表现突出，被称为“最懂中国话”的开源语音交互模型。&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;Step-Audio-Chat&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/stepfun-ai/Step-Audio-Chat&lt;/p&gt;

&lt;p&gt;Step-Audio-Tokenizer&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/stepfun-ai/Step-Audio-Tokenizer&lt;/p&gt;

&lt;p&gt;Step-Audio-TTS-3B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/stepfun-ai/Step-Audio-TTS-3B&lt;/p&gt;

&lt;p&gt;示例代码：&lt;/p&gt;

&lt;p&gt;安装依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/stepfun-ai/Step-Audio.git
conda create -n stepaudio python=3.10
conda activate stepaudio

cd Step-Audio
pip install -r requirements.txt&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推理代码如下，你需要在`Step-Audio`目录下运行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
import torchaudio
import argparse
from stepaudio import StepAudio
from modelscope import snapshot_download
os.makedirs(&#39;output&#39;, exist_ok=True)

tokenizer_path = snapshot_download(&#39;stepfun-ai/Step-Audio-Tokenizer&#39;)
tts_path = snapshot_download(&#39;stepfun-ai/Step-Audio-TTS-3B&#39;)
model_path = snapshot_download(&#39;stepfun-ai/Step-Audio-Chat&#39;)

model = StepAudio(
    tokenizer_path=tokenizer_path,
    tts_path=tts_path,
    llm_path=model_path,
)

# example for text input
text, audio, sr = model(
    [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好，我是你的朋友，我叫小明，你叫什么名字？&quot;}],
    &quot;闫雨婷&quot;,
)
print(text)
torchaudio.save(&quot;output/output_e2e_tqta.wav&quot;, audio, sr)

# example for audio input
text, audio, sr = model(
    [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;output/output_e2e_tqta.wav&quot;},
        }
    ],
    &quot;闫雨婷&quot;,
)
print(text)
torchaudio.save(&quot;output/output_e2e_aqta.wav&quot;, audio, sr)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显存资源占用：4 * 65GiB&lt;/p&gt;

&lt;p&gt;更多玩法详见：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247500969&amp;amp;idx=1&amp;amp;sn=64514ee77386c2974e92297bc5d7bfcf&amp;amp;scene=21#wechat_redirect&quot;&gt;王炸组合，阶跃星辰SOTA模型Step-Video和Step-Audio模型开源&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ovis2系列模型&lt;/p&gt;

&lt;p&gt;Ovis2是阿里巴巴国际化团队推出的新型多模态大模型架构，通过结构化对齐视觉和文本嵌入，解决了模态间嵌入策略差异的问题。相较于前序版本，Ovis2在数据构造与训练方法上显著改进，强化了小规模模型的能力密度，并通过指令微调和偏好学习提升了思维链(CoT)推理能力。其新增视频、多图像处理能力，增强多语言支持与复杂场景OCR能力，实用性大幅提升。&lt;/p&gt;

&lt;p&gt;Ovis2开源六个版本(1B至34B)，各尺寸均达SOTA水平，其中Ovis2-34B在OpenCompass评测中表现卓越，以不到一半参数量超越多个70B级开源模型。模型架构由三个关键组件构成：视觉tokenizer将图像分割并转化为概率化视觉token；视觉嵌入表存储视觉单词嵌入向量并通过加权平均生成嵌入表示；LLM将视觉与文本嵌入拼接处理，完成多模态任务。Ovis2具备高性能、多任务支持和广泛适用性，为多模态领域带来重要突破。&lt;/p&gt;

&lt;p&gt;模型链接：&lt;/p&gt;

&lt;p&gt;Ovis2-1B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/AIDC-AI/Ovis2-1B&lt;/p&gt;

&lt;p&gt;Ovis2-2B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/AIDC-AI/Ovis2-2B&lt;/p&gt;

&lt;p&gt;Ovis2-4B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/AIDC-AI/Ovis2-4B&lt;/p&gt;

&lt;p&gt;Ovis2-8B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/AIDC-AI/Ovis2-8B&lt;/p&gt;

&lt;p&gt;Ovis2-16B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/AIDC-AI/Ovis2-16B&lt;/p&gt;

&lt;p&gt;Ovis2-34B&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/models/AIDC-AI/Ovis2-34B&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;示例代码&lt;/p&gt;

&lt;p&gt;安装依赖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!pip install flash-attn==2.7.0.post2 --no-build-isolation&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;推理代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import torch
from PIL import Image
from modelscope import AutoModelForCausalLM

# load model
model = AutoModelForCausalLM.from_pretrained(&quot;AIDC-AI/Ovis2-4B&quot;,
                                             torch_dtype=torch.bfloat16,
                                             multimodal_max_length=32768,
                                             trust_remote_code=True).cuda()
text_tokenizer = model.get_text_tokenizer()
visual_tokenizer = model.get_visual_tokenizer()

# single-image input
image_path = &#39;./data/example.jpg&#39;
images = [Image.open(image_path)]
max_partition = 9
text = &#39;Describe the image.&#39;
query = f&#39;&amp;lt;image&amp;gt;\n{text}&#39;

## cot-style input
# cot_suffix = &quot;Provide a step-by-step solution to the problem, and conclude with &#39;the answer is&#39; followed by the final solution.&quot;
# image_path = &#39;/data/images/example_1.jpg&#39;
# images = [Image.open(image_path)]
# max_partition = 9
# text = &quot;What&#39;s the area of the shape?&quot;
# query = f&#39;&amp;lt;image&amp;gt;\n{text}\n{cot_suffix}&#39;

## multiple-images input
# image_paths = [
#     &#39;/data/images/example_1.jpg&#39;,
#     &#39;/data/images/example_2.jpg&#39;,
#     &#39;/data/images/example_3.jpg&#39;
# ]
# images = [Image.open(image_path) for image_path in image_paths]
# max_partition = 4
# text = &#39;Describe each image.&#39;
# query = &#39;\n&#39;.join([f&#39;Image {i+1}: &amp;lt;image&amp;gt;&#39; for i in range(len(images))]) + &#39;\n&#39; + text

## video input (require `pip install moviepy==1.0.3`)
# from moviepy.editor import VideoFileClip
# video_path = &#39;/data/videos/example_1.mp4&#39;
# num_frames = 12
# max_partition = 1
# text = &#39;Describe the video.&#39;
# with VideoFileClip(video_path) as clip:
#     total_frames = int(clip.fps * clip.duration)
#     if total_frames &amp;lt;= num_frames:
#         sampled_indices = range(total_frames)
#     else:
#         stride = total_frames / num_frames
#         sampled_indices = [min(total_frames - 1, int((stride * i + stride * (i + 1)) / 2)) for i in range(num_frames)]
#     frames = [clip.get_frame(index / clip.fps) for index in sampled_indices]
#     frames = [Image.fromarray(frame, mode=&#39;RGB&#39;) for frame in frames]
# images = frames
# query = &#39;\n&#39;.join([&#39;&amp;lt;image&amp;gt;&#39;] * len(images)) + &#39;\n&#39; + text

## text-only input
# images = []
# max_partition = None
# text = &#39;Hello&#39;
# query = text

# format conversation
prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=max_partition)
attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)
input_ids = input_ids.unsqueeze(0).to(device=model.device)
attention_mask = attention_mask.unsqueeze(0).to(device=model.device)
if pixel_values is not None:
    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)
pixel_values = [pixel_values]

# generate output
with torch.inference_mode():
    gen_kwargs = dict(
        max_new_tokens=1024,
        do_sample=False,
        top_p=None,
        top_k=None,
        temperature=None,
        repetition_penalty=None,
        eos_token_id=model.generation_config.eos_token_id,
        pad_token_id=text_tokenizer.pad_token_id,
        use_cache=True
    )
    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]
    output = text_tokenizer.decode(output_ids, skip_special_tokens=True)
    print(f&#39;Output:\n{output}&#39;)&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;02.数据集推荐&lt;/h2&gt;

&lt;p&gt;中文基于满血DeepSeek-R1蒸馏数据集&lt;/p&gt;

&lt;p&gt;本数据集为中文开源蒸馏满血R1的数据集，数据集中不仅包含math数据，还包括大量的通用类型数据，总数量为110K。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/datasets/liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/datasets/liucong/Chinese-DeepSeek-R1-Distill-data-110k&lt;/p&gt;

&lt;p&gt;更多详情请查看文章：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501200&amp;amp;idx=1&amp;amp;sn=9aaaa0636187465899a54b850ab014dc&amp;amp;scene=21#wechat_redirect&quot;&gt;强强联合，开源中文DeepSeek-R1蒸馏数据集联合魔搭社区，一起来训练中文版推理模型！&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;自我认知微调数据集(COT)&lt;/p&gt;

&lt;p&gt;该数据集主要用于微调大型语言模型，以提升模型在自我认知和思维链推理方面的能力。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/datasets/xitaosun/self-cognition&lt;/p&gt;

&lt;p&gt;NL2SQL思维链数据集&lt;/p&gt;

&lt;p&gt;该数据集通过引入思维链（Chain of Thought, CoT）的方式，帮助模型更好地理解和生成SQL查询语句，提升自然语言处理在数据库查询场景中的准确性和效率。&lt;/p&gt;

&lt;p&gt;数据集链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/datasets/ruohuaw/sql-cot&lt;/p&gt;

&lt;h2&gt;03.精选应用&lt;/h2&gt;

&lt;p&gt;通义万相Wan2.1视频生成&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/Wan-AI/Wan-2.1&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;InspireMusic音乐生成模型&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/Qwen/Qwen2.5-Max-Demo&lt;/p&gt;

&lt;p&gt;Step-Audio-TTS-3B&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/Swarmeta_AI/Step-Audio-TTS-3B&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;olmOCR PDF高精准文本提取&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/AI-ModelScope/olmOCR&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;SPO：让提示词自我进化&lt;/p&gt;

&lt;p&gt;SPO是一种用于大型语言模型（LLMs）的自动化提示工程工具，专为通用领域适配而设计。作为下一代提示工程系统，采用自监督提示优化，与传统方法相比，实现了高达17.8至90.9倍的成本效率提升，达到SoTA的性能水平。&lt;/p&gt;

&lt;p&gt;体验直达：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/studios/AI-ModelScope/SPO&lt;/p&gt;

&lt;h2&gt;04.社区精选文章&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501254&amp;amp;idx=1&amp;amp;sn=09686681a80f4172e05ecb43960f0857&amp;amp;scene=21#wechat_redirect&quot;&gt;人人都是应用开发者：AI时代的全栈产品经理实践&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501255&amp;amp;idx=1&amp;amp;sn=fc7ef40d8e4580a24c3d3a22cac7e503&amp;amp;scene=21#wechat_redirect&quot;&gt;HumanOmni：首个专注人类中心场景的多模态大模型，视觉与听觉融合的突破！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501255&amp;amp;idx=2&amp;amp;sn=8ac18fb47df80f6cf7b2932c4dd9a5fd&amp;amp;scene=21#wechat_redirect&quot;&gt;无需微调！扩散模型新突破：Attentive Eraser高效稳定移除目标物体&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501200&amp;amp;idx=1&amp;amp;sn=9aaaa0636187465899a54b850ab014dc&amp;amp;scene=21#wechat_redirect&quot;&gt;强强联合，开源中文DeepSeek-R1蒸馏数据集联合魔搭社区，一起来训练中文版推理模型！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501191&amp;amp;idx=1&amp;amp;sn=dbb345627211918abdd87ff6ed0823a9&amp;amp;scene=21#wechat_redirect&quot;&gt;英特尔显卡+魔搭社区，一行命令快速部署本地大模型&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501176&amp;amp;idx=1&amp;amp;sn=5105a4f9db9b56ecb978361c2f58357e&amp;amp;scene=21#wechat_redirect&quot;&gt;通义万相Wan2.1视频模型开源！视频生成模型新标杆，支持中文字效+高质量视频生成&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501105&amp;amp;idx=1&amp;amp;sn=bd6f15d7659e7771d2dfd28131f1cd61&amp;amp;scene=21#wechat_redirect&quot;&gt;Kimi开源Moonlight-16B-A3B：基于Muon优化器的高效大模型，性能与训练效率双突破！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501094&amp;amp;idx=1&amp;amp;sn=acf57aa2534bdd45336d81fca5611adb&amp;amp;scene=21#wechat_redirect&quot;&gt;PromptScope: 一个灵活高效的In-Context Training框架&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501093&amp;amp;idx=1&amp;amp;sn=f1e5a8bc4289b5cf602c8b93be853c4a&amp;amp;scene=21#wechat_redirect&quot;&gt;阿里国际Ovis2系列模型开源：多模态大语言模型的新突破&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247501060&amp;amp;idx=1&amp;amp;sn=85d02e6104d0e1982a565eb3390f364d&amp;amp;scene=21#wechat_redirect&quot;&gt;SPO来袭：Prompt工程师90%不存在了？AI自动优化时代开启！&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247500969&amp;amp;idx=1&amp;amp;sn=64514ee77386c2974e92297bc5d7bfcf&amp;amp;scene=21#wechat_redirect&quot;&gt;王炸组合，阶跃星辰SOTA模型Step-Video和Step-Audio模型开源&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;amp;mid=2247500867&amp;amp;idx=1&amp;amp;sn=a3eb0f54ba2308687030730af8166154&amp;amp;scene=21#wechat_redirect&quot;&gt;ModelScope魔搭25年2月版本发布月报&lt;/a&gt;&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;hr&gt; </description><link>https://community.modelscope.cn/67c53c382e30c863900a0dc6.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c53c382e30c863900a0dc6.html</guid><pubDate>Mon, 03 Mar 2025 05:20:56 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>人人都是应用开发者：AI时代的全栈产品经理实践</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/e1ea447882e94e9da43cfed318cb06ec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;本文试图最短路径、最轻模式来做一个应用，实现一个需求！仅需三大步+9小步，以下为手把手教学流程。&lt;/blockquote&gt;   &lt;h1&gt;前言说明：&lt;/h1&gt;

&lt;p&gt;本文试图最短路径、最轻模式来做一个应用，实现一个需求！仅需三大步+9小步，以下为手把手教学流程。&lt;/p&gt;

&lt;h1&gt;第一步：需求描述&lt;/h1&gt;

&lt;p&gt;以产品经理视角进行系统化描述，可参考以下模版：&lt;/p&gt;

&lt;p&gt;1、需求概述：解决什么问题，实现什么功能，有个整体的介绍。&lt;/p&gt;

&lt;p&gt;2、交互界面描述：比如输入框实现什么功能、输出框实现什么功能，如何排布及交互（传统互联网产品经理基本功）&lt;/p&gt;

&lt;p&gt;3、提供功能工具：需要找到具体可以实现某个功能的AI模型（AI产品经理基本功，知道AI都有哪些能力），比如生成文章插图可以使用一个可以生成像素风格的文生图模型，地址如下，同时也获得具体的API使用方式&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/MusePublic/326_ckpt_SD_XL/summary?version=1552&quot;&gt;https://modelscope.cn/models/MusePublic/326_ckpt_SD_XL/summary?version=1552&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1208&quot; src=&quot;https://devpress.csdnimg.cn/8d2dfc55419f498f97e95cca4ec518cb.png&quot; width=&quot;2336&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;demo成品示例：&lt;a href=&quot;https://modelscope.cn/studios/hicicada/weixinxiaozhuli&quot;&gt;https://modelscope.cn/studios/hicicada/weixinxiaozhuli&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;对应需求描述示例&lt;/p&gt;

&lt;p&gt;1、整体需求描述：&lt;/p&gt;

&lt;p&gt;基于gradio做一个应用：用户输入公众号文章内容后，点击一键生成按钮，自动生成标题、摘要内容和一个基于摘要英文promp生成的文章配图。&lt;/p&gt;

&lt;p&gt;2、前端核心模块，依次运行&lt;/p&gt;

&lt;p&gt;用户输入框：提醒用户输入公众号文章内容；&lt;/p&gt;

&lt;p&gt;1）标题输出框：根据用户输入自动生成5个适合公众号的标题，64字内；展示思考过程，思考输出和最终标题输出用两个独立框，左右布局；流式输出并多行完整展示；&lt;/p&gt;

&lt;p&gt;2）摘要输出框：根据用户输入自动生成适合公众号的摘要，120字内；展示思考过程，思考输出和最终摘要输出用两个独立框，左右布局；流式输出并多行完整展示；&lt;/p&gt;

&lt;p&gt;3）文章配图prompt输出框：根据摘要内容自动生成英文prompt，用于文生图；展示思考过程，思考输出和最终promp输出用两个独立框，左右布局；流式输出并多行完整展示；&lt;/p&gt;

&lt;p&gt;注意：最终promp输出框只输出用于文生图的英文prompt，简洁有效；&lt;/p&gt;

&lt;p&gt;promp输出框可编辑，然后可点击重新生成按钮即可重新生成图片；&lt;/p&gt;

&lt;p&gt;4）文章配图输出框：根据生成的英文prompt或者编辑的英文prompt，生成适合的配图；生成2张图，尺寸都为1024x500&lt;/p&gt;

&lt;p&gt;整体视觉风格：青色+紫色风格的科技风&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3、服务端API&lt;/p&gt;

&lt;p&gt;1）标题输出、摘要输出、prompt输出均调用以下API，格式如下，请严格遵守&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from openai import OpenAI

client = OpenAI(
    base_url=&#39;https://api-inference.modelscope.cn/v1/&#39;,
    api_key=&#39;换成你的魔搭token&#39;, # ModelScope Token
)

response = client.chat.completions.create(
    model=&#39;deepseek-ai/DeepSeek-R1&#39;, # ModelScope Model-Id
    messages=[
        {
            &#39;role&#39;: &#39;system&#39;,
            &#39;content&#39;: &#39;You are a helpful assistant.&#39;
        },
        {
            &#39;role&#39;: &#39;user&#39;,
            &#39;content&#39;: &#39;你好&#39;
        }
    ],
    stream=True
)
done_reasoning = False
for chunk in response:
    reasoning_chunk = chunk.choices[0].delta.reasoning_content
    answer_chunk = chunk.choices[0].delta.content
    if reasoning_chunk != &#39;&#39;:
        print(reasoning_chunk, end=&#39;&#39;,flush=True)
    elif answer_chunk != &#39;&#39;:
        if not done_reasoning:
            print(&#39;\n\n === Final Answer ===\n&#39;)
            done_reasoning = True
        print(answer_chunk, end=&#39;&#39;,flush=True)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2）文章配图调用API：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import requests
import json
from PIL import Image
from io import BytesIO

url = &#39;https://api-inference.modelscope.cn/v1/images/generations&#39;

payload = {
    &#39;model&#39;: &#39;djyzcp123/gjerc&#39;,#ModelScope Model-Id,required
    &#39;prompt&#39;: &#39;A golden cat&#39;# required
}
headers = {
    &#39;Authorization&#39;: &#39;Bearer 换成你的魔搭token&#39;,
    &#39;Content-Type&#39;: &#39;application/json&#39;
}

response = requests.post(url, data=json.dumps(payload, ensure_ascii=False).encode(&#39;utf-8&#39;), headers=headers)

response_data = response.json()
image = Image.open(BytesIO(requests.get(response_data[&#39;images&#39;][0][&#39;url&#39;]).content))
image.save(&#39;result_image.jpg&#39;)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;第二步：用AI生成代码，到云端Notebook测试打磨&lt;/h1&gt;

&lt;p&gt;1、生成代码&lt;/p&gt;

&lt;p&gt;可以用专业编程工具如cursor，也可以直接用大模型生成。本次评测了Cursor、DeepSeek-R1、QwenMax-QWQ、Claude3.7，均可以完成核心功能代码，将代码保存为.py文件。&lt;/p&gt;

&lt;p&gt;代码生成的思考过程&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;872&quot; src=&quot;https://devpress.csdnimg.cn/2e88bbff0b534c3e988c5a93dbc8b69f.png&quot; width=&quot;1414&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2、Notebook中运行代码&lt;/p&gt;

&lt;p&gt;Notebook预装镜像、开箱即用，不需要额外安装依赖（比如python、gradio等）；&lt;/p&gt;

&lt;p&gt;Notebook地址：&lt;a href=&quot;https://modelscope.cn/my/mynotebook&quot;&gt;https://modelscope.cn/my/mynotebook&lt;/a&gt;，本次调用API，可使用CPU环境，长期完全免费！第一次使用需完成相关账号的注册和绑定。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1010&quot; src=&quot;https://devpress.csdnimg.cn/f3ae190a1ff84f2b8ac6ae76a49920c0.png&quot; width=&quot;2320&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;上传.py文件后，运行代码!python /mnt/workspace/文件名.py 即可获取链接测试。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;916&quot; src=&quot;https://devpress.csdnimg.cn/bb5e81a638554eb48e8db03695ce142a.png&quot; width=&quot;1600&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3、测试、打磨&lt;/p&gt;

&lt;p&gt;1）前端：主要验收功能模块、布局排版&lt;/p&gt;

&lt;p&gt;实测4个工具生成的前端界面略有不同，但都满足预期。&lt;/p&gt;

&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;cursor&lt;/p&gt;
			&lt;img alt=&quot;&quot; height=&quot;1232&quot; src=&quot;https://devpress.csdnimg.cn/a074f65ae00a4f818c626f5e3ad27054.png&quot; width=&quot;1514&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;DeepSeek-R1&lt;/p&gt;
			&lt;img alt=&quot;&quot; height=&quot;1180&quot; src=&quot;https://devpress.csdnimg.cn/2917e101a71f49029beae9bc869ef55e.png&quot; width=&quot;1910&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
			&lt;p&gt;&amp;nbsp;&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;QwenMax-QWQ&lt;/p&gt;
			&lt;img alt=&quot;&quot; height=&quot;1234&quot; src=&quot;https://devpress.csdnimg.cn/f8a2f0446929406fb7b1da0e4e914b42.png&quot; width=&quot;2262&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
			&lt;p&gt;&amp;nbsp;&lt;/p&gt;

			&lt;p&gt;&amp;nbsp;&lt;/p&gt;
			&lt;/td&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;
			&lt;p&gt;Claude3.7sonnet&lt;/p&gt;
			&lt;img alt=&quot;&quot; height=&quot;1150&quot; src=&quot;https://devpress.csdnimg.cn/51d011e5802b48b49057f7cab9c0154c.png&quot; width=&quot;2270&quot; referrerpolicy=&quot;no-referrer&quot;&gt;
			&lt;p&gt;&amp;nbsp;&lt;/p&gt;

			&lt;p&gt;&amp;nbsp;&lt;/p&gt;
			&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;2）服务端：主要验收功能是否跑通&lt;/p&gt;

&lt;p&gt;实测功能均可以跑通，根据文章内容生成了标题、摘要、配图prompt及最终的文章配图。即时中间有小问题或者新需求，也是通过AI返回修改的代码也可以快速修正。&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1256&quot; src=&quot;https://devpress.csdnimg.cn/56f48caf0179489f8d1edc8907312286.png&quot; width=&quot;2268&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1132&quot; src=&quot;https://devpress.csdnimg.cn/03eb2ee0da6841299711588d34d12570.png&quot; width=&quot;1828&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;h1&gt;第三步：部署上线&lt;/h1&gt;

&lt;p&gt;1、创空间基本设置&lt;/p&gt;

&lt;p&gt;点此新建一个创空间：&lt;a href=&quot;https://modelscope.cn/studios/create&quot;&gt;https://modelscope.cn/studios/create&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;使用平台免费提供的CPU资源，按默认配置即可&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;906&quot; src=&quot;https://devpress.csdnimg.cn/ef8e5b6888fc4d62ab8be1968cf5c636.png&quot; width=&quot;1678&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2、关键步骤：将api key改为环境变量（为了保护你的key！！）&lt;/p&gt;

&lt;p&gt;将Notebook中调试成功的文件，找出存放token的代码，做如下修改&lt;/p&gt;

&lt;p&gt;before：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;108&quot; src=&quot;https://devpress.csdnimg.cn/4a5a99c3dcc14908b21541150622795c.png&quot; width=&quot;1022&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;after：&lt;/p&gt;

&lt;p&gt;# API configuration&lt;/p&gt;

&lt;p&gt;MODEL_API_KEY =os.getenv(&#39;MODEL_API_KEY&#39;)&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;将key保存在环境变量中&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1088&quot; src=&quot;https://devpress.csdnimg.cn/696f39a88460492bae27c2fcb9683bb9.png&quot; width=&quot;2224&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;3、在空间文件中上传Notebook中调试成功的文件，注意名字需改为app.py&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;908&quot; src=&quot;https://devpress.csdnimg.cn/30f0b502e4ad4b17bca3b7b674d2eeea.png&quot; width=&quot;2440&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;完成以上操作，即可发布上线并得到分享链接&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1162&quot; src=&quot;https://devpress.csdnimg.cn/f5b1f85996b141339307374c87c0fb59.png&quot; width=&quot;2336&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;点击体验：&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h1&gt;总结：人人都是应用开发者，用模型开启AI魔法，搭建万千应用！&lt;/h1&gt;

&lt;p&gt;应用可无限拓展，追求更多功能或者更好体验！&lt;/p&gt;

&lt;p&gt;1、比如增加文章润色、微信群发文案等&lt;/p&gt;

&lt;p&gt;2、比如插图风格可选&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;452&quot; src=&quot;https://devpress.csdnimg.cn/7786ae0e0ba146038e69603288d75ec2.png&quot; width=&quot;922&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;3、其他脑洞&lt;/p&gt;

&lt;p&gt;示例demo：&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;1192&quot; src=&quot;https://devpress.csdnimg.cn/8f504953295247a6a7651b41f80d1aaf.png&quot; width=&quot;2612&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;需求描述：&lt;/p&gt;

&lt;p&gt;1、需求概述：基于gradio，做一个表情包生成器&lt;/p&gt;

&lt;p&gt;2、前端界面：&lt;/p&gt;

&lt;p&gt;输入框：提醒用户输入表情包描述文本，比如形状像什么，在做什么。&lt;/p&gt;

&lt;p&gt;选择框：用户可选择图片尺寸（size）&lt;/p&gt;

&lt;p&gt;输出框1：将用户输入的文本自动翻译成英文prompt，自动都在最前面加上固定词“modalogo,A pixelated, bluish-green character,”，然后将合并后的英文用做文生图prompt；prompt支持编辑，旁边有按钮可点击重新生成。&lt;/p&gt;

&lt;p&gt;输出框2：两个图片展示框，分别展示独立生成的图片。尺寸为800*1024&lt;/p&gt;

&lt;p&gt;有独立进度条：展示图片生成进度&lt;/p&gt;

&lt;p&gt;3、服务端API&lt;/p&gt;

&lt;p&gt;1）文本生成API&lt;/p&gt;

&lt;p&gt;from openai import OpenAI client = OpenAI( base_url=&#39;https://api-inference.modelscope.cn/v1/&#39;, api_key=&#39;换成你的token, # ModelScope Token ) response = client.chat.completions.create( model=&#39;deepseek-ai/DeepSeek-V3&#39;, # ModelScope Model-Id messages=[ { &#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39; }, { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;你好&#39; } ], stream=True ) for chunk in response: print(chunk.choices[0].delta.content, end=&#39;&#39;, flush=True)&lt;/p&gt;

&lt;p&gt;2）图片生成API&lt;/p&gt;

&lt;p&gt;import requests import json from PIL import Image from io import BytesIO url = &#39;https://api-inference.modelscope.cn/v1/images/generations&#39; payload = { &#39;model&#39;: &#39;Saniac/modalogo&#39;,#ModelScope Model-Id,required &#39;prompt&#39;: &#39;A golden cat&#39;# required } headers = { &#39;Authorization&#39;: &#39;Bearer 换成你的token&#39;, &#39;Content-Type&#39;: &#39;application/json&#39; } response = requests.post(url, data=json.dumps(payload, ensure_ascii=False).encode(&#39;utf-8&#39;), headers=headers) response_data = response.json() image = Image.open(BytesIO(requests.get(response_data[&#39;images&#39;][0][&#39;url&#39;]).content)) image.save(&#39;result_image.jpg&#39;)&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;课代表的工具箱总结：&lt;/p&gt;

&lt;p&gt;模型库及API-Inference：找到合适的模型实现产品功能，&lt;a href=&quot;https://modelscope.cn/models&quot;&gt;https://modelscope.cn/models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Notebook：开箱即用的云端环境，方便前后端调试，&lt;a href=&quot;https://modelscope.cn/my/mynotebook&quot;&gt;https://modelscope.cn/my/mynotebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;创空间：可将代码部署上线，支持分享，&lt;a href=&quot;https://modelscope.cn/studios&quot;&gt;https://modelscope.cn/studios&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;看到这，你学会了嘛？&lt;/p&gt;

&lt;p&gt;现在，还可以来参加魔搭的应用挑战赛！&lt;a href=&quot;https://modelscope.cn/brand/view/Space&quot;&gt;https://modelscope.cn/brand/view/Space&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;与各位开发者过招，其乐无穷~&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c53bdd3b685529b703cdeb.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c53bdd3b685529b703cdeb.html</guid><pubDate>Mon, 03 Mar 2025 05:19:26 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>HumanOmni：首个专注人类中心场景的多模态大模型，视觉与听觉融合的突破！</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/22b735bd96e44a6e8f142c42b49a1ed4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;HumanOmni是业内首个理解以人为中心的场景，可以同时处理视觉信息、音频信息的多模态大模型。&lt;/blockquote&gt;   &lt;h2&gt;01.模型介绍&lt;/h2&gt;

&lt;p&gt;HumanOmni是业内首个理解以人为中心的场景，可以同时处理视觉信息、音频信息的多模态大模型。&lt;/p&gt;

&lt;p&gt;在人类中心场景中，理解和处理视觉与听觉信息至关重要。文章指出，当前方法大多关注于视觉-语言模型，但缺乏对音频输入的处理能力，导致对场景的理解不完整。&lt;/p&gt;

&lt;p&gt;近期一些可以同时处理视频+音频的模型被提出，但是由于缺乏大规模专门的人类中心数据集以及缺乏针对人物场景的针对性网络架构，导致其对于人物中心场景的理解能力还有较大的提升空间。&lt;/p&gt;

&lt;h3&gt;论文主要创新：&lt;/h3&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;提出了HumanOmni，首个专注于人物中心场景理解的可以同时处理视觉+声音的多模态大语言模型。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;构建了包含240万个人类中心视频片段的数据集，使用多个先进的模型进行联合校验，生成超过1400万条指令，以促进对多样化人类中心场景的理解。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;人工对5w条视频片段进行标注，产生了超过10w条高质量指令，这些数据进一步提升了我们对于人物场景的理解能力。在情绪识别、面部描述、动作理解等多个人物相关的任务下，我们的HumanOmni均达到了sota的水平，同时我们实验验证了视觉+音频的联合输入相比单独使用视觉和单独使用音频的优势。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;模型架构&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;459&quot; src=&quot;https://devpress.csdnimg.cn/53778f1b97214564bc10d86836de9f5d.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;
	&lt;p&gt;在视觉理解部分，利用三个不同的分支来分别处理面部相关问题、人体相关问题、环境交互相关问题。设计一个指令驱动的融合模块来利用用户的输入指令生成不同的权重，从而将这三个分支的视觉特征进行自适应融合。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;将视觉token、音频token、文本token结合后送入大语言模型，实现对于多模态信息的同步处理。&lt;/p&gt;
	&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;模型下载链接：&lt;/p&gt;

&lt;p&gt;https://modelscope.cn/collections/HumanOmni-shipinrenwulijie-234cdc0e5fe247&lt;/p&gt;

&lt;p&gt;https://huggingface.co/StarJiaxing/HumanOmni-7B&lt;/p&gt;

&lt;p&gt;代码地址：&lt;/p&gt;

&lt;p&gt;https://github.com/HumanMLLM/HumanOmni&lt;/p&gt;

&lt;p&gt;模型训练&lt;/p&gt;

&lt;p&gt;模型训练分为三个阶段&lt;/p&gt;

&lt;p&gt;第一阶段是视觉能力构建，在这一阶段，我们利用生成的指令数据以及人工标注的指令数据构建了模型的视觉能力，其中我们更新了三个分支的视觉映射器的参数、指令驱动的融合模块的参数以及大语言模型的参数。&lt;/p&gt;

&lt;p&gt;第二阶段是听觉能力发展，在这一阶段，我们基于第一阶段的大语言模型的参数，利用大规模音频数据进行训练，只更新音频映射器的参数，使得模型具备对语音的理解和解释能力。&lt;/p&gt;

&lt;p&gt;第三阶段是跨模态交互集成阶段。这一阶段基于前两阶段的模型参数，利用结合视觉和音频数据，提升模型处理和关联不同模态信息的能力。&lt;/p&gt;

&lt;h2&gt;02.模型效果&lt;/h2&gt;

&lt;p&gt;情绪识别：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;537&quot; src=&quot;https://devpress.csdnimg.cn/fa24b2ecd6484875a8a707705406caac.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;面部表情描述：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;811&quot; src=&quot;https://devpress.csdnimg.cn/4a224cbd0c0e45c3a5a911809065e896.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;动作理解：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;674&quot; src=&quot;https://devpress.csdnimg.cn/7df462f491fb4bc180db871429ee4505.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;在这三个人物理解相关的任务上，我们的HumanOmni均达到了SOTA的水平。&lt;/p&gt;

&lt;p&gt;ASR：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;375&quot; src=&quot;https://devpress.csdnimg.cn/add8ae847a0c42878039d629c24860db.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;在ASR任务上，在Omni模型中，达到了先进的水平。&lt;/p&gt;

&lt;p&gt;我们也对模型中不同模态的作用进行了分析：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;175&quot; src=&quot;https://devpress.csdnimg.cn/33564bf36fa741acaaa84e312979c94a.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;该表格说明了HumanOmni理解了不同模态的信息，模型可以通过不同模态的信息来完成不同类型的任务。&lt;/p&gt;

&lt;h2&gt;03.模型推理&lt;/h2&gt;

&lt;p&gt;我们开源了推理代码，只需执行inference.py即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
import argparse
from humanomni import model_init, mm_infer
from humanomni.utils import disable_torch_init
from transformers import BertTokenizer

os.environ[&#39;TRANSFORMERS_OFFLINE&#39;] = &#39;1&#39;
os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;] = &#39;0&#39;

def main():
    parser = argparse.ArgumentParser(description=&quot;HumanOmni Inference Script&quot;)
    parser.add_argument(&#39;--modal&#39;, type=str, default=&#39;video_audio&#39;, help=&#39;Modal type (video or video_audio)&#39;)
    parser.add_argument(&#39;--model_path&#39;, type=str, required=True, help=&#39;Path to the model&#39;)
    parser.add_argument(&#39;--video_path&#39;, type=str, required=True, help=&#39;Path to the video file&#39;)
    parser.add_argument(&#39;--instruct&#39;, type=str, required=True, help=&#39;Instruction for the model&#39;)

    args = parser.parse_args()

    # 初始化BERT分词器
    bert_model = &quot;bert-base-uncased&quot;
    bert_tokenizer = BertTokenizer.from_pretrained(bert_model)

    # 禁用Torch初始化
    disable_torch_init()

    # 初始化模型、处理器和分词器
    model, processor, tokenizer = model_init(args.model_path)

    # 处理视频输入
    video_tensor = processor[&#39;video&#39;](args.video_path)

    # 根据modal类型决定是否处理音频
    if args.modal == &#39;video_audio&#39; or args.modal == &#39;audio&#39;:
        audio = processor[&#39;audio&#39;](args.video_path)[0]
    else:
        audio = None

    # 执行推理
    output = mm_infer(video_tensor, args.instruct, model=model, tokenizer=tokenizer, modal=args.modal, question=args.instruct, bert_tokeni=bert_tokenizer, do_sample=False, audio=audio)
    print(output)

if __name__ == &quot;__main__&quot;:
    main()&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输入参数分别为&lt;/p&gt;

&lt;p&gt;modal，控制使用的模态，这里可以选择video_audio（使用视觉+声音），video（仅使用视觉），audio（仅使用声音）。我们推荐同时使用视觉+声音。&lt;/p&gt;

&lt;p&gt;model_path，模型的路径。&lt;/p&gt;

&lt;p&gt;video_path，视频的路径。&lt;/p&gt;

&lt;p&gt;instruct，用户的指令。&lt;/p&gt;

&lt;p&gt;我们的模型可以通过modelscope和huggingface进行获取。&lt;/p&gt;

&lt;h2&gt;04.模型微调&lt;/h2&gt;

&lt;p&gt;我们开放了模型微调的代码，供开发者使用我们的模型在数据集上进行微调。我们提供的方式是基于HumanOmni-Video（一阶段）和HumanOmni-Audio（二阶段）的参数，进行跨视觉和音频模态的微调。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;torchrun --nnodes $WORLD_SIZE \
    --nproc_per_node $NPROC_PER_NODE \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    --node_rank $RANK \
    humanomni/train_flash_attn.py \
    --deepspeed scripts/zero3.json \
    --model_type HumanOmni_qwen2 \
    --model_path /mnt/data/jiaxing.zjx/code/HumanOmni/HumanOmni_7B_Video/ \
    --vision_tower google/siglip-so400m-patch14-384 \
    --audio_tower openai/whisper-large-v3 \
    --mm_projector_type  all_in_one \
    --mm_tunable_parts &quot;mm_mlp_adapter,audio_projector,mm_language_model&quot; \
    --pretrain_audio_mlp_adapter /mnt/data/jiaxing.zjx/code/HumanOmni/HumanOmni_7B_Audio/audio_projector.bin \
    --data_path   ./yamls/oryx_audio.yaml \
    --data_folder / \
    --mm_vision_select_layer -2 \
    --image_aspect_ratio pad \
    --num_frames 32 \
    --bf16 True \
    --tf32 True \
    --fp16 False \
    --output_dir ${OUTP_DIR}/${WANDB_PROJECT}/finetune_${RUN_NAME} \
    --num_train_epochs 1 \
    --per_device_train_batch_size $LOCAL_BATCH_SIZE \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --evaluation_strategy &quot;no&quot; \
    --save_strategy &quot;steps&quot; \
    --save_steps 500 \
    --save_total_limit 99 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type &quot;cosine&quot; \
    --logging_steps 1 \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --mm_use_x_start_end True \
    --dataloader_num_workers 4 \
    --report_to tensorboard \
    --run_name $RUN_NAME \&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然用户也可以基于我们最终的HumanOmni模型进行微调。取决于用户的数据量以及数据的相关程度。&lt;/p&gt;

&lt;h2&gt;05.效果展示&lt;/h2&gt;

&lt;p&gt;我们分别在电影片段、特写视频、实拍视频三个人物场景下测试了我们的模型的理解能力。&lt;/p&gt;

&lt;p&gt;电影片段：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/466486&quot;&gt;https://live.csdn.net/v/466486&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;User:What&#39;s the major emotion?
HumanOmni:angry.

User:What&#39;s the major facial expression?
HumanOmni:frown.

User:Desribe the woman.
HumanOmni:The woman has long brown hair and is wearing a pink top with a dark cardigan.
  
User:Please describe the major action in detail.
HumanOmni:The woman is frowning and shaking her head, indicating that she is angry. She is also gesturing with her hands as if she is explaining something or emphasizing a point.

User:What did the woman say?
HumanOmni:The woman said &quot;Of course we object! I object, Steve!&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;特写视频&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/466487&quot;&gt;https://live.csdn.net/v/466487&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;User:What&#39;s the major emotion?
HumanOmni:happy.

User:What&#39;s the major facial expression?
HumanOmni:smile.

User:Desribe the woman in detail.
HumanOmni:The woman in the video has blonde hair, is wearing a black top, and is smiling.

User:What did the woman say?
HumanOmni:The woman said &quot;Dogs are sitting by the door.&quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实拍视频&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://live.csdn.net/v/466488&quot;&gt;https://live.csdn.net/v/466488&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;User:描述一下视频中男子的外貌穿着.
HumanOmni:视频中的男性有短的黑发，配戴圆框眼镜。他穿了一件棕色的毛大衣，内里是白色的.

User:视频中男子说了什么内容？
HumanOmni:男子说他穿什么颜色的衣服，现在处于什么环境，今天的天气怎么样.

User:视频中男子有什么情绪吗？
HumanOmni:没有情绪。&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在三类视频下，分别测试了模型对于主要依靠视觉内容进行回答的问题（外貌描述），主要依靠音频内容进行回答的问题（说话内容），依靠音视频共同回答的问题（情绪理解）的效果。我们的HumanOmni均有非常稳定且准确的输出。&lt;/p&gt;

&lt;p&gt;点击链接即可跳转模型链接~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://modelscope.cn/collections/HumanOmni-shipinrenwulijie-234cdc0e5fe247&quot;&gt;HumanOmni 人物音视频理解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c53aa12e30c863900a0d9d.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c53aa12e30c863900a0d9d.html</guid><pubDate>Mon, 03 Mar 2025 05:14:09 GMT</pubDate><author>魔搭ModelScope社区</author></item><item><title>无需微调！扩散模型新突破：Attentive Eraser高效稳定移除目标物体</title><description>&lt;img src=&quot;https://devpress.csdnimg.cn/ffa1f4ca102a4a75add85bc6b7ad6b01.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;   &lt;blockquote&gt;最近，扩散模型在生成模型领域异军突起，凭借其独特的生成机制在图像生成方面大放异彩，尤其在处理高维复杂数据时优势明显。然而，尽管扩散模型在图像生成任务中表现优异，但在图像目标移除任务中仍然面临诸多挑战。&lt;/blockquote&gt;   &lt;p&gt;&lt;span class=&quot;img-upload-box&quot; contenteditable=&quot;false&quot; id=&quot;tZcRdtn5&quot;&gt;&lt;span class=&quot;info-box&quot;&gt;&lt;img class=&quot;img-loading&quot; src=&quot;https://csdnimg.cn/release/devpress/public/img/uploading.4e448015.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;span class=&quot;operate-box&quot;&gt;&lt;span class=&quot;upload-msg&quot;&gt;正在上传…&lt;/span&gt;&lt;span class=&quot;btn btn-redo&quot;&gt;重新上传&lt;/span&gt;&lt;span class=&quot;btn btn-cancle&quot;&gt;取消&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;img alt=&quot;&quot; src=&quot;https://pic1.zhimg.com/80/v2-c1dbd6a1b5025e23dab8d65d767c68af_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;242&quot; src=&quot;https://devpress.csdnimg.cn/3b5b17d3b41e4f67ba671797c3fc2c91.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;最近，扩散模型在生成模型领域异军突起，凭借其独特的生成机制在图像生成方面大放异彩，尤其在处理高维复杂数据时优势明显。然而，尽管扩散模型在图像生成任务中表现优异，但在图像目标移除任务中仍然面临诸多挑战。现有方法在移除前景目标后，可能会留下残影或伪影，难以实现与背景的自然融合。&lt;/p&gt;

&lt;p&gt;为了解决这些问题，浙江工商大学和阿里巴巴研究团队联合提出了一种基于扩散模型且无需微调的方法——Attentive Eraser，以增强预训练扩散模型目标移除的能力，从而实现稳定有效的目标移除。实验结果表明，该方法在多种预训练扩散模型中均表现出优异的目标移除能力，甚至优于基于训练的方法，且无需微调，具有很强的可扩展性。&lt;/p&gt;

&lt;p&gt;该研究论文已被人工智能顶会AAAI2025录用并选为OralPresentation。&lt;/p&gt;

&lt;p&gt;论文标题：&lt;/p&gt;

&lt;p&gt;Attentive Eraser: Unleashing Diffusion Model&#39;s Object Removal Potential via Self-Attention Redirection Guidance&lt;/p&gt;

&lt;p&gt;论文链接：&lt;/p&gt;

&lt;p&gt;https://arxiv.org/pdf/2412.12974&lt;/p&gt;

&lt;p&gt;Github地址：&lt;/p&gt;

&lt;p&gt;https://github.com/Anonym0u3/AttentiveEraser&lt;/p&gt;

&lt;p&gt;体验链接：&lt;/p&gt;

&lt;p&gt;https://www.modelscope.cn/studios/Anonymou3/AttentiveEraser&lt;/p&gt;

&lt;p&gt;Diffusers Pipeline：https://github.com/huggingface/diffusers/tree/main/examples/community#stable-diffusion-xl-attentive-eraser-pipeline&lt;/p&gt;

&lt;h2&gt;01.背景&lt;/h2&gt;

&lt;p&gt;目前，扩散模型的广泛应用使得生成与真实照片质量相媲美的高质量图像成为可能，并能够根据用户的需求提供逼真的视觉呈现。这引发了一个自然的问题：这些模型的图像生成能力是否可以被用于从图像中移除特定目标。这个被称为“目标移除”的任务是图像重绘（Image Inpainting）的一种特殊形式，并需要解决两个关键问题。首先，用户指定的目标必须能够被成功且有效地从图像中移除。其次，被移除的区域需要填充内容，这些内容必须真实、合理，并与图像整体保持一致性以确保视觉上的连贯性。&lt;/p&gt;

&lt;p&gt;近年来扩散模型中最具代表性的开源预训练模型是Stable Diffusion（SD），其作为一种隐变量扩散模型在多种图像生成任务中表现优异。然而，直接将其应用在重绘pipeline上进行目标移除时效果却不尽人意，往往会出现伪影导致目标移除不成功，如图 1（SD w/o SARG）所示：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1747&quot; src=&quot;https://devpress.csdnimg.cn/0034e1c2be954376ac20a03496769d6f.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;img-upload-box&quot; contenteditable=&quot;false&quot; id=&quot;x2eAPzQI&quot;&gt;&lt;span class=&quot;info-box&quot;&gt;&lt;img class=&quot;img-loading&quot; src=&quot;https://csdnimg.cn/release/devpress/public/img/uploading.4e448015.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;span class=&quot;operate-box&quot;&gt;&lt;span class=&quot;upload-msg&quot;&gt;正在上传…&lt;/span&gt;&lt;span class=&quot;btn btn-redo&quot;&gt;重新上传&lt;/span&gt;&lt;span class=&quot;btn btn-cancle&quot;&gt;取消&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;img alt=&quot;&quot; src=&quot;https://pica.zhimg.com/80/v2-2df100eb56b48baa86a54a01180ea572_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图1 Stable Diffusion模型应用SARG前后目标移除效果对比图&lt;/p&gt;

&lt;p&gt;为了将SD应用于目标移除任务，SD-inpainting通过在模型中引入掩码作为附加条件并进行微调，构建成了一个端到端的图像重绘模型。然而，即使付出了大量资源成本，SD-inpainting在目标移除任务中的性能依然不够稳定，经常无法完全移除目标，还是会生成随机伪影。除了基于模型微调的方法外，还有一种通过提示工程（prompt engineering）引导扩散模型完成目标移除的技术。尽管这类方法在某些场景下可以取得令人满意的结果，但其显著缺点在于，需要投入大量精力进行提示构建，同时难以与前景目标区域实现精确交互。此外，这类方法同样需要耗费大量资源进行模型微调，进一步限制了其实用性。&lt;/p&gt;

&lt;p&gt;为了解决上述问题，本文提出了一种基于扩散模型且无需微调的目标移除方法，具体贡献如下：&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;
	&lt;p&gt;本文提出了一种无需微调的方法——Attentive Eraser，旨在激发预训练扩散模型的目标移除潜能。该方法由两个关键组成部分构成：&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;注意力激活和抑制（Attention Activation and Suppression，AAS），这是一种专门设计用于修改预训练扩散模型中自注意力机制的方法，可在生成图像时增强模型对背景的注意力，同时降低对前景目标物体的注意力。同时针对生成过程中自注意力本身带来的对相似物体的高依赖性问题，本文提出了相似性抑制（Similarity Suppression，SS），有效地解决了该问题。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;自注意力重定向引导（Self-Attention Redirection Guidance，SARG），这是一种新颖的逆向扩散采样过程引导方法，利用所提出的AAS将采样过程引导到目标移除的方向，进一步提升了目标移除的效果。&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;通过一系列实验和用户偏好研究，本文验证了所提出方法的有效性、鲁棒性和可扩展性。实验结果表明，本文的方法在目标移除的质量和稳定性方面均超越了现有的最先进方法。&lt;/p&gt;
	&lt;/li&gt;&lt;/ol&gt;

&lt;h2&gt;02.AttentiveEraser核心创新&lt;/h2&gt;

&lt;p&gt;本文提出的Attentive Eraser免微调目标移除方法的总体框架图如图 2所示，其中有两个主要部分：&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;
	&lt;p&gt;AAS，这是一种专为目标移除任务设计的自注意力机制修改操作，针对目标移除任务中固有的挑战，AAS通过对自注意力机制进行精细调整，使得模型在生成前景目标区域时能够更加关注背景内容，而非前景目标，进而在生成结果图中消除目标的外观信息。此外，SS可抑制由于自注意力的固有特性而可能导致的对相似物体的过度关注；&lt;/p&gt;
	&lt;/li&gt;&lt;li&gt;
	&lt;p&gt;SARG，这是一种应用于逆向扩散采样过程的引导方法，它利用通过AAS重定向的自注意力来引导采样过程指向目标移除的方向。在这种引导下，扩散模型能够更好地消除掩码区域内的前景目标，并生成与背景自然融洽的图像内容。&lt;/p&gt;
	&lt;/li&gt;&lt;/ol&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;607&quot; src=&quot;https://devpress.csdnimg.cn/0242c1e14c04491093f12f1c8b58aa63.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-a854c078c4749733923dbf1257866a11_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图2 Attentive Eraser的总体框架图&lt;/p&gt;

&lt;h2&gt;创新一：注意力抑制与激活（AAS）&lt;/h2&gt;

&lt;h3&gt;动机分析&lt;/h3&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;540&quot; src=&quot;https://devpress.csdnimg.cn/8f2c556acf104aa1b694c3a3b9d21218.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-a678d5a351e8c705e4f1382144bc6ab5_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图3 去噪网络不同层在所有时间步上的平均自注意力图的可视化图&lt;/p&gt;

&lt;p&gt;图 3中，在输入图像经过DDIM inversion后，利用主成分分析和聚类技术，对逆向扩散去噪过程中去噪网络的不同层在所有时间步上的平均自注意力图进行了可视化。通过这些可视化结果，可以观察到自注意力图显示出类似于图像各个组成部分的语义布局。这种布局清晰地展示了前景物体和背景在生成过程中所对应的自注意力的显著差异，表明它们在模型中的处理方式存在明确的区分。&lt;/p&gt;

&lt;p&gt;这种语义布局为目标移除任务提供了重要的启示，为了在生成过程中有效地去除前景目标，一个直观的方法是在生成过程中将前景物体的自注意力逐渐“融合”到背景中，使其与背景区域的注意力更加趋同。换句话说，在生成过程中与前景目标相关的区域应更关注背景区域，同时减少对自身的关注。前景目标的自注意力逐渐向背景转移有助于消除前景物体，使其自然地消隐于背景之中。&lt;/p&gt;

&lt;p&gt;此外，考虑到目标移除任务的特殊性，前景目标是处理的核心，背景区域应在生成过程中保持固定不变，且不受前景区域变化的影响。因此，为了实现更自然的生成效果，背景区域对前景区域的关注度也应适当地降低，从而避免生成过程中背景被不必要地干扰。这一策略确保了生成结果与背景的自然融合，使生成图像显得更加和谐、真实。&lt;/p&gt;

&lt;h3&gt;专为目标移除设计的自注意力机制修改方法&lt;/h3&gt;

&lt;p&gt;结合上述分析，本文提出了一种针对目标移除任务设计的简单而有效的方法——AAS，如图 2（a）所示，AAS方法的核心在于通过调整自注意力机制，灵活控制前景目标区域与背景区域之间的关系，从而实现更为自然的目标移除效果。&lt;/p&gt;

&lt;p&gt;注意力激活的目的是通过增加前景目标区域生成内容对背景区域的注意力，即增加，从而确保前景目标被移除后，生成的内容能够与背景自然融合。这一过程的关键在于增强前景区域生成内容对背景的关注度，使得前景区域在生成过程中更多地参考背景的特征信息，进而生成与背景风格一致的图像内容。这种增强能够有效地避免前景移除后出现与背景不协调的情况，确保生成图像的整体连贯性和视觉一致性。&lt;/p&gt;

&lt;p&gt;与此相反，注意力抑制是指抑制前景目标区域关于其外观及其对背景影响的信息，即减少和，达到抹除前景目标的效果。降低的目的是抑制前景区域对自身外观信息的关注，逐步抹除前景目标的原始语义信息。这意味着在逆向扩散去噪过程中，前景目标的特征信息将被逐步削弱直至完全消失而背景区域的生成过程则保持不变，以确保背景的完整性。此外，降低的作用在于减少背景区域对前景区域的依赖，避免背景区域在前景移除过程中受到不必要的影响。&lt;/p&gt;

&lt;h3&gt;相似性抑制&lt;/h3&gt;

&lt;p&gt;尽管上述理论在目标移除任务中展现了显著的效果，但其仍存在一个重要的局限性。具体而言，当背景中包含与前景目标相似的内容时，由于自注意力机制的固有特性，在生成过程中这些相似部分的注意力可能会高于其他区域。这种情况会导致扩散模型在去除前景目标时，误将背景中相似的部分保留，从而无法彻底去除目标（见图 2（a）右侧的一个例子）。这一问题的存在表明，单纯依靠上述理论可能不足以应对复杂场景中具有相似特征的前景和背景目标的区分与处理。&lt;/p&gt;

&lt;p&gt;因此，为了减少对相似目标的关注并将其分散到其他区域，本文提出了一种较为直接且有效的扩展策略引入到AAS中来解决上述问题：通过简单地引入一个小于1的相似性抑制系数来降低相似性矩阵方差。本文将该方法称为相似性抑制（SS）。基于SoftMax函数的权重计算机制，减少方差可以在一定程度上削弱生成过程过度关注高相似度区域的倾向，同时增加对背景其他区域的注意力，由此来抑制生成过程中可能出现的相似物体，从而减少目标去除不彻底的情况。&lt;/p&gt;

&lt;h2&gt;创新二：自注意力重定向引导（SARG）&lt;/h2&gt;

&lt;p&gt;为了进一步提高目标去除能力以及生成图像的整体质量，本文受Ahn等人提出的PAG（PAG：Ahn D, Cho H, Min J, et al. Self-rectifying diffusion sampling with perturbed-attention guidance[C]. European Conference on Computer Vision. Springer, Cham, 2025: 1-17.）启发，将经过AAS处理后的去噪网络看作是噪声预测过程中的一种扰动形式，通过这种扰动来引导采样过程朝向理想方向。因此，修正后的扩散模型的预测噪声可以定义如下：&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;143&quot; src=&quot;https://devpress.csdnimg.cn/5d02d81c896f40d789a4dc556746ec6a.png&quot; width=&quot;982&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;这种引导过程的优势在于，通过调整自注意力机制，SARG能够在生成过程中不断优化生成策略，使模型更加灵活地适应不同场景下的目标移除需求。同时，通过优化生成过程的各个时间步，SARG还提高了最终生成图像的质量，通过合理地控制生成过程中的注意力分配，SARG确保了最终生成图像与背景之间的自然融合，减少了它们之间的突兀感，从而提高了图像的视觉一致性和自然度，确保了高质量的目标移除效果。&lt;/p&gt;

&lt;h2&gt;03.AttentiveEraser的稳定目标擦除能力及高拓展性&lt;/h2&gt;

&lt;h2&gt;对比实验的定量和定性结果&lt;/h2&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;350&quot; src=&quot;https://devpress.csdnimg.cn/6f9a6430b7aa4620aafeea2e19afb3d0.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-f920826f89c5f3e926df19c3aab3940e_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;表 1 对比实验定量结果表&lt;/p&gt;

&lt;p&gt;从定量分析的结果来看，尽管在全局质量指标FID和LPIPS上，本文的方法表现处于平均水平，但这两个指标并不能充分反映目标去除的效果。进一步分析Local-FID指标，该指标评估生成区域的视觉质量与背景的真实分布的吻合程度，可以看到我们的方法在局部移除方面展现出色的表现，显著优于其他方法，说明在局部区域细节上我们方法生成的内容与真实图像分布更接近。同时，CLIP consensus指标通过计算多次生成结果的标准偏差，揭示了方法在不同随机种子下生成结果的一致性。&lt;/p&gt;

&lt;p&gt;从结果可以明显看到其他基于扩散模型的方法的标准偏差较大，说明了它们应用在目标移除任务时的不稳定性，而我们的结果展现了显著更低的标准偏差，说明我们方法的稳定性显著优于对比方法，更倾向于生成一致性高的图像。而CLIP Score指标直接反映目标是否被有效去除且背景是否被合理重建，实验结果表明我们的方法能够高效地去除目标，并在重绘前景区域时与背景高度一致。&lt;/p&gt;

&lt;p&gt;在CLIP Score指标上，我们的方法与当前领先的基于快速傅立叶卷积的重绘模型LAMA达到了相当的竞争水平，并在特定场景中表现出更强的背景适配能力。&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;561&quot; src=&quot;https://devpress.csdnimg.cn/c0ca1edb5d5e417cb6aab6c7bb1a200f.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-a9b114e7fd54d8906d5374b6d2d65e57_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图4 对比实验定性结果图&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;473&quot; src=&quot;https://devpress.csdnimg.cn/2b1604dcf0474295b0e67dcd9d90cbde.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-4fac5161633cd5bf24bd32ed886bddb2_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图5 目标移除稳定性实验结果对比图&lt;/p&gt;

&lt;p&gt;对比实验的定性结果如图 4所示，其中输入图像中的掩码以红色高亮显示，本文的方法以粗体标出。从图中可以观察到本文的方法与其他方法之间的显著差异。LAMA由于缺乏生成能力，虽然能成功去除目标，但生成的内容显得模糊且不清晰。相比之下，其他基于扩散模型的方法都存在一个普遍的问题，即目标移除的不稳定性，这种不稳定性往往导致随机伪影的出现，无法生成与背景连贯一致的内容。&lt;/p&gt;

&lt;p&gt;为了进一步证实这一问题，本文进行了目标移除稳定性实验，结果如图 5所示。图中展示了每种方法在使用三种不同随机种子下的目标移除结果。可以清楚地看到，本文的方法在各个版本的SD模型中都能实现稳定的目标移除，生成的内容一致且连贯。而其他方法则难以保持这种稳定性，目标移除效果因随机性而产生较大的波动，难以实现与背景一致的生成效果。&lt;/p&gt;

&lt;h2&gt;用户偏好研究和GPT-4o评估&lt;/h2&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;471&quot; src=&quot;https://devpress.csdnimg.cn/f468728964b644a8a7984fbb7d6844aa.png&quot; width=&quot;1010&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://pic1.zhimg.com/80/v2-9666d88cddd8a231dc1359379a10ac8b_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;表2 用户偏好研究和GPT-4o评估结果表&lt;/p&gt;

&lt;p&gt;由于缺乏针对目标移除任务的有效指标，上述指标可能不足以证明本文方法的优越性。因此，本文进一步进行了用户偏好研究（User Study）以验证本文方法的实际效果。表 2列出了用户对各种方法的偏好百分比，结果表明本文的方法比其他方法更受用户青睐。这一发现与对比试验的结果一致，进一步验证了本文方法在目标移除任务中的有效性和优越性，突出表明该方法在实际应用中的表现优于其他现有方法。&lt;/p&gt;

&lt;p&gt;此外为了进一步表明本文提出方法的优越性，我们还利用 GPT-4o对本文的方法和用户偏好研究中排名第二方法LAMA之间的目标移除性能进行了进一步评估。在该对比实验中，我们要求GPT-4o根据设计好的公平合理的文本提示选择目标移除效果最佳的图像。具体的文本提示如下：“你是一个生成图像评估专家。现有两张图和对应的掩码,请从以下方面进行评估:1.生成图像是否有效移除了掩码内目标且在掩码区域内生成和背景一致的内容, 2.掩码内目标的生成内容的真实感。根据以上标准，请告诉我哪张图片更好。”最终，计算了本文的方法和LAMA被选择的频率，以此衡量各自的目标移除性能。评估结果在表 2中，结果也表明本文的方法明显优于LAMA，表现出卓越的性能。通过这些实验，本文的方法不仅在生成图像的整体质量上优于LAMA，还在目标移除的准确性和生成内容的真实感方面展现了显著的优势。&lt;/p&gt;

&lt;h2&gt;鲁棒性和可拓展性分析&lt;/h2&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;582&quot; src=&quot;https://devpress.csdnimg.cn/655d54318ca84f31af976892b712164f.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://pica.zhimg.com/80/v2-5de045a698d0a46a2d4dfa11e4d504ae_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图6 Attentive Eraser对输入掩码的鲁棒性实验结果图&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://picx.zhimg.com/80/v2-89a6567d127c667018699514482748bb_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;图7 在solarsync模型上应用Attentive Eraser去除卡通图像目标的结果图&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;573&quot; src=&quot;https://devpress.csdnimg.cn/9560ac7ed1a143eb969833146499cb0c.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;由于Attentive Eraser是一个基于掩码的方法，因此我们在实验中进一步证明了其对输入掩码的鲁棒性，并展示了其在其他预训练扩散模型上的可拓展性。&lt;/p&gt;

&lt;p&gt;如图 6所示，我们通过三种不同精细度的掩码类型来评估该方法的鲁棒性，按从细致到粗糙可以将掩码分为：实例分割掩码、分割边界框掩码和手绘掩码。可以看出，即使使用较为粗糙的手绘掩码，我们的方法依然能够有效去除目标并生成合理的前景内容。这表明，Attentive Eraser的性能并不依赖于掩码的精细程度，具有极高的鲁棒性。同时，这种鲁棒性也为用户提供了更多的灵活性和便捷性，无论是使用精细的自动生成的分割掩码，还是手工绘制的粗略掩码，用户都能够获得理想的目标移除效果。&lt;/p&gt;

&lt;p&gt;此外，如图 7所示，我们的方法不仅适用于生成自然图像的预训练扩散模型（例如SD1.5、SD2.1等），还可以扩展到生成动漫图像的模型，如Civital 平台上的solarsync模型，体现出了Attentive Eraser在不同预训练扩散模型和架构上的可拓展性和广泛适用性，无论是用于自然图像还是动漫图像的目标移除任务，均能发挥出色的效果。&lt;/p&gt;

&lt;p&gt;通过这些实验，我们充分展示了Attentive Eraser的鲁棒性和可拓展性，为其在实际应用中的广泛应用提供了坚实的理论与实验支持。&lt;/p&gt;

&lt;h2&gt;03.Demo演示&lt;/h2&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; height=&quot;805&quot; src=&quot;https://devpress.csdnimg.cn/a1513f0907ee4d3cb96f52936da6ad1d.png&quot; width=&quot;1080&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://pica.zhimg.com/80/v2-535c8c4e7ab95cf60e8ccdc3cda2162e_1440w.jpg?source=d16d100b&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;

&lt;p&gt;Demo已发布在魔搭社区创空间：https://www.modelscope.cn/studios/Anonymou3/AttentiveEraser&lt;/p&gt;

&lt;p&gt;点击链接即可跳转体验~&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/Anonymou3/AttentiveEraser&quot;&gt;AttentiveEraser&lt;/a&gt;&lt;/p&gt;
 </description><link>https://community.modelscope.cn/67c53a272e30c863900a0d7e.html</link><guid isPermaLink="false">https://community.modelscope.cn/67c53a272e30c863900a0d7e.html</guid><pubDate>Mon, 03 Mar 2025 05:12:07 GMT</pubDate><author>魔搭ModelScope社区</author></item></channel></rss>